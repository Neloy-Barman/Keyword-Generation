abstracts,ieee_keywords,author_keywords
"We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam....",[],[]
High Efficiency Video Coding (HEVC) is currently being prepared as the newest video coding standard of the ITU-T Video Coding Experts Group and the ISO/IEC Moving Picture Experts Group. The main goal of the HEVC standardization effort is to enable significantly improved compression performance relative to existing standards-in the range of 50% bit-rate reduction for equal perceptual video quality. This paper provides an overview of the technical features and characteristics of the HEVC standard.,"['Video coding', 'ISO standards', 'Video compression', 'MPEG 4 Standard', 'MPEG standards']","['Advanced video coding (AVC)', 'H.264', 'High Efficiency Video Coding (HEVC)', 'Joint Collaborative Team on Video Coding (JCT-VC)', 'Moving Picture Experts Group (MPEG)', 'MPEG-4', 'standards', 'Video Coding Experts Group (VCEG)', 'video compression']"
"The global bandwidth shortage facing wireless carriers has motivated the exploration of the underutilized millimeter wave (mm-wave) frequency spectrum for future broadband cellular communication networks. There is, however, little knowledge about cellular mm-wave propagation in densely populated indoor and outdoor environments. Obtaining this information is vital for the design and operation of future fifth generation cellular networks that use the mm-wave spectrum. In this paper, we present the motivation for new mm-wave cellular systems, methodology, and hardware for measurements and offer a variety of measurement results that show 28 and 38 GHz frequencies can be used when employing steerable directional antennas at base stations and mobile devices.","['Mobile communication', 'Wireless communication', 'Computer architecture', 'Base stations', 'Microprocessors', 'Directional antennas', 'Millimeter wave technology', 'MIMO']","['28 GHz', '38 GHz', 'millimeter wave propagation measurements', 'directional antennas', 'channel models', '5G', 'cellular', 'mobile communications', 'MIMO']"
"The Internet of Things (IoT) shall be able to incorporate transparently and seamlessly a large number of different and heterogeneous end systems, while providing open access to selected subsets of data for the development of a plethora of digital services. Building a general architecture for the IoT is hence a very complex task, mainly because of the extremely large variety of devices, link layer technologies, and services that may be involved in such a system. In this paper, we focus specifically to an urban IoT system that, while still being quite a broad category, are characterized by their specific application domain. Urban IoTs, in fact, are designed to support the Smart City vision, which aims at exploiting the most advanced communication technologies to support added-value services for the administration of the city and for the citizens. This paper hence provides a comprehensive survey of the enabling technologies, protocols, and architecture for an urban IoT. Furthermore, the paper will present and discuss the technical solutions and best-practice guidelines adopted in the Padova Smart City project, a proof-of-concept deployment of an IoT island in the city of Padova, Italy, performed in collaboration with the city municipality.","['Urban areas', 'Smart buildings', 'Monitoring', 'Smart homes', 'Business', 'IEEE 802.15 Standards']","['Constrained Application Protocol (CoAP)', 'Efficient XML Interchange (EXI)', 'network architecture', 'sensor system integration', 'service functions and management', 'Smart Cities', 'testbed and trials', '6lowPAN']"
"In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients—manually annotated by up to four raters—and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%–85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.","['Image segmentation', 'Educational institutions', 'Benchmark testing', 'Biomedical imaging', 'Lesions']","['MRI', 'Brain', 'Oncology/tumor', 'Image segmentation', 'Benchmark']"
"Motivated by the recent explosion of interest around blockchains, we examine whether they make a good fit for the Internet of Things (IoT) sector. Blockchains allow us to have a distributed peer-to-peer network where non-trusting members can interact with each other without a trusted intermediary, in a verifiable manner. We review how this mechanism works and also look into smart contracts-scripts that reside on the blockchain that allow for the automation of multi-step processes. We then move into the IoT domain, and describe how a blockchain-IoT combination: 1) facilitates the sharing of services and resources leading to the creation of a marketplace of services between devices and 2) allows us to automate in a cryptographically verifiable manner several existing, time-consuming workflows. We also point out certain issues that should be considered before the deployment of a blockchain network in an IoT setting: from transactional privacy to the expected value of the digitized assets traded on the network. Wherever applicable, we identify solutions and workarounds. Our conclusion is that the blockchain-IoT combination is powerful and can cause significant transformations across several industries, paving the way for new business models and novel, distributed applications.","['Distributed processing', 'Internet of things', 'Cryptography', 'Privacy', 'Blockchains', 'Automation', 'Peer-to-peer computing']","['blockchain', 'distributed systems', 'Internet of Things']"
"Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.","['Big data', 'Bayes methods', 'Linear programming', 'Decision making', 'Design of experiments', 'Optimization', 'Genomes', 'Statistical analysis']","['decision making', 'design of experiments', 'optimization', 'response surface methodology', 'statistical learning', 'genomic medicine']"
"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","['Conferences', 'Machine learning', 'Market research', 'Prediction algorithms', 'Machine learning algorithms', 'Biological system modeling']","['Explainable artificial intelligence', 'interpretable machine learning', 'black-box models']"
"The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges.","['Internet of things', 'Medical services', 'Network security', 'Network architecture', 'Biological system modeling', 'Market research']","['Internet of Things', 'Health Care', 'Services', 'Applications', 'Networks', 'Architectures', 'Platforms', 'Security', 'Technologies', 'Industries', 'Policies', 'Challenges']"
"Disturbance-observer-based control (DOBC) and related methods have been researched and applied in various industrial sectors in the last four decades. This survey, at first time, gives a systematic and comprehensive tutorial and summary on the existing disturbance/uncertainty estimation and attenuation techniques, most notably, DOBC, active disturbance rejection control, disturbance accommodation control, and composite hierarchical antidisturbance control. In all of these methods, disturbance and uncertainty are, in general, lumped together, and an observation mechanism is employed to estimate the total disturbance. This paper first reviews a number of widely used linear and nonlinear disturbance/uncertainty estimation techniques and then discusses and compares various compensation techniques and the procedures of integrating disturbance/uncertainty compensation with a (predesigned) linear/nonlinear controller. It also provides concise tutorials of the main methods in this area with clear descriptions of their features. The application of this group of methods in various industrial sections is reviewed, with emphasis on the commercialization of some algorithms. The survey is ended with the discussion of future directions.","['Uncertainty', 'Observers', 'Robustness', 'Attenuation', 'Frequency estimation', 'Nonlinear systems']","['Disturbances', 'uncertainties', 'estimation', 'motion control', 'robustness', 'linear systems', 'nonlinear systems']"
"Millimeter-wave (mmW) frequencies between 30 and 300 GHz are a new frontier for cellular communication that offers the promise of orders of magnitude greater bandwidths combined with further gains via beamforming and spatial multiplexing from multielement antenna arrays. This paper surveys measurements and capacity studies to assess this technology with a focus on small cell deployments in urban environments. The conclusions are extremely encouraging; measurements in New York City at 28 and 73 GHz demonstrate that, even in an urban canyon environment, significant non-line-of-sight (NLOS) outdoor, street-level coverage is possible up to approximately 200 m from a potential low-power microcell or picocell base station. In addition, based on statistical channel models from these measurements, it is shown that mmW systems can offer more than an order of magnitude increase in capacity over current state-of-the-art 4G cellular networks at current cell densities. Cellular systems, however, will need to be significantly redesigned to fully achieve these gains. Specifically, the requirement of highly directional and adaptive transmissions, directional isolation between links, and significant possibilities of outage have strong implications on multiple access, channel structure, synchronization, and receiver design. To address these challenges, the paper discusses how various technologies including adaptive beamforming, multihop relaying, heterogeneous network architectures, and carrier aggregation can be leveraged in the mmW context.","['Millimeter wave communication', 'Bandwidth', 'Antennas', 'Shadow mapping', 'Atmospheric measurements', 'Radio spectrum management', 'Cellular networks']","['Cellular systems', 'channel models', 'millimeter-wave radio', 'urban deployments', 'wireless propagation', '28 GHz', '3GPP LTE', '73 GHz']"
"With the severe spectrum shortage in conventional cellular bands, millimeter wave (mmW) frequencies between 30 and 300 GHz have been attracting growing attention as a possible candidate for next-generation micro- and picocellular wireless networks. The mmW bands offer orders of magnitude greater spectrum than current cellular allocations and enable very high-dimensional antenna arrays for further gains via beamforming and spatial multiplexing. This paper uses recent real-world measurements at 28 and 73 GHz in New York, NY, USA, to derive detailed spatial statistical models of the channels and uses these models to provide a realistic assessment of mmW micro- and picocellular networks in a dense urban deployment. Statistical models are derived for key channel parameters, including the path loss, number of spatial clusters, angular dispersion, and outage. It is found that, even in highly non-line-of-sight environments, strong signals can be detected 100-200 m from potential cell sites, potentially with multiple clusters to support spatial multiplexing. Moreover, a system simulation based on the models predicts that mmW systems can offer an order of magnitude increase in capacity over current state-of-the-art 4G cellular networks with no increase in cell density from current urban deployments.","['Power measurement', 'Antenna measurements', 'Standards', 'Clustering algorithms', 'Gain', 'Delays', 'Mobile communication']","['Millimeter wave radio', '3GPP LTE', 'cellular systems', 'wireless propagation', '28 GHz', '73 GHz', 'urban deployments']"
"This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.","['IP networks', 'Protocols', 'Measurement', 'Ports (Computers)', 'Data models', 'Data mining', 'Computer security']","['Cyber Analytics', 'Data Mining', 'Machine Learning']"
"In the near future, i.e., beyond 4G, some of the prime objectives or demands that need to be addressed are increased capacity, improved data rate, decreased latency, and better quality of service. To meet these demands, drastic improvements need to be made in cellular network architecture. This paper presents the results of a detailed survey on the fifth generation (5G) cellular network architecture and some of the key emerging technologies that are helpful in improving the architecture and meeting the demands of users. In this detailed survey, the prime focus is on the 5G cellular network architecture, massive multiple input multiple output technology, and device-to-device communication (D2D). Along with this, some of the emerging technologies that are addressed in this paper include interference management, spectrum sharing with cognitive radio, ultra-dense networks, multi-radio access technology association, full duplex radios, millimeter wave solutions for 5G cellular networks, and cloud technologies for 5G radio access networks and software defined networks. In this paper, a general probable 5G cellular network architecture is proposed, which shows that D2D, small cell access points, network cloud, and the Internet of Things can be a part of 5G cellular network architecture. A detailed survey is included regarding current research projects being conducted in different countries by research groups and institutions that are working on 5G technologies.","['5G mobile communication', 'Cloud computing', 'MIMO', 'Radio access networks', 'Cellular networks']","['5G', 'Cloud', 'D2D', 'Massive MIMO', 'mm-wave', 'Relay', 'Small-cel']"
"The future of mobile communications looks exciting with the potential new use cases and challenging requirements of future 6th generation (6G) and beyond wireless networks. Since the beginning of the modern era of wireless communications, the propagation medium has been perceived as a randomly behaving entity between the transmitter and the receiver, which degrades the quality of the received signal due to the uncontrollable interactions of the transmitted radio waves with the surrounding objects. The recent advent of reconfigurable intelligent surfaces in wireless communications enables, on the other hand, network operators to control the scattering, reflection, and refraction characteristics of the radio waves, by overcoming the negative effects of natural wireless propagation. Recent results have revealed that reconfigurable intelligent surfaces can effectively control the wavefront, e.g., the phase, amplitude, frequency, and even polarization, of the impinging signals without the need of complex decoding, encoding, and radio frequency processing operations. Motivated by the potential of this emerging technology, the present article is aimed to provide the readers with a detailed overview and historical perspective on state-of-the-art solutions, and to elaborate on the fundamental differences with other technologies, the most important open research issues to tackle, and the reasons why the use of reconfigurable intelligent surfaces necessitates to rethink the communication-theoretic models currently employed in wireless networks. This article also explores theoretical performance limits of reconfigurable intelligent surface-assisted communication systems using mathematical techniques and elaborates on the potential use cases of intelligent surfaces in 6G and beyond wireless networks.","['Wireless networks', '5G mobile communication', 'Surface waves', 'STEM', '6G mobile communication']","['6G', 'large intelligent surfaces', 'meta-surfaces', 'reconfigurable intelligent surfaces', 'smart reflect-arrays', 'software-defined surfaces', 'wireless communications', 'wireless networks']"
"Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.","['Cameras', 'Geometry', 'Three-dimensional displays', 'Optimization', 'Robustness', 'Computational modeling', 'Visualization']",['Visual odometry SLAM 3D reconstruction structure from motion']
"In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyperparameter selection. The starting point of this paper is the observation that unrolled iterative methods have the form of a CNN (filtering followed by pointwise nonlinearity) when the normal operator (H*H, where H* is the adjoint of the forward imaging operator, H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 × 512 image on the GPU.","['Image reconstruction', 'Convolution', 'Inverse problems', 'Computed tomography', 'Neural networks', 'Iterative methods']","['Image restoration', 'image reconstruction', 'tomography', 'computed tomography', 'magnetic resonance imaging', 'biomedical signal processing', 'biomedical imaging', 'reconstruction algorithms']"
"Single image haze removal has been a challenging problem due to its ill-posed nature. In this paper, we propose a simple but powerful color attenuation prior for haze removal from a single input hazy image. By creating a linear model for modeling the scene depth of the hazy image under this novel prior and learning the parameters of the model with a supervised learning method, the depth information can be well recovered. With the depth map of the hazy image, we can easily estimate the transmission and restore the scene radiance via the atmospheric scattering model, and thus effectively remove the haze from a single image. Experimental results show that the proposed approach outperforms state-of-the-art haze removal algorithms in terms of both efficiency and the dehazing effect.","['Mathematical model', 'Image color analysis', 'Atmospheric modeling', 'Brightness', 'Attenuation', 'Image restoration', 'Scattering']","['Dehazing', 'defog', 'image restoration', 'depth restoration']"
"High-frequency-link (HFL) power conversion systems (PCSs) are attracting more and more attentions in academia and industry for high power density, reduced weight, and low noise without compromising efficiency, cost, and reliability. In HFL PCSs, dual-active-bridge (DAB) isolated bidirectional dc-dc converter (IBDC) serves as the core circuit. This paper gives an overview of DAB-IBDC for HFL PCSs. First, the research necessity and development history are introduced. Second, the research subjects about basic characterization, control strategy, soft-switching solution and variant, as well as hardware design and optimization are reviewed and analyzed. On this basis, several typical application schemes of DAB-IBDC for HPL PCSs are presented in a worldwide scope. Finally, design recommendations and future trends are presented. As the core circuit of HFL PCSs, DAB-IBDC has wide prospects. The large-scale practical application of DAB-IBDC for HFL PCSs is expected with the recent advances in solid-state semiconductors, magnetic and capacitive materials, and microelectronic technologies.","['Bridge circuits', 'Topology', 'Integrated circuit modeling', 'Switches', 'Circuit faults', 'Inductors']","['Bidirectional converters', 'dc–dc conversion', 'dual active bridge', 'efficiency', 'high-frequency link', 'isolated converters', 'nanocrystalline magnetic material', 'power conversion', 'power density', 'wide-band-gap semiconductor']"
"Understanding relationships between sets is an important analysis task that has received widespread attention in the visualization community. The major challenge in this context is the combinatorial explosion of the number of set intersections if the number of sets exceeds a trivial threshold. In this paper we introduce UpSet, a novel visualization technique for the quantitative analysis of sets, their intersections, and aggregates of intersections. UpSet is focused on creating task-driven aggregates, communicating the size and properties of aggregates and intersections, and a duality between the visualization of the elements in a dataset and their set membership. UpSet visualizes set intersections in a matrix layout and introduces aggregates based on groupings and queries. The matrix layout enables the effective representation of associated data, such as the number of elements in the aggregates and intersections, as well as additional summary statistics derived from subset or element attributes. Sorting according to various measures enables a task-driven analysis of relevant intersections and aggregates. The elements represented in the sets and their associated attributes are visualized in a separate view. Queries based on containment in specific intersections, aggregates or driven by attribute filters are propagated between both views. We also introduce several advanced visual encodings and interaction methods to overcome the problems of varying scales and to address scalability. UpSet is web-based and open source. We demonstrate its general utility in multiple use cases from various domains.","['Data visualization', 'Visualization', 'Power generation', 'Sorting', 'Information analysis']","['Computer Graphics', 'Databases Factual', 'Informatics']"
"DC–DC converters with voltage boost capability are widely used in a large number of power conversion applications, from fraction-of-volt to tens of thousands of volts at power levels from milliwatts to megawatts. The literature has reported on various voltage-boosting techniques, in which fundamental energy storing elements (inductors and capacitors) and/or transformers in conjunction with switch(es) and diode(s) are utilized in the circuit. These techniques include switched capacitor (charge pump), voltage multiplier, switched inductor/voltage lift, magnetic coupling, and multistage/-level, and each has its own merits and demerits depending on application, in terms of cost, complexity, power density, reliability, and efficiency. To meet the growing demand for such applications, new power converter topologies that use the above voltage-boosting techniques, as well as some active and passive components, are continuously being proposed. The permutations and combinations of the various voltage-boosting techniques with additional components in a circuit allow for numerous new topologies and configurations, which are often confusing and difficult to follow. Therefore, to present a clear picture on the general law and framework of the development of next-generation step-up dc–dc converters, this paper aims to comprehensively review and classify various step-up dc–dc converters based on their characteristics and voltage-boosting techniques. In addition, the advantages and disadvantages of these voltage-boosting techniques and associated converters are discussed in detail. Finally, broad applications of dc–dc converters are presented and summarized with comparative study of different voltage-boosting techniques.","['Inductors', 'Switches', 'Boosting', 'Topology', 'Pulse width modulation', 'Capacitors', 'Couplings']","['Coupled inductors', 'multilevel converter', 'multistage converter', 'pulse width modulated (PWM) boost converter', 'switched capacitor (SC)', 'switched inductor', 'switched mode step-up dc–dc converter', 'transformer', 'voltage lift (VL)', 'voltage multiplier']"
"With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.","['Machine learning', 'Informatics', 'Training', 'Biomedical imaging', 'Neurons', 'Artificial neural networks', 'Biological neural networks']","['Bioinformatics', 'deep learning', 'health informatics', 'machine learning', 'medical imaging', 'public health', 'wearable devices']"
"This letter presents our initial results in deep learning for channel estimation and signal detection in orthogonal frequency-division multiplexing (OFDM) systems. In this letter, we exploit deep learning to handle wireless OFDM channels in an end-to-end manner. Different from existing OFDM receivers that first estimate channel state information (CSI) explicitly and then detect/recover the transmitted symbols using the estimated CSI, the proposed deep learning-based approach estimates CSI implicitly and recovers the transmitted symbols directly. To address channel distortion, a deep learning model is first trained offline using the data generated from simulation based on channel statistics and then used for recovering the online transmitted data directly. From our simulation results, the deep learning based approach can address channel distortion and detect the transmitted symbols with performance comparable to the minimum mean-square error estimator. Furthermore, the deep learning-based approach is more robust than conventional methods when fewer training pilots are used, the cyclic prefix is omitted, and nonlinear clipping noise exists. In summary, deep learning is a promising tool for channel estimation and signal detection in wireless communications with complicated channel distortion and interference.","['Machine learning', 'OFDM', 'Channel estimation', 'Data models', 'Wireless communication', 'Training', 'Nonlinear distortion']","['Deep learning', 'channel estimation', 'OFDM']"
"A key challenge of future mobile communication research is to strike an attractive compromise between wireless network's area spectral efficiency and energy efficiency. This necessitates a clean-slate approach to wireless system design, embracing the rich body of existing knowledge, especially on multiple-input-multiple-ouput (MIMO) technologies. This motivates the proposal of an emerging wireless communications concept conceived for single-radio-frequency (RF) large-scale MIMO communications, which is termed as SM. The concept of SM has established itself as a beneficial transmission paradigm, subsuming numerous members of the MIMO system family. The research of SM has reached sufficient maturity to motivate its comparison to state-of-the-art MIMO communications, as well as to inspire its application to other emerging wireless systems such as relay-aided, cooperative, small-cell, optical wireless, and power-efficient communications. Furthermore, it has received sufficient research attention to be implemented in testbeds, and it holds the promise of stimulating further vigorous interdisciplinary research in the years to come. This tutorial paper is intended to offer a comprehensive state-of-the-art survey on SM-MIMO research, to provide a critical appraisal of its potential advantages, and to promote the discussion of its beneficial application areas and their research challenges leading to the analysis of the technological issues associated with the implementation of SM-MIMO. The paper is concluded with the description of the world's first experimental activities in this vibrant research field.","['Modulation', 'Tutorials', 'Modulation', 'MIMO', 'Spatial resolution']","['Green and sustainable wireless communications', 'heterogenous cellular networks', 'large-scale multiantenna systems', 'multiantenna wireless systems', 'network-coded cooperative wireless networks', 'relay-aided wireless communications', 'single-radio-frequency (RF) multiantenna systems', 'spatial modulation', 'testbed implementation', 'visible light communications']"
"Frequencies from 100 GHz to 3 THz are promising bands for the next generation of wireless communication systems because of the wide swaths of unused and unexplored spectrum. These frequencies also offer the potential for revolutionary applications that will be made possible by new thinking, and advances in devices, circuits, software, signal processing, and systems. This paper describes many of the technical challenges and opportunities for wireless communication and sensing applications above 100 GHz, and presents a number of promising discoveries, novel approaches, and recent results that will aid in the development and implementation of the sixth generation (6G) of wireless networks, and beyond. This paper shows recent regulatory and standard body rulings that are anticipating wireless products and services above 100 GHz and illustrates the viability of wireless cognition, hyper-accurate position location, sensing, and imaging. This paper also presents approaches and results that show how long distance mobile communications will be supported to above 800 GHz since the antenna gains are able to overcome air-induced attenuation, and present methods that reduce the computational complexity and simplify the signal processing used in adaptive antenna arrays, by exploiting the Special Theory of Relativity to create a cone of silence in over-sampled antenna arrays that improve performance for digital phased array antennas. Also, new results that give insights into power efficient beam steering algorithms, and new propagation and partition loss models above 100 GHz are given, and promising imaging, array processing, and position location results are presented. The implementation of spatial consistency at THz frequencies, an important component of channel modeling that considers minute changes and correlations over space, is also discussed. This paper offers the first in-depth look at the vast applications of THz wireless products and applications and provides approaches for how to reduce power and increase performance across several problem domains, giving early evidence that THz techniques are compelling and available for future wireless communications.","['Wireless communication', 'Wireless sensor networks', 'Antenna arrays', 'Bandwidth', 'Communication system security', 'Cognition', 'Imaging']","['mmWave', 'millimeter wave', '5G', 'D-band', '6G', 'channel sounder', 'propagation measurements', 'Terahertz (THz)', 'array processing', 'imaging', 'scattering theory', 'cone of silence', 'digital phased arrays', 'digital beamformer', 'signal processing for THz', 'position location', 'channel modeling', 'THz applications', 'wireless cognition', 'network offloading']"
"The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than $45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.","['Unmanned aerial vehicles', 'Market research', 'Wireless sensor networks', 'Wireless communication', 'Communication system security', 'Security', 'Surveillance']","['Civil infrastructure inspection', 'delivery of goods', 'precision agriculture', 'real-time monitoring', 'remote sensing', 'search and rescue', 'security and surveillance', 'UAVs', 'wireless coverage']"
"The relatively unused millimeter-wave (mmWave) spectrum offers excellent opportunities to increase mobile capacity due to the enormous amount of available raw bandwidth. This paper presents experimental measurements and empirically-based propagation channel models for the 28, 38, 60, and 73 GHz mmWave bands, using a wideband sliding correlator channel sounder with steerable directional horn antennas at both the transmitter and receiver from 2011 to 2013. More than 15,000 power delay profiles were measured across the mmWave bands to yield directional and omnidirectional path loss models, temporal and spatial channel models, and outage probabilities. Models presented here offer side-by-side comparisons of propagation characteristics over a wide range of mmWave bands, and the results and models are useful for the research and standardization process of future mmWave systems. Directional and omnidirectional path loss models with respect to a 1 m close-in free space reference distance over a wide range of mmWave frequencies and scenarios using directional antennas in real-world environments are provided herein, and are shown to simplify mmWave path loss models, while allowing researchers to globally compare and standardize path loss parameters for emerging mmWave wireless networks. A new channel impulse response modeling framework, shown to agree with extensive mmWave measurements over several bands, is presented for use in link-layer simulations, using the observed fact that spatial lobes contain multipath energy that arrives at many different propagation time intervals. The results presented here may assist researchers in analyzing and simulating the performance of next-generation mmWave wireless networks that will rely on adaptive antennas and multiple-input and multiple-output (MIMO) antenna systems.","['Antenna measurements', 'Wideband', 'Delays', 'Wireless communication', 'Loss measurement', 'Mobile communication']","['Millimeter-wave', 'path loss', 'multipath', 'RMS delay spread', 'small cell', 'channel sounder', 'statistical spatial channel models', '28 GHz', '38 GHz', '60 GHz', '73 GHz', 'propagation measurements', 'MIMO', '5G', 'SSCM']"
"Feature selection is an important task in data mining and machine learning to reduce the dimensionality of the data and increase the performance of an algorithm, such as a classification algorithm. However, feature selection is a challenging task due mainly to the large search space. A variety of methods have been applied to solve feature selection problems, where evolutionary computation (EC) techniques have recently gained much attention and shown some success. However, there are no comprehensive guidelines on the strengths and weaknesses of alternative approaches. This leads to a disjointed and fragmented field with ultimately lost opportunities for improving performance and successful applications. This paper presents a comprehensive survey of the state-of-the-art work on EC for feature selection, which identifies the contributions of these different algorithms. In addition, current issues and challenges are also discussed to identify promising areas for future research.","['Search problems', 'Machine learning algorithms', 'Evolutionary computation', 'Optimization', 'Data mining', 'Feature extraction', 'Computational efficiency']","['Classification', 'data mining', 'evolutionary computation', 'feature selection', 'machine learning']"
"Finding the sparsest solution to underdetermined systems of linear equations y = Φ x is NP-hard in general. We show here that for systems with “typical”/“random” Φ, a good approximation to the sparsest solution is obtained by applying a fixed number of standard operations from linear algebra. Our proposal, Stagewise Orthogonal Matching Pursuit (StOMP), successively transforms the signal into a negligible residual. Starting with initial residual r 0 = y , at the s -th stage it forms the “matched filter” Φ T rs-1 , identifies all coordinates with amplitudes exceeding a specially chosen threshold, solves a least-squares problem using the selected coordinates, and subtracts the least-squares fit, producing a new residual. After a fixed number of stages (e.g., 10), it stops. In contrast to Orthogonal Matching Pursuit (OMP), many coefficients can enter the model at each stage in StOMP while only one enters per stage in OMP; and StOMP takes a fixed number of stages (e.g., 10), while OMP can take many (e.g., n ). We give both theoretical and empirical support for the large-system effectiveness of StOMP. We give numerical examples showing that StOMP rapidly and reliably finds sparse solutions in compressed sensing, decoding of error-correcting codes, and overcomplete representation.","['Matching pursuit algorithms', 'Vectors', 'Approximation methods', 'Minimization', 'Sparse matrices', 'Gaussian noise', 'Equations']","['Compressed sensing', 'decoding error-correcting codes', 'false alarm rate', 'false discovery rate', 'iterative thresholding', '$\\ell _{1}$ minimization', 'mutual access interference', 'phase transition', 'sparse overcomplete representation', 'stepwise regression']"
"With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video/audio surveillance. More recently, with the proliferation of mobile computing and Internet of Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new interdiscipline, edge AI or edge intelligence (EI), is beginning to receive a tremendous amount of interest. However, research on EI is still in its infancy stage, and a dedicated venue for exchanging the recent advances of EI is highly desired by both the computer system and AI communities. To this end, we conduct a comprehensive survey of the recent research efforts on EI. Specifically, we first review the background and motivation for AI running at the network edge. We then provide an overview of the overarching architectures, frameworks, and emerging key technologies for deep learning model toward training/inference at the network edge. Finally, we discuss future research opportunities on EI. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI.","['Deep learning', 'Edge computing', 'Computational modeling', 'Computer architecture', 'Training', 'Task analysis']","['Artificial intelligence', 'deep learning', 'edge computing', 'edge intelligence']"
"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.","['Machine learning', 'Perturbation methods', 'Computer vision', 'Computational modeling', 'Neural networks', 'Task analysis', 'Predictive models']","['Deep learning', 'adversarial perturbation', 'black-box attack', 'white-box attack', 'adversarial learning', 'perturbation detection']"
"Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.","['Intrusion detection', 'Machine learning', 'Recurrent neural networks', 'Training', 'Computational modeling', 'Testing', 'Support vector machines']","['Recurrent neural networks', 'RNN-IDS', 'intrusion detection', 'deep learning', 'machine learning']"
"There is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. Object tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. A good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. However, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. In this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. We selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. We demonstrate that trackers can be evaluated objectively by survival curves, Kaplan Meier statistics, and Grubs testing. We find that in the evaluation practice the F-score is as effective as the object tracking accuracy (OTA) score. The analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers.","['Target tracking', 'Videos', 'Radar tracking', 'Educational institutions', 'Robustness', 'Object tracking']","['Object tracking', 'Tracking evaluation', 'Tracking dataset', 'Camera surveillance', 'Video understanding', 'Computer vision', 'Image processing']"
"This paper provides an overview of the features of fifth generation (5G) wireless communication systems now being developed for use in the millimeter wave (mmWave) frequency bands. Early results and key concepts of 5G networks are presented, and the channel modeling efforts of many international groups for both licensed and unlicensed applications are described here. Propagation parameters and channel models for understanding mmWave propagation, such as line-of-sight (LOS) probabilities, large-scale path loss, and building penetration loss, as modeled by various standardization bodies, are compared over the 0.5-100 GHz range.","['5G mobile communication', 'Computer architecture', 'Microprocessors', 'Wireless communication', 'Wireless fidelity', 'Atmospheric modeling']","['Cellular network', 'channel model standards', 'channel modeling', 'fifth generation (5G)', 'millimeter wave (mmWave)', 'path loss', 'propagation']"
"Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to the healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively. The high accuracy of this computer-aided diagnostic tool can significantly improve the speed and accuracy of COVID-19 diagnosis. This would be extremely useful in this pandemic where disease burden and need for preventive measures are at odds with available resources.","['Diseases', 'Lung', 'Databases', 'X-ray imaging', 'Machine learning', 'Tools', 'COVID-19']","['Artificial intelligence', 'COVID-19 pneumonia', 'machine learning', 'transfer learning', 'viral pneumonia', 'computer-aided diagnostic tool']"
"DC-link capacitors are an important part in the majority of power electronic converters which contribute to cost, size and failure rate on a considerable scale. From capacitor users' viewpoint, this paper presents a review on the improvement of reliability of dc link in power electronic converters from two aspects: 1) reliability-oriented dc-link design solutions; 2) conditioning monitoring of dc-link capacitors during operation. Failure mechanisms, failure modes and lifetime models of capacitors suitable for the applications are also discussed as a basis to understand the physics-of-failure. This review serves to provide a clear picture of the state-of-the-art research in this area and to identify the corresponding challenges and future research directions for capacitors and their dc-link applications.","['Capacitors', 'Stress', 'Capacitance', 'Reliability', 'Films', 'Integrated circuits']","['Ceramic capacitors', 'dc link', 'electrolytic capacitors', 'film capacitors', 'power converters', 'reliability']"
"Inductive power transfer (IPT) has progressed to be a power distribution system offering significant benefits in modern automation systems and particularly so in stringent environments. Here, the same technology may be used in very dirty environments and in a clean room manufacture. This paper reviews the development of simple factory automation (FA) IPT systems for both today's complex applications and onward to a much more challenging application—IPT roadway. The underpinning of all IPT technology is two strongly coupled coils operating at resonance to transfer power efficiently. Over time the air-gap, efficiency, coupling factor, and power transfer capability have significantly improved. New magnetic concepts are introduced to allow misalignment, enabling IPT systems to migrate from overhead monorails to the floor. However, the demands of IPT roadway bring about significant challenges. Here, compared with the best FA practice, air-gaps need to be 100 times larger, power levels greater than ten times, system losses ten times lower to meet efficiency requirements, and systems from different manufacturers must be interoperable over the full range of operation. This paper describes how roadway challenges are being met and outlines the problems that still exist and the solutions designers are finding to them.","['Inductive power transmission', 'Magnetic resonance', 'Electric vehicles']","['Inductive power transfer (IPT)', 'roadway powered electric vehicles', 'strongly coupled magnetic resonance', 'wireless charging systems']"
"Automated tissue characterization is one of the most crucial components of a computer aided diagnosis (CAD) system for interstitial lung diseases (ILDs). Although much research has been conducted in this field, the problem remains challenging. Deep learning techniques have recently achieved impressive results in a variety of computer vision problems, raising expectations that they might be applied in other domains, such as medical image analysis. In this paper, we propose and evaluate a convolutional neural network (CNN), designed for the classification of ILD patterns. The proposed network consists of 5 convolutional layers with 2 × 2 kernels and LeakyReLU activations, followed by average pooling with size equal to the size of the final feature maps and three dense layers. The last dense layer has 7 outputs, equivalent to the classes considered: healthy, ground glass opacity (GGO), micronodules, consolidation, reticulation, honeycombing and a combination of GGO/reticulation. To train and evaluate the CNN, we used a dataset of 14696 image patches, derived by 120 CT scans from different scanners and hospitals. To the best of our knowledge, this is the first deep CNN designed for the specific problem. A comparative analysis proved the effectiveness of the proposed CNN against previous methods in a challenging dataset. The classification performance ( ~ 85.5%) demonstrated the potential of CNNs in analyzing lung patterns. Future work includes, extending the CNN to three-dimensional data provided by CT volume scans and integrating the proposed method into a CAD system that aims to provide differential diagnosis for ILDs as a supportive tool for radiologists.","['Lungs', 'Computed tomography', 'Diseases', 'Feature extraction', 'Convolution', 'Design automation', 'Neural networks']","['Convolutional neural networks', 'interstitial lung diseases', 'texture classification']"
"Due to the broadcast nature of radio propagation, the wireless air interface is open and accessible to both authorized and illegitimate users. This completely differs from a wired network, where communicating devices are physically connected through cables and a node without direct association is unable to access the network for illicit activities. The open communications environment makes wireless transmissions more vulnerable than wired communications to malicious attacks, including both the passive eavesdropping for data interception and the active jamming for disrupting legitimate transmissions. Therefore, this paper is motivated to examine the security vulnerabilities and threats imposed by the inherent open nature of wireless communications and to devise efficient defense mechanisms for improving the wireless network security. We first summarize the security requirements of wireless networks, including their authenticity, confidentiality, integrity, and availability issues. Next, a comprehensive overview of security attacks encountered in wireless networks is presented in view of the network protocol architecture, where the potential security threats are discussed at each protocol layer. We also provide a survey of the existing security protocols and algorithms that are adopted in the existing wireless network standards, such as the Bluetooth, Wi-Fi, WiMAX, and the long-term evolution (LTE) systems. Then, we discuss the state of the art in physical-layer security, which is an emerging technique of securing the open communications environment against eavesdropping attacks at the physical layer. Several physical-layer security techniques are reviewed and compared, including information-theoretic security, artificial-noise-aided security, security-oriented beamforming, diversity-assisted security, and physical-layer key generation approaches. Since a jammer emitting radio signals can readily interfere with the legitimate wireless users, we also introduce the family of various jamming attacks and their countermeasures, including the constant jammer, intermittent jammer, reactive jammer, adaptive jammer, and intelligent jammer. Additionally, we discuss the integration of physical-layer security into existing authentication and cryptography mechanisms for further securing wireless networks. Finally, some technical challenges which remain unresolved at the time of writing are summarized and the future trends in wireless security are discussed.","['Communication system security', 'Wireless networks', 'Jamming', 'Authentication', 'Noise measurement', 'Array signal processing', 'Network security']","['Artificial noise', 'beamforming', 'denial of service (DoS)', 'diversity', 'eavesdropping attack', 'information-theoretic security', 'jamming', 'network protocol', 'wireless jamming', 'wireless networks', 'wireless security']"
"Printing sensors and electronics over flexible substrates are an area of significant interest due to low-cost fabrication and possibility of obtaining multifunctional electronics over large areas. Over the years, a number of printing technologies have been developed to pattern a wide range of electronic materials on diverse substrates. As further expansion of printed technologies is expected in future for sensors and electronics, it is opportune to review the common features, the complementarities, and the challenges associated with various printing technologies. This paper presents a comprehensive review of various printing technologies, commonly used substrates and electronic materials. Various solution/dry printing and contact/noncontact printing technologies have been assessed on the basis of technological, materials, and process-related developments in the field. Critical challenges in various printing techniques and potential research directions have been highlighted. Possibilities of merging various printing methodologies have been explored to extend the lab developed standalone systems to high-speed roll-to-roll production lines for system level integration.","['Printing', 'Substrates', 'Sensors', 'Polymers', 'Flexible electronics', 'Thermal stability']","['Printed Sensors', 'Printed Electronics', 'Flexible Electronics', 'Large Area Electronics', 'Roll-to-Roll', 'Dispersion Solutions']"
"We propose a branch flow model for the analysis and optimization of mesh as well as radial networks. The model leads to a new approach to solving optimal power flow (OPF) that consists of two relaxation steps. The first step eliminates the voltage and current angles and the second step approximates the resulting problem by a conic program that can be solved efficiently. For radial networks, we prove that both relaxation steps are always exact, provided there are no upper bounds on loads. For mesh networks, the conic relaxation is always exact but the angle relaxation may not be exact, and we provide a simple way to determine if a relaxed solution is globally optimal. We propose convexification of mesh networks using phase shifters so that OPF for the convexified network can always be solved efficiently for an optimal solution. We prove that convexification requires phase shifters only outside a spanning tree of the network and their placement depends only on network topology, not on power flows, generation, loads, or operating constraints. Part I introduces our branch flow model, explains the two relaxation steps, and proves the conditions for exact relaxation. Part II describes convexification of mesh networks, and presents simulation results.","['Mesh networks', 'Mathematical model', 'Integrated circuit modeling', 'Load modeling', 'Upper bound', 'Analytical models', 'Optimization']","['Convex relaxation', 'load flow control', 'optimal power flow', 'phase control', 'power system management']"
"Driven by the rapid escalation of the wireless capacity requirements imposed by advanced multimedia applications (e.g., ultrahigh-definition video, virtual reality, etc.), as well as the dramatically increasing demand for user access required for the Internet of Things (IoT), the fifth-generation (5G) networks face challenges in terms of supporting large-scale heterogeneous data traffic. Nonorthogonal multiple access (NOMA), which has been recently proposed for the third-generation partnership projects long-term evolution advanced (3GPP-LTE-A), constitutes a promising technology of addressing the aforementioned challenges in 5G networks by accommodating several users within the same orthogonal resource block. By doing so, significant bandwidth efficiency enhancement can be attained over conventional orthogonal multiple-access (OMA) techniques. This motivated numerous researchers to dedicate substantial research contributions to this field. In this context, we provide a comprehensive overview of the state of the art in power-domain multiplexing-aided NOMA, with a focus on the theoretical NOMA principles, multiple-antenna-aided NOMA design, on the interplay between NOMA and cooperative transmission, on the resource control of NOMA, on the coexistence of NOMA with other emerging potential 5G techniques and on the comparison with other NOMA variants. We highlight the main advantages of power-domain multiplexing NOMA compared to other existing NOMA techniques. We summarize the challenges of existing research contributions of NOMA and provide potential solutions. Finally, we offer some design guidelines for NOMA systems and identify promising research opportunities for the future.","['NOMA', '5G mobile communication', 'Multiaccess communication', 'MIMO', 'Encoding', 'Cooperative communication', 'Resource management', 'Streaming media', 'Multimedia communication']","['Cooperative communication', 'fifth generation (5G)', 'multiple-input-multiple-output (MIMO)', 'nonorthogonal multiple access (NOMA)', 'power multiplexing', 'resource allocation']"
"The Internet of Things (IoT) now permeates our daily lives, providing important measurement and collection tools to inform our every decision. Millions of sensors and devices are continuously producing data and exchanging important messages via complex networks supporting machine-to-machine communications and monitoring and controlling critical smart-world infrastructures. As a strategy to mitigate the escalation in resource congestion, edge computing has emerged as a new paradigm to solve IoT and localized computing needs. Compared with the well-known cloud computing, edge computing will migrate data computation or storage to the network “edge”, near the end users. Thus, a number of computation nodes distributed across the network can offload the computational stress away from the centralized data center, and can significantly reduce the latency in message exchange. In addition, the distributed structure can balance network traffic and avoid the traffic peaks in IoT networks, reducing the transmission latency between edge/cloudlet servers and end users, as well as reducing response times for real-time IoT applications in comparison with traditional cloud services. Furthermore, by transferring computation and communication overhead from nodes with limited battery supply to nodes with significant power resources, the system can extend the lifetime of the individual nodes. In this paper, we conduct a comprehensive survey, analyzing how edge computing improves the performance of IoT networks. We categorize edge computing into different groups based on architecture, and study their performance by comparing network latency, bandwidth occupation, energy consumption, and overhead. In addition, we consider security issues in edge computing, evaluating the availability, integrity, and the confidentiality of security strategies of each group, and propose a framework for security evaluation of IoT networks with edge computing. Finally, we compare the performance of various IoT applications (smart city, smart grid, smart transportation, and so on) in edge computing and traditional cloud computing architectures.","['Edge computing', 'Cloud computing', 'Logic gates', 'Servers', 'Security', 'Intelligent sensors']","['Edge computing', 'Internet of Things', 'survey']"
"For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.","['Approximation algorithms', 'Clustering algorithms', 'Vegetation', 'Partitioning algorithms', 'Approximation methods', 'Machine learning algorithms', 'Computer vision']","['Nearest neighbor search', 'big data', 'approximate search', 'algorithm configuration']"
"Tensors or multiway arrays are functions of three or more indices (i, j, k, . . . )-similar to matrices (two-way arrays), which are functions of two indices (r, c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.","['Tensile stress', 'Signal processing algorithms', 'Matrix decomposition', 'Signal processing', 'Optimization', 'Tutorials']","['Tensor decomposition', 'tensor factorization', 'rank', 'canonical polyadic decomposition (CPD)', 'parallel factor analysis (PARAFAC)', 'Tucker model', 'higher-order singular value decomposition (HOSVD)', 'multilinear singular value decomposition (MLSVD)', 'uniqueness', 'NP-hard problems', 'alternating optimization', 'alternating direction method of multipliers', 'gradient descent', 'Gauss–Newton', 'stochastic gradient', 'Cramér–Rao bound', 'communications', 'source separation', 'harmonic retrieval', 'speech separation', 'collaborative filtering', 'mixture modeling', 'topic modeling', 'classification', 'subspace learning']"
"Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.","['Deep learning', 'Training', 'Computer architecture', 'Feature extraction', 'Recurrent neural networks', 'Feedforward neural networks']","['Machine learning algorithm', 'optimization', 'artificial intelligence', 'deep neural network architectures', 'convolution neural network', 'backpropagation', 'supervised and unsupervised learning']"
"Inductive power transfer (IPT) was an engineering curiosity less than 30 years ago, but, at that time, it has grown to be an important technology in a variety of applications. The paper looks at the background to IPT and how its development was based on sound engineering principles leading on to factory automation and growing to a $1 billion industry in the process. Since then applications for the technology have diversified and at the same time become more technically challenging, especially for the static and dynamic charging of electric vehicles (EVs), where IPT offers possibilities that no other technology can match. Here, systems that are ten times more powerful, more tolerant of misalignment, safer, and more efficient may be achievable, and if they are, IPT can transform our society. The challenges are significant but the technology is promising.","['Wireless communication', 'Power transmission', 'Electric vehicles', 'Road transportation', 'Inductive power transmission', 'Intelligent vehicles', 'Wireless sensor networks']","['Electric vehicles (EVs)', 'inductive power transfer (IPT)', 'resonant coupling', 'roadway-powered electric vehicles']"
"In recent years, vector-based machine learning algorithms, such as random forests, support vector machines, and 1-D convolutional neural networks, have shown promising results in hyperspectral image classification. Such methodologies, nevertheless, can lead to information loss in representing hyperspectral pixels, which intrinsically have a sequence-based data structure. A recurrent neural network (RNN), an important branch of the deep learning family, is mainly designed to handle sequential data. Can sequence-based RNN be an effective method of hyperspectral image classification? In this paper, we propose a novel RNN model that can effectively analyze hyperspectral pixels as sequential data and then determine information categories via network reasoning. As far as we know, this is the first time that an RNN framework has been proposed for hyperspectral image classification. Specifically, our RNN makes use of a newly proposed activation function, parametric rectified tanh (PRetanh), for hyperspectral sequential data analysis instead of the popular tanh or rectified linear unit. The proposed activation function makes it possible to use fairly high learning rates without the risk of divergence during the training procedure. Moreover, a modified gated recurrent unit, which uses PRetanh for hidden representation, is adopted to construct the recurrent layer in our network to efficiently process hyperspectral data and reduce the total number of parameters. Experimental results on three airborne hyperspectral images suggest competitive performance in the proposed mode. In addition, the proposed network architecture opens a new window for future research, showcasing the huge potential of deep recurrent networks for hyperspectral data analysis.","['Hyperspectral imaging', 'Recurrent neural networks', 'Logic gates', 'Support vector machines', 'Data models']","['Convolutional neural network (CNN)', 'deep learning', 'gated recurrent unit (GRU)', 'hyperspectral image classification', 'long short-term memory (LSTM)', 'recurrent neural network (RNN)']"
"The Internet of Things (IoT) is a promising technology which tends to revolutionize and connect the global world via heterogeneous smart devices through seamless connectivity. The current demand for machine-type communications (MTC) has resulted in a variety of communication technologies with diverse service requirements to achieve the modern IoT vision. More recent cellular standards like long-term evolution (LTE) have been introduced for mobile devices but are not well suited for low-power and low data rate devices such as the IoT devices. To address this, there is a number of emerging IoT standards. Fifth generation (5G) mobile network, in particular, aims to address the limitations of previous cellular standards and be a potential key enabler for future IoT. In this paper, the state-of-the-art of the IoT application requirements along with their associated communication technologies are surveyed. In addition, the third generation partnership project cellular-based low-power wide area solutions to support and enable the new service requirements for Massive to Critical IoT use cases are discussed in detail, including extended coverage global system for mobile communications for the Internet of Things, enhanced machine-type communications, and narrowband-Internet of Things. Furthermore, 5G new radio enhancements for new service requirements and enabling technologies for the IoT are introduced. This paper presents a comprehensive review related to emerging and enabling technologies with main focus on 5G mobile networks that is envisaged to support the exponential traffic growth for enabling the IoT. The challenges and open research directions pertinent to the deployment of massive to critical IoT applications are also presented in coming up with an efficient context-aware congestion control mechanism.","['5G mobile communication', 'Machine-to-machine communications', 'Mobile computing', 'Internet of Things']","['Internet of Things', 'long-term evolution', 'machine-type communications', '5G new radio']"
"With the advances in new-generation information technologies, especially big data and digital twin, smart manufacturing is becoming the focus of global manufacturing transformation and upgrading. Intelligence comes from data. Integrated analysis for the manufacturing big data is beneficial to all aspects of manufacturing. Besides, the digital twin paves a way for the cyber-physical integration of manufacturing, which is an important bottleneck to achieve smart manufacturing. In this paper, the big data and digital twin in manufacturing are reviewed, including their concept as well as their applications in product design, production planning, manufacturing, and predictive maintenance. On this basis, the similarities and differences between big data and digital twin are compared from the general and data perspectives. Since the big data and digital twin can be complementary, how they can be integrated to promote smart manufacturing are discussed.","['Big Data', 'Data mining', 'Internet', 'Product design', 'Real-time systems']","['Big data', 'digital twin', 'smart manufacturing', 'comprehensive comparison', 'convergence']"
