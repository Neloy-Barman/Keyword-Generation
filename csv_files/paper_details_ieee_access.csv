abstracts,ieee_keywords,author_keywords
"The global bandwidth shortage facing wireless carriers has motivated the exploration of the underutilized millimeter wave (mm-wave) frequency spectrum for future broadband cellular communication networks. There is, however, little knowledge about cellular mm-wave propagation in densely populated indoor and outdoor environments. Obtaining this information is vital for the design and operation of future fifth generation cellular networks that use the mm-wave spectrum. In this paper, we present the motivation for new mm-wave cellular systems, methodology, and hardware for measurements and offer a variety of measurement results that show 28 and 38 GHz frequencies can be used when employing steerable directional antennas at base stations and mobile devices.",[],[]
"Motivated by the recent explosion of interest around blockchains, we examine whether they make a good fit for the Internet of Things (IoT) sector. Blockchains allow us to have a distributed peer-to-peer network where non-trusting members can interact with each other without a trusted intermediary, in a verifiable manner. We review how this mechanism works and also look into smart contracts-scripts that reside on the blockchain that allow for the automation of multi-step processes. We then move into the IoT domain, and describe how a blockchain-IoT combination: 1) facilitates the sharing of services and resources leading to the creation of a marketplace of services between devices and 2) allows us to automate in a cryptographically verifiable manner several existing, time-consuming workflows. We also point out certain issues that should be considered before the deployment of a blockchain network in an IoT setting: from transactional privacy to the expected value of the digitized assets traded on the network. Wherever applicable, we identify solutions and workarounds. Our conclusion is that the blockchain-IoT combination is powerful and can cause significant transformations across several industries, paving the way for new business models and novel, distributed applications.","['Distributed processing', 'Internet of things', 'Cryptography', 'Privacy', 'Blockchains', 'Automation', 'Peer-to-peer computing']","['blockchain', 'distributed systems', 'Internet of Things']"
"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","['Conferences', 'Machine learning', 'Market research', 'Prediction algorithms', 'Machine learning algorithms', 'Biological system modeling']","['Explainable artificial intelligence', 'interpretable machine learning', 'black-box models']"
"The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges.","['Internet of things', 'Medical services', 'Network security', 'Network architecture', 'Biological system modeling', 'Market research']","['Internet of Things', 'Health Care', 'Services', 'Applications', 'Networks', 'Architectures', 'Platforms', 'Security', 'Technologies', 'Industries', 'Policies', 'Challenges']"
"In the near future, i.e., beyond 4G, some of the prime objectives or demands that need to be addressed are increased capacity, improved data rate, decreased latency, and better quality of service. To meet these demands, drastic improvements need to be made in cellular network architecture. This paper presents the results of a detailed survey on the fifth generation (5G) cellular network architecture and some of the key emerging technologies that are helpful in improving the architecture and meeting the demands of users. In this detailed survey, the prime focus is on the 5G cellular network architecture, massive multiple input multiple output technology, and device-to-device communication (D2D). Along with this, some of the emerging technologies that are addressed in this paper include interference management, spectrum sharing with cognitive radio, ultra-dense networks, multi-radio access technology association, full duplex radios, millimeter wave solutions for 5G cellular networks, and cloud technologies for 5G radio access networks and software defined networks. In this paper, a general probable 5G cellular network architecture is proposed, which shows that D2D, small cell access points, network cloud, and the Internet of Things can be a part of 5G cellular network architecture. A detailed survey is included regarding current research projects being conducted in different countries by research groups and institutions that are working on 5G technologies.","['5G mobile communication', 'Cloud computing', 'MIMO', 'Radio access networks', 'Cellular networks']","['5G', 'Cloud', 'D2D', 'Massive MIMO', 'mm-wave', 'Relay', 'Small-cel']"
"The future of mobile communications looks exciting with the potential new use cases and challenging requirements of future 6th generation (6G) and beyond wireless networks. Since the beginning of the modern era of wireless communications, the propagation medium has been perceived as a randomly behaving entity between the transmitter and the receiver, which degrades the quality of the received signal due to the uncontrollable interactions of the transmitted radio waves with the surrounding objects. The recent advent of reconfigurable intelligent surfaces in wireless communications enables, on the other hand, network operators to control the scattering, reflection, and refraction characteristics of the radio waves, by overcoming the negative effects of natural wireless propagation. Recent results have revealed that reconfigurable intelligent surfaces can effectively control the wavefront, e.g., the phase, amplitude, frequency, and even polarization, of the impinging signals without the need of complex decoding, encoding, and radio frequency processing operations. Motivated by the potential of this emerging technology, the present article is aimed to provide the readers with a detailed overview and historical perspective on state-of-the-art solutions, and to elaborate on the fundamental differences with other technologies, the most important open research issues to tackle, and the reasons why the use of reconfigurable intelligent surfaces necessitates to rethink the communication-theoretic models currently employed in wireless networks. This article also explores theoretical performance limits of reconfigurable intelligent surface-assisted communication systems using mathematical techniques and elaborates on the potential use cases of intelligent surfaces in 6G and beyond wireless networks.","['Wireless networks', '5G mobile communication', 'Surface waves', 'STEM', '6G mobile communication']","['6G', 'large intelligent surfaces', 'meta-surfaces', 'reconfigurable intelligent surfaces', 'smart reflect-arrays', 'software-defined surfaces', 'wireless communications', 'wireless networks']"
"Frequencies from 100 GHz to 3 THz are promising bands for the next generation of wireless communication systems because of the wide swaths of unused and unexplored spectrum. These frequencies also offer the potential for revolutionary applications that will be made possible by new thinking, and advances in devices, circuits, software, signal processing, and systems. This paper describes many of the technical challenges and opportunities for wireless communication and sensing applications above 100 GHz, and presents a number of promising discoveries, novel approaches, and recent results that will aid in the development and implementation of the sixth generation (6G) of wireless networks, and beyond. This paper shows recent regulatory and standard body rulings that are anticipating wireless products and services above 100 GHz and illustrates the viability of wireless cognition, hyper-accurate position location, sensing, and imaging. This paper also presents approaches and results that show how long distance mobile communications will be supported to above 800 GHz since the antenna gains are able to overcome air-induced attenuation, and present methods that reduce the computational complexity and simplify the signal processing used in adaptive antenna arrays, by exploiting the Special Theory of Relativity to create a cone of silence in over-sampled antenna arrays that improve performance for digital phased array antennas. Also, new results that give insights into power efficient beam steering algorithms, and new propagation and partition loss models above 100 GHz are given, and promising imaging, array processing, and position location results are presented. The implementation of spatial consistency at THz frequencies, an important component of channel modeling that considers minute changes and correlations over space, is also discussed. This paper offers the first in-depth look at the vast applications of THz wireless products and applications and provides approaches for how to reduce power and increase performance across several problem domains, giving early evidence that THz techniques are compelling and available for future wireless communications.","['Wireless communication', 'Wireless sensor networks', 'Antenna arrays', 'Bandwidth', 'Communication system security', 'Cognition', 'Imaging']","['mmWave', 'millimeter wave', '5G', 'D-band', '6G', 'channel sounder', 'propagation measurements', 'Terahertz (THz)', 'array processing', 'imaging', 'scattering theory', 'cone of silence', 'digital phased arrays', 'digital beamformer', 'signal processing for THz', 'position location', 'channel modeling', 'THz applications', 'wireless cognition', 'network offloading']"
"The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than $45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.","['Unmanned aerial vehicles', 'Market research', 'Wireless sensor networks', 'Wireless communication', 'Communication system security', 'Security', 'Surveillance']","['Civil infrastructure inspection', 'delivery of goods', 'precision agriculture', 'real-time monitoring', 'remote sensing', 'search and rescue', 'security and surveillance', 'UAVs', 'wireless coverage']"
"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.","['Machine learning', 'Perturbation methods', 'Computer vision', 'Computational modeling', 'Neural networks', 'Task analysis', 'Predictive models']","['Deep learning', 'adversarial perturbation', 'black-box attack', 'white-box attack', 'adversarial learning', 'perturbation detection']"
"Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.","['Intrusion detection', 'Machine learning', 'Recurrent neural networks', 'Training', 'Computational modeling', 'Testing', 'Support vector machines']","['Recurrent neural networks', 'RNN-IDS', 'intrusion detection', 'deep learning', 'machine learning']"
"Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to the healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively. The high accuracy of this computer-aided diagnostic tool can significantly improve the speed and accuracy of COVID-19 diagnosis. This would be extremely useful in this pandemic where disease burden and need for preventive measures are at odds with available resources.","['Diseases', 'Lung', 'Databases', 'X-ray imaging', 'Machine learning', 'Tools', 'COVID-19']","['Artificial intelligence', 'COVID-19 pneumonia', 'machine learning', 'transfer learning', 'viral pneumonia', 'computer-aided diagnostic tool']"
"The Internet of Things (IoT) now permeates our daily lives, providing important measurement and collection tools to inform our every decision. Millions of sensors and devices are continuously producing data and exchanging important messages via complex networks supporting machine-to-machine communications and monitoring and controlling critical smart-world infrastructures. As a strategy to mitigate the escalation in resource congestion, edge computing has emerged as a new paradigm to solve IoT and localized computing needs. Compared with the well-known cloud computing, edge computing will migrate data computation or storage to the network “edge”, near the end users. Thus, a number of computation nodes distributed across the network can offload the computational stress away from the centralized data center, and can significantly reduce the latency in message exchange. In addition, the distributed structure can balance network traffic and avoid the traffic peaks in IoT networks, reducing the transmission latency between edge/cloudlet servers and end users, as well as reducing response times for real-time IoT applications in comparison with traditional cloud services. Furthermore, by transferring computation and communication overhead from nodes with limited battery supply to nodes with significant power resources, the system can extend the lifetime of the individual nodes. In this paper, we conduct a comprehensive survey, analyzing how edge computing improves the performance of IoT networks. We categorize edge computing into different groups based on architecture, and study their performance by comparing network latency, bandwidth occupation, energy consumption, and overhead. In addition, we consider security issues in edge computing, evaluating the availability, integrity, and the confidentiality of security strategies of each group, and propose a framework for security evaluation of IoT networks with edge computing. Finally, we compare the performance of various IoT applications (smart city, smart grid, smart transportation, and so on) in edge computing and traditional cloud computing architectures.","['Edge computing', 'Cloud computing', 'Logic gates', 'Servers', 'Security', 'Intelligent sensors']","['Edge computing', 'Internet of Things', 'survey']"
"Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.","['Deep learning', 'Training', 'Computer architecture', 'Feature extraction', 'Recurrent neural networks', 'Feedforward neural networks']","['Machine learning algorithm', 'optimization', 'artificial intelligence', 'deep neural network architectures', 'convolution neural network', 'backpropagation', 'supervised and unsupervised learning']"
"The Internet of Things (IoT) is a promising technology which tends to revolutionize and connect the global world via heterogeneous smart devices through seamless connectivity. The current demand for machine-type communications (MTC) has resulted in a variety of communication technologies with diverse service requirements to achieve the modern IoT vision. More recent cellular standards like long-term evolution (LTE) have been introduced for mobile devices but are not well suited for low-power and low data rate devices such as the IoT devices. To address this, there is a number of emerging IoT standards. Fifth generation (5G) mobile network, in particular, aims to address the limitations of previous cellular standards and be a potential key enabler for future IoT. In this paper, the state-of-the-art of the IoT application requirements along with their associated communication technologies are surveyed. In addition, the third generation partnership project cellular-based low-power wide area solutions to support and enable the new service requirements for Massive to Critical IoT use cases are discussed in detail, including extended coverage global system for mobile communications for the Internet of Things, enhanced machine-type communications, and narrowband-Internet of Things. Furthermore, 5G new radio enhancements for new service requirements and enabling technologies for the IoT are introduced. This paper presents a comprehensive review related to emerging and enabling technologies with main focus on 5G mobile networks that is envisaged to support the exponential traffic growth for enabling the IoT. The challenges and open research directions pertinent to the deployment of massive to critical IoT applications are also presented in coming up with an efficient context-aware congestion control mechanism.","['5G mobile communication', 'Machine-to-machine communications', 'Mobile computing', 'Internet of Things']","['Internet of Things', 'long-term evolution', 'machine-type communications', '5G new radio']"
"With the advances in new-generation information technologies, especially big data and digital twin, smart manufacturing is becoming the focus of global manufacturing transformation and upgrading. Intelligence comes from data. Integrated analysis for the manufacturing big data is beneficial to all aspects of manufacturing. Besides, the digital twin paves a way for the cyber-physical integration of manufacturing, which is an important bottleneck to achieve smart manufacturing. In this paper, the big data and digital twin in manufacturing are reviewed, including their concept as well as their applications in product design, production planning, manufacturing, and predictive maintenance. On this basis, the similarities and differences between big data and digital twin are compared from the general and data perspectives. Since the big data and digital twin can be complementary, how they can be integrated to promote smart manufacturing are discussed.","['Big Data', 'Data mining', 'Internet', 'Product design', 'Real-time systems']","['Big data', 'digital twin', 'smart manufacturing', 'comprehensive comparison', 'convergence']"
"Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.","['Machine learning', 'Pattern recognition', 'Big data', 'Natural language processing', 'Data processing', 'Information analysis']","['Classifier design and evaluation', 'feature representation', 'machine learning', 'neural nets models', 'parallel processing']"
"The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.","['Image analysis', 'Machine learning algorithms', 'Medical diagnostic imaging', 'Convolution']","['Convolutional neural networks', 'medical image analysis', 'machine learning', 'deep learning']"
"Underwater wireless information transfer is of great interest to the military, industry, and the scientific community, as it plays an important role in tactical surveillance, pollution monitoring, oil control and maintenance, offshore explorations, climate change monitoring, and oceanography research. In order to facilitate all these activities, there is an increase in the number of unmanned vehicles or devices deployed underwater, which require high bandwidth and high capacity for information transfer underwater. Although tremendous progress has been made in the field of acoustic communication underwater, however, it is limited by bandwidth. All this has led to the proliferation of underwater optical wireless communication (UOWC), as it provides higher data rates than the traditional acoustic communication systems with significantly lower power consumption and simpler computational complexities for short-range wireless links. UOWC has many potential applications ranging from deep oceans to coastal waters. However, the biggest challenge for underwater wireless communication originates from the fundamental characteristics of ocean or sea water; addressing these challenges requires a thorough understanding of complex physio-chemical biological systems. In this paper, the main focus is to understand the feasibility and the reliability of high data rate underwater optical links due to various propagation phenomena that impact the performance of the system. This paper provides an exhaustive overview of recent advances in UOWC. Channel characterization, modulation schemes, coding techniques, and various sources of noise which are specific to UOWC are discussed. This paper not only provides exhaustive research in underwater optical communication but also aims to provide the development of new ideas that would help in the growth of future underwater communication. A hybrid approach to an acousto-optic communication system is presented that complements the existing acoustic system, resulting in high data rates, low latency, and an energy-efficient system.","['Wireless communication', 'Optical fiber communication', 'Bandwidth allocation', 'Acoustic communication', 'Oceans', 'Underwater communication', 'Radio frequency', 'Modulation', 'Climate change']","['Underwater optical wireless', 'optical beam propagation', 'visible light', 'radio frequency', 'acoustic communication', 'hybrid optical-acoustic system', 'modulation and coding']"
"Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision, and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this paper is to provide a comprehensive study and an updated review on sparse representation and to supply guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: 1) sparse representation with l 0 -norm minimization; 2) sparse representation with lp-norm (0 <; p <; 1) minimization; 3) sparse representation with l 1 -norm minimization; 4) sparse representation with l 2 ,1-norm minimization; and 5) sparse representation with l2-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: 1) greedy strategy approximation; 2) constrained optimization; 3) proximity algorithm-based optimization; and 4) homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. In particular, an experimentally comparative study of these sparse representation algorithms was presented.","['Sparse matrices', 'Algorithm design and analysis', 'Signal processing algorithms', 'Approximation algorithms', 'Approximation methods', 'Signal processing']","['Sparse representation', 'compressive sensing', 'greedy algorithm', 'constrained optimization', 'proximal algorithm', 'homotopy algorithm', 'dictionary learning']"
"Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01-0.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data by passing them into many hidden layers. Through a rigorous experimental testing, it is confirmed that DNNs perform well in comparison with the classical machine learning classifiers. Finally, we propose a highly scalable and hybrid DNNs framework called scale-hybrid-IDS-AlertNet which can be used in real-time to effectively monitor the network traffic and host-level events to proactively alert possible cyberattacks.","['Intrusion detection', 'Computer security', 'Deep learning', 'Malware', 'Benchmark testing']","['Cyber security', 'intrusion detection', 'malware', 'big data', 'machine learning', 'deep learning', 'deep neural networks', 'cyberattacks', 'cybercrime']"
"The significant benefits associated with microgrids have led to vast efforts to expand their penetration in electric power systems. Although their deployment is rapidly growing, there are still many challenges to efficiently design, control, and operate microgrids when connected to the grid, and also when in islanded mode, where extensive research activities are underway to tackle these issues. It is necessary to have an across-the-board view of the microgrid integration in power systems. This paper presents a review of issues concerning microgrids and provides an account of research in areas related to microgrids, including distributed generation, microgrid value propositions, applications of power electronics, economic issues, microgrid operation and control, microgrid clusters, and protection and communications issues.",[],[]
"Digital Twin technology is an emerging concept that has become the centre of attention for industry and, in more recent years, academia. The advancements in industry 4.0 concepts have facilitated its growth, particularly in the manufacturing industry. The Digital Twin is defined extensively but is best described as the effortless integration of data between a physical and virtual machine in either direction. The challenges, applications, and enabling technologies for Artificial Intelligence, Internet of Things (IoT) and Digital Twins are presented. A review of publications relating to Digital Twins is performed, producing a categorical review of recent papers. The review has categorised them by research areas: manufacturing, healthcare and smart cities, discussing a range of papers that reflect these areas and the current state of research. The paper provides an assessment of the enabling technologies, challenges and open research for Digital Twins.","['Smart cities', 'Data analysis', 'Manufacturing', 'Data models', 'Internet of Things', 'Computational modeling']","['Digital twins', 'applications', 'enabling technologies', 'industrial Internet of Things (IIoT)', 'Internet of Things (IoT)', 'machine learning', 'deep learning', 'literature review']"
"The Internet of Things (IoT) is the next era of communication. Using the IoT, physical objects can be empowered to create, receive, and exchange data in a seamless manner. Various IoT applications focus on automating different tasks and are trying to empower the inanimate physical objects to act without any human intervention. The existing and upcoming IoT applications are highly promising to increase the level of comfort, efficiency, and automation for the users. To be able to implement such a world in an ever-growing fashion requires high security, privacy, authentication, and recovery from attacks. In this regard, it is imperative to make the required changes in the architecture of the IoT applications for achieving end-to-end secure IoT environments. In this paper, a detailed review of the security-related challenges and sources of threat in the IoT applications is presented. After discussing the security issues, various emerging and existing technologies focused on achieving a high degree of trust in the IoT applications are discussed. Four different technologies, blockchain, fog computing, edge computing, and machine learning, to increase the level of security in IoT are discussed.","['Internet of Things', 'Security', 'Edge computing', 'Computer architecture', 'Privacy', 'Blockchain']","['Internet of Things (IoT)', 'IoT security', 'blockchain', 'fog computing', 'edge computing', 'machine learning', 'IoT applications', 'distributed systems']"
"The dissemination of patients' medical records results in diverse risks to patients' privacy as malicious activities on these records cause severe damage to the reputation, finances, and so on of all parties related directly or indirectly to the data. Current methods to effectively manage and protect medical records have been proved to be insufficient. In this paper, we propose MeDShare, a system that addresses the issue of medical data sharing among medical big data custodians in a trust-less environment. The system is blockchain-based and provides data provenance, auditing, and control for shared medical data in cloud repositories among big data entities. MeDShare monitors entities that access data for malicious use from a data custodian system. In MeDShare, data transitions and sharing from one entity to the other, along with all actions performed on the MeDShare system, are recorded in a tamper-proof manner. The design employs smart contracts and an access control mechanism to effectively track the behavior of the data and revoke access to offending entities on detection of violation of permissions on data. The performance of MeDShare is comparable to current cutting edge solutions to data sharing among cloud service providers. By implementing MeDShare, cloud service providers and other data guardians will be able to achieve data provenance and auditing while sharing medical data with entities such as research and medical institutions with minimal risk to data privacy.","['Cloud computing', 'Contracts', 'Access control', 'Electronic mail', 'Monitoring', 'Public key']","['Access control', 'blockchain', 'cloud computing', 'data sharing', 'electronic medical records', 'privacy']"
"Recent technological advancements have led to a deluge of data from distinctive domains (e.g., health care and scientific sensors, user-generated data, Internet and financial companies, and supply chain systems) over the past two decades. The term big data was coined to capture the meaning of this emerging trend. In addition to its sheer volume, big data also exhibits other unique characteristics as compared with traditional data. For instance, big data is commonly unstructured and require more real-time analysis. This development calls for new system architectures for data acquisition, transmission, storage, and large-scale data processing mechanisms. In this paper, we present a literature survey and system tutorial for big data analytics platforms, aiming to provide an overall picture for nonexpert readers and instill a do-it-yourself spirit for advanced audiences to customize their own big-data solutions. First, we present the definition of big data and discuss big data challenges. Next, we present a systematic framework to decompose big data systems into four sequential modules, namely data generation, data acquisition, data storage, and data analytics. These four modules form a big data value chain. Following that, we present a detailed survey of numerous approaches and mechanisms from research and industry communities. In addition, we present the prevalent Hadoop framework for addressing big data challenges. Finally, we outline several evaluation benchmarks and potential research directions for big data systems.","['Big data', 'Information analysis', 'Scalability', 'Data acquisition', 'Medical services', 'Supply chain management', 'Sensor phenomena and characterization', 'Sensor systems', 'Real-time systems', 'Tutorials']","['Big data analytics', 'cloud computing', 'data acquisition', 'data storage', 'data analytics', 'Hadoop']"
"With the developments and applications of the new information technologies, such as cloud computing, Internet of Things, big data, and artificial intelligence, a smart manufacturing era is coming. At the same time, various national manufacturing development strategies have been put forward, such as Industry 4.0, Industrial Internet, manufacturing based on Cyber-Physical System, and Made in China 2025. However, one of specific challenges to achieve smart manufacturing with these strategies is how to converge the manufacturing physical world and the virtual world, so as to realize a series of smart operations in the manufacturing process, including smart interconnection, smart interaction, smart control and management, etc. In this context, as a basic unit of manufacturing, shop-floor is required to reach the interaction and convergence between physical and virtual spaces, which is not only the imperative demand of smart manufacturing, but also the evolving trend of itself. Accordingly, a novel concept of digital twin shopfloor (DTS) based on digital twin is explored and its four key components are discussed, including physical shop-floor, virtual shop-floor, shop-floor service system, and shop-floor digital twin data. What is more, the operation mechanisms and implementing methods for DTS are studied and key technologies as well as challenges ahead are investigated, respectively.","['Production', 'Manufacturing', 'Convergence', 'Data models', 'Aerospace electronics', 'Optimization', 'Cloud computing']","['Smart manufacturing', 'digital twin shop-floor (DTS)', 'digital twin', 'virtual shop-floor (VS)', 'shop-floor service system (SSS)', 'shop-floor digital twin data (SDTD)', 'convergence', 'cyber-physical system (CPS)']"
"Fully convolutional neural networks (FCNs) have been shown to achieve the state-of-the-art performance on the task of classifying time series sequences. We propose the augmentation of fully convolutional networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the data set. The proposed long short term memory fully convolutional network (LSTM-FCN) achieves the state-of-the-art performance compared with others. We also explore the usage of attention mechanism to improve time series classification with the attention long short term memory fully convolutional network (ALSTM-FCN). The attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose refinement as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared with other techniques.","['Time series analysis', 'Recurrent neural networks', 'Feature extraction', 'Convolution', 'Computer architecture', 'Machine learning', 'Machine learning algorithms']","['Convolutional neural network', 'long short term memory recurrent neural network', 'time series classification']"
"The unprecedented outbreak of the 2019 novel coronavirus, termed as COVID-19 by the World Health Organization (WHO), has placed numerous governments around the world in a precarious position. The impact of the COVID-19 outbreak, earlier witnessed by the citizens of China alone, has now become a matter of grave concern for virtually every country in the world. The scarcity of resources to endure the COVID-19 outbreak combined with the fear of overburdened healthcare systems has forced a majority of these countries into a state of partial or complete lockdown. The number of laboratory-confirmed coronavirus cases has been increasing at an alarming rate throughout the world, with reportedly more than 3 million confirmed cases as of 30 April 2020. Adding to these woes, numerous false reports, misinformation, and unsolicited fears in regards to coronavirus, are being circulated regularly since the outbreak of the COVID-19. In response to such acts, we draw on various reliable sources to present a detailed review of all the major aspects associated with the COVID-19 pandemic. In addition to the direct health implications associated with the outbreak of COVID-19, this study highlights its impact on the global economy. In drawing things to a close, we explore the use of technologies such as the Internet of Things (IoT), Unmanned Aerial Vehicles (UAVs), blockchain, Artificial Intelligence (AI), and 5G, among others, to help mitigate the impact of COVID-19 outbreak.","['COVID-19', 'Viruses (medical)', 'Pandemics', 'Artificial intelligence', 'Blockchain', '5G mobile communication']","['Coronavirus', 'COVID-19', 'pandemic', 'transmission stages', 'global economic impact', 'UAVs for disaster management', 'Blockchain', 'IoMT applications', 'IoT', 'AI', '5G']"
"Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).","['Diseases', 'Heart', 'Data mining', 'Support vector machines', 'Feature extraction', 'Machine learning', 'Predictive models']","['Machine learning', 'heart disease prediction', 'feature selection', 'prediction model', 'classification algorithms', 'cardiovascular disease (CVD)']"
"Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in people’s life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning algorithms for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline thoroughly and deeply, in this survey, we analyze the methods of existing typical detection models and describe the benchmark datasets at first. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.","['Deep learning', 'Training data', 'Image classification', 'Transportation', 'Object detection', 'Detectors', 'Computer architecture', 'Computer vision', 'Pipelines']","['Classification', 'deep learning', 'localization', 'object detection', 'typical pipelines']"
"With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.","['Diseases', 'Hospitals', 'Prediction algorithms', 'Machine learning algorithms', 'Big Data', 'Data models']","['Big data analytics', 'machine learning', 'healthcare']"
"As the explosive growth of smart devices and the advent of many new applications, traffic volume has been growing exponentially. The traditional centralized network architecture cannot accommodate such user demands due to heavy burden on the backhaul links and long latency. Therefore, new architectures, which bring network functions and contents to the network edge, are proposed, i.e., mobile edge computing and caching. Mobile edge networks provide cloud computing and caching capabilities at the edge of cellular networks. In this survey, we make an exhaustive review on the state-of-the-art research efforts on mobile edge networks. We first give an overview of mobile edge networks, including definition, architecture, and advantages. Next, a comprehensive survey of issues on computing, caching, and communication techniques at the network edge is presented. The applications and use cases of mobile edge networks are discussed. Subsequently, the key enablers of mobile edge networks, such as cloud technology, SDN/NFV, and smart devices are discussed. Finally, open research challenges and future directions are presented as well.","['Mobile communication', 'Cloud computing', 'Mobile computing', 'Edge computing', 'Computer architecture', 'Network architecture', 'Mobile handsets']","['Mobile edge computing', 'mobile edge caching', 'D2D', 'SDN', 'NFV', 'content delivery', 'computational offloading']"
"Internet of Things (IoT) technology has attracted much attention in recent years for its potential to alleviate the strain on healthcare systems caused by an aging population and a rise in chronic illness. Standardization is a key issue limiting progress in this area, and thus this paper proposes a standard model for application in future IoT healthcare systems. This survey paper then presents the state-of-the-art research relating to each area of the model, evaluating their strengths, weaknesses, and overall suitability for a wearable IoT healthcare system. Challenges that healthcare IoT faces including security, privacy, wearability, and low-power operation are presented, and recommendations are made for future research directions.","['Medical services', 'Monitoring', 'Internet of Things', 'Biomedical monitoring', 'Cloud computing', 'Strain', 'Standards']","['Biomedical engineering', 'body sensor networks', 'intelligent systems', 'Internet of Things (IoT)', 'communications standards', 'security', 'wearable sensors']"
"Due to the current structure of digital factory, it is necessary to build the smart factory to upgrade the manufacturing industry. Smart factory adopts the combination of physical technology and cyber technology and deeply integrates previously independent discrete systems making the involved technologies more complex and precise than they are now. In this paper, a hierarchical architecture of the smart factory was proposed first, and then the key technologies were analyzed from the aspects of the physical resource layer, the network layer, and the data application layer. In addition, we discussed the major issues and potential solutions to key emerging technologies, such as Internet of Things (IoT), big data, and cloud computing, which are embedded in the manufacturing process. Finally, a candy packing line was used to verify the key technologies of smart factory, which showed that the overall equipment effectiveness of the equipment is significantly improved.","['Manufacturing', 'Production facilities', 'Computer architecture', 'Cloud computing', 'Robot kinematics']","['Smart factory', 'big data', 'cloud computing', 'cyber-physical systems', 'industrial Internet of Things']"
"The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks. Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance. Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin. In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented. After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications. Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application. Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications.","['Peer-to-peer computing', 'Bitcoin', 'Internet of Things', 'Cloud computing', 'Computer architecture']","['IoT', 'blockchain', 'traceability', 'consensus', 'distributed systems', 'BIoT', 'fog computing', 'edge computing']"
"Digital twin can be defined as a virtual representation of a physical asset enabled through data and simulators for real-time prediction, optimization, monitoring, controlling, and improved decision making. Recent advances in computational pipelines, multiphysics solvers, artificial intelligence, big data cybernetics, data processing and management tools bring the promise of digital twins and their impact on society closer to reality. Digital twinning is now an important and emerging trend in many applications. Also referred to as a computational megamodel, device shadow, mirrored system, avatar or a synchronized virtual prototype, there can be no doubt that a digital twin plays a transformative role not only in how we design and operate cyber-physical intelligent systems, but also in how we advance the modularity of multi-disciplinary systems to tackle fundamental barriers not addressed by the current, evolutionary modeling practices. In this work, we review the recent status of methodologies and techniques related to the construction of digital twins mostly from a modeling perspective. Our aim is to provide a detailed coverage of the current challenges and enabling technologies along with recommendations and reflections for various stakeholders.","['Digital twin', 'Real-time systems', 'Monitoring', 'Solid modeling', 'Biological system modeling', 'Big Data', 'Buildings']","['Digital twin', 'artificial intelligence', 'machine learning', 'big data cybernetics', 'hybrid analysis and modeling']"
"Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.","['Automation', 'Task analysis', 'Systems architecture', 'Accidents', 'Planning', 'Vehicle dynamics', 'Robot sensing systems']","['Autonomous vehicles', 'control', 'robotics', 'automation', 'intelligent vehicles', 'intelligent transportation systems']"
"This paper promotes the concept of smart and connected communities SCC, which is evolving from the concept of smart cities. SCC are envisioned to address synergistically the needs of remembering the past (preservation and revitalization), the needs of living in the present (livability), and the needs of planning for the future (attainability). Therefore, the vision of SCC is to improve livability, preservation, revitalization, and attainability of a community. The goal of building SCC for a community is to live in the present, plan for the future, and remember the past. We argue that Internet of Things (IoT) has the potential to provide a ubiquitous network of connected devices and smart sensors for SCC, and big data analytics has the potential to enable the move from IoT to real-time control desired for SCC. We highlight mobile crowdsensing and cyber-physical cloud computing as two most important IoT technologies in promoting SCC. As a case study, we present TreSight, which integrates IoT and big data analytics for smart tourism and sustainable cultural heritage in the city of Trento, Italy.","['Internet of things', 'Smart cities', 'Big data', 'Sensors', 'Cultural differences', 'Economics', 'Urban areas', 'Data analytics', 'Sustainable development']","['Internet of Things', 'Big Data Analytics', 'Smart and Connected Communities', 'Smart Cities', 'Smart Tourism', 'Sustainable Cultural Heritage']"
"With the development of the Internet, cyber-attacks are changing rapidly and the cyber security situation is not optimistic. This survey report describes key literature surveys on machine learning (ML) and deep learning (DL) methods for network analysis of intrusion detection and provides a brief tutorial description of each ML/DL method. Papers representing each method were indexed, read, and summarized based on their temporal or thermal correlations. Because data are so important in ML/DL methods, we describe some of the commonly used network datasets used in ML/DL, discuss the challenges of using ML/DL for cybersecurity and provide suggestions for research directions.","['Machine learning', 'Intrusion detection', 'Feature extraction', 'Machine learning algorithms', 'Computer security']","['Cybersecurity', 'intrusion detection', 'deep learning', 'machine learning']"
"Mobile edge computing (MEC) is a promising paradigm to provide cloud-computing capabilities in close proximity to mobile devices in fifth-generation (5G) networks. In this paper, we study energy-efficient computation offloading (EECO) mechanisms for MEC in 5G heterogeneous networks. We formulate an optimization problem to minimize the energy consumption of the offloading system, where the energy cost of both task computing and file transmission are taken into consideration. Incorporating the multi-access characteristics of the 5G heterogeneous network, we then design an EECO scheme, which jointly optimizes offloading and radio resource allocation to obtain the minimal energy consumption under the latency constraints. Numerical results demonstrate energy efficiency improvement of our proposed EECO scheme.","['Cloud computing', 'Mobile handsets', '5G mobile communication', 'Energy consumption', 'Energy efficiency', 'Heterogeneous networks']","['Energy-efficiency', 'offloading', 'mobile edge computing', '5G']"
"Data mining and analytics have played an important role in knowledge discovery and decision making/supports in the process industry over the past several decades. As a computational engine to data mining and analytics, machine learning serves as basic tools for information extraction, data pattern recognition and predictions. From the perspective of machine learning, this paper provides a review on existing data mining and analytics applications in the process industry over the past several decades. The state-of-the-art of data mining and analytics are reviewed through eight unsupervised learning and ten supervised learning algorithms, as well as the application status of semi-supervised learning algorithms. Several perspectives are highlighted and discussed for future researches on data mining and analytics in the process industry.","['Data mining', 'Industries', 'Data models', 'Machine learning algorithms', 'Analytical models', 'Manufacturing', 'Predictive models']","['Data mining', 'data analytics', 'machine learning', 'process industry']"
"Python has become the programming language of choice for research and industry projects related to data science, machine learning, and deep learning. Since optimization is an inherent part of these research fields, more optimization related frameworks have arisen in the past few years. Only a few of them support optimization of multiple conflicting objectives at a time, but do not provide comprehensive tools for a complete multi-objective optimization task. To address this issue, we have developed pymoo, a multi-objective optimization framework in Python. We provide a guide to getting started with our framework by demonstrating the implementation of an exemplary constrained multi-objective optimization scenario. Moreover, we give a high-level overview of the architecture of pymoo to show its capabilities followed by an explanation of each module and its corresponding sub-modules. The implementations in our framework are customizable and algorithms can be modified/extended by supplying custom operators. Moreover, a variety of single, multi- and many-objective test problems are provided and gradients can be retrieved by automatic differentiation out of the box. Also, pymoo addresses practical needs, such as the parallelization of function evaluations, methods to visualize low and high-dimensional spaces, and tools for multi-criteria decision making. For more information about pymoo, readers are encouraged to visit: https://pymoo.org.","['Optimization', 'Python', 'Tools', 'Task analysis', 'Data visualization', 'Evolutionary computation']","['Customization', 'genetic algorithm', 'multi-objective optimization', 'python']"
"The Internet of Things (IoT)-centric concepts like augmented reality, high-resolution video streaming, self-driven cars, smart environment, e-health care, etc. have a ubiquitous presence now. These applications require higher data-rates, large bandwidth, increased capacity, low latency and high throughput. In light of these emerging concepts, IoT has revolutionized the world by providing seamless connectivity between heterogeneous networks (HetNets). The eventual aim of IoT is to introduce the plug and play technology providing the end-user, ease of operation, remotely access control and configurability. This paper presents the IoT technology from a bird's eye view covering its statistical/architectural trends, use cases, challenges and future prospects. The paper also presents a detailed and extensive overview of the emerging 5G-IoT scenario. Fifth Generation (5G) cellular networks provide key enabling technologies for ubiquitous deployment of the IoT technology. These include carrier aggregation, multiple-input multiple-output (MIMO), massive-MIMO (M-MIMO), coordinated multipoint processing (CoMP), device-to-device (D2D) communications, centralized radio access network (CRAN), software-defined wireless sensor networking (SD-WSN), network function virtualization (NFV) and cognitive radios (CRs). This paper presents an exhaustive review for these key enabling technologies and also discusses the new emerging use cases of 5G-IoT driven by the advances in artificial intelligence, machine and deep learning, ongoing 5G initiatives, quality of service (QoS) requirements in 5G and its standardization issues. Finally, the paper discusses challenges in the implementation of 5G-IoT due to high data-rates requiring both cloud-based platforms and IoT devices based edge computing.","['5G mobile communication', 'Market research', 'Protocols', 'Internet of Things', 'Quality of service', 'Security', 'Next generation networking']","['Internet of Things (IoT)', '5G', 'carrier aggregation', 'CoMP', 'CRAN', 'CRs', 'HetNets', 'MIMO', 'M-MIMO', 'NFV', 'SD-WSN', 'QoS']"
"Hybrid analog/digital multiple-input multiple-output architectures were recently proposed as an alternative for fully digital-precoding in millimeter wave wireless communication systems. This is motivated by the possible reduction in the number of RF chains and analog-to-digital converters. In these architectures, the analog processing network is usually based on variable phase shifters. In this paper, we propose hybrid architectures based on switching networks to reduce the complexity and the power consumption of the structures based on phase shifters. We define a power consumption model and use it to evaluate the energy efficiency of both structures. To estimate the complete MIMO channel, we propose an open-loop compressive channel estimation technique that is independent of the hardware used in the analog processing stage. We analyze the performance of the new estimation algorithm for hybrid architectures based on phase shifters and switches. Using the estimate, we develop two algorithms for the design of the hybrid combiner based on switches and analyze the achieved spectral efficiency. Finally, we study the tradeoffs between power consumption, hardware complexity, and spectral efficiency for hybrid architectures based on phase shifting networks and switching networks. Numerical results show that architectures based on switches obtain equal or better channel estimation performance to that obtained using phase shifters, while reducing hardware complexity and power consumption. For equal power consumption, all the hybrid architectures provide similar spectral efficiencies.","['Channel estimation', 'Radio frequency', 'Power demand', 'Phase shifters', 'MIMO', 'Switches', 'Antenna arrays', 'Millimeter wave communication', 'Hybrid systems']","['Millimeter wave', 'hybrid architecture', 'switches', 'channel estimation', 'precoding']"
"What is index modulation (IM)? This is an interesting question that we have started to hear more and more frequently over the past few years. The aim of this paper is to answer this question in a comprehensive manner by covering not only the basic principles and emerging variants of IM, but also reviewing the most recent as well as promising advances in this field toward the application scenarios foreseen in next-generation wireless networks. More specifically, we investigate three forms of IM: spatial modulation, channel modulation and orthogonal frequency division multiplexing (OFDM) with IM, which consider the transmit antennas of a multiple-input multiple-output system, the radio frequency mirrors (parasitic elements) mounted at a transmit antenna and the subcarriers of an OFDM system for IM techniques, respectively. We present the up-to-date advances in these three promising frontiers and discuss possible future research directions for IM-based schemes toward low-complexity, spectrum- and energy-efficient next-generation wireless networks.","['OFDM', 'Indexes', 'Phase modulation', 'MIMO', 'Optical transmitters', 'Optical modulation']","['5G wireless networks', 'channel modulation', 'cognitive radio networks', 'cooperative networks', 'full-duplex networks', 'index modulation', 'MIMO systems', 'multi-carrier systems', 'multi-user systems', 'OFDM', 'OFDM with index modulation', 'practical implementations', 'reconfigurable antennas', 'spatial modulation', 'vehicular communications', 'visible light communications']"
"Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics.","['Hidden Markov models', 'Speech recognition', 'Neural networks', 'Deep learning', 'Feature extraction', 'Computer architecture', 'Acoustics']","['Speech recognition', 'deep neural network', 'systematic review']"
"The k-means algorithm is generally the most known and used clustering method. There are various extensions of k-means to be proposed in the literature. Although it is an unsupervised learning to clustering in pattern recognition and machine learning, the k-means algorithm and its extensions are always influenced by initializations with a necessary number of clusters a priori. That is, the k-means algorithm is not exactly an unsupervised clustering method. In this paper, we construct an unsupervised learning schema for the k-means algorithm so that it is free of initializations without parameter selection and can also simultaneously find an optimal number of clusters. That is, we propose a novel unsupervised k-means (U-k-means) clustering algorithm with automatically finding an optimal number of clusters without giving any initialization and parameter selection. The computational complexity of the proposed U-k-means clustering algorithm is also analyzed. Comparisons between the proposed U-k-means and other existing methods are made. Experimental results and comparisons actually demonstrate these good aspects of the proposed U-k-means clustering algorithm.","['Clustering algorithms', 'Indexes', 'Linear programming', 'Entropy', 'Clustering methods', 'Unsupervised learning', 'Machine learning algorithms']","['Clustering', 'K-means', 'number of clusters', 'initializations', 'unsupervised learning schema', 'Unsupervised k-means (U-k-means)']"
"The past decade has witnessed the rapid evolution in blockchain technologies, which has attracted tremendous interests from both the research communities and industries. The blockchain network was originated from the Internet financial sector as a decentralized, immutable ledger system for transactional data ordering. Nowadays, it is envisioned as a powerful backbone/framework for decentralized data processing and data-driven self-organization in flat, open-access networks. In particular, the plausible characteristics of decentralization, immutability, and self-organization are primarily owing to the unique decentralized consensus mechanisms introduced by blockchain networks. This survey is motivated by the lack of a comprehensive literature review on the development of decentralized consensus mechanisms in blockchain networks. In this paper, we provide a systematic vision of the organization of blockchain networks. By emphasizing the unique characteristics of decentralized consensus in blockchain networks, our in-depth review of the state-of-the-art consensus protocols is focused on both the perspective of distributed consensus system design and the perspective of incentive mechanism design. From a game-theoretic point of view, we also provide a thorough review of the strategy adopted for self-organization by the individual nodes in the blockchain backbone networks. Consequently, we provide a comprehensive survey of the emerging applications of blockchain networks in a broad area of telecommunication. We highlight our special interest in how the consensus mechanisms impact these applications. Finally, we discuss several open issues in the protocol design for blockchain consensus and the related potential research directions.","['Blockchain', 'Protocols', 'Organizations', 'Bitcoin', 'Scalability', 'Standards organizations']","['Blockchain', 'permissionless consensus', 'Byzantine fault tolerance', 'block mining', 'incentive mechanisms', 'game theory', 'P2P networks']"
"Voluminous amounts of data have been produced, since the past decade as the miniaturization of Internet of things (IoT) devices increases. However, such data are not useful without analytic power. Numerous big data, IoT, and analytics solutions have enabled people to obtain valuable insight into large data generated by IoT devices. However, these solutions are still in their infancy, and the domain lacks a comprehensive survey. This paper investigates the state-of-the-art research efforts directed toward big IoT data analytics. The relationship between big data analytics and IoT is explained. Moreover, this paper adds value by proposing a new architecture for big IoT data analytics. Furthermore, big IoT data analytic types, methods, and technologies for big data mining are discussed. Numerous notable use cases are also presented. Several opportunities brought by data analytics in IoT paradigm are then discussed. Finally, open research challenges, such as privacy, big data mining, visualization, and integration, are presented as future research directions.","['Big Data', 'Data analysis', 'Tools', 'Computer architecture', 'Business', 'Sensors', 'Data mining']","['Big data', 'Internet of Things', 'data analytics', 'distributed computing', 'smart city']"
"The grand objective of 5G wireless technology is to support three generic services with vastly heterogeneous requirements: enhanced mobile broadband (eMBB), massive machine-type communications (mMTCs), and ultra-reliable low-latency communications (URLLCs). Service heterogeneity can be accommodated by network slicing, through which each service is allocated resources to provide performance guarantees and isolation from the other services. Slicing of the radio access network (RAN) is typically done by means of orthogonal resource allocation among the services. This paper studies the potential advantages of allowing for non-orthogonal sharing of RAN resources in uplink communications from a set of eMBB, mMTC, and URLLC devices to a common base station. The approach is referred to as heterogeneous nonorthogonal multiple access (H-NOMA), in contrast to the conventional NOMA techniques that involve users with homogeneous requirements and hence can be investigated through a standard multiple access channel. The study devises a communication-theoretic model that accounts for the heterogeneous requirements and characteristics of the three services. The concept of reliability diversity is introduced as a design principle that leverages the different reliability requirements across the services in order to ensure performance guarantees with non-orthogonal RAN slicing. This paper reveals that H-NOMA can lead, in some regimes, to significant gains in terms of performance tradeoffs among the three generic services as compared to orthogonal slicing.","['NOMA', 'Reliability', 'Wireless communication', 'Resource management', 'Radio spectrum management', 'Time-frequency analysis', '5G mobile communication']","['5G mobile communication', 'machine-to-machine communications', 'multiaccess communication', 'NOMA', 'wireless communication']"
"6G and beyond will fulfill the requirements of a fully connected world and provide ubiquitous wireless connectivity for all. Transformative solutions are expected to drive the surge for accommodating a rapidly growing number of intelligent devices and services. Major technological breakthroughs to achieve connectivity goals within 6G include: (i) a network operating at the THz band with much wider spectrum resources, (ii) intelligent communication environments that enable a wireless propagation environment with active signal transmission and reception, (iii) pervasive artificial intelligence, (iv) large-scale network automation, (v) an all-spectrum reconfigurable front-end for dynamic spectrum access, (vi) ambient backscatter communications for energy savings, (vii) the Internet of Space Things enabled by CubeSats and UAVs, and (viii) cell-free massive MIMO communication networks. In this roadmap paper, use cases for these enabling techniques as well as recent advancements on related topics are highlighted, and open problems with possible solutions are discussed, followed by a development timeline outlining the worldwide efforts in the realization of 6G. Going beyond 6G, promising early-stage technologies such as the Internet of NanoThings, the Internet of BioNanoThings, and quantum communications, which are expected to have a far-reaching impact on wireless communications, have also been discussed at length in this paper.","['6G mobile communication', 'Wireless communication', '5G mobile communication', 'Automation', 'Internet', 'Measurement', 'Communication system security']","['6G', 'wireless communications', 'terahertz band', 'intelligent communication environments', 'pervasive artificial intelligence', 'network automation', 'all-spectrum reconfigurable transceivers', 'ambient backscatter communications', 'cell-free massive MIMO', 'Internet of NanoThings', 'Internet of BioNanoThings', 'quantum communications']"
"Recently, artificial intelligence (AI) and blockchain have become two of the most trending and disruptive technologies. Blockchain technology has the ability to automate payment in cryptocurrency and to provide access to a shared ledger of data, transactions, and logs in a decentralized, secure, and trusted manner. Also with smart contracts, blockchain has the ability to govern interactions among participants with no intermediary or a trusted third party. AI, on the other hand, offers intelligence and decision-making capabilities for machines similar to humans. In this paper, we present a detailed survey on blockchain applications for AI. We review the literature, tabulate, and summarize the emerging blockchain applications, platforms, and protocols specifically targeting AI area. We also identify and discuss open research challenges of utilizing blockchain technologies for AI.","['Blockchain', 'Smart contracts', 'Machine learning', 'Decision making', 'Machine learning algorithms', 'Data mining']","['Artificial intelligence', 'machine learning', 'blockchain', 'cybersecurity', 'smart contracts', 'consensus protocols']"
"A variety of rechargeable batteries are now available in world markets for powering electric vehicles (EVs). The lithium-ion (Li-ion) battery is considered the best among all battery types and cells because of its superior characteristics and performance. The positive environmental impacts and recycling potential of lithium batteries have influenced the development of new research for improving Li-ion battery technologies. However, the cost reduction, safe operation, and mitigation of negative ecological impacts are now a common concern for advancement. This paper provides a comprehensive study on the state of the art of Li-ion batteries including the fundamentals, structures, and overall performance evaluations of different types of lithium batteries. A study on a battery management system for Li-ion battery storage in EV applications is demonstrated, which includes a cell condition monitoring, charge, and discharge control, states estimation, protection and equalization, temperature control and heat management, battery fault diagnosis, and assessment aimed at enhancing the overall performance of the system. It is observed that the Li-ion batteries are becoming very popular in vehicle applications due to price reductions and lightweight with high power density. However, the management of the charging and discharging processes, CO2 and greenhouse gases emissions, health effects, and recycling and refurbishing processes have still not been resolved satisfactorily. Consequently, this review focuses on the many factors, challenges, and problems and provides recommendations for sustainable battery manufacturing for future EVs. This review will hopefully lead to increasing efforts toward the development of an advanced Li-ion battery in terms of economics, longevity, specific power, energy density, safety, and performance in vehicle applications.","['Lithium-ion batteries', 'Lithium', 'Cathodes', 'Ions', 'Anodes', 'Electrolytes']","['Lithium-ion battery', 'state-of-the-art of lithium-ion battery', 'energy management system', 'electric vehicle']"
"Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offline selves are not different. With the technological development of deep learning-based high-precision recognition models and natural generation models, Metaverse is being strengthened with various factors, from mobile-based always-on access to connectivity with reality using virtual currency. The integration of enhanced social activities and neural-net methods requires a new definition of Metaverse suitable for the present, different from the previous Metaverse. This paper divides the concepts and essential techniques necessary for realizing the Metaverse into three components (i.e., hardware, software, and contents) and three approaches (i.e., user interaction, implementation, and application) rather than marketing or hardware approach to conduct a comprehensive analysis. Furthermore, we describe essential methods based on three components and techniques to Metaverse’s representative Ready Player One, Roblox, and Facebook research in the domain of films, games, and studies. Finally, we summarize the limitations and directions for implementing the immersive Metaverse as social influences, constraints, and open challenges.","['Augmented reality', 'Avatars', 'Metaverse', 'Artificial intelligence', 'Solid modeling', 'Games', 'Virtual reality']","['Artificial intelligence', 'metaverse', 'cyber world', 'avatar', 'extended reality']"
"When, in 1956, Artificial Intelligence (AI) was officially declared a research field, no one would have ever predicted the huge influence and impact its description, prediction, and prescription capabilities were going to have on our daily lives. In parallel to continuous advances in AI, the past decade has seen the spread of broadband and ubiquitous connectivity, (embedded) sensors collecting descriptive high dimensional data, and improvements in big data processing techniques and cloud computing. The joint usage of such technologies has led to the creation of digital twins, artificial intelligent virtual replicas of physical systems. Digital Twin (DT) technology is nowadays being developed and commercialized to optimize several manufacturing and aviation processes, while in the healthcare and medicine fields this technology is still at its early development stage. This paper presents the results of a study focused on the analysis of the state-of-the-art definitions of DT, the investigation of the main characteristics that a DT should possess, and the exploration of the domains in which DT applications are currently being developed. The design implications derived from the study are then presented: they focus on socio-technical design aspects and DT lifecycle. Open issues and challenges that require to be addressed in the future are finally discussed.","['Artificial intelligence', 'Data models', 'Atmospheric modeling', 'Military aircraft', 'Big Data', 'Sensors']","['Artificial intelligence', 'digital twin', 'human-computer interaction', 'Internet of Things', 'machine learning', 'sensor systems']"
"Blockchain is the underlying technology of a number of digital cryptocurrencies. Blockchain is a chain of blocks that store information with digital signatures in a decentralized and distributed network. The features of blockchain, including decentralization, immutability, transparency and auditability, make transactions more secure and tamper proof. Apart from cryptocurrency, blockchain technology can be used in financial and social services, risk management, healthcare facilities, and so on. A number of research studies focus on the opportunity that blockchain provides in various application domains. This paper presents a comparative study of the tradeoffs of blockchain and also explains the taxonomy and architecture of blockchain, provides a comparison among different consensus mechanisms and discusses challenges, including scalability, privacy, interoperability, energy consumption and regulatory issues. In addition, this paper also notes the future scope of blockchain technology.","['Blockchain', 'Bitcoin', 'Peer-to-peer computing', 'Digital signatures', 'Computer architecture', 'Government']","['Blockchain', 'distributed ledger', 'consensus procedures', 'cryptocurrency', 'smart contract', 'selfish mining', 'energy consumption']"
"Battery technology is the bottleneck of the electric vehicles (EVs). It is important, both in theory and practical application, to do research on the modeling and state estimation of batteries, which is essential to optimizing energy management, extending the life cycle, reducing cost, and safeguarding the safe application of batteries in EVs. However, the batteries, with strong time-variables and nonlinear characteristics, are further influenced by such random factors such as driving loads, operational conditions, in the application of EVs. The real-time, accurate estimation of their state is challenging. The classification of the estimation methodologies for estimating state-of-charge (SoC) of battery focusing with the estimation method/algorithm, advantages, drawbacks, and estimation error are systematically and separately discussed. Especially for the battery packs existing of the inevitable inconsistency in cell capacity, resistance and voltage, the advanced characterizing monomer selection, and bias correction-based method has been described and discussed. The review also presents the key feedback factors that are indispensable for accurate estimation of battery SoC, it will be helpful for ensuring the SoC estimation accuracy. It will be very helpful for choosing an appropriate method to develop a reliable and safe battery management system and energy management strategy of the EVs. Finally, the paper also highlights a number of key factors and challenges, and presents the possible recommendations for the development of next generation of smart SoC estimation and battery management systems for electric vehicles and battery energy storage system.","['Batteries', 'Estimation', 'Integrated circuit modeling', 'Mathematical model', 'Battery charge measurement', 'State of charge', 'Electric vehicles']","['Batteries', 'data-driven estimation', 'electric vehicles', 'model based estimation', 'multi-scale', 'state of charge']"
"The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause–effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.","['Big Data', 'Machine learning algorithms', 'Data mining', 'Algorithm design and analysis', 'Data analysis', 'Support vector machines', 'Classification algorithms']","['Big Data', 'Big Data Vs', 'data analysis', 'data analytics', 'deep learning', 'distributed computing', 'machine learning', 'neural networks']"
"To meet the fast-growing energy demand and, at the same time, tackle environmental concerns resulting from conventional energy sources, renewable energy sources are getting integrated in power networks to ensure reliable and affordable energy for the public and industrial sectors. However, the integration of renewable energy in the ageing electrical grids can result in new risks/challenges, such as security of supply, base load energy capacity, seasonal effects, and so on. Recent research and development in microgrids have proved that microgrids, which are fueled by renewable energy sources and managed by smart grids (use of smart sensors and smart energy management system), can offer higher reliability and more efficient energy systems in a cost-effective manner. Further improvement in the reliability and efficiency of electrical grids can be achieved by utilizing dc distribution in microgrid systems. DC microgrid is an attractive technology in the modern electrical grid system because of its natural interface with renewable energy sources, electric loads, and energy storage systems. In the recent past, an increase in research work has been observed in the area of dc microgrid, which brings this technology closer to practical implementation. This paper presents the state-of-the-art dc microgrid technology that covers ac interfaces, architectures, possible grounding schemes, power quality issues, and communication systems. The advantages of dc grids can be harvested in many applications to improve their reliability and efficiency. This paper also discusses benefits and challenges of using dc grid systems in several applications. This paper highlights the urgent need of standardizations for dc microgrid technology and presents recent updates in this area.","['Microgrids', 'Power system reliability', 'Reliability', 'Grounding', 'Renewable energy sources', 'Batteries', 'Power quality']","['DC microgrid', 'architectures', 'power quality', 'grounding', 'communication network', 'smart grid and standardization']"
"The 5G System is being developed and enhanced to provide unparalleled connectivity to connect everyone and everything, everywhere. The first version of the 5G System, based on the Release 15 (“Rel-15”) version of the specifications developed by 3GPP, comprising the 5G Core (5GC) and 5G New Radio (NR) with 5G User Equipment (UE), is currently being deployed commercially throughout the world both at sub-6 GHz and at mmWave frequencies. Concurrently, the second phase of 5G is being standardized by 3GPP in the Release 16 (“Rel-16”) version of the specifications which will be completed by March 2020. While the main focus of Rel-15 was on enhanced mobile broadband services, the focus of Rel-16 is on new features for URLLC (Ultra-Reliable Low Latency Communication) and Industrial IoT, including Time Sensitive Communication (TSC), enhanced Location Services, and support for Non-Public Networks (NPNs). In addition, some crucial new features, such as NR on unlicensed bands (NR-U), Integrated Access & Backhaul (IAB) and NR Vehicle-to-X (V2X), are also being introduced as part of Rel-16, as well as enhancements for massive MIMO, wireless and wireline convergence, the Service Based Architecture (SBA) and Network Slicing. Finally, the number of use cases, types of connectivity and users, and applications running on top of 5G networks, are all expected to increase dramatically, thus motivating additional security features to counter security threats which are expected to increase in number, scale and variety. In this paper, we discuss the Rel-16 features and provide an outlook towards Rel-17 and beyond, covering both new features and enhancements of existing features. 5G Evolution will focus on three main areas: enhancements to features introduced in Rel-15 and Rel-16, features that are needed for operational enhancements, and new features to further expand the applicability of the 5G System to new markets and use cases.","['5G mobile communication', 'Computer architecture', '3GPP', 'Data analysis', 'Wireless communication', 'Convergence', 'Security']","['5G new radio', '5G core', '5G system', 'non-public network', 'industrial IoT', 'time sensitive communication', 'ultra-reliable low-latency communications', 'integrated access and backhaul', 'converged edge and core clouds', 'positioning', 'NR-unlicensed', 'non-terrestrial network']"
"This paper presents an in-depth analysis of the majority of the deep neural networks (DNNs) proposed in the state of the art for image recognition. For each DNN, multiple performance indices are observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and inference time. The behavior of such performance indices and some combinations of them are analyzed and discussed. To measure the indices, we experiment the use of DNNs on two different computer architectures, a workstation equipped with a NVIDIA Titan X Pascal, and an embedded system based on a NVIDIA Jetson TX1 board. This experimentation allows a direct comparison between DNNs running on machines with very different computational capacities. This paper is useful for researchers to have a complete view of what solutions have been explored so far and in which research directions are worth exploring in the future, and for practitioners to select the DNN architecture(s) that better fit the resource constraints of practical deployments and applications. To complete this work, all the DNNs, as well as the software used for the analysis, are available online.","['Computational modeling', 'Graphics processing units', 'Computational complexity', 'Memory management', 'Embedded systems']","['Deep neural networks', 'convolutional neural networks', 'image recognition']"
"Motion planning is a fundamental research area in robotics. Sampling-based methods offer an efficient solution for what is otherwise a rather challenging dilemma of path planning. Consequently, these methods have been extended further away from basic robot planning into further difficult scenarios and diverse applications. A comprehensive survey of the growing body of work in sampling-based planning is given here. Simulations are executed to evaluate some of the proposed planners and highlight some of the implementation details that are often left unspecified. An emphasis is placed on contemporary research directions in this field. We address planners that tackle current issues in robotics. For instance, real-life kinodynamic planning, optimal planning, replanning in dynamic environments, and planning under uncertainty are discussed. The aim of this paper is to survey the state of the art in motion planning and to assess selected planners, examine implementation details and above all shed a light on the current challenges in motion planning and the promising approaches that will potentially overcome those problems.","['Planning', 'Measurement', 'Heuristic algorithms', 'Robot sensing systems', 'Path planning', 'Vegetation']","['Planning', 'sampling', 'randomization', 'RRT', 'PRM', 'path', 'motion', 'autonomous robots']"
"Despite the perception people may have regarding the agricultural process, the reality is that today's agriculture industry is data-centered, precise, and smarter than ever. The rapid emergence of the Internet-of-Things (IoT) based technologies redesigned almost every industry including “smart agriculture” which moved the industry from statistical to quantitative approaches. Such revolutionary changes are shaking the existing agriculture methods and creating new opportunities along a range of challenges. This article highlights the potential of wireless sensors and IoT in agriculture, as well as the challenges expected to be faced when integrating this technology with the traditional farming practices. IoT devices and communication techniques associated with wireless sensors encountered in agriculture applications are analyzed in detail. What sensors are available for specific agriculture application, like soil preparation, crop status, irrigation, insect and pest detection are listed. How this technology helping the growers throughout the crop stages, from sowing until harvesting, packing and transportation is explained. Furthermore, the use of unmanned aerial vehicles for crop surveillance and other favorable applications such as optimizing crop yield is considered in this article. State-of-the-art IoT-based architectures and platforms used in agriculture are also highlighted wherever suitable. Finally, based on this thorough review, we identify current and future trends of IoT in agriculture and highlight potential research challenges.","['Agriculture', 'Soil', 'Production', 'Intelligent sensors', 'Sociology']","['Food quality and quantity', 'Internet-of-Things (IoTs)', 'smart agriculture', 'advanced agriculture practices', 'urban farming', 'agriculture robots', 'automation', 'future food expectation']"
"Non-orthogonal multiple access (NOMA) has recently been considered as a key enabling technique for 5G cellular systems. In NOMA, by exploiting the channel gain differences, multiple users are multiplexed into transmission power domain and then non-orthogonally scheduled for transmission on the same spectrum resources. Successive interference cancellation (SIC) is then applied at the receivers to decode the message signals. In this paper, first, we briefly describe the differences in the working principles of uplink and downlink NOMA transmissions in a cellular wireless system. Then, for both uplink and downlink NOMAs, we formulate a sum-throughput maximization problem in a cell such that the user clustering (i.e., grouping users into a single cluster or multiple clusters) and power allocations in NOMA clusters can be optimized under transmission power constraints, minimum rate requirements of the users, and SIC constraints. Due to the combinatorial nature of the formulated mixed integer non-linear programming problem, we solve the problem in two steps, i.e., by first grouping users into clusters and then optimizing their respective power allocations. In particular, we propose a low-complexity sub-optimal user grouping scheme. The proposed scheme exploits the channel gain differences among users in an NOMA cluster and groups them into a single cluster or multiple clusters in order to enhance the sum-throughput of the system. For a given set of NOMA clusters, we then derive the optimal power allocation policy that maximizes the sum-throughput per NOMA cluster and in turn maximizes the overall system throughput. Using Karush-Kuhn-Tucker optimality conditions, closed-form solutions for optimal power allocations are derived for any cluster size, considering both uplink and downlink NOMA systems. Numerical results compare the performances of NOMA and OMA and illustrate the significance of NOMA in various network scenarios.","['NOMA', 'Resource management', 'Uplink', 'Downlink', '5G mobile communication', 'Multiplexing', 'Silicon carbide']","['5G cellular', 'non-orthogonal multiple access (NOMA)', 'orthogonal multiple access (OMA)', 'power allocation', 'throughput maximization', 'user grouping']"
"The recent expansion of the Internet of Things (IoT) and the consequent explosion in the volume of data produced by smart devices have led to the outsourcing of data to designated data centers. However, to manage these huge data stores, centralized data centers, such as cloud storage cannot afford auspicious way. There are many challenges that must be addressed in the traditional network architecture due to the rapid growth in the diversity and number of devices connected to the internet, which is not designed to provide high availability, real-time data delivery, scalability, security, resilience, and low latency. To address these issues, this paper proposes a novel blockchain-based distributed cloud architecture with a software defined networking (SDN) enable controller fog nodes at the edge of the network to meet the required design principles. The proposed model is a distributed cloud architecture based on blockchain technology, which provides low-cost, secure, and on-demand access to the most competitive computing infrastructures in an IoT network. By creating a distributed cloud infrastructure, the proposed model enables cost-effective high-performance computing. Furthermore, to bring computing resources to the edge of the IoT network and allow low latency access to large amounts of data in a secure manner, we provide a secure distributed fog node architecture that uses SDN and blockchain techniques. Fog nodes are distributed fog computing entities that allow the deployment of fog services, and are formed by multiple computing resources at the edge of the IoT network. We evaluated the performance of our proposed architecture and compared it with the existing models using various performance measures. The results of our evaluation show that performance is improved by reducing the induced delay, reducing the response time, increasing throughput, and the ability to detect real-time attacks in the IoT network with low performance overheads.","['Cloud computing', 'Computer architecture', 'Edge computing', 'Performance evaluation', 'Peer-to-peer computing', 'Distributed databases', 'Security']","['Internet of things', 'software defined networking', 'security', 'blockchain', 'cloud computing', 'fog computing', 'edge computing']"
"Recurrent neural network (RNN) and long short-term memory (LSTM) have achieved great success in processing sequential multimedia data and yielded the state-of-the-art results in speech recognition, digital signal processing, video processing, and text data analysis. In this paper, we propose a novel action recognition method by processing the video data using convolutional neural network (CNN) and deep bidirectional LSTM (DB-LSTM) network. First, deep features are extracted from every sixth frame of the videos, which helps reduce the redundancy and complexity. Next, the sequential information among frame features is learnt using DB-LSTM network, where multiple layers are stacked together in both forward pass and backward pass of DB-LSTM to increase its depth. The proposed method is capable of learning long term sequences and can process lengthy videos by analyzing features for a certain time interval. Experimental results show significant improvements in action recognition using the proposed method on three benchmark data sets including UCF-101, YouTube 11 Actions, and HMDB51 compared with the state-of-the-art action recognition methods.","['Feature extraction', 'Computer architecture', 'Streaming media', 'Visualization', 'Microprocessors', 'Logic gates', 'Shape']","['Action recognition', 'deep learning', 'recurrent neural network', 'deep bidirectional long short-term memory', 'convolution neural network']"
"Internet of Things is smartly changing various existing research areas into new themes, including smart health, smart home, smart industry, and smart transport. Relying on the basis of “smart transport,” Internet of Vehicles (IoV) is evolving as a new theme of research and development from vehicular ad hoc networks (VANETs). This paper presents a comprehensive framework of IoV with emphasis on layered architecture, protocol stack, network model, challenges, and future aspects. Specifically, following the background on the evolution of VANETs and motivation on IoV an overview of IoV is presented as the heterogeneous vehicular networks. The IoV includes five types of vehicular communications, namely, vehicle-to-vehicle, vehicle-to-roadside, vehicle-to-infrastructure of cellular networks, vehicle-to-personal devices, and vehicle-to-sensors. A five layered architecture of IoV is proposed considering functionalities and representations of each layer. A protocol stack for the layered architecture is structured considering management, operational, and security planes. A network model of IoV is proposed based on the three network elements, including cloud, connection, and client. The benefits of the design and development of IoV are highlighted by performing a qualitative comparison between IoV and VANETs. Finally, the challenges ahead for realizing IoV are discussed and future aspects of IoV are envisioned.","['Intelligent vehicles', 'Internet of things', 'Vehicular ad hoc networks', 'Cloud computing', 'Computer architecture', 'Heterogeneous networks', 'Reliability']","['Vehicular adhoc networks', 'Internet of Vehicles', 'cloud computing', 'heterogeneous networks']"
"The use of renewable energy resources, such as solar, wind, and biomass will not diminish their availability. Sunlight being a constant source of energy is used to meet the ever-increasing energy need. This review discusses the world's energy needs, renewable energy technologies for domestic use, and highlights public opinions on renewable energy. A systematic review of the literature was conducted from 2009 to 2018. During this process, more than 300 articles were classified and 42 papers were filtered for critical review. The literature analysis showed that despite serious efforts at all levels to reduce reliance on fossil fuels by promoting renewable energy as its alternative, fossil fuels continue to contribute 73.5% to the worldwide electricity production in 2017. Conversely, renewable sources contributed only 26.5%. Furthermore, this study highlights that the lack of public awareness is a major barrier to the acceptance of renewable energy technologies. The results of this study show that worldwide energy crises can be managed by integrating renewable energy sources in the power generation. Moreover, in order to facilitate the development of renewable energy technologies, this systematic review has highlighted the importance of public opinion and performed a real-time analysis of public tweets. This example of tweet analysis is a relatively novel initiative in a review study that will seek to direct the attention of future researchers and policymakers toward public opinion and recommend the implications to both academia and industries.","['Fossil fuels', 'Systematics', 'Information technology', 'Computer science', 'Wind', 'Biomass']","['Energy policies', 'public opinion', 'renewable energy sources (RES)', 'renewable energy technology (RET)', 'solar energy', 'wind energy']"
"Coronavirus (COVID-19) is a viral disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The spread of COVID-19 seems to have a detrimental effect on the global economy and health. A positive chest X-ray of infected patients is a crucial step in the battle against COVID-19. Early results suggest that abnormalities exist in chest X-rays of patients suggestive of COVID-19. This has led to the introduction of a variety of deep learning systems and studies have shown that the accuracy of COVID-19 patient detection through the use of chest X-rays is strongly optimistic. Deep learning networks like convolutional neural networks (CNNs) need a substantial amount of training data. Because the outbreak is recent, it is difficult to gather a significant number of radiographic images in such a short time. Therefore, in this research, we present a method to generate synthetic chest X-ray (CXR) images by developing an Auxiliary Classifier Generative Adversarial Network (ACGAN) based model called CovidGAN. In addition, we demonstrate that the synthetic images produced from CovidGAN can be utilized to enhance the performance of CNN for COVID-19 detection. Classification using CNN alone yielded 85% accuracy. By adding synthetic images produced by CovidGAN,the accuracy increased to 95%. We hope this method will speed up COVID-19 detection and lead to more robust systems of radiology.","['Generative adversarial networks', 'Training', 'Biomedical imaging', 'X-ray imaging', 'Computer architecture', 'Machine learning', 'COVID-19']","['Deep learning', 'convolutional neural networks', 'generative adversarial networks', 'synthetic data augmentation', 'COVID-19 detection']"
"Driven by the emergence of new compute-intensive applications and the vision of the Internet of Things (IoT), it is foreseen that the emerging 5G network will face an unprecedented increase in traffic volume and computation demands. However, end users mostly have limited storage capacities and finite processing capabilities, thus how to run compute-intensive applications on resource-constrained users has recently become a natural concern. Mobile edge computing (MEC), a key technology in the emerging fifth generation (5G) network, can optimize mobile resources by hosting compute-intensive applications, process large data before sending to the cloud, provide the cloud-computing capabilities within the radio access network (RAN) in close proximity to mobile users, and offer context-aware services with the help of RAN information. Therefore, MEC enables a wide variety of applications, where the real-time response is strictly required, e.g., driverless vehicles, augmented reality, robotics, and immerse media. Indeed, the paradigm shift from 4G to 5G could become a reality with the advent of new technological concepts. The successful realization of MEC in the 5G network is still in its infancy and demands for constant efforts from both academic and industry communities. In this survey, we first provide a holistic overview of MEC technology and its potential use cases and applications. Then, we outline up-to-date researches on the integration of MEC with the new technologies that will be deployed in 5G and beyond. We also summarize testbeds and experimental evaluations, and open source activities, for edge computing. We further summarize lessons learned from state-of-the-art research works as well as discuss challenges and potential future directions for MEC research.","['Cloud computing', '5G mobile communication', 'Edge computing', 'Internet of Things', 'Radio access networks', 'NOMA', 'Wireless communication']","['5G and beyond network', 'heterogeneous networks', 'Internet of Things', 'machine learning', 'edge computing', 'non-orthogonal multiple access', 'testbeds', 'unmanned aerial vehicle', 'wireless power transfer and energy harvesting']"
"U-net is an image segmentation technique developed primarily for image segmentation tasks. These traits provide U-net with a high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in nearly all major image modalities, from CT scans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. Given that U-net's potential is still increasing, this narrative literature review examines the numerous developments and breakthroughs in the U-net architecture and provides observations on recent trends. We also discuss the many innovations that have advanced in deep learning and discuss how these tools facilitate U-net. In addition, we review the different image modalities and application areas that have been enhanced by U-net.","['Image segmentation', 'Convolution', 'Biomedical imaging', 'Three-dimensional displays', 'Logic gates', 'Deep learning', 'Computer architecture']","['Biomedical imaging', 'deep learning', 'neural network architecture', 'segmentation', 'U-net']"
"In new product development, time to market (TTM) is critical for the success and profitability of next generation products. When these products include sophisticated electronics encased in 3D packaging with complex geometries and intricate detail, TTM can be compromised - resulting in lost opportunity. The use of advanced 3D printing technology enhanced with component placement and electrical interconnect deposition can provide electronic prototypes that now can be rapidly fabricated in comparable time frames as traditional 2D bread-boarded prototypes; however, these 3D prototypes include the advantage of being embedded within more appropriate shapes in order to authentically prototype products earlier in the development cycle. The fabrication freedom offered by 3D printing techniques, such as stereolithography and fused deposition modeling have recently been explored in the context of 3D electronics integration - referred to as 3D structural electronics or 3D printed electronics. Enhanced 3D printing may eventually be employed to manufacture end-use parts and thus offer unit-level customization with local manufacturing; however, until the materials and dimensional accuracies improve (an eventuality), 3D printing technologies can be employed to reduce development times by providing advanced geometrically appropriate electronic prototypes. This paper describes the development process used to design a novelty six-sided gaming die. The die includes a microprocessor and accelerometer, which together detect motion and upon halting, identify the top surface through gravity and illuminate light-emitting diodes for a striking effect. By applying 3D printing of structural electronics to expedite prototyping, the development cycle was reduced from weeks to hours.","['Product development', 'Three dimensional displays', 'Printing', 'Product life cycle management', 'Economics', 'Consumer electronics', 'Time to market', 'Prototypes', 'Packaging']","['3D printed electronics', 'additive manufacturing', 'direct-print', 'electronic gaming die', 'hybrid manufacturing', 'rapid prototyping', 'structural electronics', 'three-dimensional electronics']"
"A network traffic classifier (NTC) is an important part of current network monitoring systems, being its task to infer the network service that is currently used by a communication flow (e.g., HTTP and SIP). The detection is based on a number of features associated with the communication flow, for example, source and destination ports and bytes transmitted per packet. NTC is important, because much information about a current network flow can be learned and anticipated just by knowing its network service (required latency, traffic volume, and possible duration). This is of particular interest for the management and monitoring of Internet of Things (IoT) networks, where NTC will help to segregate traffic and behavior of heterogeneous devices and services. In this paper, we present a new technique for NTC based on a combination of deep learning models that can be used for IoT traffic. We show that a recurrent neural network (RNN) combined with a convolutional neural network (CNN) provides best detection results. The natural domain for a CNN, which is image processing, has been extended to NTC in an easy and natural way. We show that the proposed method provides better detection results than alternative algorithms without requiring any feature engineering, which is usual when applying other models. A complete study is presented on several architectures that integrate a CNN and an RNN, including the impact of the features chosen and the length of the network flows used for training.","['Ports (Computers)', 'Telecommunication traffic', 'Feature extraction', 'Recurrent neural networks', 'Machine learning', 'Payloads', 'Biological neural networks']","['Convolutional neural network', 'deep learning', 'network traffic classification', 'recurrent neural network']"
"Cyber-physical system (CPS) is a new trend in the Internet-of-Things related research works, where physical systems act as the sensors to collect real-world information and communicate them to the computation modules (i.e. cyber layer), which further analyze and notify the findings to the corresponding physical systems through a feedback loop. Contemporary researchers recommend integrating cloud technologies in the CPS cyber layer to ensure the scalability of storage, computation, and cross domain communication capabilities. Though there exist a few descriptive models of the cloud-based CPS architecture, it is important to analytically describe the key CPS properties: computation, control, and communication. In this paper, we present a digital twin architecture reference model for the cloud-based CPS, C2PS, where we analytically describe the key properties of the C2PS. The model helps in identifying various degrees of basic and hybrid computation-interaction modes in this paradigm. We have designed C2PS smart interaction controller using a Bayesian belief network, so that the system dynamically considers current contexts. The composition of fuzzy rule base with the Bayes network further enables the system with reconfiguration capability. We also describe analytically, how C2PS subsystem communications can generate even more complex system-of-systems. Later, we present a telematics-based prototype driving assistance application for the vehicular domain of C2PS, VCPS, to demonstrate the efficacy of the architecture reference model.","['Cloud computing', 'Computer architecture', 'Sensors', 'Social network services', 'Analytical models', 'Urban areas', 'Computational modeling']","['Digital twin', 'cyber-physical systems', 'Internet-of-Things', 'social internet of vehicles', 'sensing-as-a-service', 'analytical model']"
"Ultra-wideband millimeter-wave (mmWave) propagation measurements were conducted in the 28- and 73-GHz frequency bands in a typical indoor office environment in downtown Brooklyn, New York, on the campus of New York University. The measurements provide large-scale path loss and temporal statistics that will be useful for ultra-dense indoor wireless networks for future mmWave bands. This paper presents the details of measurements that employed a 400 Megachips-per-second broadband sliding correlator channel sounder, using rotatable highly directional horn antennas for both co-polarized and cross-polarized antenna configurations. The measurement environment was a closed-plan in-building scenario that included a line-of-sight and non-line-of-sight corridor, a hallway, a cubicle farm, and adjacent-room communication links. Well-known and new single-frequency and multi-frequency directional and omnidirectional large-scale path loss models are presented and evaluated based on more than 14 000 directional power delay profiles acquired from unique transmitter and receiver antenna pointing angle combinations. Omnidirectional path loss models, synthesized from the directional measurements, are provided for the case of arbitrary polarization coupling, as well as for the specific cases of co-polarized and cross-polarized antenna orientations. The results show that novel large-scale path loss models provided here are simpler and more physically based compared to previous 3GPP and ITU indoor propagation models that require more model parameters and offer very little additional accuracy and lack a physical basis. Multipath time dispersion statistics for mmWave systems using directional antennas are presented for co-polarization, crosspolarization, and combined-polarization scenarios, and show that the multipath root mean square delay spread can be reduced when using transmitter and receiver antenna pointing angles that result in the strongest received power. Raw omnidirectional path loss data and closed-form optimization formulas for all path loss models are given in the Appendices.","['Millimeter wave communication', 'Wideband', 'Propagation measurements', 'Ultra wideband communication', '5G mobile communication', 'Loss measurement', 'Path planning', 'Polarization', 'Multipath channels']","['Millimeter-wave', 'mmWave', 'path loss', '5G', 'indoor hotspot', 'RMS delay spread', 'small cell', 'channel sounder', 'propagation', '28 GHz', '73 GHz', 'multipath', 'polarization']"
"The Internet of Things (IoT) is a dynamic global information network consisting of Internet-connected objects, such as radio frequency identifications, sensors, and actuators, as well as other instruments and smart appliances that are becoming an integral component of the Internet. Over the last few years, we have seen a plethora of IoT solutions making their way into the industry marketplace. Context-aware communications and computing have played a critical role throughout the last few years of ubiquitous computing and are expected to play a significant role in the IoT paradigm as well. In this paper, we examine a variety of popular and innovative IoT solutions in terms of context-aware technology perspectives. More importantly, we evaluate these IoT solutions using a framework that we built around well-known context-aware computing theories. This survey is intended to serve as a guideline and a conceptual framework for context-aware product development and research in the IoT paradigm. It also provides a systematic exploration of existing IoT products in the marketplace and highlights a number of potentially significant research directions and trends.","['Context awareness', 'Internet of things', 'Market research', 'Information networks', 'Internet', 'Globalization', 'Interconnections', 'Product development', 'Futures research']","['Internet of things', 'industry solutions', 'contextawareness', 'product review', 'IoT marketplace']"
"Over the past three decades, significant developments have been made in hyperspectral imaging due to which it has emerged as an effective tool in numerous civil, environmental, and military applications. Modern sensor technologies are capable of covering large surfaces of earth with exceptional spatial, spectral, and temporal resolutions. Due to these features, hyperspectral imaging has been effectively used in numerous remote sensing applications requiring estimation of physical parameters of many complex surfaces and identification of visually similar materials having fine spectral signatures. In the recent years, ground based hyperspectral imaging has gained immense interest in the research on electronic imaging for food inspection, forensic science, medical surgery and diagnosis, and military applications. This review focuses on the fundamentals of hyperspectral image analysis and its modern applications such as food quality and safety assessment, medical diagnosis and image guided surgery, forensic document examination, defense and homeland security, remote sensing applications such as precision agriculture and water resource management and material identification and mapping of artworks. Moreover, recent research on the use of hyperspectral imaging for examination of forgery detection in questioned documents, aided by deep learning, is also presented. This review can be a useful baseline for future research in hyperspectral image analysis.","['Hyperspectral imaging', 'Spatial resolution', 'Imaging', 'Safety']","['Agriculture', 'document images', 'food quality and safety', 'hyperspectral imaging', 'medical imaging', 'remote sensing']"
"In this paper, an improved ant colony optimization (ICMPACO) algorithm based on the multi-population strategy, co-evolution mechanism, pheromone updating strategy, and pheromone diffusion mechanism is proposed to balance the convergence speed and solution diversity, and improve the optimization performance in solving the large-scale optimization problem. In the proposed ICMPACO algorithm, the optimization problem is divided into several sub-problems and the ants in the population are divided into elite ants and common ants in order to improve the convergence rate, and avoid to fall into the local optimum value. The pheromone updating strategy is used to improve optimization ability. The pheromone diffusion mechanism is used to make the pheromone released by ants at a certain point, which gradually affects a certain range of adjacent regions. The co-evolution mechanism is used to interchange information among different sub-populations in order to implement information sharing. In order to verify the optimization performance of the ICMPACO algorithm, the traveling salesmen problem (TSP) and the actual gate assignment problem are selected here. The experiment results show that the proposed ICMPACO algorithm can effectively obtain the best optimization value in solving TSP and effectively solve the gate assignment problem, obtain better assignment result, and it takes on better optimization ability and stability.","['Optimization', 'Convergence', 'Scheduling', 'Sociology', 'Statistics', 'Heuristic algorithms', 'Ant colony optimization']","['Co-evolution mechanism', 'ACO', 'pheromone updating strategy', 'pheromone diffusion mechanism', 'hybrid strategy', 'assignment problem']"
"5G is the next cellular generation and is expected to quench the growing thirst for taxing data rates and to enable the Internet of Things. Focused research and standardization work have been addressing the corresponding challenges from the radio perspective while employing advanced features, such as network densification, massive multiple-input-multiple-output antennae, coordinated multi-point processing, inter-cell interference mitigation techniques, carrier aggregation, and new spectrum exploration. Nevertheless, a new bottleneck has emerged: the backhaul. The ultra-dense and heavy traffic cells should be connected to the core network through the backhaul, often with extreme requirements in terms of capacity, latency, availability, energy, and cost efficiency. This pioneering survey explains the 5G backhaul paradigm, presents a critical analysis of legacy, cutting-edge solutions, and new trends in backhauling, and proposes a novel consolidated 5G backhaul framework. A new joint radio access and backhaul perspective is proposed for the evaluation of backhaul technologies which reinforces the belief that no single solution can solve the holistic 5G backhaul problem. This paper also reveals hidden advantages and shortcomings of backhaul solutions, which are not evident when backhaul technologies are inspected as an independent part of the 5G network. This survey is key in identifying essential catalysts that are believed to jointly pave the way to solving the beyond-2020 backhauling challenge. Lessons learned, unsolved challenges, and a new consolidated 5G backhaul vision are thus presented.","['5G mobile communication', 'Cellular networks', 'Backhaul communication', 'Microcells', 'Heterogeneous networks', 'Internet of things', 'MIMO', 'Market research', 'Software defined radio', 'Interference (signal)']","['5G', 'backhaul', 'fronthaul', 'small cells', 'heterogeneous network', 'C-RAN', 'SDN', 'SON', 'backhaul as a service']"
"The recent outbreak of COVID-19 has taken the world by surprise, forcing lockdowns and straining public health care systems. COVID-19 is known to be a highly infectious virus, and infected individuals do not initially exhibit symptoms, while some remain asymptomatic. Thus, a non-negligible fraction of the population can, at any given time, be a hidden source of transmissions. In response, many governments have shown great interest in smartphone contact tracing apps that help automate the difficult task of tracing all recent contacts of newly identified infected individuals. However, tracing apps have generated much discussion around their key attributes, including system architecture, data management, privacy, security, proximity estimation, and attack vulnerability. In this article, we provide the first comprehensive review of these much-discussed tracing app attributes. We also present an overview of many proposed tracing app examples, some of which have been deployed countrywide, and discuss the concerns users have reported regarding their usage. We close by outlining potential research directions for next-generation app design, which would facilitate improved tracing and security performance, as well as wide adoption by the population at large.","['Servers', 'Security', 'Privacy', 'Computer architecture', 'Viruses (medical)', 'Data privacy', 'Bluetooth', 'COVID-19']","['Contact tracing', 'privacy', 'security']"
"Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.","['Machine learning', 'Data models', 'Mathematical model', 'Kernel', 'Biological system modeling', 'Approximation algorithms', 'Data mining']","['Explainable machine learning', 'informed machine learning', 'interpretability', 'scientific consistency', 'transparency']"
"Deep learning has exploded in the public consciousness, primarily as predictive and analytical products suffuse our world, in the form of numerous human-centered smart-world systems, including targeted advertisements, natural language assistants and interpreters, and prototype self-driving vehicle systems. Yet to most, the underlying mechanisms that enable such human-centered smart products remain obscure. In contrast, researchers across disciplines have been incorporating deep learning into their research to solve problems that could not have been approached before. In this paper, we seek to provide a thorough investigation of deep learning in its applications and mechanisms. Specifically, as a categorical collection of state of the art in deep learning research, we hope to provide a broad reference for those seeking a primer on deep learning and its various implementations, platforms, algorithms, and uses in a variety of smart-world systems. Furthermore, we hope to outline recent key advancements in the technology, and provide insight into areas, in which deep learning can improve investigation, as well as highlight new areas of research that have yet to see the application of deep learning, but could nonetheless benefit immensely. We hope this survey provides a valuable reference for new deep learning practitioners, as well as those seeking to innovate in the application of deep learning.","['Machine learning', 'Neurons', 'Neural networks', 'Task analysis', 'Learning systems', 'Computational modeling', 'Training']","['Human-centered smart systems', 'deep learning', 'platform', 'neural networks', 'emergent applications', 'Internet of Things', 'cyber-physical systems', 'survey', 'networking', 'security']"
"In the past two decades, reversible data hiding (RDH), also referred to as lossless or invertible data hiding, has gradually become a very active research area in the field of data hiding. This has been verified by more and more papers on increasingly wide-spread subjects in the field of RDH research that have been published these days. In this paper, the various RDH algorithms and researches have been classified into the following six categories: 1) RDH into image spatial domain; 2) RDH into image compressed domain (e.g., JPEG); 3) RDH suitable for image semi-fragile authentication; 4) RDH with image contrast enhancement; 5) RDH into encrypted images, which is expected to have wide application in the cloud computation; and 6) RDH into video and into audio. For each of these six categories, the history of technical developments, the current state of the arts, and the possible future researches are presented and discussed. It is expected that the RDH technology and its applications in the real word will continue to move ahead.","['Data hiding', 'Video communication', 'Reversible data hiding', 'Classification algorithms', 'Image coding', 'Transform coding', 'Cryptography', 'Authentication']","['Reversible data hiding', 'lossless data hiding', 'invertible data hiding', 'histogram shifting', 'difference expansion', 'prediction-error', 'sorting', 'robust reversible data hiding', 'video reversible data hiding', 'audio reversible data hiding']"
"The focus of wireless research is increasingly shifting toward 6G as 5G deployments get underway. At this juncture, it is essential to establish a vision of future communications to provide guidance for that research. In this paper, we attempt to paint a broad picture of communication needs and technologies in the timeframe of 6G. The future of connectivity is in the creation of digital twin worlds that are a true representation of the physical and biological worlds at every spatial and time instant, unifying our experience across these physical, biological and digital worlds. New themes are likely to emerge that will shape 6G system requirements and technologies, such as: (i) new man-machine interfaces created by a collection of multiple local devices acting in unison; (ii) ubiquitous universal computing distributed among multiple local devices and the cloud; (iii) multi-sensory data fusion to create multi-verse maps and new mixed-reality experiences; and (iv) precision sensing and actuation to control the physical world. With rapid advances in artificial intelligence, it has the potential to become the foundation for the 6G air interface and network, making data, compute and energy the new resources to be exploited for achieving superior performance. In addition, in this paper we discuss the other major technology transformations that are likely to define 6G: (i) cognitive spectrum sharing methods and new spectrum bands; (ii) the integration of localization and sensing capabilities into the system definition, (iii) the achievement of extreme performance requirements on latency and reliability; (iv) new network architecture paradigms involving sub-networks and RAN-Core convergence; and (v) new security and privacy schemes.","['6G mobile communication', '5G mobile communication', 'Robot sensing systems', 'Biology', 'Digital twin', 'User interfaces']","['6G', 'AI/ML driven air interface', 'network localization and sensing', 'cognitive spectrum sharing', 'sub-terahertz', 'RAN-Core convergence', 'subnetworks', 'security', 'privacy', 'network as a platform']"
"Due to the significant advancement of the Internet of Things (IoT) in the healthcare sector, the security, and the integrity of the medical data became big challenges for healthcare services applications. This paper proposes a hybrid security model for securing the diagnostic text data in medical images. The proposed model is developed through integrating either 2-D discrete wavelet transform 1 level (2D-DWT-1L) or 2-D discrete wavelet transform 2 level (2D-DWT-2L) steganography technique with a proposed hybrid encryption scheme. The proposed hybrid encryption schema is built using a combination of Advanced Encryption Standard, and Rivest, Shamir, and Adleman algorithms. The proposed model starts by encrypting the secret data; then it hides the result in a cover image using 2D-DWT-1L or 2D-DWT-2L. Both color and gray-scale images are used as cover images to conceal different text sizes. The performance of the proposed system was evaluated based on six statistical parameters; the peak signal-to-noise ratio (PSNR), mean square error (MSE), bit error rate (BER), structural similarity (SSIM), structural content (SC), and correlation. The PSNR values were relatively varied from 50.59 to 57.44 in case of color images and from 50.52 to 56.09 with the gray scale images. The MSE values varied from 0.12 to 0.57 for the color images and from 0.14 to 0.57 for the gray scale images. The BER values were zero for both images, while SSIM, SC, and correlation values were ones for both images. Compared with the state-of-the-art methods, the proposed model proved its ability to hide the confidential patient's data into a transmitted cover image with high imperceptibility, capacity, and minimal deterioration in the received stego-image.","['Encryption', 'Medical diagnostic imaging', 'Medical services', 'Data models']","['Cryptography', 'DWT-1level', 'DWT-2level', 'encryption', 'healthcare services', 'Internet of Things', 'medical images', 'steganography']"
"Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.","['Dimensionality reduction', 'Principal component analysis', 'Machine learning algorithms', 'Support vector machines', 'Medical diagnostic imaging', 'Feature extraction']","['Cardiotocography dataset', 'dimensionality reduction', 'feature engineering', 'linear discriminant analysis', 'machine learning', 'principal component analysis']"
"One of the key enablers of future wireless communications is constituted by massive multiple-input multiple-output (MIMO) systems, which can improve the spectral efficiency by orders of magnitude. In existing massive MIMO systems, however, conventional phased arrays are used for beamforming. This method results in excessive power consumption and high hardware costs. Recently, reconfigurable intelligent surface (RIS) has been considered as one of the revolutionary technologies to enable energy-efficient and smart wireless communications, which is a two-dimensional structure with a large number of passive elements. In this paper, we develop a new type of high-gain yet low-cost RIS that bears 256 elements. The proposed RIS combines the functions of phase shift and radiation together on an electromagnetic surface, where positive intrinsic-negative (PIN) diodes are used to realize 2-bit phase shifting for beamforming. This radical design forms the basis for the world’s first wireless communication prototype using RIS having 256 two-bit elements. The prototype consists of modular hardware and flexible software that encompass the following: the hosts for parameter setting and data exchange, the universal software radio peripherals (USRPs) for baseband and radio frequency (RF) signal processing, as well as the RIS for signal transmission and reception. Our performance evaluation confirms the feasibility and efficiency of RISs in wireless communications. We show that, at 2.3 GHz, the proposed RIS can achieve a 21.7 dBi antenna gain. At the millimeter wave (mmWave) frequency, that is, 28.5 GHz, it attains a 19.1 dBi antenna gain. Furthermore, it has been shown that the RIS-based wireless communication prototype developed is capable of significantly reducing the power consumption.","['Wireless communication', 'Phased arrays', 'Prototypes', 'Radio frequency', 'MIMO communication']","['Massive MIMO', 'prototype', 'reconfigurable intelligent surface (RIS)', 'wireless communication']"
"A microgrid (MG) is a local entity that consists of distributed energy resources (DERs) to achieve local power reliability and sustainable energy utilization. The MG concept or renewable energy technologies integrated with energy storage systems (ESS) have gained increasing interest and popularity because it can store energy at off-peak hours and supply energy at peak hours. However, existing ESS technology faces challenges in storing energy due to various issues, such as charging/discharging, safety, reliability, size, cost, life cycle, and overall management. Thus, an advanced ESS is required with regard to capacity, protection, control interface, energy management, and characteristics to enhance the performance of ESS in MG applications. This paper comprehensively reviews the types of ESS technologies, ESS structures along with their configurations, classifications, features, energy conversion, and evaluation process. Moreover, details on the advantages and disadvantages of ESS in MG applications have been analyzed based on the process of energy formations, material selection, power transfer mechanism, capacity, efficiency, and cycle period. Existing reviews critically demonstrate the current technologies for ESS in MG applications. However, the optimum management of ESSs for efficient MG operation remains a challenge in modern power system networks. This review also highlights the key factors, issues, and challenges with possible recommendations for the further development of ESS in future MG applications. All the highlighted insights of this review significantly contribute to the increasing effort toward the development of a cost-effective and efficient ESS model with a prolonged life cycle for sustainable MG implementation.","['Batteries', 'Power system reliability', 'Reliability', 'Renewable energy sources', 'Lithium', 'Fuel cells']","['Energy storage system', 'microgrid', 'distributed energy resources', 'ESS technologies', 'energy management']"
"Multi-agent systems (MASs) have received tremendous attention from scholars in different disciplines, including computer science and civil engineering, as a means to solve complex problems by subdividing them into smaller tasks. The individual tasks are allocated to autonomous entities, known as agents. Each agent decides on a proper action to solve the task using multiple inputs, e.g., history of actions, interactions with its neighboring agents, and its goal. The MAS has found multiple applications, including modeling complex systems, smart grids, and computer networks. Despite their wide applicability, there are still a number of challenges faced by MAS, including coordination between agents, security, and task allocation. This survey provides a comprehensive discussion of all aspects of MAS, starting from definitions, features, applications, challenges, and communications to evaluation. A classification on MAS applications and challenges is provided along with references for further studies. We expect this paper to serve as an insightful and comprehensive resource on the MAS for researchers and practitioners in the area.","['Task analysis', 'Multi-agent systems', 'Computer science', 'Security', 'Australia', 'Computational modeling', 'Decision making']","['Multi-agent systems', 'survey', 'MAS applications', 'challenges']"
"Internet of things (IoT) is a promising technology which provides efficient and reliable solutions towards the modernization of several domains. IoT based solutions are being developed to automatically maintain and monitor agricultural farms with minimal human involvement. The article presents many aspects of technologies involved in the domain of IoT in agriculture. It explains the major components of IoT based smart farming. A rigorous discussion on network technologies used in IoT based agriculture has been presented, that involves network architecture and layers, network topologies used, and protocols. Furthermore, the connection of IoT based agriculture systems with relevant technologies including cloud computing, big data storage and analytics has also been presented. In addition, security issues in IoT agriculture have been highlighted. A list of smart phone based and sensor based applications developed for different aspects of farm management has also been presented. Lastly, the regulations and policies made by several countries to standardize IoT based agriculture have been presented along with few available success stories. In the end, some open research issues and challenges in IoT agriculture field have been presented.","['Agriculture', 'Monitoring', 'Internet of Things', 'Protocols', 'Temperature sensors', 'Temperature measurement', 'Data acquisition']","['IoT', 'smart farming', 'applications', 'protocols', 'network', 'architecture', 'platforms', 'industries', 'security', 'challenges', 'technologies', 'policies']"
"Fog computing paradigm extends the storage, networking, and computing facilities of the cloud computing toward the edge of the networks while offloading the cloud data centers and reducing service latency to the end users. However, the characteristics of fog computing arise new security and privacy challenges. The existing security and privacy measurements for cloud computing cannot be directly applied to the fog computing due to its features, such as mobility, heterogeneity, and large-scale geo-distribution. This paper provides an overview of existing security and privacy concerns, particularly for the fog computing. Afterward, this survey highlights ongoing research effort, open challenges, and research trends in privacy and security issues for fog computing.","['Edge computing', 'Cloud computing', 'Privacy', 'Computational modeling', 'Authentication', 'Electronic mail']","['Fog', 'fog computing', 'fog networking', 'security', 'privacy', 'IoT', 'privacy threats', 'security threats']"
"In traditional cloud storage systems, attribute-based encryption (ABE) is regarded as an important technology for solving the problem of data privacy and fine-grained access control. However, in all ABE schemes, the private key generator has the ability to decrypt all data stored in the cloud server, which may bring serious problems such as key abuse and privacy data leakage. Meanwhile, the traditional cloud storage model runs in a centralized storage manner, so single point of failure may leads to the collapse of system. With the development of blockchain technology, decentralized storage mode has entered the public view. The decentralized storage approach can solve the problem of single point of failure in traditional cloud storage systems and enjoy a number of advantages over centralized storage, such as low price and high throughput. In this paper, we study the data storage and sharing scheme for decentralized storage systems and propose a framework that combines the decentralized storage system interplanetary file system, the Ethereum blockchain, and ABE technology. In this framework, the data owner has the ability to distribute secret key for data users and encrypt shared data by specifying access policy, and the scheme achieves fine-grained access control over data. At the same time, based on smart contract on the Ethereum blockchain, the keyword search function on the cipher text of the decentralized storage systems is implemented, which solves the problem that the cloud server may not return all of the results searched or return wrong results in the traditional cloud storage systems. Finally, we simulated the scheme in the Linux system and the Ethereum official test network Rinkeby, and the experimental results show that our scheme is feasible.","['Encryption', 'Cloud computing', 'Access control', 'Contracts', 'Data privacy']","['ABE', 'Ethereum blockchain', 'smart contract', 'IPFS', 'access control', 'keyword searchable']"
"For more than a decade now, radio frequency identification (RFID) technology has been quite effective in providing anti-counterfeits measures in the supply chain. However, the genuineness of RFID tags cannot be guaranteed in the post supply chain, since these tags can be rather easily cloned in the public space. In this paper, we propose a novel product ownership management system (POMS) of RFID-attached products for anti-counterfeits that can be used in the post supply chain. For this purpose, we leverage the idea of Bitcoin's blockchain that anyone can check the proof of possession of balance. With the proposed POMS, a customer can reject the purchase of counterfeits even with genuine RFID tag information, if the seller does not possess their ownership. We have implemented a proof-of-concept experimental system employing a blockchain-based decentralized application platform, Ethereum, and evaluated its cost performance. Results have shown that, typically, the cost of managing the ownership of a product with up to six transfers is less than U.S. $1.","['Supply chains', 'Bitcoin', 'RFID tags', 'Protocols', 'Internet']","['Anti-counterfeits technology', 'POMS (products ownership management system)', 'blockchain', 'Ethereum', 'security']"
"With the rising interest in autonomous vehicles, developing radio access technologies (RATs) that enable reliable and low-latency vehicular communications has become of paramount importance. Dedicated short-range communications (DSRCs) and cellular V2X (C-V2X) are two present-day technologies that are capable of supporting day-1 vehicular applications. However, these RATs fall short of supporting communication requirements of many advanced vehicular applications, which are believed to be critical in enabling fully autonomous vehicles. Both the DSRC and C-V2X are undergoing extensive enhancements in order to support advanced vehicular applications that are characterized by high reliability, low latency, and high throughput requirements. These RAT evolutions—the IEEE 802.11bd for the DSRC and NR V2X for C-V2X—can supplement today’s vehicular sensors in enabling autonomous driving. In this paper, we survey the latest developments in the standardization of 802.11bd and NR V2X. We begin with a brief description of the two present-day vehicular RATs. In doing so, we highlight their inability to guarantee the quality of service requirements of many advanced vehicular applications. We then look at the two RAT evolutions, i.e., the IEEE 802.11bd and NR V2X, outline their objectives, describe their salient features, and provide an in-depth description of key mechanisms that enable these features. While both, the IEEE 802.11bd and NR V2X, are in their initial stages of development, we shed light on their preliminary performance projections and compare and contrast the two evolutionary RATs with their respective predecessors.","['Radio access technologies', 'Vehicle-to-everything', '3GPP', 'Communication channels', '5G mobile communication', 'Reliability']","['C-V2X', 'DSRC', 'IEEE 802.11bd', 'NR V2X']"
"Alternaria leaf spot, Brown spot, Mosaic, Grey spot, and Rust are five common types of apple leaf diseases that severely affect apple yield. However, the existing research lacks an accurate and fast detector of apple diseases for ensuring the healthy development of the apple industry. This paper proposes a deep learning approach that is based on improved convolutional neural networks (CNNs) for the real-time detection of apple leaf diseases. In this paper, the apple leaf disease dataset (ALDD), which is composed of laboratory images and complex images under real field conditions, is first constructed via data augmentation and image annotation technologies. Based on this, a new apple leaf disease detection model that uses deep-CNNs is proposed by introducing the GoogLeNet Inception structure and Rainbow concatenation. Finally, under the hold-out testing dataset, using a dataset of 26,377 images of diseased apple leaves, the proposed INAR-SSD (SSD with Inception module and Rainbow concatenation) model is trained to detect these five common apple leaf diseases. The experimental results show that the INAR-SSD model realizes a detection performance of 78.80% mAP on ALDD, with a high-detection speed of 23.13 FPS. The results demonstrate that the novel INAR-SSD model provides a high-performance solution for the early diagnosis of apple leaf diseases that can perform real-time detection of these diseases with higher accuracy and faster detection speed than previous methods.","['Diseases', 'Feature extraction', 'Real-time systems', 'Convolutional neural networks', 'Training', 'Deep learning', 'Agriculture']","['Apple leaf diseases', 'real-time detection', 'deep learning', 'convolutional neural networks', 'feature fusion']"
"The globalized production and the distribution of agriculture production bring a renewed focus on the safety, quality, and the validation of several important criteria in agriculture and food supply chains. The growing number of issues related to food safety and contamination risks has established an immense need for effective traceability solution that acts as an essential quality management tool ensuring adequate safety of products in the agricultural supply chain. Blockchain is a disruptive technology that can provide an innovative solution for product traceability in agriculture and food supply chains. Today's agricultural supply chains are complex ecosystem involving several stakeholders making it cumbersome to validate several important criteria such as country of origin, stages in crop development, conformance to quality standards, and monitor yields. In this paper, we propose an approach that leverages the Ethereum blockchain and smart contracts efficiently perform business transactions for soybean tracking and traceability across the agricultural supply chain. Our proposed solution eliminates the need for a trusted centralized authority, intermediaries and provides transactions records, enhancing efficiency and safety with high integrity, reliability, and security. The proposed solution focuses on the utilization of smart contracts to govern and control all interactions and transactions among all the participants involved within the supply chain ecosystem. All transactions are recorded and stored in the blockchain's immutable ledger with links to a decentralized file system (IPFS) and thus providing to all a high level of transparency and traceability into the supply chain ecosystem in a secure, trusted, reliable, and efficient manner.","['Supply chains', 'Blockchain', 'Smart contracts', 'Agriculture', 'Safety', 'Data mining', 'Contamination']","['Blockchain', 'Ethereum', 'smart contracts', 'traceability', 'Soybean', 'agricultural supply chain', 'food safety']"
"Fog computing-enhanced Internet of Things (IoT) has recently received considerable attention, as the fog devices deployed at the network edge can not only provide low latency, location awareness but also improve real-time and quality of services in IoT application scenarios. Privacy-preserving data aggregation is one of typical fog computing applications in IoT, and many privacy-preserving data aggregation schemes have been proposed in the past years. However, most of them only support data aggregation for homogeneous IoT devices, and cannot aggregate hybrid IoT devices' data into one in some real IoT applications. To address this challenge, in this paper, we present a lightweight privacy-preserving data aggregation scheme, called Lightweight Privacy-preserving Data Aggregation, for fog computing-enhanced IoT. The proposed LPDA is characterized by employing the homomorphic Paillier encryption, Chinese Remainder Theorem, and one-way hash chain techniques to not only aggregate hybrid IoT devices' data into one, but also early filter injected false data at the network edge. Detailed security analysis shows LPDA is really secure and privacy-enhanced with differential privacy techniques. In addition, extensive performance evaluations are conducted, and the results indicate LPDA is really lightweight in fog computing-enhanced IoT.","['Security', 'Data aggregation', 'Edge computing', 'Real-time systems', 'Internet of Things', 'Aggregates', 'Computational modeling']","['Internet of Things', 'fog computing', 'privacy-preserving aggregation', 'lightweight', 'differential privacy']"
"The development of an anomaly-based intrusion detection system (IDS) is a primary research direction in the field of intrusion detection. An IDS learns normal and anomalous behavior by analyzing network traffic and can detect unknown and new attacks. However, the performance of an IDS is highly dependent on feature design, and designing a feature set that can accurately characterize network traffic is still an ongoing research issue. Anomaly-based IDSs also have the problem of a high false alarm rate (FAR), which seriously restricts their practical applications. In this paper, we propose a novel IDS called the hierarchical spatial-temporal features-based intrusion detection system (HAST-IDS), which first learns the low-level spatial features of network traffic using deep convolutional neural networks (CNNs) and then learns high-level temporal features using long short-term memory networks. The entire process of feature learning is completed by the deep neural networks automatically; no feature engineering techniques are required. The automatically learned traffic features effectively reduce the FAR. The standard DARPA1998 and ISCX2012 data sets are used to evaluate the performance of the proposed system. The experimental results show that the HAST-IDS outperforms other published approaches in terms of accuracy, detection rate, and FAR, which successfully demonstrates its effectiveness in both feature learning and FAR reduction.","['Telecommunication traffic', 'Feature extraction', 'Intrusion detection', 'Recurrent neural networks', 'Natural language processing']","['Network intrusion detection', 'deep neural networks', 'representation learning']"
"Blockchain technology enables the creation of a decentralized environment, where transactions and data are not under the control of any third party organization. Any transaction ever completed is recorded in a public ledger in a verifiable and permanent way. Based on the blockchain technology, we propose a global higher education credit platform, named EduCTX. This platform is based on the concept of the European Credit Transfer and Accumulation System (ECTS). It constitutes a globally trusted, decentralized higher education credit, and grading system that can offer a globally unified viewpoint for students and higher education institutions (HEIs), as well as for other potential stakeholders, such as companies, institutions, and organizations. As a proof of concept, we present a prototype implementation of the environment, based on the open-source Ark Blockchain Platform. Based on a globally distributed peer-to-peer network, EduCTX will process, manage, and control ECTX tokens, which represent credits that students gain for completed courses, such as ECTS. HEIs are the peers of the blockchain network. The platform is a first step toward a more transparent and technologically advanced form of higher education systems. The EduCTX platform represents the basis of the EduCTX initiative, which anticipates that various HEIs would join forces in order to create a globally efficient, simplified, and ubiquitous environment in order to avoid language and administrative barriers. Therefore, we invite and encourage HEIs to join the EduCTX initiative and the EduCTX blockchain network.","['Education', 'Prototypes', 'Europe', 'Bitcoin', 'Open source software', 'Organizations']","['Blockchain', 'higher education', 'ECTS', 'tokens']"
"Due to the proliferation of ICT during the last few decades, there is an exponential increase in the usage of various smart applications such as smart farming, smart healthcare, supply-chain & logistics, business, tourism and hospitality, energy management etc. However, for all the aforementioned applications, security and privacy are major concerns keeping in view of the usage of the open channel, i.e., Internet for data transfer. Although many security solutions and standards have been proposed over the years to enhance the security levels of aforementioned smart applications, but the existing solutions are either based upon the centralized architecture (having single point of failure) or having high computation and communication costs. Moreover, most of the existing security solutions have focussed only on few aspects and fail to address scalability, robustness, data storage, network latency, auditability, immutability, and traceability. To handle the aforementioned issues, blockchain technology can be one of the solutions. Motivated from these facts, in this paper, we present a systematic review of various blockchain-based solutions and their applicability in various Industry 4.0-based applications. Our contributions in this paper are in four fold. Firstly, we explored the current state-of-the-art solutions in the blockchain technology for the smart applications. Then, we illustrated the reference architecture used for the blockchain applicability in various Industry 4.0 applications. Then, merits and demerits of the traditional security solutions are also discussed in comparison to their countermeasures. Finally, we provided a comparison of existing blockchain-based security solutions using various parameters to provide deep insights to the readers about its applicability in various applications.","['Industries', 'Medical services', 'Computer architecture', 'Data privacy', 'Internet']","['Blockchain', 'consensus algorithms', 'cyber-physical systems', 'IoT', 'smart grid', 'supply chain management', 'intelligent transportation']"
"In the field of agricultural information, the automatic identification and diagnosis of maize leaf diseases is highly desired. To improve the identification accuracy of maize leaf diseases and reduce the number of network parameters, the improved GoogLeNet and Cifar10 models based on deep learning are proposed for leaf disease recognition in this paper. Two improved models that are used to train and test nine kinds of maize leaf images are obtained by adjusting the parameters, changing the pooling combinations, adding dropout operations and rectified linear unit functions, and reducing the number of classifiers. In addition, the number of parameters of the improved models is significantly smaller than that of the VGG and AlexNet structures. During the recognition of eight kinds of maize leaf diseases, the GoogLeNet model achieves a top - 1 average identification accuracy of 98.9%, and the Cifar10 model achieves an average accuracy of 98.8%. The improved methods are possibly improved the accuracy of maize leaf disease, and reduced the convergence iterations, which can effectively improve the model training and recognition efficiency.","['Diseases', 'Training', 'Machine learning', 'Support vector machines', 'Testing', 'Convolutional neural networks']","['Deep learning', 'deep convolutional neural networks', 'identification', 'image processing', 'leaf diseases']"
"The vision of Industry 4.0, otherwise known as the fourth industrial revolution, is the integration of massively deployed smart computing and network technologies in industrial production and manufacturing settings for the purposes of automation, reliability, and control, implicating the development of an Industrial Internet of Things (I-IoT). Specifically, I-IoT is devoted to adopting the IoT to enable the interconnection of anything, anywhere, and at any time in the manufacturing system context to improve the productivity, efficiency, safety, and intelligence. As an emerging technology, I-IoT has distinct properties and requirements that distinguish it from consumer IoT, including the unique types of smart devices incorporated, network technologies and quality-of-service requirements, and strict needs of command and control. To more clearly understand the complexities of I-IoT and its distinct needs and to present a unified assessment of the technology from a systems’ perspective, in this paper, we comprehensively survey the body of existing research on I-IoT. Particularly, we first present the I-IoT architecture, I-IoT applications (i.e., factory automation and process automation), and their characteristics. We then consider existing research efforts from the three key system aspects of control, networking, and computing. Regarding control, we first categorize industrial control systems and then present recent and relevant research efforts. Next, considering networking, we propose a three-dimensional framework to explore the existing research space and investigate the adoption of some representative networking technologies, including 5G, machine-to-machine communication, and software-defined networking. Similarly, concerning computing, we again propose a second three-dimensional framework that explores the problem space of computing in I-IoT and investigate the cloud, edge, and hybrid cloud and edge computing platforms. Finally, we outline particular challenges and future research needs in control, networking, and computing systems, as well as for the adoption of machine learning in an I-IoT context.","['Manufacturing', 'Automation', 'Machine learning', 'Sensors', 'Production', 'Control systems', 'Cloud computing']","['Industrial Internet of Things', 'industrial cyber physical systems', 'application and service', 'control', 'networking', 'computing', 'machine learning', 'big data analytics', 'survey', 'future research directions']"
"Smart world is envisioned as an era in which objects (e.g., watches, mobile phones, computers, cars, buses, and trains) can automatically and intelligently serve people in a collaborative manner. Paving the way for smart world, Internet of Things (IoT) connects everything in the smart world. Motivated by achieving a sustainable smart world, this paper discusses various technologies and issues regarding green IoT, which further reduces the energy consumption of IoT. Particularly, an overview regarding IoT and green IoT is performed first. Then, the hot green information and communications technologies (ICTs) (e.g., green radio-frequency identification, green wireless sensor network, green cloud computing, green machine to machine, and green data center) enabling green IoT are studied, and general green ICT principles are summarized. Furthermore, the latest developments and future vision about sensor cloud, which is a novel paradigm in green IoT, are reviewed and introduced, respectively. Finally, future research directions and open problems about green IoT are presented. Our work targets to be an enlightening and latest guidance for research with respect to green IoT and smart world.","['Internet of things', 'Radio frequency identification', 'Wireless sensor networks', 'Machine-to-machine communications', 'Cloud computing', 'Data centers']","['Smart world', 'internet of things', 'green', 'radio frequency identification', 'wireless sensor network', 'cloud computing', 'machine to machine', 'data center', 'sensor-cloud']"
"New high-data-rate multimedia services and applications are evolving continuously and exponentially increasing the demand for wireless capacity of fifth-generation (5G) and beyond. The existing radio frequency (RF) communication spectrum is insufficient to meet the demands of future high-datarate 5G services. Optical wireless communication (OWC), which uses an ultra-wide range of unregulated spectrum, has emerged as a promising solution to overcome the RF spectrum crisis. It has attracted growing research interest worldwide in the last decade for indoor and outdoor applications. OWC offloads huge data traffic applications from RF networks. A 100 Gb/s data rate has already been demonstrated through OWC. It offers services indoors as well as outdoors, and communication distances range from several nm to more than 10000 km. This paper provides a technology overview and a review on optical wireless technologies, such as visible light communication, light fidelity, optical camera communication, free space optical communication, and light detection and ranging. We survey the key technologies for understanding OWC and present state-of-the-art criteria in aspects, such as classification, spectrum use, architecture, and applications. The key contribution of this paper is to clarify the differences among different promising optical wireless technologies and between these technologies and their corresponding similar existing RF technologies.","['Wireless communication', '5G mobile communication', 'Radio frequency', 'Optical transmitters', 'Optical fiber communication', 'Optical sensors']","['Free space optical communication', 'infrared', 'light detection and ranging', 'light fidelity', 'optical camera communication', 'optical wireless communication', 'radio frequency', 'ultraviolet', 'visible light', 'visible light communication']"
"COVID-19 outbreak has put the whole world in an unprecedented difficult situation bringing life around the world to a frightening halt and claiming thousands of lives. Due to COVID-19's spread in 212 countries and territories and increasing numbers of infected cases and death tolls mounting to 5,212,172 and 334,915 (as of May 22 2020), it remains a real threat to the public health system. This paper renders a response to combat the virus through Artificial Intelligence (AI). Some Deep Learning (DL) methods have been illustrated to reach this goal, including Generative Adversarial Networks (GANs), Extreme Learning Machine (ELM), and Long/Short Term Memory (LSTM). It delineates an integrated bioinformatics approach in which different aspects of information from a continuum of structured and unstructured data sources are put together to form the user-friendly platforms for physicians and researchers. The main advantage of these AI-based platforms is to accelerate the process of diagnosis and treatment of the COVID-19 disease. The most recent related publications and medical reports were investigated with the purpose of choosing inputs and targets of the network that could facilitate reaching a reliable Artificial Neural Network-based tool for challenges associated with COVID-19. Furthermore, there are some specific inputs for each platform, including various forms of the data, such as clinical data and medical imaging which can improve the performance of the introduced approaches toward the best responses in practical applications.","['Artificial intelligence', 'COVID-19', 'Medical diagnostic imaging', 'Databases']","['Artificial intelligence', 'big data', 'bioinformatics', 'biomedical informatics', 'COVID-19', 'deep learning', 'diagnosis', 'machine learning', 'treatment']"
"The growing popularity and development of data mining technologies bring serious threat to the security of individual,'s sensitive information. An emerging research topic in data mining, known as privacy-preserving data mining (PPDM), has been extensively studied in recent years. The basic idea of PPDM is to modify the data in such a way so as to perform data mining algorithms effectively without compromising the security of sensitive information contained in the data. Current studies of PPDM mainly focus on how to reduce the privacy risk brought by data mining operations, while in fact, unwanted disclosure of sensitive information may also happen in the process of data collecting, data publishing, and information (i.e., the data mining results) delivering. In this paper, we view the privacy issues related to data mining from a wider perspective and investigate various approaches that can help to protect sensitive information. In particular, we identify four different types of users involved in data mining applications, namely, data provider, data collector, data miner, and decision maker. For each type of user, we discuss his privacy concerns and the methods that can be adopted to protect sensitive information. We briefly introduce the basics of related research topics, review state-of-the-art approaches, and present some preliminary thoughts on future research directions. Besides exploring the privacy-preserving approaches for each type of user, we also review the game theoretical approaches, which are proposed for analyzing the interactions among different users in a data mining scenario, each of whom has his own valuation on the sensitive information. By differentiating the responsibilities of different users with respect to security of sensitive information, we would like to provide some useful insights into the study of PPDM.","['Privacy', 'Data mining', 'Game theory', 'Tracking', 'Computer security', 'Data privacy', 'Algorithm design and analysis']","['data mining', 'sensitive information', 'privacypreserving data mining', 'anonymization', 'provenance', 'game theory', 'privacy auction', 'anti-tracking']"
"Diverse proprietary network appliances increase both the capital and operational expense of service providers, meanwhile causing problems of network ossification. Network function virtualization (NFV) is proposed to address these issues by implementing network functions as pure software on commodity and general hardware. NFV allows flexible provisioning, deployment, and centralized management of virtual network functions. Integrated with SDN, the software-defined NFV architecture further offers agile traffic steering and joint optimization of network functions and resources. This architecture benefits a wide range of applications (e.g., service chaining) and is becoming the dominant form of NFV. In this survey, we present a thorough investigation of the development of NFV under the software-defined NFV architecture, with an emphasis on service chaining as its application. We first introduce the software-defined NFV architecture as the state of the art of NFV and present relationships between NFV and SDN. Then, we provide a historic view of the involvement from middlebox to NFV. Finally, we introduce significant challenges and relevant solutions of NFV, and discuss its future research directions by different application domains.","['Computer architecture', 'Virtualization', 'Hardware', 'Software', 'Home appliances', 'Control systems', 'Servers']","['Software-defined networks', 'network function virtualization', 'middlebox', 'service chain', 'network virtualization']"
"Emerging technologies such as the Internet of Things (IoT) require latency-aware computation for real-time application processing. In IoT environments, connected things generate a huge amount of data, which are generally referred to as big data. Data generated from IoT devices are generally processed in a cloud infrastructure because of the on-demand services and scalability features of the cloud computing paradigm. However, processing IoT application requests on the cloud exclusively is not an efficient solution for some IoT applications, especially time-sensitive ones. To address this issue, Fog computing, which resides in between cloud and IoT devices, was proposed. In general, in the Fog computing environment, IoT devices are connected to Fog devices. These Fog devices are located in close proximity to users and are responsible for intermediate computation and storage. One of the key challenges in running IoT applications in a Fog computing environment are resource allocation and task scheduling. Fog computing research is still in its infancy, and taxonomy-based investigation into the requirements of Fog infrastructure, platform, and applications mapped to current research is still required. This survey will help the industry and research community synthesize and identify the requirements for Fog computing. This paper starts with an overview of Fog computing in which the definition of Fog computing, research trends, and the technical differences between Fog and cloud are reviewed. Then, we investigate numerous proposed Fog computing architectures and describe the components of these architectures in detail. From this, the role of each component will be defined, which will help in the deployment of Fog computing. Next, a taxonomy of Fog computing is proposed by considering the requirements of the Fog computing paradigm. We also discuss existing research works and gaps in resource allocation and scheduling, fault tolerance, simulation tools, and Fog-based microservices. Finally, by addressing the limitations of current research works, we present some open issues, which will determine the future research direction for the Fog computing paradigm.","['Edge computing', 'Cloud computing', 'Computer architecture', 'Market research', 'Internet of Things', 'Resource management', 'Taxonomy']","['Fog computing', 'Internet of Things (IoT)', 'fog devices', 'fault tolerance', 'IoT application', 'microservices']"
"Intrusion detection is a fundamental part of security tools, such as adaptive security appliances, intrusion detection systems, intrusion prevention systems, and firewalls. Various intrusion detection techniques are used, but their performance is an issue. Intrusion detection performance depends on accuracy, which needs to improve to decrease false alarms and to increase the detection rate. To resolve concerns on performance, multilayer perceptron, support vector machine (SVM), and other techniques have been used in recent work. Such techniques indicate limitations and are not efficient for use in large data sets, such as system and network data. The intrusion detection system is used in analyzing huge traffic data; thus, an efficient classification technique is necessary to overcome the issue. This problem is considered in this paper. Well-known machine learning techniques, namely, SVM, random forest, and extreme learning machine (ELM) are applied. These techniques are well-known because of their capability in classification. The NSL–knowledge discovery and data mining data set is used, which is considered a benchmark in the evaluation of intrusion detection mechanisms. The results indicate that ELM outperforms other approaches.","['Intrusion detection', 'Support vector machines', 'Radio frequency', 'Training', 'Forestry', 'Kernel']","['Detection rate', 'extreme learning machine', 'false alarms', 'NSL–KDD', 'random forest', 'support vector machine']"
"In the past years, traditional pattern recognition methods have made great progress. However, these methods rely heavily on manual feature extraction, which may hinder the generalization model performance. With the increasing popularity and success of deep learning methods, using these techniques to recognize human actions in mobile and wearable computing scenarios has attracted widespread attention. In this paper, a deep neural network that combines convolutional layers with long short-term memory (LSTM) was proposed. This model could extract activity features automatically and classify them with a few model parameters. LSTM is a variant of the recurrent neural network (RNN), which is more suitable for processing temporal sequences. In the proposed architecture, the raw data collected by mobile sensors was fed into a two-layer LSTM followed by convolutional layers. In addition, a global average pooling layer (GAP) was applied to replace the fully connected layer after convolution for reducing model parameters. Moreover, a batch normalization layer (BN) was added after the GAP layer to speed up the convergence, and obvious results were achieved. The model performance was evaluated on three public datasets (UCI, WISDM, and OPPORTUNITY). Finally, the overall accuracy of the model in the UCI-HAR dataset is 95.78%, in the WISDM dataset is 95.85%, and in the OPPORTUNITY dataset is 92.63%. The results show that the proposed model has higher robustness and better activity detection capability than some of the reported results. It can not only adaptively extract activity features, but also has fewer parameters and higher accuracy.","['Feature extraction', 'Activity recognition', 'Acceleration', 'Deep learning', 'Sensor phenomena and characterization', 'Accelerometers']","['Human activity recognition', 'convolution', 'long short-term memory', 'mobile sensors']"
"Brain tumor classification is a crucial task to evaluate the tumors and make a treatment decision according to their classes. There are many imaging techniques used to detect brain tumors. However, MRI is commonly used due to its superior image quality and the fact of relying on no ionizing radiation. Deep learning (DL) is a subfield of machine learning and recently showed a remarkable performance, especially in classification and segmentation problems. In this paper, a DL model based on a convolutional neural network is proposed to classify different brain tumor types using two publicly available datasets. The former one classifies tumors into (meningioma, glioma, and pituitary tumor). The other one differentiates between the three glioma grades (Grade II, Grade III, and Grade IV). The datasets include 233 and 73 patients with a total of 3064 and 516 images on T1-weighted contrast-enhanced images for the first and second datasets, respectively. The proposed network structure achieves a significant performance with the best overall accuracy of 96.13% and 98.7%, respectively, for the two studies. The results indicate the ability of the model for brain tumor multi-classification purposes.","['Tumors', 'Feature extraction', 'Cancer', 'Task analysis', 'Magnetic resonance imaging', 'Convolutional neural networks', 'Training']","['Brain tumor', 'convolutional neural network', 'data augmentation', 'deep learning', 'MRI']"
"Wind energy has seen great development during the past decade. However, wind turbine availability and reliability, especially for offshore sites, still need to be improved, which strongly affect the cost of wind energy. Wind turbine operational cost is closely depending on component failure and repair rate, while fault detection and isolation will be very helpful to improve the availability and reliability factors. In this paper, an efficient machine learning method, random forests (RFs) in combination with extreme gradient boosting (XGBoost), is used to establish the data-driven wind turbine fault detection framework. In the proposed design, RF is used to rank the features by importance, which are either direct sensor signals or constructed variables from prior knowledge. Then, based on the top-ranking features, XGBoost trains the ensemble classifier for each specific fault. In order to verify the effectiveness of the proposed approach, numerical simulations using the state-of-the-art wind turbine simulator FAST are conducted for three different types of wind turbines in both the below and above rated conditions. It is shown that the proposed approach is robust to various wind turbine models including offshore ones in different working conditions. Besides, the proposed ensemble classifier is able to protect against overfitting, and it achieves better wind turbine fault detection results than the support vector machine method when dealing with multidimensional data.","['Wind turbines', 'Generators', 'Actuators', 'Blades', 'Radio frequency', 'Fault detection', 'Support vector machines']","['Wind turbines', 'fault detection', 'data-driven', 'random forests', 'extreme gradient boosting']"
"Generative adversarial network (GANs) is one of the most important research avenues in the field of artificial intelligence, and its outstanding data generation capacity has received wide attention. In this paper, we present the recent progress on GANs. First, the basic theory of GANs and the differences among different generative models in recent years were analyzed and summarized. Then, the derived models of GANs are classified and introduced one by one. Third, the training tricks and evaluation metrics were given. Fourth, the applications of GANs were introduced. Finally, the problem, we need to address, and future directions were discussed.","['Gallium nitride', 'Generators', 'Generative adversarial networks', 'Training', 'Feature extraction', 'Data models', 'Unsupervised learning']","['Deep learning', 'machine learning', 'unsupervised learning', 'generative adversarial networks']"
"Software defined networking (SDN) brings about innovation, simplicity in network management, and configuration in network computing. Traditional networks often lack the flexibility to bring into effect instant changes because of the rigidity of the network and also the over dependence on proprietary services. SDN decouples the control plane from the data plane, thus moving the control logic from the node to a central controller. A wireless sensor network (WSN) is a great platform for low-rate wireless personal area networks with little resources and short communication ranges. However, as the scale of WSN expands, it faces several challenges, such as network management and heterogeneous-node networks. The SDN approach to WSNs seeks to alleviate most of the challenges and ultimately foster efficiency and sustainability in WSNs. The fusion of these two models gives rise to a new paradigm: Software defined wireless sensor networks (SDWSN). The SDWSN model is also envisioned to play a critical role in the looming Internet of Things paradigm. This paper presents a comprehensive review of the SDWSN literature. Moreover, it delves into some of the challenges facing this paradigm, as well as the major SDWSN design requirements that need to be considered to address these challenges.","['Wireless sensor networks', 'Software', 'Protocols', 'Iron', 'Internet of Things', 'Switches']","['Software defined wireless sensor networks', 'software defined networking', 'wireless sensor networks']"
"The recent advances in embedded processing have enabled the vision based systems to detect fire during surveillance using convolutional neural networks (CNNs). However, such methods generally need more computational time and memory, restricting its implementation in surveillance networks. In this research paper, we propose a cost-effective fire detection CNN architecture for surveillance videos. The model is inspired from GoogleNet architecture, considering its reasonable computational complexity and suitability for the intended problem compared to other computationally expensive networks such as AlexNet. To balance the efficiency and accuracy, the model is fine-tuned considering the nature of the target problem and fire data. Experimental results on benchmark fire datasets reveal the effectiveness of the proposed framework and validate its suitability for fire detection in CCTV surveillance systems compared to state-of-the-art methods.","['Fires', 'Surveillance', 'Computer architecture', 'Videos', 'Color', 'Feature extraction', 'Convolution']","['Fire detection', 'image classification', 'real-world applications', 'deep learning', 'CCTV video analysis']"
"With the purpose of identifying cyber threats and possible incidents, intrusion detection systems (IDSs) are widely deployed in various computer networks. In order to enhance the detection capability of a single IDS, collaborative intrusion detection networks (or collaborative IDSs) have been developed, which allow IDS nodes to exchange data with each other. However, data and trust management still remain two challenges for current detection architectures, which may degrade the effectiveness of such detection systems. In recent years, blockchain technology has shown its adaptability in many fields, such as supply chain management, international payment, interbanking, and so on. As blockchain can protect the integrity of data storage and ensure process transparency, it has a potential to be applied to intrusion detection domain. Motivated by this, this paper provides a review regarding the intersection of IDSs and blockchains. In particular, we introduce the background of intrusion detection and blockchain, discuss the applicability of blockchain to intrusion detection, and identify open challenges in this direction.","['Intrusion detection', 'Collaboration', 'Peer-to-peer computing', 'Monitoring', 'Resistance']","['Blockchain technology', 'intrusion detection', 'collaborative network', 'trust management', 'data sharing and management']"
"Internet of Things (IoT) is one of the evolutionary directions of the Internet. This paper focuses on the low earth orbit (LEO) satellite constellation-based IoT services for their irreplaceable functions. In many cases, IoT devices are distributed in remote areas (e.g., desert, ocean, and forest) in some special applications, they are placed in some extreme topography, where are unable to have direct terrestrial network accesses and can only be covered by satellite. Comparing with the traditional geostationary earth orbit (GEO) systems, LEO satellite constellation has the advantages of low propagation delay, small propagation loss and global coverage. Furthermore, revision of existing IoT protocol are necessary to enhance the compatibility of the LEO satellite constellation-based IoT with terrestrial IoT systems. In this paper, we provide an overview of the architecture of the LEO satellite constellation-based IoT including the following topics: LEO satellite constellation structure, efficient spectrum allocation, heterogeneous networks compatibility, and access and routing protocols.","['Low earth orbit satellites', 'Monitoring', 'Satellite constellations', 'Protocols', 'Machine-to-machine communications', 'Sensors']","['Internet of things (IoT)', 'LEO satellite constellation', 'low-power wide-area network (LPWAN)', 'long range (LoRa)', 'machine-to-machine (M2M) communications', 'narrow band internet of things (NB-IoT)']"
"After many years of rigid conventional procedures of production, industrial manufacturing is going through a process of change toward flexible and intelligent manufacturing, the so-called Industry 4.0. In this paper, human-robot collaboration has an important role in smart factories since it contributes to the achievement of higher productivity and greater efficiency. However, this evolution means breaking with the established safety procedures as the separation of workspaces between robot and human is removed. These changes are reflected in safety standards related to industrial robotics since the last decade, and have led to the development of a wide field of research focusing on the prevention of human-robot impacts and/or the minimization of related risks or their consequences. This paper presents a review of the main safety systems that have been proposed and applied in industrial robotic environments that contribute to the achievement of safe collaborative human-robot work. Additionally, a review is provided of the current regulations along with new concepts that have been introduced in them. The discussion presented in this paper includes multidisciplinary approaches, such as techniques for estimation and the evaluation of injuries in human-robot collisions, mechanical and software devices designed to minimize the consequences of human-robot impact, impact detection systems, and strategies to prevent collisions or minimize their consequences when they occur.","['Service robots', 'Safety', 'Robot sensing systems', 'Collaboration', 'Standards', 'Collision avoidance']","['Safety', 'industrial robot', 'human-robot collaboration', 'industrial standards', 'Industry 4.0']"
"Blockchain technologies have recently come to the forefront of the research and industrial communities as they bring potential benefits for many industries. This is due to their practical capabilities in solving many issues currently inhibiting further advances in various industrial domains. Securely recording and sharing transactional data, establishing automated and efficient supply chain processes, and enhancing transparency across the whole value chain are some examples of these issues. Blockchain offers an effective way to tackle these issues using distributed, shared, secure, and permissioned transactional ledgers. The employment of blockchain technologies and the possibility of applying them in different situations enables many industrial applications through increased efficiency and security; enhanced traceability and transparency; and reduced costs. In this paper, different industrial application domains where the use of blockchain technologies has been proposed are reviewed. This paper explores the opportunities, benefits, and challenges of incorporating blockchain in different industrial applications. Furthermore, the paper attempts to identify the requirements that support the implementation of blockchain for different industrial applications. The review reveals that several opportunities are available for utilizing blockchain in various industrial sectors; however, there are still some challenges to be addressed to achieve better utilization of this technology.",[],[]
"In the era of smart cities, there are a plethora of applications where the localization of indoor environments is important, from monitoring and tracking in smart buildings to proximity marketing and advertising in shopping malls. The success of these applications is based on the development of a cost-efficient and robust real-time system capable of accurately localizing objects. In most outdoor localization systems, global positioning system (GPS) is used due to its ease of implementation and accuracy up to five meters. However, due to the limited space that comes with performing localization of indoor environments and the large number of obstacles found indoors, GPS is not a suitable option. Hence, accurately and efficiently locating objects is a major challenge in indoor environments. Recent advancements in the Internet of Things (IoT) along with novel wireless technologies can alleviate the problem. Small-size and cost-efficient IoT devices which use wireless protocols can provide an attractive solution. In this paper, we compare four wireless technologies for indoor localization: Wi-Fi (IEEE 802.11n-2009 at the 2.4 GHz band), Bluetooth low energy, Zigbee, and long-range wide-area network. These technologies are compared in terms of localization accuracy and power consumption when IoT devices are used. The received signal strength indicator (RSSI) values from each modality were used and trilateration was performed for localization. The RSSI data set is available online. The experimental results can be used as an indicator in the selection of a wireless technology for an indoor localization system following application requirements.","['Wireless fidelity', 'Wireless communication', 'Performance evaluation', 'Global Positioning System', 'Receivers', 'ZigBee', 'Hardware']","['Indoor localization accuracy', 'power consumption', 'Internet of Things', 'RSSI', 'WiFi', 'Bluetooth low energy', 'Zigbee', 'LoRaWAN']"
"The Blockchain technology can be defined as a distributed ledger database for recording transactions between parties verifiably and permanently. Blockchain emerged as a leading technology layer for financial applications. Nevertheless, in the past years, the attention of researchers and practitioners moved to the application of the Blockchain technologies to other domains. Recently, it represents the backbone of a new digital supply chain. Thanks to its capability of ensuring data immutability and public accessibility of data streams, Blockchain can increase the efficiency, reliability, and transparency of the overall supply chain, and optimize the inbound processes. The literature concerning Blockchain in non-financial applications mainly focused on the technological part and the Business Process Modeling, lacking in terms of standard methodology for designing a strategy to develop and validate the overall Blockchain solution and integrate it in the Business Strategy. Thus, this paper aims to overcome this lack. First, we integrate the current literature filling the lack concerning the digital strategy, creating a standard methodology to design Blockchain technology use cases, which are not related to finance applications. Second, we present the results of a use case in the fresh food delivery, showing the critical aspects of implementing a Blockchain solution. Moreover, the paper discusses how the Blockchain will help in reducing the logistics costs and in optimizing the operations and the research challenges.","['Supply chains', 'Standards']","['Blockchain', 'hyperledger', 'supply chain']"
"The Internet of Drones (IoD) is a layered network control architecture designed mainly for coordinating the access of unmanned aerial vehicles to controlled airspace, and providing navigation services between locations referred to as nodes. The IoD provides generic services for various drone applications, such as package delivery, traffic surveillance, search and rescue, and more. In this paper, we present a conceptual model of how such an architecture can be organized and we specify the features that an IoD system based on our architecture should implement. For doing so, we extract key concepts from three existing large scale networks, namely the air traffic control network, the cellular network, and the Internet, and explore their connections to our novel architecture for drone traffic management. A simulation platform for IoD is being implemented, which can be accessed from www.IoDnet.org in the future.","['Internet of things', 'Drones', 'Network architecture', 'Internet', 'Navigation', 'Atmospheric modeling', 'Surveillance', 'Traffic control']","['Layered architecture', 'Internet of Drones (IoD)', 'Internet', 'cellular network', 'air traffic control (ATC)', 'low altitude air traffic management', 'unmanned aerial vehicle (UAV)']"
"Wireless mediums, such as RF, optical, or acoustical, provide finite resources for the purposes of remote sensing (such as radar) and data communications. Often, these two functions are at odds with one another and compete for these resources. Applications for wireless technology are growing rapidly, and RF convergence is already presenting itself as a requirement for both users as consumer and military system requirements evolve. The broad solution space to this complex problem encompasses cooperation or codesigning of systems with both sensing and communications functions. By jointly considering the systems during the design phase, rather than perpetuating a notion of mutual interference, both system's performance can be improved. We provide a point of departure for future researchers that will be required to solve this problem by presenting the applications, topologies, levels of system integration, the current state of the art, and outlines of future information-centric systems.","['Radio frequency', 'Radar communication', 'Wireless sensor networks', 'Laser radar', 'Remote sensing', 'Radar remote sensing']","['RF convergence', 'radar communications co-existence', 'joint sensing-communications', 'wireless resources']"
"This paper reviews the basic concepts of rays, ray tracing algorithms, and radio propagation modeling using ray tracing methods. We focus on the fundamental concepts and the development of practical ray tracing algorithms. The most recent progress and a future perspective of ray tracing are also discussed. We envision propagation modeling in the near future as an intelligent, accurate, and real-time system in which ray tracing plays an important role. This review is especially useful for experts who are developing new ray tracing algorithms to enhance modeling accuracy and improve computational speed.","['Ray tracing', 'Radio propagation', 'Modeling', 'Acceleration', 'Algorithm design and analysis', 'Radio propagation', 'Propagation modeling']","['Radio propagation', 'propagation modeling', 'acceleration algorithm']"
"Lithium-ion battery is an appropriate choice for electric vehicle (EV) due to its promising features of high voltage, high energy density, low self-discharge and long lifecycles. The successful operation of EV is highly dependent on the operation of battery management system (BMS). State of charge (SOC) is one of the vital paraments of BMS which signifies the amount of charge left in a battery. A good estimation of SOC leads to long battery life and prevention of catastrophe from battery failure. Besides, an accurate and robust SOC estimation has great significance towards an efficient EV operation. However, SOC estimation is a complex process due to its dependency on various factors such as battery age, ambient temperature, and many unknown factors. This review presents the recent SOC estimation methods highlighting the model-based and data-driven approaches. Model-based methods attempt to model the battery behavior incorporating various factors into complex mathematical equations in order to accurately estimate the SOC while the data-driven methods adopt an approach of learning the battery's behavior by running complex algorithms with a large amount of measured battery data. The classifications of model-based and data-driven based SOC estimation are explained in terms of estimation model/algorithm, benefits, drawbacks, and estimation error. In addition, the review highlights many factors and challenges and delivers potential recommendations for the development of SOC estimation methods in EV applications. All the highlighted insights of this review will hopefully lead to increased efforts toward the enhancement of SOC estimation method of lithium-ion battery for the future high-tech EV applications.","['State of charge', 'Batteries', 'Estimation', 'Temperature measurement', 'Mathematical model', 'Integrated circuit modeling']","['State of charge', 'lithium-ion battery', 'electric vehicle', 'model-based approaches', 'data-driven approaches']"
"With the rapid development of the Internet of Everything (IoE), the number of smart devices connected to the Internet is increasing, resulting in large-scale data, which has caused problems such as bandwidth load, slow response speed, poor security, and poor privacy in traditional cloud computing models. Traditional cloud computing is no longer sufficient to support the diverse needs of today's intelligent society for data processing, so edge computing technologies have emerged. It is a new computing paradigm for performing calculations at the edge of the network. Unlike cloud computing, it emphasizes closer to the user and closer to the source of the data. At the edge of the network, it is lightweight for local, small-scale data storage and processing. This article mainly reviews the related research and results of edge computing. First, it summarizes the concept of edge computing and compares it with cloud computing. Then summarize the architecture of edge computing, keyword technology, security and privacy protection, and finally summarize the applications of edge computing.","['Cloud computing', 'Edge computing', 'Real-time systems', 'Internet of Things', 'Bandwidth', 'Security', 'Data privacy']","['Edge computing', 'cloud computing', 'Internet of Things']"
"Blockchain-based decentralized cryptocurrencies have drawn much attention and been widely-deployed in recent years. Bitcoin, the first application of blockchain, achieves great success and promotes more development in this field. However, Bitcoin encounters performance problems of low throughput and high transaction latency. Other cryptocurrencies based on proof-of-work also inherit the flaws, leading to more concerns about the scalability of blockchain. This paper attempts to cover the existing scaling solutions for blockchain and classify them by level. In addition, we make comparisons between different methods and list some potential directions for solving the scalability problem of blockchain.","['Blockchain', 'Scalability', 'Bitcoin', 'Throughput', 'Measurement']","['Blockchain', 'scalability']"
"In this paper, we review the background and state-of-the-art of the narrow-band Internet of Things (NB-IoT). We first introduce NB-IoT general background, development history, and standardization. Then, we present NB-IoT features through the review of current national and international studies on NB-IoT technology, where we focus on basic theories and key technologies, i.e., connection count analysis theory, delay analysis theory, coverage enhancement mechanism, ultra-low power consumption technology, and coupling relationship between signaling and data. Subsequently, we compare several performances of NB-IoT and other wireless and mobile communication technologies in aspects of latency, security, availability, data transmission rate, energy consumption, spectral efficiency, and coverage area. Moreover, we analyze five intelligent applications of NB-IoT, including smart cities, smart buildings, intelligent environment monitoring, intelligent user services, and smart metering. Finally, we summarize security requirements of NB-IoT, which need to be solved urgently. These discussions aim to provide a comprehensive overview of NB-IoT, which can help readers to understand clearly the scientific problems and future research directions of NB-IoT.","['Couplings', 'Security', '3GPP', 'Long Term Evolution', 'Uplink', 'Internet of Things', 'GSM']","['Intelligent application', 'Internet of Things', 'LPWAN', 'LTE', 'NB-IoT']"
"The purpose of this study was to assess the impact of Artificial Intelligence (AI) on education. Premised on a narrative and framework for assessing AI identified from a preliminary analysis, the scope of the study was limited to the application and effects of AI in administration, instruction, and learning. A qualitative research approach, leveraging the use of literature review as a research design and approach was used and effectively facilitated the realization of the study purpose. Artificial intelligence is a field of study and the resulting innovations and developments that have culminated in computers, machines, and other artifacts having human-like intelligence characterized by cognitive abilities, learning, adaptability, and decision-making capabilities. The study ascertained that AI has extensively been adopted and used in education, particularly by education institutions, in different forms. AI initially took the form of computer and computer related technologies, transitioning to web-based and online intelligent education systems, and ultimately with the use of embedded computer systems, together with other technologies, the use of humanoid robots and web-based chatbots to perform instructors' duties and functions independently or with instructors. Using these platforms, instructors have been able to perform different administrative functions, such as reviewing and grading students' assignments more effectively and efficiently, and achieve higher quality in their teaching activities. On the other hand, because the systems leverage machine learning and adaptability, curriculum and content has been customized and personalized in line with students' needs, which has fostered uptake and retention, thereby improving learners experience and overall quality of learning.","['Education', 'Technological innovation', 'Learning (artificial intelligence)', 'Microcomputers', 'Robots']","['Education', 'artificial intelligence', 'leaner']"
"The upcoming fifth generation (5G) of wireless networks is expected to lay a foundation of intelligent networks with the provision of some isolated artificial intelligence (AI) operations. However, fully intelligent network orchestration and management for providing innovative services will only be realized in Beyond 5G (B5G) networks. To this end, we envisage that the sixth generation (6G) of wireless networks will be driven by on-demand self-reconfiguration to ensure a many-fold increase in the network performance and service types. The increasingly stringent performance requirements of emerging networks may finally trigger the deployment of some interesting new technologies, such as large intelligent surfaces, electromagnetic-orbital angular momentum, visible light communications, and cell-free communications, to name a few. Our vision for 6G is a massively connected complex network capable of rapidly responding to the users' service calls through real-time learning of the network state as described by the network edge (e.g., base-station locations and cache contents), air interface (e.g., radio spectrum and propagation channel), and the user-side (e.g., battery-life and locations). The multi-state, multi-dimensional nature of the network state, requiring the real-time knowledge, can be viewed as a quantum uncertainty problem. In this regard, the emerging paradigms of machine learning (ML), quantum computing (QC), and quantum ML (QML) and their synergies with communication networks can be considered as core 6G enablers. Considering these potentials, starting with the 5G target services and enabling technologies, we provide a comprehensive review of the related state of the art in the domains of ML (including deep learning), QC, and QML and identify their potential benefits, issues, and use cases for their applications in the B5G networks. Subsequently, we propose a novel QC-assisted and QML-based framework for 6G communication networks while articulating its challenges and potential enabling technologies at the network infrastructure, network edge, air interface, and user end. Finally, some promising future research directions for the quantum- and QML-assisted B5G networks are identified and discussed.","['5G mobile communication', 'Communication networks', 'Quantum computing', 'Machine learning', 'Wireless networks', 'Parallel processing', 'Quantum communication']","['6G', 'B5G', 'machine learning', 'quantum communications', 'quantum machine learning']"
"The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.","['Image processing', 'Tomography', 'Data acquisition', 'Image reconstruction', 'Machine learning', 'Deep learning']","['Tomographic imaging', 'medical imaging', 'data acquisition', 'image reconstruction', 'image analysis', 'big data', 'machine learning', 'deep learning']"
"Internet of Things (IoT) is a network of all devices that can be accessed through the Internet. These devices can be remotely accessed and controlled using existing network infrastructure, thus allowing a direct integration of computing systems with the physical world. This also reduces human involvement along with improving accuracy and efficiency, resulting in economic benefit. The devices in IoT facilitate the day-to-day life of people. However, the IoT has an enormous threat to security and privacy due to its heterogeneous and dynamic nature. Authentication is one of the most challenging security requirements in the IoT environment, where a user (external party) can directly access information from the devices, provided the mutual authentication between user and devices happens. In this paper, we present a new signature-based authenticated key establishment scheme for the IoT environment. The proposed scheme is tested for security with the help of the widely used Burrows-Abadi-Needham logic, informal security analysis, and also the formal security verification using the broadly accepted automated validation of Internet security protocols and applications tool. The proposed scheme is also implemented using the widely accepted NS2 simulator, and the simulation results demonstrate the practicability of the scheme. Finally, the proposed scheme provides more functionality features, and its computational and communication costs are also comparable with other existing approaches.","['Authentication', 'Privacy', 'Elliptic curves', 'Protocols', 'Adaptation models', 'Sensors']","['Internet of things (IoT)', 'authentication', 'key establishment', 'Burrows-Abadi-Needham (BAN) logic', 'AVISPA', 'NS2 simulation', 'security']"
"With the development of technologies, such as big data, cloud computing, and the Internet of Things (IoT), digital twin is being applied in industry as a precision simulation technology from concept to practice. Further, simulation plays a very important role in the healthcare field, especially in research on medical pathway planning, medical resource allocation, medical activity prediction, etc. By combining digital twin and healthcare, there will be a new and efficient way to provide more accurate and fast services for elderly healthcare. However, how to achieve personal health management throughout the entire lifecycle of elderly patients, and how to converge the medical physical world and the virtual world to realize real smart healthcare, are still two key challenges in the era of precision medicine. In this paper, a framework of the cloud healthcare system is proposed based on digital twin healthcare (CloudDTH). This is a novel, generalized, and extensible framework in the cloud environment for monitoring, diagnosing and predicting aspects of the health of individuals using, for example, wearable medical devices, toward the goal of personal health management, especially for the elderly. CloudDTH aims to achieve interaction and convergence between medical physical and virtual spaces. Accordingly, a novel concept of digital twin healthcare (DTH) is proposed and discussed, and a DTH model is implemented. Next, a reference framework of CloudDTH based on DTH is constructed, and its key enabling technologies are explored. Finally, the feasibility of some application scenarios and a case study for real-time supervision are demonstrated.","['Medical services', 'Cloud computing', 'Senior citizens', 'Medical diagnostic imaging', 'Real-time systems', 'Computational modeling']","['Digital twin', 'elderly healthcare', 'personal health management', 'cloud computing', 'precision medicine', 'interaction', 'convergence']"
"Google Colaboratory (also known as Colab) is a cloud service based on Jupyter Notebooks for disseminating machine learning education and research. It provides a runtime fully configured for deep learning and free-of-charge access to a robust GPU. This paper presents a detailed analysis of Colaboratory regarding hardware resources, performance, and limitations. This analysis is performed through the use of Colaboratory for accelerating deep learning for computer vision and other GPU-centric applications. The chosen test-cases are a parallel tree-based combinatorial search and two computer vision applications: object detection/classification and object localization/segmentation. The hardware under the accelerated runtime is compared with a mainstream workstation and a robust Linux server equipped with 20 physical cores. Results show that the performance reached using this cloud service is equivalent to the performance of the dedicated testbeds, given similar resources. Thus, this service can be effectively exploited to accelerate not only deep learning but also other classes of GPU-centric applications. For instance, it is faster to train a CNN on Colaboratory's accelerated runtime than using 20 physical cores of a Linux server. The performance of the GPU made available by Colaboratory may be enough for several profiles of researchers and students. However, these free-of-charge hardware resources are far from enough to solve demanding real-world problems and are not scalable. The most significant limitation found is the lack of CPU cores. Finally, several strengths and limitations of this cloud service are discussed, which might be useful for helping potential users.","['Google', 'Machine learning', 'Hardware', 'Graphics processing units', 'Acceleration', 'Runtime', 'Computer vision']","['Deep learning', 'Colab', 'convolutional neural networks', 'Google colaboratory', 'GPU computing']"
"Machine learning (ML) based forecasting mechanisms have proved their significance to anticipate in perioperative outcomes to improve the decision making on the future course of actions. The ML models have long been used in many application domains which needed the identification and prioritization of adverse factors for a threat. Several prediction methods are being popularly used to handle forecasting problems. This study demonstrates the capability of ML models to forecast the number of upcoming patients affected by COVID-19 which is presently considered as a potential threat to mankind. In particular, four standard forecasting models, such as linear regression (LR), least absolute shrinkage and selection operator (LASSO), support vector machine (SVM), and exponential smoothing (ES) have been used in this study to forecast the threatening factors of COVID-19. Three types of predictions are made by each of the models, such as the number of newly infected cases, the number of deaths, and the number of recoveries in the next 10 days. The results produced by the study proves it a promising mechanism to use these methods for the current scenario of the COVID-19 pandemic. The results prove that the ES performs best among all the used models followed by LR and LASSO which performs well in forecasting the new confirmed cases, death rate as well as recovery rate, while SVM performs poorly in all the prediction scenarios given the available dataset.","['Predictive models', 'Forecasting', 'Linear regression', 'Support vector machines', 'Diseases', 'Machine learning', 'Prediction algorithms', 'COVID-19']","['COVID-19', 'exponential smoothing method', 'future forecasting', 'adjusted R² score', 'supervised machine learning']"
"Supporting high mobility in millimeter wave (mmWave) systems enables a wide range of important applications, such as vehicular communications and wireless virtual/augmented reality. Realizing this in practice, though, requires overcoming several challenges. First, the use of narrow beams and the sensitivity of mmWave signals to blockage greatly impact the coverage and reliability of highly-mobile links. Second, highly-mobile users in dense mmWave deployments need to frequently hand-off between base stations (BSs), which is associated with critical control and latency overhead. Furthermore, identifying the optimal beamforming vectors in large antenna array mmWave systems requires considerable training overhead, which significantly affects the efficiency of these mobile systems. In this paper, a novel integrated machine learning and coordinated beamforming solution is developed to overcome these challenges and enable highly-mobile mmWave applications. In the proposed solution, a number of distributed yet coordinating BSs simultaneously serve a mobile user. This user ideally needs to transmit only one uplink training pilot sequence that will be jointly received at the coordinating BSs using omni or quasi-omni beam patterns. These received signals draw a defining signature not only for the user location, but also for its interaction with the surrounding environment. The developed solution then leverages a deep learning model that learns how to use these signatures to predict the beamforming vectors at the BSs. This renders a comprehensive solution that supports highly mobile mmWave applications with reliable coverage, low latency, and negligible training overhead. Extensive simulation results based on accurate ray-tracing, show that the proposed deep-learning coordinated beamforming strategy approaches the achievable rate of the genie-aided solution that knows the optimal beamforming vectors with no training overhead. Compared with traditional beamforming solutions, the results show that the proposed deep learning-based strategy attains higher rates, especially in high-mobility large-array regimes.","['Array signal processing', 'Training', 'Machine learning', 'Channel estimation', 'Radio frequency', 'Sensors', 'Wireless communication']","['Millimeter wave', 'deep learning', 'machine learning', 'beamforming', 'channel estimation', 'vehicular communications', 'wireless virtual/augmented reality']"
"Compressive Sensing (CS) is a new sensing modality, which compresses the signal being acquired at the time of sensing. Signals can have sparse or compressible representation either in original domain or in some transform domain. Relying on the sparsity of the signals, CS allows us to sample the signal at a rate much below the Nyquist sampling rate. Also, the varied reconstruction algorithms of CS can faithfully reconstruct the original signal back from fewer compressive measurements. This fact has stimulated research interest toward the use of CS in several fields, such as magnetic resonance imaging, high-speed video acquisition, and ultrawideband communication. This paper reviews the basic theoretical concepts underlying CS. To bridge the gap between theory and practicality of CS, different CS acquisition strategies and reconstruction approaches are elaborated systematically in this paper. The major application areas where CS is currently being used are reviewed here. This paper also highlights some of the challenges and research directions in this field.","['Sensors', 'Transforms', 'Mathematical model', 'Sparse matrices', 'Compressed sensing', 'Reconstruction algorithms', 'Image reconstruction']","['Compressive sensing', 'sparsity', 'CS acquisition strategies', 'random demodulator', 'CS reconstruction algorithms', 'OMP', 'CS applications']"
"Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper, we show that the outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a BadNet) that has the state-of-the-art performance on the user's training and validation samples but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our U.S. street sign detector can persist even if the network is later retrained for another task and cause a drop in an accuracy of 25% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and-because the behavior of neural networks is difficult to explicate-stealthy. This paper provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.","['Training', 'Machine learning', 'Perturbation methods', 'Computational modeling', 'Biological neural networks', 'Security']","['Computer security', 'machine learning', 'neural networks']"
"Recent years have witnessed a paradigm shift in the storage of Electronic Health Records (EHRs) on mobile cloud environments, where mobile devices are integrated with cloud computing to facilitate medical data exchanges among patients and healthcare providers. This advanced model enables healthcare services with low operational cost, high flexibility, and EHRs availability. However, this new paradigm also raises concerns about data privacy and network security for e-health systems. How to reliably share EHRs among mobile users while guaranteeing high-security levels in the mobile cloud is a challenging issue. In this paper, we propose a novel EHRs sharing framework that combines blockchain and the decentralized interplanetary file system (IPFS) on a mobile cloud platform. Particularly, we design a trustworthy access control mechanism using smart contracts to achieve secure EHRs sharing among different patients and medical providers. We present a prototype implementation using Ethereum blockchain in a real data sharing scenario on a mobile app with Amazon cloud computing. The empirical results show that our proposal provides an effective solution for reliable data exchanges on mobile clouds while preserving sensitive health information against potential threats. The system evaluation and security analysis also demonstrate the performance improvements in lightweight access control design, minimum network latency with high security and data privacy levels, compared to the existing data sharing models.","['Blockchain', 'Cloud computing', 'Access control', 'Smart contracts', 'Medical services', 'Data privacy']","['Electronic health records (EHRs)', 'EHRs sharing', 'mobile cloud computing (MCC)', 'Internet of Medical Things (IoMT)', 'blockchain', 'smart contracts', 'access control', 'privacy', 'security']"
"In this paper, we propose a new metric to measure goodness-of-fit for classifiers: the Real World Cost function. This metric factors in information about a real world problem, such as financial impact, that other measures like accuracy or F1 do not. This metric is also more directly interpretable for users. To optimize for this metric, we introduce the Real-World-Weight Cross-Entropy loss function, in both binary classification and single-label multiclass classification variants. Both variants allow direct input of real world costs as weights. For single-label, multiclass classification, our loss function also allows direct penalization of probabilistic false positives, weighted by label, during the training of a machine learning model. We compare the design of our loss function to the binary cross-entropy and categorical cross-entropy functions, as well as their weighted variants, to discuss the potential for improvement in handling a variety of known shortcomings of machine learning, ranging from imbalanced classes to medical diagnostic error to reinforcement of social bias. We create scenarios that emulate those issues using the MNIST data set and demonstrate empirical results of our new loss function. Finally, we discuss our intuition about why this approach works and sketch a proof based on Maximum Likelihood Estimation.","['Training', 'Neural networks', 'Machine learning', 'Probabilistic logic', 'Measurement', 'Maximum likelihood estimation', 'Standards']","['Machine learning', 'class imbalance', 'oversampling', 'undersampling', 'ethnic stereotypes', 'social bias', 'maximum likelihood estimation', 'cross-entropy', 'softmax']"
"A hybrid antenna is proposed for future 4G/5G multiple input multiple output (MIMO) applications. The proposed antenna is composed of two antenna modules, namely, 4G antenna module and 5G antenna module. The 4G antenna module is a two-antenna array capable of covering the GSM850/900/1800/1900, UMTS2100, and LTE2300/2500 operating bands, while the 5G antenna module is an eight-antenna array operating in the 3.5-GHz band capable of covering the C-band (3400-3600 MHz), which could meet the demand of future 5G application. Compared with ideal uncorrelated antennas in an 8 × 8 MIMO system, the 5G antenna module has shown good ergodic channel capacity of ~40 b/s/Hz, which is only 6 b/s/Hz lower than ideal case. This multi-mode hybrid antenna is fabricated, and typically, experimental results such as S-parameter, antenna efficiency, radiation pattern, and envelope correlation coefficient are presented.","['MIMO', 'Channel capacity', '5G mobile communication', 'Smart phones', 'Antenna arrays', 'Antenna radiation', 'Channel capacity', 'Envelope correlation coefficient', 'Radiation patterns']","['MIMO', 'multi-antenna', 'multi-mode', 'channel capacity', '5G application']"
"Clustering is a fundamental problem in many data-driven application domains, and clustering performance highly depends on the quality of data representation. Hence, linear or non-linear feature transformations have been extensively used to learn a better data representation for clustering. In recent years, a lot of works focused on using deep neural networks to learn a clustering-friendly representation, resulting in a significant increase of clustering performance. In this paper, we give a systematic survey of clustering with deep learning in views of architecture. Specifically, we first introduce the preliminary knowledge for better understanding of this field. Then, a taxonomy of clustering with deep learning is proposed and some representative methods are introduced. Finally, we propose some interesting future opportunities of clustering with deep learning and give some conclusion remarks.","['Clustering methods', 'Machine learning', 'Clustering algorithms', 'Network architecture', 'Neural networks', 'Neurons', 'Gallium nitride']","['Clustering', 'deep learning', 'data representation', 'network architecture']"
"This paper aims to understand, identify, and mitigate the impacts of residential electric vehicle (EV) charging on distribution system voltages. A thorough literature review on the impacts of residential EV charging is presented, followed by a proposed method for evaluating the impacts of EV loads on the distribution system voltage quality. Practical solutions to mitigate EV load impacts are discussed as well, including infrastructural changes and indirect controlled charging with time-of-use (TOU) pricing. An optimal TOU schedule is also presented, with the aim of maximizing both customer and utility benefits. This paper also presents a discussion on implementing smart charging algorithms to directly control EV charging rates and EV charging starting times. Finally, a controlled charging algorithm is proposed to improve the voltage quality at the EV load locations while avoiding customer inconvenience. The proposed method significantly decreases the impacts of EV load charging on system peak load demand and feeder voltages.","['IEEE Standards', 'Electric vehicles', 'Pricing', 'Power system quality', 'Voltage control', 'Dynamic programming', 'Controlled charging']","['Electric vehicle charging', 'TOU pricing', 'controlled charging', 'power quality', 'voltage quality', 'distribution system', 'dynamic programming']"
"A binary version of the hybrid grey wolf optimization (GWO) and particle swarm optimization (PSO) is proposed to solve feature selection problems in this paper. The original PSOGWO is a new hybrid optimization algorithm that benefits from the strengths of both GWO and PSO. Despite the superior performance, the original hybrid approach is appropriate for problems with a continuous search space. Feature selection, however, is a binary problem. Therefore, a binary version of hybrid PSOGWO called BGWOPSO is proposed to find the best feature subset. To find the best solutions, the wrapper-based method K-nearest neighbors classifier with Euclidean separation matric is utilized. For performance evaluation of the proposed binary algorithm, 18 standard benchmark datasets from UCI repository are employed. The results show that BGWOPSO significantly outperformed the binary GWO (BGWO), the binary PSO, the binary genetic algorithm, and the whale optimization algorithm with simulated annealing when using several performance measures including accuracy, selecting the best optimal features, and the computational time.","['Feature extraction', 'Optimization', 'Classification algorithms', 'Particle swarm optimization', 'Search problems', 'Genetic algorithms', 'Data mining']","['Feature selection', 'hybrid binary optimization', 'grey wolf optimization', 'particle swarm optimization', 'classification']"
"Detecting COVID-19 early may help in devising an appropriate treatment plan and disease containment decisions. In this study, we demonstrate how transfer learning from deep learning models can be used to perform COVID-19 detection using images from three most commonly used medical imaging modes X-Ray, Ultrasound, and CT scan. The aim is to provide over-stressed medical professionals a second pair of eyes through intelligent deep learning image classification models. We identify a suitable Convolutional Neural Network (CNN) model through initial comparative study of several popular CNN models. We then optimize the selected VGG19 model for the image modalities to show how the models can be used for the highly scarce and challenging COVID-19 datasets. We highlight the challenges (including dataset size and quality) in utilizing current publicly available COVID-19 datasets for developing useful deep learning models and how it adversely impacts the trainability of complex models. We also propose an image pre-processing stage to create a trustworthy image dataset for developing and testing the deep learning models. The new approach is aimed to reduce unwanted noise from the images so that deep learning models can focus on detecting diseases with specific features from them. Our results indicate that Ultrasound images provide superior detection accuracy compared to X-Ray and CT scans. The experimental results highlight that with limited data, most of the deeper networks struggle to train well and provides less consistency over the three imaging modes we are using. The selected VGG19 model, which is then extensively tuned with appropriate parameters, performs in considerable levels of COVID-19 detection against pneumonia or normal for all three lung image modes with the precision of up to 86% for X-Ray, 100% for Ultrasound and 84% for CT scans.","['Computed tomography', 'Lung', 'X-ray imaging', 'Ultrasonic imaging', 'Machine learning', 'Diseases']","['COVID-19 detection', 'image processing', 'model comparison', 'CNN models', 'X-ray', 'ultrasound and CT based detection']"
"In this survey paper, we systematically summarize existing literature on bearing fault diagnostics with deep learning (DL) algorithms. While conventional machine learning (ML) methods, including artificial neural network, principal component analysis, support vector machines, etc., have been successfully applied to the detection and categorization of bearing faults for decades, recent developments in DL algorithms in the last five years have sparked renewed interest in both industry and academia for intelligent machine health monitoring. In this paper, we first provide a brief review of conventional ML methods, before taking a deep dive into the state-of-the-art DL algorithms for bearing fault applications. Specifically, the superiority of DL based methods are analyzed in terms of fault feature extraction and classification performances; many new functionalities enabled by DL techniques are also summarized. In addition, to obtain a more intuitive insight, a comparative study is conducted on the classification accuracy of different algorithms utilizing the open source Case Western Reserve University (CWRU) bearing dataset. Finally, to facilitate the transition on applying various DL algorithms to bearing fault diagnostics, detailed recommendations and suggestions are provided for specific application conditions. Future research directions to further enhance the performance of DL algorithms on health monitoring are also discussed.","['Induction motors', 'Vibrations', 'Sensors', 'Stators', 'Fault diagnosis', 'Machine learning algorithms', 'Monitoring']","['Bearing fault', 'deep learning', 'diagnostics', 'feature extraction', 'machine learning']"
"Various new national advanced manufacturing strategies, such as Industry 4.0, Industrial Internet, and Made in China 2025, are issued to achieve smart manufacturing, resulting in the increasing number of newly designed production lines in both developed and developing countries. Under the individualized designing demands, more realistic virtual models mirroring the real worlds of production lines are essential to bridge the gap between design and operation. This paper presents a digital twin-based approach for rapid individualized designing of the hollow glass production line. The digital twin merges physics-based system modeling and distributed real-time process data to generate an authoritative digital design of the system at pre-production phase. A digital twin-based analytical decoupling framework is also developed to provide engineering analysis capabilities and support the decision-making over the system designing and solution evaluation. Three key enabling techniques as well as a case study in hollow glass production line are addressed to validate the proposed approach.","['Optimization', 'Data models', 'Couplings', 'Glass', 'Conferences']","['Digital twin', 'individualized designing', 'mass individualization', 'multi-view synchronization', 'semi-physical simulation']"
"In this paper, we introduce an intelligent reflecting surface (IRS) to provide a programmable wireless environment for physical layer security. By adjusting the reflecting coefficients, the IRS can change the attenuation and scattering of the incident electromagnetic wave so that it can propagate in the desired way toward the intended receiver. Specifically, we consider a downlink multiple-input single-output (MISO) broadcast system, where the base station (BS) transmits independent data streams to multiple legitimate receivers and keeps them secret from multiple eavesdroppers. By jointly optimizing the beamformers at the BS and reflecting coefficients at the IRS, we formulate a minimum-secrecy-rate maximization problem under various practical constraints on the reflecting coefficients. The constraints capture the scenarios of both continuous and discrete reflecting coefficients of the reflecting elements. Due to the non-convexity of the formulated problem, we propose an efficient algorithm based on the alternating optimization and the path-following algorithm to solve it in an iterative manner. Besides, we show that the proposed algorithm can converge to a local (global) optimum. Furthermore, we develop two suboptimal algorithms with some forms of closed-form solutions to reduce computational complexity. Finally, the simulation results validate the advantages of the introduced IRS and the effectiveness of the proposed algorithms.","['Receivers', 'Signal to noise ratio', 'Wireless communication', 'MISO communication', 'Communication system security', 'Downlink', 'Optimization']","['Intelligent reflecting surface', 'programmable wireless environment', 'physical layer security', 'beamforming']"
"This paper considers a downlink cloud radio access network (C-RAN) in which all the base-stations (BSs) are connected to a central computing cloud via digital backhaul links with finite capacities. Each user is associated with a user-centric cluster of BSs; the central processor shares the user's data with the BSs in the cluster, which then cooperatively serve the user through joint beamforming. Under this setup, this paper investigates the user scheduling, BS clustering, and beamforming design problem from a network utility maximization perspective. Differing from previous works, this paper explicitly considers the per-BS backhaul capacity constraints. We formulate the network utility maximization problem for the downlink C-RAN under two different models depending on whether the BS clustering for each user is dynamic or static over different user scheduling time slots. In the former case, the user-centric BS cluster is dynamically optimized for each scheduled user along with the beamforming vector in each time-frequency slot, whereas in the latter case, the user-centric BS cluster is fixed for each user and we jointly optimize the user scheduling and the beamforming vector to account for the backhaul constraints. In both cases, the nonconvex per-BS backhaul constraints are approximated using the reweighted ℓ 1 -norm technique. This approximation allows us to reformulate the per-BS backhaul constraints into weighted per-BS power constraints and solve the weighted sum rate maximization problem through a generalized weighted minimum mean square error approach. This paper shows that the proposed dynamic clustering algorithm can achieve significant performance gain over existing naive clustering schemes. This paper also proposes two heuristic static clustering schemes that can already achieve a substantial portion of the gain.","['Array signal processing', 'Dynamic scheduling', 'Radio access networks', 'Downlink', 'Heuristic algorithms', 'Approximation methods', 'Cloud computing']","['Cloud radio access network (C-RAN)', 'network multiple-input multiple-output (MIMO)', 'multi-point (CoMP)', 'limited backhaul', 'user scheduling', 'base station clustering', 'beamforming', 'weighted sum rate', 'weighted minimum mean square error (WMMSE)']"
"The classification of electrocardiogram (ECG) signals is very important for the automatic diagnosis of heart disease. Traditionally, it is divided into two steps, including the step of feature extraction and the step of pattern classification. Owing to recent advances in artificial intelligence, it has been demonstrated that deep neural network, which trained on a huge amount of data, can carry out the task of feature extraction directly from the data and recognize cardiac arrhythmias better than professional cardiologists. This paper proposes an ECG arrhythmia classification method using two-dimensional (2D) deep convolutional neural network (CNN). The time domain signals of ECG, belonging to five heart beat types including normal beat (NOR), left bundle branch block beat (LBB), right bundle branch block beat (RBB), premature ventricular contraction beat (PVC), and atrial premature contraction beat (APC), were first transformed into time-frequency spectrograms by short-time Fourier transform. Subsequently, the spectrograms of the five arrhythmia types were utilized as input to the 2D-CNN such that the ECG arrhythmia types were identified and classified finally. Using ECG recordings from the MIT-BIH arrhythmia database as the training and testing data, the classification results show that the proposed 2D-CNN model can reach an averaged accuracy of 99.00%. On the other hand, in order to achieve optimal classification performances, the model parameter optimization was investigated. It was found when the learning rate is 0.001 and the batch size parameter is 2500, the classifier achieved the highest accuracy and the lowest loss. We also compared the proposed 2D-CNN model with a conventional one-dimensional CNN model. Comparison results show that the 1D-CNN classifier can achieve an averaged accuracy of 90.93%. Therefore, it is validated that the proposed CNN classifier using ECG spectrograms as input can achieve improved classification accuracy without additional manual pre-processing of the ECG signals.","['Electrocardiography', 'Feature extraction', 'Spectrogram', 'Convolutional neural networks', 'Heart beat', 'Diseases', 'Time-frequency analysis']","['Electrocardiogram (ECG)', 'arrhythmia classification', 'short-time Fourier transform (STFT)', 'convolutional neural network', 'model parameter optimization']"
"Prognostics and health management is an emerging discipline to scientifically manage the health condition of engineering systems and their critical components. It mainly consists of three main aspects: construction of health indicators, remaining useful life prediction, and health management. Construction of health indicators aims to evaluate the system's current health condition and its critical components. Given the observations of a health indicator, prediction of the remaining useful life is used to infer the time when an engineering systems or a critical component will no longer perform its intended function. Health management involves planning the optimal maintenance schedule according to the system's current and future health condition, its critical components and the replacement costs. Construction of health indicators is the key to predicting the remaining useful life. Bearings and gears are the most common mechanical components in rotating machines, and their health conditions are of great concern in practice. Because it is difficult to measure and quantify the health conditions of bearings and gears in many cases, numerous vibration-based methods have been proposed to construct bearing and gear health indicators. This paper presents a thorough review of vibration-based bearing and gear health indicators constructed from mechanical signal processing, modeling, and machine learning. This review paper will be helpful for designing further advanced bearing and gear health indicators and provides a basis for predicting the remaining useful life of bearings and gears. Most of the bearing and gear health indicators reviewed in this paper are highly relevant to simulated and experimental run-to-failure data rather than artificially seeded bearing and gear fault data. Finally, some problems in the literature are highlighted and areas for future study are identified.","['Gears', 'Vibrations', 'Degradation', 'Predictive models', 'Root mean square', 'Prognostics and health management', 'Signal processing']","['Ball bearings', 'condition monitoring', 'feature extraction', 'gears', 'prognostics and health management', 'signal processing algorithms', 'vibrations']"
"With the rapid development of Internet technology and social networks, a large number of comment texts are generated on the Web. In the era of big data, mining the emotional tendency of comments through artificial intelligence technology is helpful for the timely understanding of network public opinion. The technology of sentiment analysis is a part of artificial intelligence, and its research is very meaningful for obtaining the sentiment trend of the comments. The essence of sentiment analysis is the text classification task, and different words have different contributions to classification. In the current sentiment analysis studies, distributed word representation is mostly used. However, distributed word representation only considers the semantic information of word, but ignore the sentiment information of the word. In this paper, an improved word representation method is proposed, which integrates the contribution of sentiment information into the traditional TF-IDF algorithm and generates weighted word vectors. The weighted word vectors are input into bidirectional long short term memory (BiLSTM) to capture the context information effectively, and the comment vectors are better represented. The sentiment tendency of the comment is obtained by feedforward neural network classifier. Under the same conditions, the proposed sentiment analysis method is compared with the sentiment analysis methods of RNN, CNN, LSTM, and NB. The experimental results show that the proposed sentiment analysis method has higher precision, recall, and F1 score. The method is proved to be effective with high accuracy on comments.","['Sentiment analysis', 'Dictionaries', 'Neural networks', 'Semantics', 'Data mining', 'Internet', 'Task analysis']","['Sentiment analysis', 'artificial intelligence', 'social network', 'weighted word vectors', 'BiLSTM']"
"While 5G is being commercialized worldwide, research institutions around the world have started to look beyond 5G and 6G is expected to evolve into green networks, which deliver high Quality of Service and energy efficiency. To meet the demands of future applications, significant improvements need to be made in mobile network architecture. We envision 6G undergoing unprecedented breakthrough and integrating traditional terrestrial mobile networks with emerging space, aerial and underwater networks to provide anytime anywhere network access. This paper presents a detailed survey on wireless evolution towards 6G networks. In this survey, the prime focus is on the new architectural changes associated with 6G networks, characterized by ubiquitous 3D coverage, introduction of pervasive AI and enhanced network protocol stack. Along with this, we discuss related potential technologies that are helpful in forming sustainable and socially seamless networks, encompassing terahertz and visible light communication, new communication paradigm, blockchain and symbiotic radio. Our work aims to provide enlightening guidance for subsequent research of green 6G.","['5G mobile communication', 'Satellite broadcasting', 'Low earth orbit satellites', 'Wireless networks']","['6G', 'architecture', 'green networks', 'VLC', 'blockchain']"
"Network intrusion detection systems (NIDSs) provide a better solution to network security than other traditional network defense technologies, such as firewall systems. The success of NIDS is highly dependent on the performance of the algorithms and improvement methods used to increase the classification accuracy and decrease the training and testing times of the algorithms. We propose an effective deep learning approach, self-taught learning (STL)-IDS, based on the STL framework. The proposed approach is used for feature learning and dimensionality reduction. It reduces training and testing time considerably and effectively improves the prediction accuracy of support vector machines (SVM) with regard to attacks. The proposed model is built using the sparse autoencoder mechanism, which is an effective learning algorithm for reconstructing a new feature representation in an unsupervised manner. After the pre-training stage, the new features are fed into the SVM algorithm to improve its detection capability for intrusion and classification accuracy. Moreover, the efficiency of the approach in binary and multiclass classification is studied and compared with that of shallow classification methods, such as J48, naive Bayesian, random forest, and SVM. Results show that our approach has accelerated SVM training and testing times and performed better than most of the previous approaches in terms of performance metrics in binary and multiclass classification. The proposed STL-IDS approach improves network intrusion detection and provides a new research method for intrusion detection.","['Support vector machines', 'Machine learning', 'Intrusion detection', 'Feature extraction', 'Training', 'Computational modeling', 'Classification algorithms']","['Network security', 'network intrusion detection system', 'deep learning', 'sparse autoencoder', 'SVM', 'self-taught learning', 'NSL-KDD']"
"A centralized infrastructure system carries out existing data analytics and decision-making processes from our current highly virtualized platform of wireless networks and the Internet of Things (IoT) applications. There is a high possibility that these existing methods will encounter more challenges and issues in relation to network dynamics, resulting in a high overhead in the network response time, leading to latency and traffic. In order to avoid these problems in the network and achieve an optimum level of resource utilization, a new paradigm called edge computing (EC) is proposed to pave the way for the evolution of new age applications and services. With the integration of EC, the processing capabilities are pushed to the edge of network devices such as smart phones, sensor nodes, wearables, and on-board units, where data analytics and knowledge generation are performed which removes the necessity for a centralized system. Many IoT applications, such as smart cities, the smart grid, smart traffic lights, and smart vehicles, are rapidly upgrading their applications with EC, significantly improving response time as well as conserving network resources. Irrespective of the fact that EC shifts the workload from a centralized cloud to the edge, the analogy between EC and the cloud pertaining to factors such as resource management and computation optimization are still open to research studies. Hence, this paper aims to validate the efficiency and resourcefulness of EC. We extensively survey the edge systems and present a comparative study of cloud computing systems. After analyzing the different network properties in the system, the results show that EC systems perform better than cloud computing systems. Finally, the research challenges in implementing an EC system and future research directions are discussed.","['Cloud computing', 'Computer architecture', 'Edge computing', 'Ad hoc networks', 'Servers', 'Mobile computing', 'Resource management']","['IoT', 'cloud computing', 'edge computing', 'fog computing', 'multi-cloud']"
"The next-generation wireless networks are evolving into very complex systems because of the very diversified service requirements, heterogeneity in applications, devices, and networks. The network operators need to make the best use of the available resources, for example, power, spectrum, as well as infrastructures. Traditional networking approaches, i.e., reactive, centrally-managed, one-size-fits-all approaches, and conventional data analysis tools that have limited capability (space and time) are not competent anymore and cannot satisfy and serve that future complex networks regarding operation and optimization cost effectively. A novel paradigm of proactive, self-aware, self-adaptive, and predictive networking is much needed. The network operators have access to large amounts of data, especially from the network and the subscribers. Systematic exploitation of the big data dramatically helps in making the system smart, intelligent, and facilitates efficient as well as cost-effective operation and optimization. We envision data-driven next-generation wireless networks, where the network operators employ advanced data analytics, machine learning (ML), and artificial intelligence. We discuss the data sources and strong drivers for the adoption of the data analytics, and the role of ML, artificial intelligence in making the system intelligent regarding being self-aware, self-adaptive, proactive and prescriptive. A set of network design and optimization schemes are presented concerning data analytics. This paper concludes with a discussion of challenges and the benefits of adopting big data analytics, ML, and artificial intelligence in the next-generation communication systems.","['Big Data', 'Next generation networking', 'Optimization', 'Machine learning', 'Data analysis', 'Tools', 'Wireless communication']","['Big data analytics', 'machine learning', 'artificial intelligence', 'next-generation wireless']"
"Clean and environment-friendly energy harvesting are of prime interest today as it is one of the key enablers in achieving the Sustainable Development Goals (SDGs) as well as accelerates social progress and enhances living standards. India, the second-most populous nation with a population of 1.353 billion, is one of the largest consumers of fossil fuels in the world which is responsible for global warming. An ever-increasing population is projected until 2050, and consequently, the energy demand in the upcoming decades will be co-accelerated by the rapid industrial growth. The Ministry of New and Renewable Energy (MNRE) with the support of National Institution for Transforming India (NITI) Aayog is working to achieve the Indian Government's target of attaining 175 GW through renewable energy resources. Many Indian states are currently increasing their renewable energy capacity in an objective to meet future energy demand. The review paper discusses in-depth about the three Indian states, namely Karnataka, Gujarat, Tamil Nadu, which pioneers the renewable energy production in India. The global energy scenario was discussed in detail with Indian contrast. Further, the barriers to the development of renewable energy generation and policies of the Indian government are discussed in detail to promote renewable energy generation throughout India as well as globally since the challenges are similar for other nations. This study analyzed various prospects of the country in renewable energy which has been done in a purpose to help the scholars, researchers, and policymakers of the nation, as it gives an insight into the present renewable energy scenario of the country.","['Renewable energy sources', 'Production', 'Global warming', 'Fossil fuels', 'Meteorology', 'Electric potential', 'Sociology']","['Renewable energy potential', 'global energy scenario', 'Energy policy in India', 'renewable energy barriers', 'prospects of renewables in India', 'renewable energy in India']"
"Traditional power grids are being transformed into smart grids (SGs) to address the issues in the existing power system due to uni-directional information flow, energy wastage, growing energy demand, reliability, and security. SGs offer bi-directional energy flow between service providers and consumers, involving power generation, transmission, distribution, and utilization systems. SGs employ various devices for the monitoring, analysis, and control of the grid, deployed at power plants, distribution centers, and in consumers’ premises in a very large number. Hence, an SG requires connectivity, automation, and the tracking of such devices. This is achieved with the help of the Internet of Things (IoT). The IoT helps SG systems to support various network functions throughout the generation, transmission, distribution, and consumption of energy by incorporating the IoT devices (such as sensors, actuators, and smart meters), as well as by providing the connectivity, automation, and tracking for such devices. In this paper, we provide a comprehensive survey on the IoT-aided SG systems, which includes the existing architectures, applications, and prototypes of the IoT-aided SG systems. This survey also highlights the open issues, challenges, and future research directions for the IoT-aided SG systems.","['Internet of Things', 'Security', 'Power generation', 'Power grids', 'Monitoring']","['Home area network (HAN)', 'Internet of Things (IoT)', 'neighborhood area network (NAN)', 'smart grid (SG)', 'wide area network (WAN)']"
"In this paper, we investigate the feasibility of recognizing human hand gestures using micro-Doppler signatures measured by Doppler radar with a deep convolutional neural network (DCNN). Hand gesture recognition using radar can be applied to control electronic appliances. Compared with an optical recognition system, radar can work regardless of light conditions and it can be embedded in a case. We classify ten different hand gestures, with only micro-Doppler signatures on spectrograms without range information. The ten gestures, which included swiping from left to right, swiping from right to left, rotating clockwise, rotating counterclockwise, pushing, double pushing, holding, and double holding, were measured using Doppler radar and their spectrograms investigated. A DCNN was employed to classify the spectrograms, with 90% of the data utilized for training and the remaining 10% for validation. After five-fold validation, the classification accuracy of the proposed method was found to be 85.6%. With seven gestures, the accuracy increased to 93.1%.","['Doppler radar', 'Spectrogram', 'Laser radar', 'Neural networks', 'Gesture recognition', 'Spectrograms']","['Hand gesture', 'micro-Doppler signatures', 'Doppler radar', 'deep convolutional neural networks']"
"In recent decades, we have witnessed the evolution of information technologies from the development of VLSI technologies, to communication and networking infrastructure, to the standardization of multimedia compression and coding schemes, to effective multimedia content search and retrieval. As a result, multimedia devices and digital content have become ubiquitous. This path of technological evolution has naturally led to a critical issue that must be addressed next, namely, to ensure that content, devices, and intellectual property are being used by authorized users for legitimate purposes, and to be able to forensically prove with high confidence when otherwise. When security is compromised, intellectual rights are violated, or authenticity is forged, forensic methodologies and tools are employed to reconstruct what has happened to digital content in order to answer who has done what, when, where, and how. The goal of this paper is to provide an overview on what has been done over the last decade in the new and emerging field of information forensics regarding theories, methodologies, state-of-the-art techniques, major applications, and to provide an outlook of the future.","['Forensics', 'Forgery', 'Multimedia communication', 'Feature extraction', 'Fingerprint recognition', 'Computer Security', 'Information processing', 'Digital forensics', 'Very large scale integration', 'Data privacy', 'Intellectual property']","['Anti-forensics', 'information forensics', 'multimedia fingerprint', 'tampering detection']"
"Diabetic Retinopathy (DR) is an ophthalmic disease that damages retinal blood vessels. DR causes impaired vision and may even lead to blindness if it is not diagnosed in early stages. DR has five stages or classes, namely normal, mild, moderate, severe and PDR (Proliferative Diabetic Retinopathy). Normally, highly trained experts examine the colored fundus images to diagnose this fatal disease. This manual diagnosis of this condition (by clinicians) is tedious and error-prone. Therefore, various computer vision-based techniques have been proposed to automatically detect DR and its different stages from retina images. However, these methods are unable to encode the underlying complicated features and can only classify DR’s different stages with very low accuracy particularly, for the early stages. In this research, we used the publicly available Kaggle dataset of retina images to train an ensemble of five deep Convolution Neural Network (CNN) models (Resnet50, Inceptionv3, Xception, Dense121, Dense169) to encode the rich features and improve the classification for different stages of DR. The experimental results show that the proposed model detects all the stages of DR unlike the current methods and performs better compared to state-of-the-art methods on the same Kaggle dataset.","['Diabetes', 'Retinopathy', 'Training', 'Predictive models', 'Retina', 'Biomedical imaging']","['CNN', 'diabetic retinopathy', 'deep learning', 'ensemble model', 'fundus images', 'medical image analysis']"
"Emotion recognition from speech signals is an important but challenging component of Human-Computer Interaction (HCI). In the literature of speech emotion recognition (SER), many techniques have been utilized to extract emotions from signals, including many well-established speech analysis and classification techniques. Deep Learning techniques have been recently proposed as an alternative to traditional techniques in SER. This paper presents an overview of Deep Learning techniques and discusses some recent literature where these methods are utilized for speech-based emotion recognition. The review covers databases used, emotions extracted, contributions made toward speech emotion recognition and limitations related to it.","['Databases', 'Emotion recognition', 'Feature extraction', 'Speech recognition', 'Deep learning', 'Human computer interaction', 'Hidden Markov models']","['Speech emotion recognition', 'deep learning', 'deep neural network', 'deep Boltzmann machine', 'recurrent neural network', 'deep belief network', 'convolutional neural network']"
"The main vision of the Internet of Things (IoT) is to equip real-life physical objects with computing and communication power so that they can interact with each other for the social good. As one of the key members of IoT, Internet of Vehicles (IoV) has seen steep advancement in communication technologies. Now, vehicles can easily exchange safety, efficiency, infotainment, and comfort-related information with other vehicles and infrastructures using vehicular ad hoc networks (VANETs). We leverage on the cloud-based VANETs theme to propose cyber-physical architecture for the Social IoV (SIoV). SIoV is a vehicular instance of the Social IoT (SIoT), where vehicles are the key social entities in the machine-to-machine vehicular social networks. We have identified the social structures of SIoV components, their relationships, and the interaction types. We have mapped VANETs components into IoT-A architecture reference model to offer better integration of SIoV with other IoT domains. We also present a communication message structure based on automotive ontologies, the SAE J2735 message set, and the advanced traveler information system events schema that corresponds to the social graph. Finally, we provide the implementation details and the experimental analysis to demonstrate the efficacy of the proposed system as well as include different application scenarios for various user groups.","['Internet of things', 'Computer architecture', 'Ad hoc networks', 'Social network services', 'Vehicles', 'Intelligent vehicles']","['Social Network of Vehicles', 'Cyber-Physical Systems', 'Internet of Things', 'Internet of Vehicles', 'IoT Architecture Reference Model', 'Intelligent Transport Systems', 'SAE J2735']"
"Key generation from the randomness of wireless channels is a promising alternative to public key cryptography for the establishment of cryptographic keys between any two users. This paper reviews the current techniques for wireless key generation. The principles, performance metrics and key generation procedure are comprehensively surveyed. Methods for optimizing the performance of key generation are also discussed. Key generation applications in various environments are then introduced along with the challenges of applying the approach in each scenario. The paper concludes with some suggestions for future studies.","['Random processes', 'Physical layer', 'Network security', 'Wireless communication', 'Key generation', 'Cryptography']","['Physical layer security', 'key generation', 'wireless communication']"
"Electronic Health Records (EHRs) are entirely controlled by hospitals instead of patients, which complicates seeking medical advices from different hospitals. Patients face a critical need to focus on the details of their own healthcare and restore management of their own medical data. The rapid development of blockchain technology promotes population healthcare, including medical records as well as patient-related data. This technology provides patients with comprehensive, immutable records, and access to EHRs free from service providers and treatment websites. In this paper, to guarantee the validity of EHRs encapsulated in blockchain, we present an attribute-based signature scheme with multiple authorities, in which a patient endorses a message according to the attribute while disclosing no information other than the evidence that he has attested to it. Furthermore, there are multiple authorities without a trusted single or central one to generate and distribute public/private keys of the patient, which avoids the escrow problem and conforms to the mode of distributed data storage in the blockchain. By sharing the secret pseudorandom function seeds among authorities, this protocol resists collusion attack out of N from N -1 corrupted authorities. Under the assumption of the computational bilinear Diffie-Hellman, we also formally demonstrate that, in terms of the unforgeability and perfect privacy of the attribute-signer, this attribute-based signature scheme is secure in the random oracle model. The comparison shows the efficiency and properties between the proposed method and methods proposed in other studies.","['Privacy', 'Security', 'Hospitals', 'Electronic medical records', 'Standards']","['Attribute-based signature (ABS)', 'blockchain', 'electronic health records (EHRs)', 'multiple authorities', 'preserve privacy']"
"Twitter sentiment analysis technology provides the methods to survey public emotion about the events or products related to them. Most of the current researches are focusing on obtaining sentiment features by analyzing lexical and syntactic features. These features are expressed explicitly through sentiment words, emoticons, exclamation marks, and so on. In this paper, we introduce a word embeddings method obtained by unsupervised learning based on large twitter corpora, this method using latent contextual semantic relationships and co-occurrence statistical characteristics between words in tweets. These word embeddings are combined with n-grams features and word sentiment polarity score features to form a sentiment feature set of tweets. The feature set is integrated into a deep convolution neural network for training and predicting sentiment classification labels. We experimentally compare the performance of our model with the baseline model that is a word n-grams model on five Twitter data sets, the results indicate that our model performs better on the accuracy and F1-measure for twitter sentiment classification.","['Twitter', 'Neural networks', 'Convolution', 'Sentiment analysis', 'Terminology', 'Semantics']","['Twitter', 'sentiment analysis', 'word embeddings', 'convolution neural network']"
"Presently, educational institutions compile and store huge volumes of data, such as student enrolment and attendance records, as well as their examination results. Mining such data yields stimulating information that serves its handlers well. Rapid growth in educational data points to the fact that distilling massive amounts of data requires a more sophisticated set of algorithms. This issue led to the emergence of the field of educational data mining (EDM). Traditional data mining algorithms cannot be directly applied to educational problems, as they may have a specific objective and function. This implies that a preprocessing algorithm has to be enforced first and only then some specific data mining methods can be applied to the problems. One such preprocessing algorithm in EDM is clustering. Many studies on EDM have focused on the application of various data mining algorithms to educational attributes. Therefore, this paper provides over three decades long (1983-2016) systematic literature review on clustering algorithm and its applicability and usability in the context of EDM. Future insights are outlined based on the literature reviewed, and avenues for further research are identified.","['Clustering algorithms', 'Data mining', 'Partitioning algorithms', 'Classification algorithms', 'Clustering methods', 'Systematics', 'Big data']","['Data mining', 'clustering methods', 'educational technology', 'systematic review']"
"Blockchain have been an interesting research area for a long time and the benefits it provides have been used by a number of various industries. Similarly, the healthcare sector stands to benefit immensely from the blockchain technology due to security, privacy, confidentiality and decentralization. Nevertheless, the Electronic Health Record (EHR) systems face problems regarding data security, integrity and management. In this paper, we discuss how the blockchain technology can be used to transform the EHR systems and could be a solution of these issues. We present a framework that could be used for the implementation of blockchain technology in healthcare sector for EHR. The aim of our proposed framework is firstly to implement blockchain technology for EHR and secondly to provide secure storage of electronic records by defining granular access rules for the users of the proposed framework. Moreover, this framework also discusses the scalability problem faced by the blockchain technology in general via use of off-chain storage of the records. This framework provides the EHR system with the benefits of having a scalable, secure and integral blockchain-based solution.","['Blockchain', 'Peer-to-peer computing', 'Hospitals', 'Cryptography', 'Electronic medical records']","['Blockchain', 'health records', 'electronic health records', 'decentralization', 'and scalability']"
"The Internet of Things (IoT) is set to become one of the key technological developments of our times provided we are able to realize its full potential. The number of objects connected to IoT is expected to reach 50 billion by 2020 due to the massive influx of diverse objects emerging progressively. IoT, hence, is expected to be a major producer of big data. Sharing and collaboration of data and other resources would be the key for enabling sustainable ubiquitous environments, such as smart cities and societies. A timely fusion and analysis of big data, acquired from IoT and other sources, to enable highly efficient, reliable, and accurate decision making and management of ubiquitous environments would be a grand future challenge. Computational intelligence would play a key role in this challenge. A number of surveys exist on data fusion. However, these are mainly focused on specific application areas or classifications. The aim of this paper is to review literature on data fusion for IoT with a particular focus on mathematical methods (including probabilistic methods, artificial intelligence, and theory of belief) and specific IoT environments (distributed, heterogeneous, nonlinear, and object tracking environments). The opportunities and challenges for each of the mathematical methods and environments are given. Future developments, including emerging areas that would intrinsically benefit from data fusion and IoT, autonomous vehicles, deep learning for data fusion, and smart cities, are discussed.","['Data integration', 'Smart cities', 'Internet of Things', 'Big Data', 'Decision making', 'Computer science', 'Information technology']","['Internet of Things', 'big data', 'data fusion', 'computational and artificial intelligence', 'high performance computing', 'smart cities', 'smart societies', 'ubiquitous environments']"
"With the explosive growth of Internet of Things devices and massive data produced at the edge of the network, the traditional centralized cloud computing model has come to a bottleneck due to the bandwidth limitation and resources constraint. Therefore, edge computing, which enables storing and processing data at the edge of the network, has emerged as a promising technology in recent years. However, the unique features of edge computing, such as content perception, real-time computing, and parallel processing, has also introduced several new challenges in the field of data security and privacy-preserving, which are also the key concerns of the other prevailing computing paradigms, such as cloud computing, mobile cloud computing, and fog computing. Despites its importance, there still lacks a survey on the recent research advance of data security and privacy-preserving in the field of edge computing. In this paper, we present a comprehensive analysis of the data security and privacy threats, protection technologies, and countermeasures inherent in edge computing. Specifically, we first make an overview of edge computing, including forming factors, definition, architecture, and several essential applications. Next, a detailed analysis of data security and privacy requirements, challenges, and mechanisms in edge computing are presented. Then, the cryptography-based technologies for solving data security and privacy issues are summarized. The state-of-the-art data security and privacy solutions in edge-related paradigms are also surveyed. Finally, we propose several open research directions of data security in the field of edge computing.","['Edge computing', 'Cloud computing', 'Data privacy', 'Encryption', 'Computer architecture', 'Authentication']","['Edge computing', 'data security', 'cryptography', 'authentication', 'access control', 'privacy']"
"The Internet of Things (IoT) is one of the most promising technologies for the near future. Healthcare and well-being will receive great benefits with the evolution of this technology. This paper presents a review of techniques based on IoT for healthcare and ambient-assisted living, defined as the Internet of Health Things (IoHT), based on the most recent publications and products available in the market from industry for this segment. Also, this paper identifies the technological advances made so far, analyzing the challenges to be overcome and provides an approach of future trends. Through selected works, it is possible to notice that further studies are important to improve current techniques and that novel concept and technologies of IoHT are needed to overcome the identified challenges. The presented results aim to serve as a source of information for healthcare providers, researchers, technology specialists, and the general population to improve the IoHT.","['Medical services', 'Monitoring', 'Biomedical monitoring', 'Industries', 'Sensors', 'Internet of Things']","['Ambient assisted living', 'Internet of Things', 'Internet of Health Things', 'mobile health', 'remote healthcare monitoring', 'wearable']"
"Industry 4.0 can make a factory smart by applying intelligent information processing approaches, communication systems, future-oriented techniques, and more. However, the high complexity, automation, and flexibility of an intelligent factory bring new challenges to reliability and safety. Industrial big data generated by multisource sensors, intercommunication within the system and external-related information, and so on, might provide new solutions for predictive maintenance to improve system reliability. This paper puts forth attributes of industrial big data processing and actively explores industrial big data processing-based predictive maintenance. A novel framework is proposed for structuring multisource heterogeneous information, characterizing structured data with consideration of the spatiotemporal property, and modeling invisible factors, which would make the production process transparent and eventually implement predictive maintenance on facilities and energy saving in the industry 4.0 era. The effectiveness of the proposed scheme was verified by analyzing multisource heterogeneous industrial data for the remaining life prediction of key components of machining equipment.","['Big Data', 'Predictive maintenance', 'Data mining', 'Manufacturing processes', 'Industries', 'Feature extraction', 'Data models']","['Industrial big data', 'multisource heterogeneous data', 'structuralization and characterization', 'multiple invisible factors', 'predictive maintenance']"
"The whale optimization algorithm (WOA) has been shown to be powerful in searching for an optimal solution. This paper proposes an improvement to the whale optimization algorithm that is based on a Lévy flight trajectory and called the Lévy flight trajectory-based whale optimization algorithm (LWOA). The LWOA makes the WOA faster and more robust and avoids premature convergence. The Lévy flight trajectory is helpful for increasing the diversity of the population against premature convergence and enhancing the capability of jumping out of local optimal optima. This method helps obtaining a better tradeoff between the exploration and exploitation of the WOA. The proposed algorithm is characterized by quick convergence and high precision, and it can effectively get rid of a local optimum. The LWOA is further compared with other well-known nature-inspired algorithms on 23 benchmarks and solving infinite impulse response model identification. The statistical results on the benchmark functions show that the LWOA can significantly outperform others on a majority of the benchmark functions, especially in solving an optimization problem that has high dimensionality. Additionally, the superior identification capability of the proposed algorithm is evident from the results obtained through the simulation study compared with other algorithms. All the results prove the superiority of the LWOA.","['Whales', 'Optimization', 'Trajectory', 'Mathematical model', 'Convergence', 'Algorithm design and analysis', 'Spirals']","['Lévy flight trajectory', 'whale optimization algorithm', 'global optimization', 'IIR system']"
"Even after 16 years of existence, low energy adaptive clustering hierarchy (LEACH) protocol is still gaining the attention of the research community working in the area of wireless sensor network (WSN). This itself shows the importance of this protocol. Researchers have come up with various and diverse modifications of the LEACH protocol. Successors of LEACH protocol are now available from single hop to multi-hop scenarios. Extensive work has already been done related to LEACH and it is a good idea for a new research in the field of WSN to go through LEACH and its variants over the years. This paper surveys the variants of LEACH routing protocols proposed so far and discusses the enhancement and working of them. This survey classifies all the protocols in two sections, namely, single hop communication and multi-hop communication based on data transmission from the cluster head to the base station. A comparitive analysis using nine different parameters, such as energy efficiency, overhead, scalability complexity, and so on, has been provided in a chronological fashion. The article also discusses the strong and the weak points of each and every variants of LEACH. Finally the paper concludes with suggestions on future research domains in the area of WSN.","['Wireless sensor networks', 'Routing protocols', 'Routing', 'Time division multiple access', 'Schedules', 'Scalability']","['LEACH', 'Wireless Sensor Network', 'Clustering Protocol', 'Cluster Head', 'Routing']"
"The purpose of this paper is to survey and assess the state-of-the-art in automatic target recognition for synthetic aperture radar imagery (SAR-ATR). The aim is not to develop an exhaustive survey of the voluminous literature, but rather to capture in one place the various approaches for implementing the SAR-ATR system. This paper is meant to be as self-contained as possible, and it approaches the SAR-ATR problem from a holistic end-to-end perspective. A brief overview for the breadth of the SAR-ATR challenges is conducted. This is couched in terms of a single-channel SAR, and it is extendable to multi-channel SAR systems. Stages pertinent to the basic SAR-ATR system structure are defined, and the motivations of the requirements and constraints on the system constituents are addressed. For each stage in the SAR-ATR processing chain, a taxonomization methodology for surveying the numerous methods published in the open literature is proposed. Carefully selected works from the literature are presented under the taxa proposed. Novel comparisons, discussions, and comments are pinpointed throughout this paper. A two-fold benchmarking scheme for evaluating existing SAR-ATR systems and motivating new system designs is proposed. The scheme is applied to the works surveyed in this paper. Finally, a discussion is presented in which various interrelated issues, such as standard operating conditions, extended operating conditions, and target-model design, are addressed. This paper is a contribution toward fulfilling an objective of end-to-end SAR-ATR system design.","['Synthetic aperture radar', 'Classification', 'Feature recognition', 'Target recognition', 'System analysis and design', 'Benchmark testing', 'Radar tracking']","['SAR', 'radar', 'target', 'classification', 'recognition', 'features', 'model']"
"In recent decades, we have witnessed the evolution of biometric technology from the first pioneering works in face and voice recognition to the current state of development wherein a wide spectrum of highly accurate systems may be found, ranging from largely deployed modalities, such as fingerprint, face, or iris, to more marginal ones, such as signature or hand. This path of technological evolution has naturally led to a critical issue that has only started to be addressed recently: the resistance of this rapidly emerging technology to external attacks and, in particular, to spoofing. Spoofing, referred to by the term presentation attack in current standards, is a purely biometric vulnerability that is not shared with other IT security solutions. It refers to the ability to fool a biometric system into recognizing an illegitimate user as a genuine one by means of presenting a synthetic forged version of the original biometric trait to the sensor. The entire biometric community, including researchers, developers, standardizing bodies, and vendors, has thrown itself into the challenging task of proposing and developing efficient protection methods against this threat. The goal of this paper is to provide a comprehensive overview on the work that has been carried out over the last decade in the emerging field of antispoofing, with special attention to the mature and largely deployed face modality. The work covers theories, methodologies, state-of-the-art techniques, and evaluation databases and also aims at providing an outlook into the future of this very active field of research.","['Biometrics', 'Distance measurement', 'Computer security', 'Fingerprint recognition', 'Iris recognition', 'Access control', 'Speech recognition', 'Immune system', 'Biomedical monitoring', 'Authentication']","['Biometrics', 'Security', 'Anti-Spoofing']"
"The increasing demand for electricity and the emergence of smart grids have presented new opportunities for a home energy management system (HEMS) that can reduce energy usage. The HEMS incorporates a demand response (DR) tool that shifts and curtails demand to improve home energy consumption. This system commonly creates optimal consumption schedules by considering several factors, such as energy costs, environmental concerns, load profiles, and consumer comfort. With the deployment of smart meters, performing load control using the HEMS with DR-enabled appliances has become possible. This paper provides a comprehensive review on previous and current research related to the HEMS by considering various DR programs, smart technologies, and load scheduling controllers. The application of artificial intelligence for load scheduling controllers, such as artificial neural network, fuzzy logic, and adaptive neural fuzzy inference system, is also reviewed. Heuristic optimization techniques, which are widely used for optimal scheduling of various electrical devices in a smart home, are also discussed.","['Home appliances', 'Energy management', 'Energy consumption', 'Optimization', 'Schedules', 'Water heating']","['Home energy management system', 'demand response', 'smart technologies', 'integrated wireless technology', 'intelligent scheduling controller']"
"Industrial systems always prefer to reduce their operational expenses. To support such reductions, they need solutions that are capable of providing stability, fault tolerance, and flexibility. One such solution for industrial systems is cyber physical system (CPS) integration with the Internet of Things (IoT) utilizing cloud computing services. These CPSs can be considered as smart industrial systems, with their most prevalent applications in smart transportation, smart grids, smart medical and eHealthcare systems, and many more. These industrial CPSs mostly utilize supervisory control and data acquisition (SCADA) systems to control and monitor their critical infrastructure (CI). For example, WebSCADA is an application used for smart medical technologies, making improved patient monitoring and more timely decisions possible. The focus of the study presented in this paper is to highlight the security challenges that the industrial SCADA systems face in an IoT-cloud environment. Classical SCADA systems are already lacking in proper security measures; however, with the integration of complex new architectures for the future Internet based on the concepts of IoT, cloud computing, mobile wireless sensor networks, and so on, there are large issues at stakes in the security and deployment of these classical systems. Therefore, the integration of these future Internet concepts needs more research effort. This paper, along with highlighting the security challenges of these CI's, also provides the existing best practices and recommendations for improving and maintaining security. Finally, this paper briefly describes future research directions to secure these critical CPSs and help the research community in identifying the research gaps in this regard.","['Cloud computing', 'Security', 'SCADA systems', 'Power system stability', 'Fault tolerant systems', 'Wireless sensor networks', 'Internet of things', 'Stability analysis']","['APT', 'Industrial Control System', 'Internet of Things (IoT)', 'NIST', 'PRECYSE', 'Supervisory Control and Data Acquisition', 'SOA']"
"Traditional distance and density-based anomaly detection techniques are unable to detect periodic and seasonality related point anomalies which occur commonly in streaming data, leaving a big gap in time series anomaly detection in the current era of the IoT. To address this problem, we present a novel deep learning-based anomaly detection approach (DeepAnT) for time series data, which is equally applicable to the non-streaming cases. DeepAnT is capable of detecting a wide range of anomalies, i.e., point anomalies, contextual anomalies, and discords in time series data. In contrast to the anomaly detection methods where anomalies are learned, DeepAnT uses unlabeled data to capture and learn the data distribution that is used to forecast the normal behavior of a time series. DeepAnT consists of two modules: time series predictor and anomaly detector. The time series predictor module uses deep convolutional neural network (CNN) to predict the next time stamp on the defined horizon. This module takes a window of time series (used as a context) and attempts to predict the next time stamp. The predicted value is then passed to the anomaly detector module, which is responsible for tagging the corresponding time stamp as normal or abnormal. DeepAnT can be trained even without removing the anomalies from the given data set. Generally, in deep learning-based approaches, a lot of data are required to train a model. Whereas in DeepAnT, a model can be trained on relatively small data set while achieving good generalization capabilities due to the effective parameter sharing of the CNN. As the anomaly detection in DeepAnT is unsupervised, it does not rely on anomaly labels at the time of model generation. Therefore, this approach can be directly applied to real-life scenarios where it is practically impossible to label a big stream of data coming from heterogeneous sensors comprising of both normal as well as anomalous points. We have performed a detailed evaluation of 15 algorithms on 10 anomaly detection benchmarks, which contain a total of 433 real and synthetic time series. Experiments show that DeepAnT outperforms the state-of-the-art anomaly detection methods in most of the cases, while performing on par with others.","['Anomaly detection', 'Time series analysis', 'Clustering algorithms', 'Data models', 'Benchmark testing', 'Heuristic algorithms']","['Anomaly detection', 'artificial intelligence', 'convolutional neural network', 'deep neural networks', 'recurrent neural networks', 'time series analysis']"
"This paper provides a systematic review of the mutual coupling in multiple-input multiple-output (MIMO) systems, including the effects on performances of MIMO systems and various decoupling techniques. The mutual coupling changes the antenna characteristics in an array, and therefore, degrades the system performance of the MIMO system and causes the spectral regrowth. Although the system performance can be partially improved by calibrating out the mutual coupling in the digital domain, it is more effective to use decoupling techniques (from the antenna point) to overcome the mutual coupling effects. Some popular decoupling techniques for MIMO systems (especially for massive MIMO base station antennas) are also presented.","['Mutual coupling', 'MIMO communication', 'Antenna arrays', 'Signal to noise ratio', 'Interference', 'System performance']","['Capacity', 'error rate', 'MIMO antennas', 'mutual coupling']"
"This paper provides a comprehensive study of Federated Learning (FL) with an emphasis on enabling software and hardware platforms, protocols, real-life applications and use-cases. FL can be applicable to multiple domains but applying it to different industries has its own set of obstacles. FL is known as collaborative learning, where algorithm(s) get trained across multiple devices or servers with decentralized data samples without having to exchange the actual data. This approach is radically different from other more established techniques such as getting the data samples uploaded to servers or having data in some form of distributed infrastructure. FL on the other hand generates more robust models without sharing data, leading to privacy-preserved solutions with higher security and access privileges to data. This paper starts by providing an overview of FL. Then, it gives an overview of technical details that pertain to FL enabling technologies, protocols, and applications. Compared to other survey papers in the field, our objective is to provide a more thorough summary of the most relevant protocols, platforms, and real-life use-cases of FL to enable data scientists to build better privacy-preserving solutions for industries in critical need of FL. We also provide an overview of key challenges presented in the recent literature and provide a summary of related research work. Moreover, we explore both the challenges and advantages of FL and present detailed service use-cases to illustrate how different architectures and protocols that use FL can fit together to deliver desired results.","['Machine learning', 'Computer architecture', 'Protocols', 'Data models', 'Data privacy', 'Computational modeling', 'Industries']","['Federated learning', 'machine learning', 'collaborative AI', 'privacy', 'security', 'decentralized data', 'on-device AI', 'peer-to-peer network']"
"Early diagnosis of gear transmission has been a significant challenge, because gear faults occur primarily at microstructure or even material level but their effects can only be observed indirectly at a system level. The performance of a gear fault diagnosis system depends significantly on the features extracted and the classifier subsequently applied. Traditionally, fault-related features are extracted and identified based on domain expertise through data preprocessing which are system-specific and may not be easily generalized. On the other hand, although recently the deep neural networks based approaches featuring adaptive feature extractions and inherent classifications have attracted attention, they usually require a substantial set of training data. Aiming at tackling these issues, this paper presents a deep convolutional neural network-based transfer learning approach. The proposed transfer learning architecture consists of two parts; the first part is constructed with a pre-trained deep neural network that serves to extract the features automatically from the input, and the second part is a fully connected stage to classify the features that needs to be trained using gear fault experimental data. Case analyses using experimental data from a benchmark gear system indicate that the proposed approach not only entertains preprocessing free adaptive feature extractions, but also requires only a small set of training data.","['Gears', 'Feature extraction', 'Fault diagnosis', 'Convolution', 'Convolutional neural networks', 'Vibrations']","['Alexnet', 'deep convolutional neural network', 'gear fault diagnosis', 'transfer learning']"
"Modern technologies of mobile computing and wireless sensing prompt the concept of pervasive social network (PSN)-based healthcare. To realize the concept, the core problem is how a PSN node can securely share health data with other nodes in the network. In this paper, we propose a secure system for PSN-based healthcare. Two protocols are designed for the system. The first one is an improved version of the IEEE 802.15.6 display authenticated association. It establishes secure links with unbalanced computational requirements for mobile devices and resource-limited sensor nodes. The second protocol uses blockchain technique to share health data among PSN nodes. We realize a protocol suite to study protocol runtime and other factors. In addition, human body channels are proposed for PSN nodes in some use cases. The proposed system illustrates a potential method of using blockchain for PSN-based applications.","['Protocols', 'Medical services', 'Mobile handsets', 'Wireless communication', 'Body area networks', 'Security', 'IEEE 802.15 Standard']","['IEEE 802.15.6', 'blockchain', 'e-health', 'healthcare', 'human body channels']"
"Internet of Things (IoT) is a new technological paradigm that can connect things from various fields through the Internet. For the IoT connected healthcare applications, the wireless body area network (WBAN) is gaining popularity as wearable devices spring into the market. This paper proposes a wearable sensor node with solar energy harvesting and Bluetooth low energy transmission that enables the implementation of an autonomous WBAN. Multiple sensor nodes can be deployed on different positions of the body to measure the subject's body temperature distribution, heartbeat, and detect falls. A web-based smartphone application is also developed for displaying the sensor data and fall notification. To extend the lifetime of the wearable sensor node, a flexible solar energy harvester with an output-based maximum power point tracking technique is used to power the sensor node. Experimental results show that the wearable sensor node works well when powered by the solar energy harvester. The autonomous 24 h operation is achieved with the experimental results. The proposed system with solar energy harvesting demonstrates that long-term continuous medical monitoring based on WBAN is possible provided that the subject stays outside for a short period of time in a day.","['Wearable sensors', 'Wireless communication', 'Body area networks', 'Solar panels', 'Solar energy', 'Maximum power point trackers', 'Temperature measurement']","['Internet of Things', 'wireless body area network', 'energy harvesting', 'maximum power point tracking', 'Bluetooth']"
"Due to the monumental growth of Internet applications in the last decade, the need for security of information network has increased manifolds. As a primary defense of network infrastructure, an intrusion detection system is expected to adapt to dynamically changing threat landscape. Many supervised and unsupervised techniques have been devised by researchers from the discipline of machine learning and data mining to achieve reliable detection of anomalies. Deep learning is an area of machine learning which applies neuron-like structure for learning tasks. Deep learning has profoundly changed the way we approach learning tasks by delivering monumental progress in different disciplines like speech processing, computer vision, and natural language processing to name a few. It is only relevant that this new technology must be investigated for information security applications. The aim of this paper is to investigate the suitability of deep learning approaches for anomaly-based intrusion detection system. For this research, we developed anomaly detection models based on different deep neural network structures, including convolutional neural networks, autoencoders, and recurrent neural networks. These deep models were trained on NSLKDD training data set and evaluated on both test data sets provided by NSLKDD, namely NSLKDDTest+ and NSLKDDTest21. All experiments in this paper are performed by authors on a GPU-based test bed. Conventional machine learning-based intrusion detection models were implemented using well-known classification techniques, including extreme learning machine, nearest neighbor, decision-tree, random-forest, support vector machine, naive-bays, and quadratic discriminant analysis. Both deep and conventional machine learning models were evaluated using well-known classification metrics, including receiver operating characteristics, area under curve, precision-recall curve, mean average precision and accuracy of classification. Experimental results of deep IDS models showed promising results for real-world application in anomaly detection systems.","['Anomaly detection', 'Machine learning', 'Training', 'Intrusion detection', 'Measurement', 'Neural networks']","['Deep learning', 'convolutional neural networks', 'autoencoders', 'LSTM', 'k_NN', 'decision_tree', 'intrusion detection', 'convnets', 'information security']"
"The boom in the capabilities and features of mobile devices, like smartphones, tablets, and wearables, combined with the ubiquitous and affordable Internet access and the advances in the areas of cooperative networking, computer vision, and mobile cloud computing transformed mobile augmented reality (MAR) from science fiction to a reality. Although mobile devices are more constrained computationalwise from traditional computers, they have a multitude of sensors that can be used to the development of more sophisticated MAR applications and can be assisted from remote servers for the execution of their intensive parts. In this paper, after introducing the reader to the basics of MAR, we present a categorization of the application fields together with some representative examples. Next, we introduce the reader to the user interface and experience in MAR applications and continue with the core system components of the MAR systems. After that, we discuss advances in tracking and registration, since their functionality is crucial to any MAR application and the network connectivity of the devices that run MAR applications together with its importance to the performance of the application. We continue with the importance of data management in MAR systems and the systems performance and sustainability, and before we conclude this survey, we present existing challenging problems.","['Mobile communication', 'Mobile computing', 'Cloud computing', 'Augmented reality', 'Smart phones', 'Sensors']","['Mobile augmented reality', 'mobile computing', 'human computer interaction']"
"Due to recent advances in digital technologies, and availability of credible data, an area of artificial intelligence, deep learning, has emerged and has demonstrated its ability and effectiveness in solving complex learning problems not possible before. In particular, convolutional neural networks (CNNs) have demonstrated their effectiveness in the image detection and recognition applications. However, they require intensive CPU operations and memory bandwidth that make general CPUs fail to achieve the desired performance levels. Consequently, hardware accelerators that use application-specific integrated circuits, field-programmable gate arrays (FPGAs), and graphic processing units have been employed to improve the throughput of CNNs. More precisely, FPGAs have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism and their energy efficiency. In this paper, we review the recent existing techniques for accelerating deep learning networks on FPGAs. We highlight the key features employed by the various techniques for improving the acceleration performance. In addition, we provide recommendations for enhancing the utilization of FPGAs for CNNs acceleration. The techniques investigated in this paper represent the recent trends in the FPGA-based accelerators of deep learning networks. Thus, this paper is expected to direct the future advances on efficient hardware accelerators and to be useful for deep learning researchers.","['Deep learning', 'Field programmable gate arrays', 'Neural networks', 'Hardware', 'Acceleration', 'Convolution', 'Throughput']","['Adaptable architectures', 'convolutional neural networks (CNNs)', 'deep learning', 'dynamic reconfiguration', 'energy-efficient architecture', 'field programmable gate arrays (FPGAs)', 'hardware accelerator', 'machine learning', 'neural networks', 'optimization', 'parallel computer architecture', 'reconfigurable computing']"
"Neural architecture search (NAS) has significant progress in improving the accuracy of image classification. Recently, some works attempt to extend NAS to image segmentation which shows preliminary feasibility. However, all of them focus on searching architecture for semantic segmentation in natural scenes. In this paper, we design three types of primitive operation set on search space to automatically find two cell architecture DownSC and UpSC for semantic image segmentation especially medical image segmentation. Inspired by the U-net architecture and its variants successfully applied to various medical image segmentation, we propose NAS-Unet which is stacked by the same number of DownSC and UpSC on a U-like backbone network. The architectures of DownSC and UpSC updated simultaneously by a differential architecture strategy during the search stage. We demonstrate the good segmentation results of the proposed method on Promise12, Chaos, and ultrasound nerve datasets, which collected by magnetic resonance imaging, computed tomography, and ultrasound, respectively. Without any pretraining, our architecture searched on PASCAL VOC2012, attains better performances and much fewer parameters (about 0.8M) than U-net and one of its variants when evaluated on the above three types of medical image datasets.","['Computer architecture', 'Image segmentation', 'Magnetic resonance imaging', 'Medical diagnostic imaging', 'Task analysis', 'Microprocessors']","['Medical image segmentation', 'convolutional neural architecture search', 'deep learning']"
"An outbreak of a novel coronavirus disease (i.e., COVID-19) has been recorded in Wuhan, China since late December 2019, which subsequently became pandemic around the world. Although COVID-19 is an acutely treated disease, it can also be fatal with a risk of fatality of 4.03% in China and the highest of 13.04% in Algeria and 12.67% Italy (as of 8th April 2020). The onset of serious illness may result in death as a consequence of substantial alveolar damage and progressive respiratory failure. Although laboratory testing, e.g., using reverse transcription polymerase chain reaction (RT-PCR), is the golden standard for clinical diagnosis, the tests may produce false negatives. Moreover, under the pandemic situation, shortage of RT-PCR testing resources may also delay the following clinical decision and treatment. Under such circumstances, chest CT imaging has become a valuable tool for both diagnosis and prognosis of COVID-19 patients. In this study, we propose a weakly supervised deep learning strategy for detecting and classifying COVID-19 infection from CT images. The proposed method can minimise the requirements of manual labelling of CT images but still be able to obtain accurate infection detection and distinguish COVID-19 from non-COVID-19 cases. Based on the promising results obtained qualitatively and quantitatively, we can envisage a wide deployment of our developed technique in large-scale clinical studies.","['COVID-19', 'Computed tomography', 'Lung', 'Deep learning', 'Hospitals']","['COVID-19', 'deep learning', 'weakly supervision', 'CT~images', 'classification', 'convolutional neural network']"
"Extending the Technology Acceptance Model (TAM) for studying the e-learning acceptance is not a new research topic, and it has been tackled by many scholars. However, the development of a comprehensive TAM that could be able to examine the e-learning acceptance under any circumstances is regarded to be an essential research direction. To identify the most widely used external factors of the TAM concerning the e-learning acceptance, a literature review comprising of 120 significant published studies from the last twelve years was conducted. The review analysis indicated that computer self-efficacy, subjective/social norm, perceived enjoyment, system quality, information quality, content quality, accessibility, and computer playfulness were the most common external factors of TAM. Accordingly, the TAM has been extended by the aforementioned factors to examine the students' acceptance of e-learning in five different universities in the United Arab of Emirates (UAE). A total of 435 students participated in the study. The results indicated that system quality, computer self-efficacy, and computer playfulness have a significant impact on perceived ease of use of e-learning system. Furthermore, information quality, perceived enjoyment, and accessibility were found to have a positive influence on perceived ease of use and perceived usefulness of e-learning system.","['Electronic learning', 'Databases', 'Media', 'Bibliographies', 'Tools']","['E-learning', 'higher education', 'TAM', 'PLS-SEM']"
"Continuous practices, i.e., continuous integration, delivery, and deployment, are the software development industry practices that enable organizations to frequently and reliably release new features and products. With the increasing interest in the literature on continuous practices, it is important to systematically review and synthesize the approaches, tools, challenges, and practices reported for adopting and implementing continuous practices. This paper aimed at systematically reviewing the state of the art of continuous practices to classify approaches and tools, identify challenges and practices in this regard, and identify the gaps for future research. We used the systematic literature review method for reviewing the peer-reviewed papers on continuous practices published between 2004 and June 1, 2016. We applied the thematic analysis method for analyzing the data extracted from reviewing 69 papers selected using predefined criteria. We have identified 30 approaches and associated tools, which facilitate the implementation of continuous practices in the following ways: (1) reducing build and test time in continuous integration (CI); (2) increasing visibility and awareness on build and test results in CI; (3) supporting (semi-) automated continuous testing; (4) detecting violations, flaws, and faults in CI; (5) addressing security and scalability issues in deployment pipeline; and (6) improving dependability and reliability of deployment process. We have also determined a list of critical factors, such as testing (effort and time), team awareness and transparency, good design principles, customer, highly skilled and motivated team, application domain, and appropriate infrastructure that should be carefully considered when introducing continuous practices in a given organization. The majority of the reviewed papers were validation (34.7%) and evaluation (36.2%) research types. This paper also reveals that continuous practices have been successfully applied to both greenfield and maintenance projects. Continuous practices have become an important area of software engineering research and practice. While the reported approaches, tools, and practices are addressing a wide range of challenges, there are several challenges and gaps, which require future research work for improving the capturing and reporting of contextual information in the studies reporting different aspects of continuous practices; gaining a deep understanding of how software-intensive systems should be (re-) architected to support continuous practices; and addressing the lack of knowledge and tools for engineering processes of designing and running secure deployment pipelines.","['Software', 'Organizations', 'Software engineering', 'Systematics', 'Bibliographies', 'Testing', 'Production']","['Continuous integration', 'continuous delivery', 'continuous deployment', 'continuous software engineering', 'systematic literature review', 'empirical software engineering']"
"Electromagnetic radars have been shown potentially to be used for remote sensing of biosignals in a more comfortable and easier way than wearable and contact devices. While there is an increasing interest in using radars for health monitoring, their performance has not been tested and reported either in practical scenarios or with acceptable low errors. Therefore, we use a frequency modulated continuous wave (FMCW) radar operating at 77 GHz in a bedroom environment to extract the respiration and heart rates of a patient, who is used to lying down on the bed. Indeed, the proposed signal processing contains advanced phase unwrapping manipulation, which is unique. In addition, the results are compared with a reliable reference sensor. Our results show that the correlations between the reference sensor and the radar estimates are in 94% and 80% for breathing and heart rates, respectively.","['Heart rate', 'Phase noise', 'Monitoring', 'Radar remote sensing', 'Radar detection', 'Receivers']","['Breathing rate monitoring', 'FMCW radar', 'heart rate monitoring', 'Hexoskin', 'mm-wave', 'non-contact monitoring', 'phase analysis', 'remote sensing', 'vital signs', 'TI']"
"Shipbuilding companies are upgrading their inner workings in order to create Shipyards 4.0, where the principles of Industry 4.0 are paving the way to further digitalized and optimized processes in an integrated network. Among the different Industry 4.0 technologies, this paper focuses on augmented reality, whose application in the industrial field has led to the concept of industrial augmented reality (IAR). This paper first describes the basics of IAR and then carries out a thorough analysis of the latest IAR systems for industrial and shipbuilding applications. Then, in order to build a practical IAR system for shipyard workers, the main hardware and software solutions are compared. Finally, as a conclusion after reviewing all the aspects related to IAR for shipbuilding, it proposed an IAR system architecture that combines cloudlets and fog computing, which reduce latency response and accelerate rendering tasks while offloading compute intensive tasks from the cloud.","['Augmented reality', 'Industries', 'Task analysis', 'Software', 'Cameras', 'Companies', 'Hardware']","['Industry 4.0', 'augmented reality', 'industrial augmented reality', 'Internet of Things', 'cyber-physical systems', 'industrial operator support', 'smart factory', 'task execution', 'cloudlet', 'edge computing']"
"This paper conducts a comprehensive study on the application of big data and machine learning in the electrical power grid introduced through the emergence of the next-generation power system-the smart grid (SG). Connectivity lies at the core of this new grid infrastructure, which is provided by the Internet of Things (IoT). This connectivity, and constant communication required in this system, also introduced a massive data volume that demands techniques far superior to conventional methods for proper analysis and decision-making. The IoT-integrated SG system can provide efficient load forecasting and data acquisition technique along with cost-effectiveness. Big data analysis and machine learning techniques are essential to reaping these benefits. In the complex connected system of SG, cyber security becomes a critical issue; IoT devices and their data turning into major targets of attacks. Such security concerns and their solutions are also included in this paper. Key information obtained through literature review is tabulated in the corresponding sections to provide a clear synopsis; and the findings of this rigorous review are listed to give a concise picture of this area of study and promising future fields of academic and industrial research, with current limitations with viable solutions along with their effectiveness.","['Smart grids', 'Big Data', 'Machine learning', 'Security', 'Internet of Things']","['Big data analysis', 'cyber security', 'IoT', 'machine learning', 'smart grid']"
"Blockchain technology has attracted tremendous attention in both academia and capital market. However, overwhelming speculations on thousands of available cryptocurrencies and numerous initial coin offering scams have also brought notorious debates on this emerging technology. This paper traces the development of blockchain systems to reveal the importance of decentralized applications (dApps) and the future value of blockchain. We survey the state-of-the-art dApps and discuss the direction of blockchain development to fulfill the desirable characteristics of dApps. The readers will gain an overview of dApp research and get familiar with recent developments in the blockchain.","['Bitcoin', 'Games', 'Contracts', 'Peer-to-peer computing', 'Software systems']","['Blockchain', 'decentralized application', 'smart contract', 'software systems', 'survey']"
"Employing large intelligent surfaces (LISs) is a promising solution for improving the coverage and rate of future wireless systems. These surfaces comprise massive numbers of nearly-passive elements that interact with the incident signals, for example by reflecting them, in a smart way that improves the wireless system performance. Prior work focused on the design of the LIS reflection matrices assuming full channel knowledge. Estimating these channels at the LIS, however, is a key challenging problem. With the massive number of LIS elements, channel estimation or reflection beam training will be associated with (i) huge training overhead if all the LIS elements are passive (not connected to a baseband) or with (ii) prohibitive hardware complexity and power consumption if all the elements are connected to the baseband through a fully-digital or hybrid analog/digital architecture. This paper proposes efficient solutions for these problems by leveraging tools from compressive sensing and deep learning. First, a novel LIS architecture based on sparse channel sensors is proposed. In this architecture, all the LIS elements are passive except for a few elements that are active (connected to the baseband). We then develop two solutions that design the LIS reflection matrices with negligible training overhead. In the first approach, we leverage compressive sensing tools to construct the channels at all the LIS elements from the channels seen only at the active elements. In the second approach, we develop a deep-learning based solution where the LIS learns how to interact with the incident signal given the channels at the active elements, which represent the state of the environment and transmitter/receiver locations. We show that the achievable rates of the proposed solutions approach the upper bound, which assumes perfect channel knowledge, with negligible training overhead and with only a few active elements, making them promising for future LIS systems.","['Training', 'Deep learning', 'Wireless communication', 'Baseband', 'Tools', 'Reflection', 'Compressed sensing']","['Large intelligent surface', 'intelligent reflecting surfaces', 'reconfigurable intelligent surface', 'smart reflect-array', 'beamforming', 'millimeter wave', 'compressive sensing', 'deep learning']"
"Traditional machine learning algorithms have made great achievements in data-driven fault diagnosis. However, they assume that all the data must be in the same working condition and have the same distribution and feature space. They are not applicable for real-world working conditions, which often change with time, so the data are hard to obtain. In order to utilize data in different working conditions to improve the performance, this paper presents a transfer learning approach for fault diagnosis with neural networks. First, it learns characteristics from massive source data and adjusts the parameters of neural networks accordingly. Second, the structure of neural networks alters for the change of data distribution. In the same time, some parameters are transferred from source task to target task. Finally, the new model is trained by a small amount of target data in another working condition. The Case Western Reserve University bearing data set is used to validate the performance of the proposed transfer learning approach. Experimental results show that the proposed transfer learning approach can improve the classification accuracy and reduce the training time comparing with the conventional neural network method when there are only a small amount of target data.","['Employee welfare', 'Fault diagnosis', 'Data models', 'Training', 'Vibrations', 'Biological neural networks']","['Fault diagnosis', 'transfer learning', 'neural networks', 'machine learning']"
"For agricultural applications, regularized smart-farming solutions are being considered, including the use of unmanned aerial vehicles (UAVs). The UAVs combine information and communication technologies, robots, artificial intelligence, big data, and the Internet of Things. The agricultural UAVs are highly capable, and their use has expanded across all areas of agriculture, including pesticide and fertilizer spraying, seed sowing, and growth assessment and mapping. Accordingly, the market for agricultural UAVs is expected to continue growing with the related technologies. In this study, we consider the latest trends and applications of leading technologies related to agricultural UAVs, control technologies, equipment, and development. We discuss the use of UAVs in real agricultural environments. Furthermore, the future development of the agricultural UAVs and their challenges are presented.","['Cameras', 'Unmanned aerial vehicles', 'Agriculture', 'Wireless sensor networks', 'Wireless communication', 'Sensors', 'Rotors']","['Agricultural applications', 'agricultural UAV', 'control technology', 'smart farming', 'unmanned aerial vehicle', 'UAV platforms']"
"This paper investigates the coexistence between two key enabling technologies for fifth generation (5G) mobile networks, non-orthogonal multiple access (NOMA), and millimeter-wave (mmWave) communications. Particularly, the application of random beamforming to mmWave-NOMA systems is considered in order to avoid the requirement that the base station know all the users' channel state information. Stochastic geometry is used to characterize the performance of the proposed mmWave-NOMA transmission scheme by using key features of mmWave systems, i.e., that mmWave transmission is highly directional and potential blockages will thin the user distribution. Two random beamforming approaches that can further reduce the system overhead are also proposed, and their performance is studied analytically in terms of sum rates and outage probabilities. Simulation results are also provided to demonstrate the performance of the proposed schemes and verify the accuracy of the developed analytical results.","['Base stations', 'NOMA', 'Array signal processing', 'Precoding', '5G mobile communication', 'Interference']","['Non-orthogonal multiple access (NOMA)', 'millimeter wave (mmWave) networks', 'mmWave-NOMA communications', 'random beamforming']"
"The public key infrastructure-based authentication protocol provides basic security services for the vehicular ad hoc networks (VANETs). However, trust and privacy are still open issues due to the unique characteristics of VANETs. It is crucial to prevent internal vehicles from broadcasting forged messages while simultaneously preserving the privacy of vehicles against the tracking attacks. In this paper, we propose a blockchain-based anonymous reputation system (BARS) to establish a privacy-preserving trust model for VANETs. The certificate and revocation transparency is implemented efficiently with the proofs of presence and absence based on the extended blockchain technology. The public keys are used as pseudonyms in communications without any information about real identities for conditional anonymity. In order to prevent the distribution of forged messages, a reputation evaluation algorithm is presented relying on both direct historical interactions and indirect opinions about vehicles. A set of experiments is conducted to evaluate BARS in terms of security, validity, and performance, and the results show that BARS is able to establish a trust model with transparency, conditional anonymity, efficiency, and robustness for VANETs.","['Privacy', 'Public key', 'Bars', 'Data models', 'Authentication']","['Vehicular ad-hoc networks', 'blockchain', 'trust management', 'reputation system', 'privacy']"
"In recent years, the classification of breast cancer has been the topic of interest in the field of Healthcare informatics, because it is the second main cause of cancer-related deaths in women. Breast cancer can be identified using a biopsy where tissue is removed and studied under microscope. The diagnosis is based on the qualification of the histopathologist, who will look for abnormal cells. However, if the histopathologist is not well-trained, this may lead to wrong diagnosis. With the recent advances in image processing and machine learning, there is an interest in attempting to develop a reliable pattern recognition based systems to improve the quality of diagnosis. In this paper, we compare two machine learning approaches for the automatic classification of breast cancer histology images into benign and malignant and into benign and malignant sub-classes. The first approach is based on the extraction of a set of handcrafted features encoded by two coding models (bag of words and locality constrained linear coding) and trained by support vector machines, while the second approach is based on the design of convolutional neural networks. We have also experimentally tested dataset augmentation techniques to enhance the accuracy of the convolutional neural network as well as “handcrafted features + convolutional neural network”and “convolutional neural network features + classifier”configurations. The results show convolutional neural networks outperformed the handcrafted feature based classifier, where we achieved accuracy between 96.15% and 98.33% for the binary classification and 83.31% and 88.23% for the multi-class classification.","['Feature extraction', 'Convolutional neural networks', 'Breast cancer', 'Image coding', 'Machine learning']","['Histology images', 'convolutional neural networks', 'engineered features', 'bag of words', 'locality constrained linear coding']"
"Mobile edge computing (MEC) provides a promising approach to significantly reduce network operational cost and improve quality of service (QoS) of mobile users by pushing computation resources to the network edges, and enables a scalable Internet of Things (IoT) architecture for time-sensitive applications (e-healthcare, real-time monitoring, and so on.). However, the mobility of mobile users and the limited coverage of edge servers can result in significant network performance degradation, dramatic drop in QoS, and even interruption of ongoing edge services; therefore, it is difficult to ensure service continuity. Service migration has great potential to address the issues, which decides when or where these services are migrated following user mobility and the changes of demand. In this paper, two conceptions similar to service migration, i.e., live migration for data centers and handover in cellular networks, are first discussed. Next, the cutting-edge research efforts on service migration in MEC are reviewed, and a devisal of taxonomy based on various research directions for efficient service migration is presented. Subsequently, a summary of three technologies for hosting services on edge servers, i.e., virtual machine, container, and agent, is provided. At last, open research challenges in service migration are identified and discussed.","['Servers', 'Virtual machining', 'Edge computing', 'Cloud computing', 'Quality of service', 'Data centers', 'Handover']","['Mobile edge computing', 'service migration', 'live migration', 'migration path selection', 'cellular handover']"
"We investigate the application of non-orthogonal multiple access (NOMA) with successive interference cancellation (SIC) in downlink multiuser multiple-input multiple-output (MIMO) cellular systems, where the total number of receive antennas at user equipment (UE) ends in a cell is more than the number of transmit antennas at the base station (BS). We first dynamically group the UE receive antennas into a number of clusters equal to or more than the number of BS transmit antennas. A single beamforming vector is then shared by all the receive antennas in a cluster. We propose a linear beamforming technique in which all the receive antennas can significantly cancel the inter-cluster interference. On the other hand, the receive antennas in each cluster are scheduled on the power domain NOMA basis with SIC at the receiver ends. For inter-cluster and intra-cluster power allocation, we provide dynamic power allocation solutions with an objective to maximizing the overall cell capacity. An extensive performance evaluation is carried out for the proposed MIMO-NOMA system and the results are compared with those for conventional orthogonal multiple access (OMA)-based MIMO systems and other existing MIMO-NOMA solutions. The numerical results quantify the capacity gain of the proposed MIMO-NOMA model over MIMO-OMA and other existing MIMO-NOMA solutions.","['NOMA', 'Resource management', 'Receiving antennas', 'Downlink', 'Transmitting antennas', 'MIMO', 'Interference']","['5G cellular', 'non-orthogonal multiple access (NOMA)', 'multiple-input multiple-output (MIMO)', 'linear beamforming', 'dynamic user clustering', 'dynamic power allocation']"
"It is an exciting time for power systems as there are many ground-breaking changes happening simultaneously. There is a global consensus in increasing the share of renewable energy-based generation in the overall mix, transitioning to a more environmental-friendly transportation with electric vehicles as well as liberalizing the electricity markets, much to the distaste of traditional utility companies. All of these changes are against the status quo and introduce new paradigms in the way the power systems operate. The generation penetrates distribution networks, renewables introduce intermittency, and liberalized markets need more competitive operation with the existing assets. All of these challenges require using some sort of storage device to develop viable power system operation solutions. There are different types of storage systems with different costs, operation characteristics, and potential applications. Understanding these is vital for the future design of power systems whether it be for short-term transient operation or long-term generation planning. In this paper, the state-of-the-art storage systems and their characteristics are thoroughly reviewed along with the cutting edge research prototypes. Based on their architectures, capacities, and operation characteristics, the potential application fields are identified. Finally, the research fields that are related to energy storage systems are studied with their impacts on the future of power systems.","['Flywheels', 'Iron', 'Renewable energy sources', 'Production', 'Smart grids']","['Storage systems', 'electric vehicles', 'power system optimization', 'market liberalization', 'renewable energy', 'new operation schemes', 'power system planning']"
"With the development of satellite technology, up to date imaging mode of synthetic aperture radar (SAR) satellite can provide higher resolution SAR imageries, which benefits ship detection and instance segmentation. Meanwhile, object detectors based on convolutional neural network (CNN) show high performance on SAR ship detection even without land-ocean segmentation; but with respective shortcomings, such as the relatively small size of SAR images for ship detection, limited SAR training samples, and inappropriate annotations, in existing SAR ship datasets, related research is hampered. To promote the development of CNN based ship detection and instance segmentation, we have constructed a High-Resolution SAR Images Dataset (HRSID). In addition to object detection, instance segmentation can also be implemented on HRSID. As for dataset construction, under the overlapped ratio of 25%, 136 panoramic SAR imageries with ranging resolution from 1m to 5m are cropped to 800 × 800 pixels SAR images. To reduce wrong annotation and missing annotation, optical remote sensing imageries are applied to reduce the interferes from harbor constructions. There are 5604 cropped SAR images and 16951 ships in HRSID, and we have divided HRSID into a training set (65% SAR images) and test set (35% SAR images) with the format of Microsoft Common Objects in Context (MS COCO). 8 state-of-the-art detectors are experimented on HRSID to build the baseline; MS COCO evaluation metrics are applicated for comprehensive evaluation. Experimental results reveal that ship detection and instance segmentation can be well implemented on HRSID.","['Marine vehicles', 'Radar polarimetry', 'Image segmentation', 'Imaging', 'Synthetic aperture radar', 'Detectors', 'Image resolution']","['High-resolution SAR images dataset', 'ship detection', 'instance segmentation', 'deep learning', 'convolutional neural network']"
"Digital twin is a significant way to achieve smart manufacturing, and provides a new paradigm for fault diagnosis. Traditional data-based fault diagnosis methods mostly assume that the training data and test data are following the same distribution and can acquire sufficient data to train a reliable diagnosis model, which is unrealistic in the dynamic changing production process. In this paper, we present a two-phase digital-twin-assisted fault diagnosis method using deep transfer learning (DFDD), which realizes fault diagnosis both in the development and maintenance phases. At first, the potential problems that are not considered at design time can be discovered through front running the ultra-high-fidelity model in the virtual space, while a deep neural network (DNN)-based diagnosis model will be fully trained. In the second phase, the previously trained diagnosis model can be migrated from the virtual space to physical space using deep transfer learning for real-time monitoring and predictive maintenance. This ensures the accuracy of the diagnosis as well as avoids wasting time and knowledge. A case study about fault diagnosis using DFDD in a car body-side production line is presented. The results show the superiority and feasibility of our proposed method.","['Fault diagnosis', 'Data models', 'Manufacturing', 'Training data', 'Maintenance engineering', 'Predictive models', 'Solid modeling']","['Digital twin', 'deep transfer learning', 'fault diagnosis', 'smart manufacturing']"
"For task-scheduling problems in cloud computing, a multi-objective optimization method is proposed here. First, with an aim toward the biodiversity of resources and tasks in cloud computing, we propose a resource cost model that defines the demand of tasks on resources with more details. This model reflects the relationship between the user's resource costs and the budget costs. A multi-objective optimization scheduling method has been proposed based on this resource cost model. This method considers the makespan and the user's budget costs as constraints of the optimization problem, achieving multi-objective optimization of both performance and cost. An improved ant colony algorithm has been proposed to solve this problem. Two constraint functions were used to evaluate and provide feedback regarding the performance and budget cost. These two constraint functions made the algorithm adjust the quality of the solution in a timely manner based on feedback in order to achieve the optimal solution. Some simulation experiments were designed to evaluate this method's performance using four metrics: 1) the makespan; 2) cost; 3) deadline violation rate; and 4) resource utilization. Experimental results show that based on these four metrics, a multi-objective optimization method is better than other similar methods, especially as it increased 56.6% in the best case scenario.","['Scheduling', 'Optimization', 'Processor scheduling', 'Cloud computing', 'Ant colony optimization', 'Memory management', 'Resource management']","['Cloud computing', 'Ant colony', 'Task scheduling']"
"Texture analysis is used in a very broad range of fields and applications, from texture classification (e.g., for remote sensing) to segmentation (e.g., in biomedical imaging), passing through image synthesis or pattern recognition (e.g., for image inpainting). For each of these image processing procedures, first, it is necessary to extract—from raw images—meaningful features that describe the texture properties. Various feature extraction methods have been proposed in the last decades. Each of them has its advantages and limitations: performances of some of them are not modified by translation, rotation, affine, and perspective transform; others have a low computational complexity; others, again, are easy to implement; and so on. This paper provides a comprehensive survey of the texture feature extraction methods. The latter are categorized into seven classes: statistical approaches, structural approaches, transform-based approaches, model-based approaches, graph-based approaches, learning-based approaches, and entropy-based approaches. For each method in these seven classes, we present the concept, the advantages, and the drawbacks and give examples of application. This survey allows us to identify two classes of methods that, particularly, deserve attention in the future, as their performances seem interesting, but their thorough study is not performed yet.",[],[]
"Cell-free (CF) massive multiple-input-multiple-output (MIMO) systems have a large number of individually controllable antennas distributed over a wide area for simultaneously serving a small number of user equipments (UEs). This solution has been considered as a promising next-generation technology due to its ability to offer a similar quality of service to all UEs despite its low-complexity signal processing. In this paper, we provide a comprehensive survey of CF massive MIMO systems. To be more specific, the benefit of the so-called channel hardening and the favorable propagation conditions are exploited. Furthermore, we quantify the advantages of CF massive MIMO systems in terms of their energy- and cost-efficiency. Additionally, the signal processing techniques invoked for reducing the fronthaul burden for joint channel estimation and for transmit precoding are analyzed. Finally, the open research challenges in both its deployment and network management are highlighted.","['MIMO communication', 'Central Processing Unit', 'Power demand', 'Antenna arrays', 'Signal processing', 'Time-frequency analysis']","['Cell-free massive MIMO', 'B5G', 'signal processing', 'performance analysis']"
"The era of artificial neural network (ANN) began with a simplified application in many fields and remarkable success in pattern recognition (PR) even in manufacturing industries. Although significant progress achieved and surveyed in addressing ANN application to PR challenges, nevertheless, some problems are yet to be resolved like whimsical orientation (the unknown path that cannot be accurately calculated due to its directional position). Other problem includes; object classification, location, scaling, neurons behavior analysis in hidden layers, rule, and template matching. Also, the lack of extant literature on the issues associated with ANN application to PR seems to slow down research focus and progress in the field. Hence, there is a need for state-of-the-art in neural networks application to PR to urgently address the above-highlights problems for more successes. The study furnishes readers with a clearer understanding of the current, and new trend in ANN models that effectively addresses PR challenges to enable research focus and topics. Similarly, the comprehensive review reveals the diverse areas of the success of ANN models and their application to PR. In evaluating the performance of ANN models, some statistical indicators for measuring the performance of the ANN model in many studies were adopted. Such as the use of mean absolute percentage error (MAPE), mean absolute error (MAE), root mean squared error (RMSE), and variance of absolute percentage error (VAPE). The result shows that the current ANN models such as GAN, SAE, DBN, RBM, RNN, RBFN, PNN, CNN, SLP, MLP, MLNN, Reservoir computing, and Transformer models are performing excellently in their application to PR tasks. Therefore, the study recommends the research focus on current models and the development of new models concurrently for more successes in the field.","['Artificial neural networks', 'Task analysis', 'Fingerprint recognition', 'Computational modeling', 'Image recognition', 'Agriculture']","['Artificial neural networks', 'application to pattern recognition', 'feedforward neural networks', 'feedback neural networks', 'hybrid models']"
"Fog computing (FC) and Internet of Everything (IoE) are two emerging technological paradigms that, to date, have been considered standing-alone. However, because of their complementary features, we expect that their integration can foster a number of computing and network-intensive pervasive applications under the incoming realm of the future Internet. Motivated by this consideration, the goal of this position paper is fivefold. First, we review the technological attributes and platforms proposed in the current literature for the standing-alone FC and IoE paradigms. Second, by leveraging some use cases as illustrative examples, we point out that the integration of the FC and IoE paradigms may give rise to opportunities for new applications in the realms of the IoE, Smart City, Industry 4.0, and Big Data Streaming, while introducing new open issues. Third, we propose a novel technological paradigm, the Fog of Everything (FoE) paradigm, that integrates FC and IoE and then we detail the main building blocks and services of the corresponding technological platform and protocol stack. Fourth, as a proof-of-concept, we present the simulated energy-delay performance of a small-scale FoE prototype, namely, the V-FoE prototype. Afterward, we compare the obtained performance with the corresponding one of a benchmark technological platform, e.g., the V-D2D one. It exploits only device-to-device links to establish inter-thing “ad hoc” communication. Last, we point out the position of the proposed FoE paradigm over a spectrum of seemingly related recent research projects.","['Cloud computing', 'Big Data', 'Edge computing', 'Biological system modeling', 'Ecosystems', 'Smart cities']","['Fog of IoE', 'virtualized networked computing platforms for IoE', 'context-aware networking-plus-computing distributed resource management', 'Internet of Energy', 'Smart City', 'Industry 4.0', 'Big Data Streaming', 'future Internet']"
"Accurate prediction of remaining useful life (RUL) of lithium-ion battery plays an increasingly crucial role in the intelligent battery health management systems. The advances in deep learning introduce new data-driven approaches to this problem. This paper proposes an integrated deep learning approach for RUL prediction of lithium-ion battery by integrating autoencoder with deep neural network (DNN). First, we present a multi-dimensional feature extraction method with autoencoder model to represent battery health degradation. Then, the RUL prediction model-based DNN is trained for multi-battery remaining cycle life estimation. The proposed approach is applied to the real data set of lithium-ion battery cycle life from NASA, and the experiment results show that the proposed approach can improve the accuracy of RUL prediction.","['Lithium-ion batteries', 'Feature extraction', 'Predictive models', 'Voltage measurement', 'Temperature measurement', 'Machine learning']","['Lithium-ion battery', 'remaining useful life', 'RUL prediction model', 'deep learning', 'deep neural network']"
"Security breaches due to attacks by malicious software (malware) continue to escalate posing a major security concern in this digital age. With many computer users, corporations, and governments affected due to an exponential growth in malware attacks, malware detection continues to be a hot research topic. Current malware detection solutions that adopt the static and dynamic analysis of malware signatures and behavior patterns are time consuming and have proven to be ineffective in identifying unknown malwares in real-time. Recent malwares use polymorphic, metamorphic, and other evasive techniques to change the malware behaviors quickly and to generate a large number of new malwares. Such new malwares are predominantly variants of existing malwares, and machine learning algorithms (MLAs) are being employed recently to conduct an effective malware analysis. However, such approaches are time consuming as they require extensive feature engineering, feature learning, and feature representation. By using the advanced MLAs such as deep learning, the feature engineering phase can be completely avoided. Recently reported research studies in this direction show the performance of their algorithms with a biased training data, which limits their practical use in real-time situations. There is a compelling need to mitigate bias and evaluate these methods independently in order to arrive at a new enhanced method for effective zero-day malware detection. To fill the gap in the literature, this paper, first, evaluates the classical MLAs and deep learning architectures for malware detection, classification, and categorization using different public and private datasets. Second, we remove all the dataset bias removed in the experimental analysis by having different splits of the public and private datasets to train and test the model in a disjoint way using different timescales. Third, our major contribution is in proposing a novel image processing technique with optimal parameters for MLAs and deep learning architectures to arrive at an effective zero-day malware detection model. A comprehensive comparative study of our model demonstrates that our proposed deep learning architectures outperform classical MLAs. Our novelty in combining visualization and deep learning architectures for static, dynamic, and image processing-based hybrid approach applied in a big data environment is the first of its kind toward achieving robust intelligent zero-day malware detection. Overall, this paper paves way for an effective visual detection of malware using a scalable and hybrid deep learning framework for real-time deployments.","['Malware', 'Deep learning', 'Feature extraction', 'Computer architecture', 'Computer security']","['Cyber security', 'cybercrime', 'malware detection', 'static and dynamic analysis', 'artificial intelligence', 'machine learning', 'deep learning', 'image processing', 'scalable and hybrid framework']"
"Internet of Things (IoT) and smart computing technologies have revolutionized every sphere of 21 st century humans. IoT technologies and the data driven services they offer were beyond imagination just a decade ago. Now, they surround us and influence a variety of domains such as automobile, smart home, healthcare, etc. In particular, the Agriculture and Farming industries have also embraced this technological intervention. Smart devices are widely used by a range of people from farmers to entrepreneurs. These technologies are used in a variety of ways, from finding real-time status of crops and soil moisture content to deploying drones to assist with tasks such as applying pesticide spray. However, the use of IoT and smart communication technologies introduce a vast exposure to cybersecurity threats and vulnerabilities in smart farming environments. Such cyber attacks have the potential to disrupt the economies of countries that are widely dependent on agriculture. In this paper, we present a holistic study on security and privacy in a smart farming ecosystem. The paper outlines a multi layered architecture relevant to the precision agriculture domain and discusses the security and privacy issues in this dynamic and distributed cyber physical environment. Further more, the paper elaborates on potential cyber attack scenarios and highlights open research challenges and future directions.","['Privacy', 'Real-time systems', 'Computer security']","['Security', 'privacy', 'smart farming', 'precision agriculture', 'cloud computing', 'edge computing', 'cyber physical systems', 'IoT', 'artificial intelligence (AI)', 'machine learning', 'layered architecture']"
"Machine learning is one of the most prevailing techniques in computer science, and it has been widely applied in image processing, natural language processing, pattern recognition, cybersecurity, and other fields. Regardless of successful applications of machine learning algorithms in many scenarios, e.g., facial recognition, malware detection, automatic driving, and intrusion detection, these algorithms and corresponding training data are vulnerable to a variety of security threats, inducing a significant performance decrease. Hence, it is vital to call for further attention regarding security threats and corresponding defensive techniques of machine learning, which motivates a comprehensive survey in this paper. Until now, researchers from academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks. Thus, we revisit existing security threats and give a systematic survey on them from two aspects, the training phase and the testing/inferring phase. After that, we categorize current defensive techniques of machine learning into four groups: security assessment mechanisms, countermeasures in the training phase, those in the testing or inferring phase, data security, and privacy. Finally, we provide five notable trends in the research on security threats and defensive techniques of machine learning, which are worth doing in-depth studies in future.","['Security', 'Training', 'Machine learning algorithms', 'Training data', 'Support vector machines', 'Testing', 'Taxonomy']","['Machine learning', 'adversarial samples', 'security threats', 'defensive techniques']"
"In recent years, advanced threat attacks are increasing, but the traditional network intrusion detection system based on feature filtering has some drawbacks which make it difficult to find new attacks in time. This paper takes NSL-KDD data set as the research object, analyses the latest progress and existing problems in the field of intrusion detection technology, and proposes an adaptive ensemble learning model. By adjusting the proportion of training data and setting up multiple decision trees, we construct a MultiTree algorithm. In order to improve the overall detection effect, we choose several base classifiers, including decision tree, random forest, kNN, DNN, and design an ensemble adaptive voting algorithm. We use NSL-KDD Test+ to verify our approach, the accuracy of the MultiTree algorithm is 84.2%, while the final accuracy of the adaptive voting algorithm reaches 85.2%. Compared with other research papers, it is proved that our ensemble model effectively improves detection accuracy. In addition, through the analysis of data, it is found that the quality of data features is an important factor to determine the detection effect. In the future, we should optimize the feature selection and preprocessing of intrusion detection data to achieve better results.","['Intrusion detection', 'Feature extraction', 'Classification algorithms', 'Adaptation models', 'Machine learning algorithms', 'Neural networks', 'Prediction algorithms']","['Intrusion detection', 'ensemble learning', 'deep neural network', 'voting', 'MultiTree', 'NSL-KDD']"
"Nature computing has evolved with exciting performance to solve complex real-world combinatorial optimization problems. These problems span across engineering, medical sciences, and sciences generally. The Ebola virus has a propagation strategy that allows individuals in a population to move among susceptible, infected, quarantined, hospitalized, recovered, and dead sub-population groups. Motivated by the effectiveness of this strategy of propagation of the disease, a new bio-inspired and population-based optimization algorithm is proposed. This study presents a novel metaheuristic algorithm named Ebola Optimization Search Algorithm (EOSA) based on the propagation mechanism of the Ebola virus disease. First, we designed an improved SIR model of the disease, namely SEIR-HVQD: Susceptible (S), Exposed (E), Infected (I), Recovered (R), Hospitalized (H), Vaccinated (V), Quarantine (Q), and Death or Dead (D). Secondly, we represented the new model using a mathematical model based on a system of first-order differential equations. A combination of the propagation and mathematical models was adapted for developing the new metaheuristic algorithm. To evaluate the performance and capability of the proposed method in comparison with other optimization methods, two sets of benchmark functions consisting of forty-seven (47) classical and thirty (30) constrained IEEE-CEC benchmark functions were investigated. The results indicate that the performance of the proposed algorithm is competitive with other state-of-the-art optimization methods based on scalability, convergence, and sensitivity analyses. Extensive simulation results show that the EOSA outperforms popular metaheuristic algorithms such as the Particle Swarm Optimization Algorithm (PSO), Genetic Algorithm (GA), and Artificial Bee Colony Algorithm (ABC). Also, the algorithm was applied to address the complex problem of selecting the best combination of convolutional neural network (CNN) hyperparameters in the image classification of digital mammography. Results obtained showed the optimized CNN architecture successfully detected breast cancer from digital images at an accuracy of 96.0%. The source code of EOSA is publicly available at https://github.com/NathanielOy/EOSA_Metaheuristic .","['Optimization', 'Viruses (medical)', 'Diseases', 'Metaheuristics', 'Mathematical models', 'Heuristic algorithms', 'Statistics']","['Ebola virus', 'metaheuristic algorithm', 'optimization problems', 'constrained benchmark functions', 'image classification', 'convolutional neural network']"
"Wireless sensor networks (WSNs) will be integrated into the future Internet as one of the components of the Internet of Things, and will become globally addressable by any entity connected to the Internet. Despite the great potential of this integration, it also brings new threats, such as the exposure of sensor nodes to attacks originating from the Internet. In this context, lightweight authentication and key agreement protocols must be in place to enable end-to-end secure communication. Recently, Amin et al. proposed a three-factor mutual authentication protocol for WSNs. However, we identified several flaws in their protocol. We found that their protocol suffers from smart card loss attack where the user identity and password can be guessed using offline brute force techniques. Moreover, the protocol suffers from known session-specific temporary information attack, which leads to the disclosure of session keys in other sessions. Furthermore, the protocol is vulnerable to tracking attack and fails to fulfill user untraceability. To address these deficiencies, we present a lightweight and secure user authentication protocol based on the Rabin cryptosystem, which has the characteristic of computational asymmetry. We conduct a formal verification of our proposed protocol using ProVerif in order to demonstrate that our scheme fulfills the required security properties. We also present a comprehensive heuristic security analysis to show that our protocol is secure against all the possible attacks and provides the desired security features. The results we obtained show that our new protocol is a secure and lightweight solution for authentication and key agreement for Internet-integrated WSNs.","['Protocols', 'Wireless sensor networks', 'Internet', 'Authentication', 'Smart cards', 'Cryptography']","['Authentication', 'biometrics', 'key management', 'privacy', 'Rabin cryptosystem', 'smart card', 'wireless sensor networks']"
"The rapid development of blockchain technology and their numerous emerging applications has received huge attention in recent years. The distributed consensus mechanism is the backbone of a blockchain network. It plays a key role in ensuring the network's security, integrity, and performance. Most current blockchain networks have been deploying the proof-of-work consensus mechanisms, in which the consensus is reached through intensive mining processes. However, this mechanism has several limitations, e.g., energy inefficiency, delay, and vulnerable to security threats. To overcome these problems, a new consensus mechanism has been developed recently, namely proof of stake, which enables to achieve the consensus via proving the stake ownership. This mechanism is expected to become a cutting-edge technology for future blockchain networks. This paper is dedicated to investigating proof-of-stake mechanisms, from fundamental knowledge to advanced proof-of-stake-based protocols along with performance analysis, e.g., energy consumption, delay, and security, as well as their promising applications, particularly in the field of Internet of Vehicles. The formation of stake pools and their effects on the network stake distribution are also analyzed and simulated. The results show that the ratio between the block reward and the total network stake has a significant impact on the decentralization of the network. Technical challenges and potential solutions are also discussed.","['Blockchain', 'Cryptography', 'Games', 'Delays', 'Hash functions', 'Distributed databases']","['Blockchain', 'consensus mechanisms', 'energy', 'game theory', 'proof-of-stake', 'proof-of-work', 'security', 'and mining process']"
"In recent years, with the rapid development of Internet technology, online shopping has become a mainstream way for users to purchase and consume. Sentiment analysis of a large number of user reviews on e-commerce platforms can effectively improve user satisfaction. This paper proposes a new sentiment analysis model-SLCABG, which is based on the sentiment lexicon and combines Convolutional Neural Network (CNN) and attention-based Bidirectional Gated Recurrent Unit (BiGRU). In terms of methods, the SLCABG model combines the advantages of sentiment lexicon and deep learning technology, and overcomes the shortcomings of existing sentiment analysis model of product reviews. The SLCABG model combines the advantages of the sentiment lexicon and deep learning techniques. First, the sentiment lexicon is used to enhance the sentiment features in the reviews. Then the CNN and the Gated Recurrent Unit (GRU) network are used to extract the main sentiment features and context features in the reviews and use the attention mechanism to weight. And finally classify the weighted sentiment features. In terms of data, this paper crawls and cleans the real book evaluation of dangdang.com, a famous Chinese e-commerce website, for training and testing, all of which are based on Chinese. The scale of the data has reached 100000 orders of magnitude, which can be widely used in the field of Chinese sentiment analysis. The experimental results show that the model can effectively improve the performance of text sentiment analysis.","['Sentiment analysis', 'Analytical models', 'Feature extraction', 'Deep learning', 'Support vector machines']","['Attention mechanism', 'CNN', 'BiGRU', 'sentiment analysis', 'sentiment lexicon']"
"One of the biggest concerns of big data is privacy. However, the study on big data privacy is still at a very early stage. We believe the forthcoming solutions and theories of big data privacy root from the in place research output of the privacy discipline. Motivated by these factors, we extensively survey the existing research outputs and achievements of the privacy field in both application and theoretical angles, aiming to pave a solid starting ground for interested readers to address the challenges in the big data case. We first present an overview of the battle ground by defining the roles and operations of privacy systems. Second, we review the milestones of the current two major research categories of privacy: data clustering and privacy frameworks. Third, we discuss the effort of privacy study from the perspectives of different disciplines, respectively. Fourth, the mathematical description, measurement, and modeling on privacy are presented. We summarize the challenges and opportunities of this promising topic at the end of this paper, hoping to shed light on the exciting and almost uncharted land.","['Big data', 'Privacy', 'Clustering', 'Differential equations', 'Data privacy', 'Mathematical models']","['Big data', 'privacy', 'data clustering', 'differential privacy']"
"Driven by the demand to accommodate today's growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.","['Wireless communication', 'Training', '5G mobile communication', 'Supervised learning', 'Task analysis', 'Reinforcement learning']","['Machine learning', '5G mobile communication', 'B5G', 'wireless communication', 'mobile communication', 'artificial intelligence']"
"A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks. There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method, which we call smart augmentation and we show how to use it to increase the accuracy and reduce over fitting on a target network. Smart augmentation works, by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network. Smart augmentation has shown the potential to increase accuracy by demonstrably significant measures on all data sets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.","['Training', 'Biological neural networks', 'Informatics', 'Electronic mail', 'Machine learning', 'Data models']","['Artificial intelligence', 'artificial neural networks', 'machine learning', 'computer vision supervised learning', 'machine learning algorithms', 'image databases']"
"The concept of a digital twin has been used in some industries where an accurate digital model of the equipment can be used for predictive maintenance. The use of a digital twin for performance is critical, and for capital-intensive equipment such as jet engines it proved to be successful in terms of cost savings and reliability improvements. In this paper, we aim to study the expansion of the digital twin in including building life cycle management and explore the benefits and shortcomings of such implementation. In four rounds of experimentation, more than 25,000 sensor reading instances were collected, analyzed, and utilized to create and test a limited digital twin of an office building facade element. This is performed to point out the method of implementation, highlight the benefits gained from digital twin, and to uncover some of the technical shortcomings of the current Internet of Things systems for this purpose.","['Buildings', 'Solid modeling', 'Object oriented modeling', 'Three-dimensional displays', 'Real-time systems']","['Building information modeling', 'digital twin', 'life cycle management', 'Internet of Things', 'wireless sensor network']"
"The Internet of Energy (IoE) provides an effective networking technology for distributed green energy, which allows the connection of energy anywhere at any time. As an important part of the IoE, electric vehicles (EVs), and charging pile management are of great significance to the development of the IoE industry. Previous work has mainly focused on network performance optimization for its management, and few studies have considered the security of the management between EVs and charging piles. Therefore, this paper proposes a decentralized security model based on the lightning network and smart contract in the blockchain ecosystem; this proposed model is called the lightning network and smart contract (LNSC). The overall model involves registration, scheduling, authentication, and charging phases. The new proposed security model can be easily integrated with current scheduling mechanisms to enhance the security of trading between EVs and charging piles. Experimental results according to a realistic infrastructure are presented in this paper. These experimental results demonstrate that our scheme can effectively enhance vehicle security. Different performances of LNSC-based scheduling strategies are also presented.","['Contracts', 'Lightning', 'Charging stations', 'Electric vehicle charging']","['Blockchain', 'smart contract', 'vehicle charging', 'mutual authentication', 'Internet of Energy']"
"Detecting outliers is a significant problem that has been studied in various research and application areas. Researchers continue to design robust schemes to provide solutions to detect outliers efficiently. In this survey, we present a comprehensive and organized review of the progress of outlier detection methods from 2000 to 2019. First, we offer the fundamental concepts of outlier detection and then categorize them into different techniques from diverse outlier detection techniques, such as distance-, clustering-, density-, ensemble-, and learning-based methods. In each category, we introduce some state-of-the-art outlier detection methods and further discuss them in detail in terms of their performance. Second, we delineate their pros, cons, and challenges to provide researchers with a concise overview of each technique and recommend solutions and possible research directions. This paper gives current progress of outlier detection techniques and provides a better understanding of the different outlier detection methods. The open research issues and challenges at the end will provide researchers with a clear path for the future of outlier detection methods.","['Anomaly detection', 'Tools', 'Distributed databases', 'Trajectory', 'Deep learning']","['Outlier detection', 'distance-based', 'clustering-based', 'density-based', 'ensemble-based']"
"Bitcoin has recently attracted considerable attention in the fields of economics, cryptography, and computer science due to its inherent nature of combining encryption technology and monetary units. This paper reveals the effect of Bayesian neural networks (BNNs) by analyzing the time series of Bitcoin process. We also select the most relevant features from Blockchain information that is deeply involved in Bitcoin's supply and demand and use them to train models to improve the predictive performance of the latest Bitcoin pricing process. We conduct the empirical study that compares the Bayesian neural network with other linear and non-linear benchmark models on modeling and predicting the Bitcoin process. Our empirical studies show that BNN performs well in predicting Bitcoin price time series and explaining the high volatility of the recent Bitcoin price.","['Bitcoin', 'Predictive models', 'Neural networks', 'Time series analysis', 'Bayes methods', 'Biological system modeling']","['Bitcoin', 'blockchain', 'Bayesian neural network', 'time-series analysis', 'predictive model']"
"Today, the stability of the electric power grid is maintained through real time balancing of generation and demand. Grid scale energy storage systems are increasingly being deployed to provide grid operators the flexibility needed to maintain this balance. Energy storage also imparts resiliency and robustness to the grid infrastructure. Over the last few years, there has been a significant increase in the deployment of large scale energy storage systems. This growth has been driven by improvements in the cost and performance of energy storage technologies and the need to accommodate distributed generation, as well as incentives and government mandates. Energy management systems (EMSs) and optimization methods are required to effectively and safely utilize energy storage as a flexible grid asset that can provide multiple grid services. The EMS needs to be able to accommodate a variety of use cases and regulatory environments. In this paper, we provide a brief history of grid-scale energy storage, an overview of EMS architectures, and a summary of the leading applications for storage. These serve as a foundation for a discussion of EMS optimization methods and design.","['Energy management', 'Batteries', 'Frequency control', 'Optimization methods', 'Power system stability']","['Energy storage system (ESS)', 'energy management system (EMS)', 'battery energy storage system (BESS)', 'optimization methods', 'optimal control', 'linear programming (LP)', 'mixed integer linear programming (MILP)', 'battery management system (BMS)']"
"Although the Internet of Things (IoT) can increase efficiency and productivity through intelligent and remote management, it also increases the risk of cyber-attacks. The potential threats to IoT applications and the need to reduce risk have recently become an interesting research topic. It is crucial that effective Intrusion Detection Systems (IDSs) tailored to IoT applications be developed. Such IDSs require an updated and representative IoT dataset for training and evaluation. However, there is a lack of benchmark IoT and IIoT datasets for assessing IDSs-enabled IoT systems. This paper addresses this issue and proposes a new data-driven IoT/IIoT dataset with the ground truth that incorporates a label feature indicating normal and attack classes, as well as a type feature indicating the sub-classes of attacks targeting IoT/IIoT applications for multi-classification problems. The proposed dataset, which is named TON_IoT, includes Telemetry data of IoT/IIoT services, as well as Operating Systems logs and Network traffic of IoT network, collected from a realistic representation of a medium-scale network at the Cyber Range and IoT Labs at the UNSW Canberra (Australia). This paper also describes the proposed dataset of the Telemetry data of IoT/IIoT services and their characteristics. TON_IoT has various advantages that are currently lacking in the state-of-the-art datasets: i) it has various normal and attack events for different IoT/IIoT services, and ii) it includes heterogeneous data sources. We evaluated the performance of several popular Machine Learning (ML) methods and a Deep Learning model in both binary and multi-class classification problems for intrusion detection purposes using the proposed Telemetry dataset.","['Intrusion detection', 'Telemetry', 'Sensors', 'Internet of Things', 'Machine learning', 'Australia']","['Internet of Things (IoT)', 'Industrial Internet of Things (IIoT)', 'cybersecurity', 'intrusion detection systems (IDSs)', 'dataset']"
"The impending environmental issues and growing concerns for global energy crises are driving the need for new opportunities and technologies that can meet significantly higher demand for cleaner and sustainable energy systems. This necessitates the development of transportation and power generation systems. The electrification of the transportation system is a promising approach to green the transportation systems and to reduce the issues of climate change. This paper inspects the present status, latest deployment, and challenging issues in the implementation of Electric vehicles (EVs) infrastructural and charging systems in conjunction with several international standards and charging codes. It further analyzes EVs impacts and prospects in society. A complete assessment of charging systems for EVs with battery charging techniques is explained. Moreover, the beneficial and harmful impacts of EVs are categorized and thoroughly reviewed. Remedial measures for harmful impacts are presented and benefits obtained therefrom are highlighted. Bidirectional charging offers the fundamental feature of vehicle to grid technology. In this paper, the current challenging issues due to the massive deployment of EVs, and upcoming research trends are also presented. It is envisioned that the researchers interested in such areas can find this paper valuable and an informative one-stop source.","['Climate change', 'Electric vehicles', 'Batteries', 'Connectors', 'Fossil fuels', 'Power grids']","['Electric vehicles (EVs)', 'international standards', 'infrastructure of charging systems', 'plug-in hybrid electric vehicles (PHEVs)', 'impacts and challenging issues', 'vehicle to gird (V2G) technology']"
"In recent years, food safety issues have drawn growing concerns from society. In order to efficiently detect and prevent food safety problems and trace the accountability, building a reliable traceability system is indispensable. It is especially essential to accurately record, share, and trace the specific data within the whole food supply chain, including the process of production, processing, warehousing, transportation, and retail. The traditional traceability systems have issues, such as data invisibility, tampering, and sensitive information disclosure. The blockchain is a promising technology for the food safety traceability system because of the characteristics, such as the irreversible time vector, smart contract, and consensus algorithm. This paper proposes a food safety traceability system based on the blockchain and the EPC Information Services and develops a prototype system. The management architecture of on-chain & off-chain data is proposed as well, through which the traceability system can alleviate the data explosion issue of the blockchain for the Internet of Things. Furthermore, the enterprise-level smart contract is designed to prevent data tampering and sensitive information disclosure during information interaction among participants. The prototype system was implemented based on the Ethereum. According to the test results, the average time of information query response is around 2 ms, while the amount of on-chain data and query counts are 1 GB and 1000 times/s, respectively.","['Blockchain', 'Supply chains', 'Smart contracts', 'Explosions', 'Supply chain management', 'Prototypes']","['Food safety', 'traceability', 'blockchain', 'EPCIS', 'on-chain & off-chain', 'smart contract']"
"Agriculture plays a vital role in the economic growth of any country. With the increase of population, frequent changes in climatic conditions and limited resources, it becomes a challenging task to fulfil the food requirement of the present population. Precision agriculture also known as smart farming have emerged as an innovative tool to address current challenges in agricultural sustainability. The mechanism that drives this cutting edge technology is machine learning (ML). It gives the machine ability to learn without being explicitly programmed. ML together with IoT (Internet of Things) enabled farm machinery are key components of the next agriculture revolution. In this article, authors present a systematic review of ML applications in the field of agriculture. The areas that are focused are prediction of soil parameters such as organic carbon and moisture content, crop yield prediction, disease and weed detection in crops and species detection. ML with computer vision are reviewed for the classification of a different set of crop images in order to monitor the crop quality and yield assessment. This approach can be integrated for enhanced livestock production by predicting fertility patterns, diagnosing eating disorders, cattle behaviour based on ML models using data collected by collar sensors, etc. Intelligent irrigation which includes drip irrigation and intelligent harvesting techniques are also reviewed that reduces human labour to a great extent. This article demonstrates how knowledge-based agriculture can improve the sustainable productivity and quality of the product.","['Agriculture', 'Artificial intelligence', 'Internet of Things', 'Irrigation', 'Wireless sensor networks', 'Soil', 'Sensors']","['Agricultural engineering', 'machine learning', 'intelligent irrigation', 'IoT', 'prediction']"
"Fog computing is an architectural style in which network components between devices and the cloud execute application-specific logic. We present the first review on fog computing within healthcare informatics, and explore, classify, and discuss different application use cases presented in the literature. For that, we categorize applications into use case classes and list an inventory of application-specific tasks that can be handled by fog computing. We discuss on which level of the network such fog computing tasks can be executed, and provide tradeoffs with respect to requirements relevant to healthcare. Our review indicates that: 1) there is a significant number of computing tasks in healthcare that require or can benefit from fog computing principles; 2) processing on higher network tiers is required due to constraints in wireless devices and the need to aggregate data; and 3) privacy concerns and dependability prevent computation tasks to be completely moved to the cloud. These findings substantiate the need for a coherent approach toward fog computing in healthcare, for which we present a list of recommended research and development actions.","['Edge computing', 'Medical services', 'Wireless sensor networks', 'Cloud computing', 'Computer architecture', 'Wireless communication', 'Monitoring']","['Body sensor networks', 'fog computing', 'healthcare', 'health information management', 'internet of things', 'sensor devices', 'wireless sensor networks']"
"Multi-modality image fusion provides more comprehensive and sophisticated information in modern medical diagnosis, remote sensing, video surveillance, and so on. This paper presents a novel multi-modality medical image fusion method based on phase congruency and local Laplacian energy. In the proposed method, the non-subsampled contourlet transform is performed on medical image pairs to decompose the source images into high-pass and low-pass subbands. The high-pass subbands are integrated by a phase congruency-based fusion rule that can enhance the detailed features of the fused image for medical diagnosis. A local Laplacian energy-based fusion rule is proposed for low-pass subbands. The local Laplacian energy consists of weighted local energy and the weighted sum of Laplacian coefficients that describe the structured information and the detailed features of source image pairs, respectively. Thus, the proposed fusion rule can simultaneously integrate two key components for the fusion of low-pass subbands. The fused high-pass and low-pass subbands are inversely transformed to obtain the fused image. In the comparative experiments, three categories of multi-modality medical image pairs are used to verify the effectiveness of the proposed method. The experiment results show that the proposed method achieves competitive performance in both the image quantity and computational costs.","['Image fusion', 'Transforms', 'Laplace equations', 'Medical diagnostic imaging', 'Feature extraction', 'Medical diagnosis']","['Medical image fusion', 'multi-modality sensor fusion', 'NSCT', 'phase congruency', 'Laplacian energy']"
"The potential of blockchain has been extensively discussed in the literature and media mainly in finance and payment industry. One relatively recent trend is at the enterprise-level, where blockchain serves as the infrastructure for internet security and immutability. Emerging application domains include Industry 4.0 and Industrial Internet of Things (IIoT). Therefore, in this paper, we comprehensively review existing blockchain applications in Industry 4.0 and IIoT settings. Specifically, we present the current research trends in each of the related industrial sectors, as well as successful commercial implementations of blockchain in these relevant sectors. We also discuss industry-specific challenges for the implementation of blockchain in each sector. Further, we present currently open issues in the adoption of the blockchain technology in Industry 4.0 and discuss newer application areas. We hope that our findings pave the way for empowering and facilitating research in this domain, and assist decision-makers in their blockchain adoption and investment in Industry 4.0 and IIoT space.","['Industries', 'Medical services', 'Peer-to-peer computing', 'Internet of Things', 'Contracts', 'Production']","['Internet of Things', 'industry 40', 'industrial IoT', 'blockchain', 'smart contracts']"
"Approximate message passing (AMP) is a low-cost iterative signal recovery algorithm for linear system models. When the system transform matrix has independent identically distributed (IID) Gaussian entries, the performance of AMP can be asymptotically characterized by a simple scalar recursion called state evolution (SE). However, SE may become unreliable for other matrix ensembles, especially for ill-conditioned ones. This imposes limits on the applications of AMP. In this paper, we propose an orthogonal AMP (OAMP) algorithm based on de-correlated linear estimation (LE) and divergence-free non-linear estimation (NLE). The Onsager term in standard AMP vanishes as a result of the divergence-free constraint on NLE. We develop an SE procedure for OAMP and show numerically that the SE for OAMP is accurate for general unitarily-invariant matrices, including IID Gaussian matrices and partial orthogonal matrices. We further derive optimized options for OAMP and show that the corresponding SE fixed point coincides with the optimal performance obtained via the replica method. Our numerical results demonstrate that OAMP can be advantageous over AMP, especially for ill-conditioned matrices.","['Discrete cosine transforms', 'Estimation', 'Message passing', 'Algorithm design and analysis', 'Sparse matrices', 'Gaussian processes', 'Orthogonal matrices']","['Compressed sensing', 'approximate message passing (AMP)', 'replica method', 'state evolution', 'unitarily-invariant', 'IID Gaussian', 'partial orthogonal matrix']"
"With the accelerated development of Internet-of-Things (IoT), wireless sensor networks (WSNs) are gaining importance in the continued advancement of information and communication technologies, and have been connected and integrated with the Internet in vast industrial applications. However, given the fact that most wireless sensor devices are resource constrained and operate on batteries, the communication overhead and power consumption are therefore important issues for WSNs design. In order to efficiently manage these wireless sensor devices in a unified manner, the industrial authorities should be able to provide a network infrastructure supporting various WSN applications and services that facilitate the management of sensor-equipped real-world entities. This paper presents an overview of industrial ecosystem, technical architecture, industrial device management standards, and our latest research activity in developing a WSN management system. The key approach to enable efficient and reliable management of WSN within such an infrastructure is a cross-layer design of lightweight and cloud-based RESTful Web service.","['Urban areas', 'Wireless sensor networks', 'Monitoring', 'Industries', 'Standards', 'Security', 'Transportation']","['Internet-of-Things', 'device management', 'IEEE 802.15.4', 'RESTful', 'error correction coding (ECC)', 'cloud']"
"This paper presents the latest progress on cloud RAN (C-RAN) in the areas of centralization and virtualization. A C-RAN system centralizes the baseband processing resources into a pool and virtualizes soft base-band units on demand. The major challenges for C-RAN including front-haul and virtualization are analyzed with potential solutions proposed. Extensive field trials verify the viability of various front-haul solutions, including common public radio interface compression, single fiber bidirection and wavelength-division multiplexing. In addition, C-RANs facilitation of coordinated multipoint (CoMP) implementation is demonstrated with 50%-100% uplink CoMP gain observed in field trials. Finally, a test bed is established based on general purpose platform with assisted accelerators. It is demonstrated that this test bed can support multi-RAT, i.e., Time-Division Duplexing Long Term Evolution, Frequency-Division Duplexing Long Term Evolution, and Global System for Mobile Communications efficiently and presents similar performance to traditional systems.","['Optical fiber networks', 'Wavelength division multiplexing', 'Virtualization', 'Optical fiber devices', 'Radio access networks', 'Optical fiber testing', 'Mobile communication']","['C-RAN', 'CoMP', 'virtualization', 'cloud', 'front-haul']"
"Fog computing, an extension of cloud computing services to the edge of the network to decrease latency and network congestion, is a relatively recent research trend. Although both cloud and fog offer similar resources and services, the latter is characterized by low latency with a wider spread and geographically distributed nodes to support mobility and real-time interaction. In this paper, we describe the fog computing architecture and review its different services and applications. We then discuss security and privacy issues in fog computing, focusing on service and resource availability. Virtualization is a vital technology in both fog and cloud computing that enables virtual machines (VMs) to coexist in a physical server (host) to share resources. These VMs could be subject to malicious attacks or the physical server hosting it could experience system failure, both of which result in unavailability of services and resources. Therefore, a conceptual smart pre-copy live migration approach is presented for VM migration. Using this approach, we can estimate the downtime after each iteration to determine whether to proceed to the stop-and-copy stage during a system failure or an attack on a fog computing node. This will minimize both the downtime and the migration time to guarantee resource and service availability to the end users of fog computing. Last, future research directions are outlined.","['Edge computing', 'Cloud computing', 'Computer architecture', 'Real-time systems', 'Wireless sensor networks', 'Servers', 'Sensors']","['Cloud computing', 'edge computing', 'fog computing', 'live VM migration framework', 'virtualization']"
"Research on machine assisted text analysis follows the rapid development of digital media, and sentiment analysis is among the prevalent applications. Traditional sentiment analysis methods require complex feature engineering, and embedding representations have dominated leaderboards for a long time. However, the context-independent nature limits their representative power in rich context, hurting performance in Natural Language Processing (NLP) tasks. Bidirectional Encoder Representations from Transformers (BERT), among other pre-trained language models, beats existing best results in eleven NLP tasks (including sentence-level sentiment classification) by a large margin, which makes it the new baseline of text representation. As a more challenging task, fewer applications of BERT have been observed for sentiment classification at the aspect level. We implement three target-dependent variations of the BERT base model, with positioned output at the target terms and an optional sentence with the target built in. Experiments on three data collections show that our TD-BERT model achieves new state-of-the-art performance, in comparison to traditional feature engineering methods, embedding-based models and earlier applications of BERT. With the successful application of BERT in many NLP tasks, our experiments try to verify if its context-aware representation can achieve similar performance improvement in aspect-based sentiment analysis. Surprisingly, coupling it with complex neural networks that used to work well with embedding representations does not show much value, sometimes with performance below the vanilla BERT-FC implementation. On the other hand, incorporation of target information shows stable accuracy improvement, and the most effective way of utilizing that information is displayed through the experiment.","['Bit error rate', 'Task analysis', 'Neural networks', 'Sentiment analysis', 'Context modeling', 'Media']","['Deep learning', 'neural networks', 'sentiment analysis', 'BERT']"
"This paper presents end-to-end learning from spectrum data-an umbrella term for new sophisticated wireless signal identification approaches in spectrum monitoring applications based on deep neural networks. End-to-end learning allows to: 1) automatically learn features directly from simple wireless signal representations, without requiring design of hand-crafted expert features like higher order cyclic moments and 2) train wireless signal classifiers in one end-to-end step which eliminates the need for complex multi-stage machine learning processing pipelines. The purpose of this paper is to present the conceptual framework of end-to-end learning for spectrum monitoring and systematically introduce a generic methodology to easily design and implement wireless signal classifiers. Furthermore, we investigate the importance of the choice of wireless data representation to various spectrum monitoring tasks. In particular, two case studies are elaborated: 1) modulation recognition and 2) wireless technology interference detection. For each case study three convolutional neural networks are evaluated for the following wireless signal representations: temporal IQ data, the amplitude/phase representation, and the frequency domain representation. From our analysis, we prove that the wireless data representation impacts the accuracy depending on the specifics and similarities of the wireless signals that need to be differentiated, with different data representations resulting in accuracy variations of up to 29%. Experimental results show that using the amplitude/phase representation for recognizing modulation formats can lead to performance improvements up to 2% and 12% for medium to high SNR compared to IQ and frequency domain data, respectively. For the task of detecting interference, frequency domain representation outperformed amplitude/phase and IQ data representation up to 20%.","['Wireless communication', 'Wireless sensor networks', 'Monitoring', 'Machine learning', 'Interference', 'Modulation', 'Pipelines']","['Big spectrum data', 'spectrum monitoring', 'end-to-end learning', 'deep learning', 'convolutional neural networks', 'wireless signal identification', 'IoT']"
"We demonstrate use of iteratively pruned deep learning model ensembles for detecting pulmonary manifestations of COVID-19 with chest X-rays. This disease is caused by the novel Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) virus, also known as the novel Coronavirus (2019-nCoV). A custom convolutional neural network and a selection of ImageNet pretrained models are trained and evaluated at patient-level on publicly available CXR collections to learn modality-specific feature representations. The learned knowledge is transferred and fine-tuned to improve performance and generalization in the related task of classifying CXRs as normal, showing bacterial pneumonia, or COVID-19-viral abnormalities. The best performing models are iteratively pruned to reduce complexity and improve memory efficiency. The predictions of the best-performing pruned models are combined through different ensemble strategies to improve classification performance. Empirical evaluations demonstrate that the weighted average of the best-performing pruned models significantly improves performance resulting in an accuracy of 99.01% and area under the curve of 0.9972 in detecting COVID-19 findings on CXRs. The combined use of modality-specific knowledge transfer, iterative model pruning, and ensemble learning resulted in improved predictions. We expect that this model can be quickly adopted for COVID-19 screening using chest radiographs.","['Lung', 'COVID-19', 'Microorganisms', 'Computational modeling', 'National Institutes of Health', 'Predictive models']","['COVID-19', 'convolutional neural network', 'deep learning', 'ensemble', 'iterative pruning']"
"Nowadays synthetic aperture radar (SAR) and multiple-input-multiple-output (MIMO) antenna systems with the capability to radiate waves in more than one pattern and polarization are playing a key role in modern telecommunication and radar systems. This is possible with the use of antenna arrays as they offer advantages of high gain and beamforming capability, which can be utilized for controlling radiation pattern for electromagnetic (EM) interference immunity in wireless systems. However, with the growing demand for compact array antennas, the physical footprint of the arrays needs to be smaller and the consequent of this is severe degradation in the performance of the array resulting from strong mutual-coupling and crosstalk effects between adjacent radiating elements. This review presents a detailed systematic and theoretical study of various mutual-coupling suppression (decoupling) techniques with a strong focus on metamaterial (MTM) and metasurface (MTS) approaches. While the performance of systems employing antenna arrays can be enhanced by calibrating out the interferences digitally, however it is more efficient to apply decoupling techniques at the antenna itself. Previously various simple and cost-effective approaches have been demonstrated to effectively suppress unwanted mutual-coupling in arrays. Such techniques include the use of defected ground structure (DGS), parasitic or slot element, dielectric resonator antenna (DRA), complementary split-ring resonators (CSRR), decoupling networks, P.I.N or varactor diodes, electromagnetic bandgap (EBG) structures, etc. In this review, it is shown that the mutual-coupling reduction methods inspired By MTM and MTS concepts can provide a higher level of isolation between neighbouring radiating elements using easily realizable and cost-effective decoupling configurations that have negligible consequence on the array's characteristics such as bandwidth, gain and radiation efficiency, and physical footprint.","['Antenna arrays', 'MIMO communication', 'Mutual coupling', 'Synthetic aperture radar', 'Metamaterials']","['Decoupling methods', 'metamaterial (MTM)', 'metasurface (MTS)', 'multiple-input-multiple-output (MIMO)', 'synthetic aperture radar (SAR)', 'isolation enhancement', 'array antennas']"
"Providing reliable broadband wireless communications in high mobility environments, such as high-speed railway systems, remains one of the main challenges faced by the development of the next generation wireless systems. This paper provides a systematic review of high mobility communications. We first summarize a list of key challenges and opportunities in high mobility communication systems, then provide comprehensive reviews of techniques that can address these challenges and utilize the unique opportunities. The review covers a wide spectrum of communication operations, including the accurate modeling of high mobility channels, the transceiver structures that can exploit the properties of high mobility environments, the signal processing techniques that can harvest the benefits (e.g., Doppler diversity) and mitigate the impairments (e.g., carrier frequency offset, intercarrier interference, channel estimation errors) in high mobility systems, and the mobility management and network architectures that are designed specifically for high mobility systems. The survey focuses primarily on physical layer operations, which are affected the most by the mobile environment, with some additional discussions on higher layer operations, such as handover management and control-plane/user-plane decoupling, which are essential to high mobility operations. Future research directions on high mobility communications are summarized at the end of this paper.","['Doppler effect', 'Handover', 'Fading channels', 'OFDM', 'Wireless communication', 'Channel estimation', 'Mobility management']","['High mobility communications', 'fast time-varying fading', 'CFO', 'ICI', 'Doppler diversity', 'mobility management']"
"Internet of Things (IoT) connects sensing devices to the Internet for the purpose of exchanging information. Location information is one of the most crucial pieces of information required to achieve intelligent and context-aware IoT systems. Recently, positioning and localization functions have been realized in a large amount of IoT systems. However, security and privacy threats related to positioning in IoT have not been sufficiently addressed so far. In this paper, we survey solutions for improving the robustness, security, and privacy of location-based services in IoT systems. First, we provide an in-depth evaluation of the threats and solutions related to both global navigation satellite system (GNSS) and non-GNSS-based solutions. Second, we describe certain cryptographic solutions for security and privacy of positioning and location-based services in IoT. Finally, we discuss the state-of-the-art of policy regulations regarding security of positioning solutions and legal instruments to location data privacy in detail. This survey paper addresses a broad range of security and privacy aspects in IoT-based positioning and localization from both technical and legal points of view and aims to give insight and recommendations for future IoT systems providing more robust, secure, and privacy-preserving location-based services.","['Security', 'Robustness', 'Satellites', 'Receivers', 'Privacy', 'Law', 'Terrestrial atmosphere']","['Positioning', 'wireless localization', 'navigation', 'Internet of Things', 'GNSS', 'vulnerabilities', 'security', 'privacy', 'cryptography', 'trustworthiness', 'literature study']"
"Smart cities contain intelligent things which can intelligently automatically and collaboratively enhance life quality, save people's lives, and act a sustainable resource ecosystem. To achieve these advanced collaborative technologies such as drones, robotics, artificial intelligence, and Internet of Things (IoT) are required to increase the smartness of smart cities by improving the connectivity, energy efficiency, and quality of services (QoS). Therefore, collaborative drones and IoT play a vital role in supporting a lot of smart-city applications such as those involved in communication, transportation, agriculture,safety and security, disaster mitigation, environmental protection, service delivery, energy saving, e-waste reduction, weather monitoring, healthcare, etc. This paper presents a survey of the potential techniques and applications of collaborative drones and IoT which have recently been proposed in order to increase the smartness of smart cities. It provides a comprehensive overview highlighting the recent and ongoing research on collaborative drone and IoT in improving the real-time application of smart cities. This survey is different from previous ones in term of breadth, scope, and focus. In particular, we focus on the new concept of collaborative drones and IoT for improving smart-city applications. This survey attempts to show how collaborative drones and IoT improve the smartness of smart cities based on data collection, privacy and security, public safety, disaster management, energy consumption and quality of life in smart cities. It mainly focuses on the measurement of the smartness of smart cities, i.e., environmental aspects, life quality, public safety, and disaster management.","['Drones', 'Smart cities', 'Internet of Things', 'Collaboration', 'Monitoring', 'Safety', 'Security']","['ICT', 'smart city', 'energy consumption', 'smart drone', 'IoT', 'pollutions', 'gathering data', 'IoD', 'disaster', 'public safety', 'security and privacy', 'collaborative drone', 'IoT']"
"Deep learning is a branch of artificial intelligence. In recent years, with the advantages of automatic learning and feature extraction, it has been widely concerned by academic and industrial circles. It has been widely used in image and video processing, voice processing, and natural language processing. At the same time, it has also become a research hotspot in the field of agricultural plant protection, such as plant disease recognition and pest range assessment, etc. The application of deep learning in plant disease recognition can avoid the disadvantages caused by artificial selection of disease spot features, make plant disease feature extraction more objective, and improve the research efficiency and technology transformation speed. This review provides the research progress of deep learning technology in the field of crop leaf disease identification in recent years. In this paper, we present the current trends and challenges for the detection of plant leaf disease using deep learning and advanced imaging techniques. We hope that this work will be a valuable resource for researchers who study the detection of plant diseases and insect pests. At the same time, we also discussed some of the current challenges and problems that need to be resolved.","['Diseases', 'Deep learning', 'Feature extraction', 'Image recognition', 'Plants (biology)', 'Agriculture', 'Image color analysis']","['Deep learning', 'plant leaf disease detection', 'visualization', 'small sample', 'hyperspectral imaging']"
"Motor bearing is subjected to the joint effects of much more loads, transmissions, and shocks that cause bearing fault and machinery breakdown. A vibration signal analysis method is the most popular technique that is used to monitor and diagnose the fault of motor bearing. However, the application of the vibration signal analysis method for motor bearing is very limited in engineering practice. In this paper, on the basis of comparing fault feature extraction by using empirical wavelet transform (EWT) and Hilbert transform with the theoretical calculation, a new motor bearing fault diagnosis method based on integrating EWT, fuzzy entropy, and support vector machine (SVM) called EWTFSFD is proposed. In the proposed method, a novel signal processing method called EWT is used to decompose vibration signal into multiple components in order to extract a series of amplitude modulated-frequency modulated (AM-FM) components with supporting Fourier spectrum under an orthogonal basis. Then, fuzzy entropy is utilized to measure the complexity of vibration signal, reflect the complexity changes of intrinsic oscillation, and compute the fuzzy entropy values of AM-FM components, which are regarded as the inputs of the SVM model to train and construct an SVM classifier for fulfilling fault pattern recognition. Finally, the effectiveness of the proposed method is validated by using the simulated signal and real motor bearing vibration signals. The experiment results show that the EWT outperforms empirical mode decomposition for decomposing the signal into multiple components, and the proposed EWTFSFD method can accurately and effectively achieve the fault diagnosis of motor bearing.","['Fault diagnosis', 'Wavelet transforms', 'Entropy', 'Vibrations', 'Feature extraction', 'Support vector machines']","['Motor bearing', 'fault diagnosis', 'empirical wavelet transform', 'fuzzy entropy', 'support vector machine', 'Fourier spectrum segmentation', 'AM-FM components']"
"Fifth-generation (5G) cellular networks will almost certainly operate in the high-bandwidth, underutilized millimeter-wave (mmWave) frequency spectrum, which offers the potentiality of high-capacity wireless transmission of multi-gigabit-per-second (Gbps) data rates. Despite the enormous available bandwidth potential, mmWave signal transmissions suffer from fundamental technical challenges like severe path loss, sensitivity to blockage, directivity, and narrow beamwidth, due to its short wavelengths. To effectively support system design and deployment, accurate channel modeling comprising several 5G technologies and scenarios is essential. This survey provides a comprehensive overview of several emerging technologies for 5G systems, such as massive multiple-input multiple-output (MIMO) technologies, multiple access technologies, hybrid analog-digital precoding and combining, non-orthogonal multiple access (NOMA), cell-free massive MIMO, and simultaneous wireless information and power transfer (SWIPT) technologies. These technologies induce distinct propagation characteristics and establish specific requirements on 5G channel modeling. To tackle these challenges, we first provide a survey of existing solutions and standards and discuss the radio-frequency (RF) spectrum and regulatory issues for mmWave communications. Second, we compared existing wireless communication techniques like sub-6-GHz WiFi and sub-6 GHz 4G LTE over mmWave communications which come with benefits comprising narrow beam, high signal quality, large capacity data transmission, and strong detection potential. Third, we describe the fundamental propagation characteristics of the mmWave band and survey the existing channel models for mmWave communications. Fourth, we track evolution and advancements in hybrid beamforming for massive MIMO systems in terms of system models of hybrid precoding architectures, hybrid analog and digital precoding/combining matrices, with the potential antenna configuration scenarios and mmWave channel estimation (CE) techniques. Fifth, we extend the scope of the discussion by including multiple access technologies for mmWave systems such as non-orthogonal multiple access (NOMA) and space-division multiple access (SDMA), with limited RF chains at the base station. Lastly, we explore the integration of SWIPT in mmWave massive MIMO systems, with limited RF chains, to realize spectrally and energy-efficient communications.","['5G mobile communication', 'Millimeter wave communication', 'NOMA', 'MIMO communication', 'Wireless communication', 'Precoding', 'Radio frequency']","['Millimeter wave communications', 'propagation', 'channel measurements', 'channel models', 'MIMO', 'hybrid precoding', 'non-orthogonal multiple access (NOMA)', 'multiple access techniques', 'simultaneous wireless information and power transfer (SWIPT)', 'RF energy harvesting']"
"With the development of wireless communication technology, the need for bandwidth is increasing continuously, and the growing need makes wireless spectrum resources more and more scarce. Cognitive radio (CR) has been identified as a promising solution for the spectrum scarcity, and its core idea is the dynamic spectrum access. It can dynamically utilize the idle spectrum without affecting the rights of primary users, so that multiple services or users can share a part of the spectrum, thus achieving the goal of avoiding the high cost of spectrum resetting and improving the utilization of spectrum resources. In order to meet the critical requirements of the fifth generation (5G) mobile network, especially the Wider-Coverage , Massive-Capacity , Massive-Connectivity , and Low-Latency four application scenarios, the spectrum range used in 5G will be further expanded into the full spectrum era, possibly from 1 GHz to 100 GHz. In this paper, we conduct a comprehensive survey of CR technology and focus on the current significant research progress in the full spectrum sharing towards the four scenarios. In addition, the key enabling technologies that may be closely related to the study of 5G in the near future are presented in terms of full-duplex spectrum sensing, spectrum-database based spectrum sensing, auction based spectrum allocation, carrier aggregation based spectrum access. Subsequently, other issues that play a positive role for the development research and practical application of CR, such as common control channel, energy harvesting, non-orthogonal multiple access, and CR based aeronautical communication are discussed. The comprehensive overview provided by this survey is expected to help researchers develop CR technology in the field of 5G further.","['5G mobile communication', 'Sensors', 'Resource management', 'Internet', 'Cognitive radio']","['5G', 'cognitive radio', 'spectrum sharing', 'full-duplex spectrum sensing', 'carrier aggregation', 'energy harvesting']"
"A compact, high performance, and novel-shaped ultra-wideband (UWB) multiple-input multiple-output (MIMO) antenna with low mutual coupling is presented in this paper. The proposed antenna consists of two radiating elements with shared ground plane having an area of 50 x 30 mm 2 . F-shaped stubs are introduced in the shared ground plane of the proposed antenna to produce high isolation between the MIMO antenna elements. The designed MIMO antenna has very low mutual coupling of (S 21 <; -20 dB), low envelop correlation coefficient (ECC <; 0.04), high diversity gain (DG > 7.4 dB), high multiplexing efficiency (η Mux > -3.5), and high peak gain over the entire UWB frequencies. The antenna performance is studied in terms of S-Parameters, radiation properties, peak gain, diversity gain, envelop correlation coefficient, and multiplexing efficiency. A good agreement between the simulated and measured results is observed.","['MIMO communication', 'Mutual coupling', 'Ultra wideband antennas', 'Antenna measurements', 'Diversity methods', 'Multiplexing']","['MIMO antenna', 'diversity gain', 'multiplexing efficiency', 'microstrip patch']"
"Photovoltaic power generation forecasting is an important topic in the field of sustainable power system design, energy conversion management, and smart grid construction. Difficulties arise while the generated PV power is usually unstable due to the variability of solar irradiance, temperature, and other meteorological factors. In this paper, a hybrid ensemble deep learning framework is proposed to forecast short-term photovoltaic power generation in a time series manner. Two LSTM neural networks are employed working on temperature and power outputs forecasting, respectively. The forecasting results are flattened and combined with a fully connected layer to enhance forecasting accuracy. Moreover, we adopted the attention mechanism for the two LSTM neural networks to adaptively focus on input features that are more significant in forecasting. Comprehensive experiments are conducted with recently collected real-world photovoltaic power generation datasets. Three error metrics were adopted to compare the forecasting results produced by attention LSTM model with state-of-art methods, including the persistent model, the auto-regressive integrated moving average model with exogenous variable (ARIMAX), multi-layer perceptron (MLP), and the traditional LSTM model in all four seasons and various forecasting horizons to show the effectiveness and robustness of the proposed method.","['Forecasting', 'Predictive models', 'Logic gates', 'Power generation', 'Time series analysis', 'Neural networks', 'Deep learning']","['PV power generation', 'short-term forecasting', 'long short term memory', 'attention mechanism']"
"Since the early 1990s, a large number of chaos-based communication systems have been proposed exploiting the properties of chaotic waveforms. The motivation lies in the significant advantages provided by this class of non-linear signals. For this aim, many communication schemes and applications have been specially designed for chaos-based communication systems where energy, data rate, and synchronization awareness are considered in most designs. Recently, the major focus, however, has been given to the non-coherent chaos-based systems to benefit from the advantages of chaotic signals and non-coherent detection and to avoid the use of chaotic synchronization, which suffers from weak performance in the presence of additive noise. This paper presents a comprehensive survey of the entire wireless radio frequency chaos-based communication systems. First, it outlines the challenges of chaos implementations and synchronization methods, followed by comprehensive literature review and analysis of chaos-based coherent techniques and their applications. In the second part of the survey, we offer a taxonomy of the current literature by focusing on non-coherent detection methods. For each modulation class, this paper categorizes different transmission techniques by elaborating on its modulation, receiver type, data rate, complexity, energy efficiency, multiple access scheme, and performance. In addition, this survey reports on the analysis of tradeoff between different chaos-based communication systems. Finally, several concluding remarks are discussed.","['Chaotic communication', 'Synchronization', 'Coherent systems', 'Telecommunication services', 'Performance evaluation', 'Additive noise', 'Noise measurement', 'Nonlinear systems', 'Modulation']","['Chaos-based communication systems', 'chaos implementation', 'chaotic synchronization', 'coherent systems', 'noncoherent systems', 'applications', 'performance analysis']"
"An Internet of Vehicles (IoV) allows forming a self-organized network and broadcasting messages for the vehicles on roads. However, as the data are transmitted in an insecure network, it is essential to use an authentication mechanism to protect the privacy of vehicle users. Recently, Ying et al. proposed an authentication protocol for IoV and claimed that the protocol could resist various attacks. Unfortunately, we discovered that their protocol suffered from an offline identity guessing attack, location spoofing attack, and replay attack, and consumed a considerable amount of time for authentication. To resolve these shortcomings, we propose an improved protocol. In addition, we provide a formal proof to the proposed protocol to demonstrate that our protocol is indeed secure. Compared with previous methods, the proposed protocol performs better in terms of security and performance.","['Protocols', 'Authentication', 'Vehicular ad hoc networks', 'Smart cards', 'Cloud computing', 'Privacy']","['Internet of Vehicles', 'authentication', 'anonymity', 'smart card']"
"Internet of Things (IoT) is an emerging concept, which aims to connect billions of devices with each other. The IoT devices sense, collect, and transmit important information from their surroundings. This exchange of very large amount of information amongst billions of devices creates a massive energy need. Green IoT envisions the concept of reducing the energy consumption of IoT devices and making the environment safe. Inspired by achieving a sustainable environment for IoT, we first give the overview of green IoT and the challenges that are faced due to excessive usage of energy hungry IoT devices. We then discuss and evaluate the strategies that can be used to minimize the energy consumption in IoT, such as designing energy efficient datacenters, energy efficient transmission of data from sensors, and design of energy efficient policies. Moreover, we critically analyze the green IoT strategies and propose five principles that can be adopted to achieve green IoT. Finally, we consider a case study of very important aspect of IoT, i.e., smart phones and we provide an easy and concise view for improving the current practices to make the IoT greener for the world in 2020 and beyond.","['Green products', 'Air pollution', 'Energy efficiency', 'Radiofrequency identification', 'Internet', 'Energy consumption']","['Internet of things', 'green IoT', 'datacenter', 'green computing', 'smart phones']"
"In recent years, the emergence of blockchain technology (BT) has become a unique, most disruptive, and trending technology. The decentralized database in BT emphasizes data security and privacy. Also, the consensus mechanism in it makes sure that data is secured and legitimate. Still, it raises new security issues such as majority attack and double-spending. To handle the aforementioned issues, data analytics is required on blockchain based secure data. Analytics on these data raises the importance of arisen technology Machine Learning (ML). ML involves the rational amount of data to make precise decisions. Data reliability and its sharing are very crucial in ML to improve the accuracy of results. The combination of these two technologies (ML and BT) can provide highly precise results. In this paper, we present a detailed study on ML adoption for making BT-based smart applications more resilient against attacks. There are various traditional ML techniques, for instance, Support Vector Machines (SVM), clustering, bagging, and Deep Learning (DL) algorithms such as Convolutional Neural Network (CNN) and Long short-term memory (LSTM) can be used to analyse the attacks on a blockchain-based network. Further, we include how both the technologies can be applied in several smart applications such as Unmanned Aerial Vehicle (UAV), Smart Grid (SG), healthcare, and smart cities. Then, future research issues and challenges are explored. At last, a case study is presented with a conclusion.","['Blockchain', 'Security', 'Machine learning', 'Taxonomy', 'Databases', 'Prediction algorithms', 'Malware']","['Blockchain', 'machine learning', 'smart grid', 'data security and privacy', 'data analytics', 'smart applications']"
"Brain cancer classification is an important step that depends on the physician's knowledge and experience. An automated tumor classification system is very essential to support radiologists and physicians to identify brain tumors. However, the accuracy of current systems needs to be improved for suitable treatments. In this paper, we propose a hybrid feature extraction method with a regularized extreme learning machine (RELM) for developing an accurate brain tumor classification approach. The approach starts by preprocessing the brain images by using a min-max normalization rule to enhance the contrast of brain edges and regions. Then, the brain tumor features are extracted based on a hybrid method of feature extraction. Finally, a RELM is used for classifying the type of brain tumor. To evaluate and compare the proposed approach, a set of experiments is conducted on a new public dataset of brain images. The experimental results proved that the approach is more effective compared with the existing state-of-the-art approaches, and the performance in terms of classification accuracy improved from 91.51% to 94.233% for the experiment of the random holdout technique.","['Feature extraction', 'Tumors', 'Brain', 'Image segmentation', 'Gabor filters', 'Principal component analysis', 'Training']","['Brain tumor classification', 'hybrid feature extraction', 'NGIST features', 'PCA', 'regularized extreme learning machine']"
"Forecasting of fast fluctuated and high-frequency financial data is always a challenging problem in the field of economics and modelling. In this study, a novel hybrid model with the strength of fractional order derivative is presented with their dynamical features of deep learning, long-short term memory (LSTM) networks, to predict the abrupt stochastic variation of the financial market. Stock market prices are dynamic, highly sensitive, nonlinear and chaotic. There are different techniques for forecast prices in the time-variant domain and due to variability and uncertain behavior in stock prices, traditional methods, such as data mining, statistical approaches, and non-deep neural networks models are not suited for prediction and generalized forecasting stock prices. While autoregressive fractional integrated moving average (ARFIMA) model provides a flexible tool for classes of long-memory models. The advancement of machine learning-based deep non-linear modelling confirms that the hybrid model efficiently extracts profound features and model non-linear functions. LSTM networks are a special kind of recurrent neural network (RNN) that map sequences of input observations to output observations with capabilities of long-term dependencies. A novel ARFIMA-LSTM hybrid recurrent network is presented in which ARFIMA model-based filters having the linear tendencies better than ARIMA model in the data and passes the residual to the LSTM model that captures nonlinearity in the residual values with the help of exogenous dependent variables. The model not only minimizes the volatility problem but also overcome the over fitting problem of neural networks. The model is evaluated using PSX company data of the stock market based on RMSE, MSE and MAPE along with a comparison of ARIMA, LSTM model and generalized regression radial basis neural network (GRNN) ensemble method independently. The forecasting performance indicates the effectiveness of the proposed AFRIMA-LSTM hybrid model to improve around 80% accuracy on RMSE as compared to traditional forecasting counterparts.","['Data models', 'Predictive models', 'Forecasting', 'Time series analysis', 'Neural networks', 'Stock markets', 'Mathematical model']","['ARIMA model', 'ARFIMA model', 'GARCH model', 'RNN', 'LSTM model', 'RMSE', 'MSE', 'MAPE']"
"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper, we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.","['Task analysis', 'Feature extraction', 'Computational modeling', 'Data models', 'Machine learning', 'Learning systems', 'Natural language processing']","['Contrastive learning', 'representation learning', 'self-supervised learning', 'unsupervised learning', 'deep learning', 'machine learning']"
"Electroencephalogram (EEG) comprises valuable details related to the different physiological state of the brain. In this paper, a framework is offered for detecting the epileptic seizures from EEG data recorded from normal subjects and epileptic patients. This framework is based on a discrete wavelet transform (DWT) analysis of EEG signals using linear and nonlinear classifiers. The performance of the 14 different combinations of two-class epilepsy detection is studied using naïve Bayes (NB) and k-nearest neighbor (k-NN) classifiers for the derived statistical features from DWT. It has been found that the NB classifier performs better and shows an accuracy of 100% for the individual and combined statistical features derived from the DWT values of normal eyes open and epileptic EEG data provided by the University of Bonn, Germany. It has been found that the computation time of NB classifier is lesser than k-NN to provide better accuracy. So, the detection of an epileptic seizure based on DWT statistical features using NB classifiers is more suitable in real time for a reliable, automatic epileptic seizure detection system to enhance the patient's care and the quality of life.","['Discrete wavelet transforms', 'Electroencephalography', 'Feature extraction', 'Bayes methods', 'Physiology', 'Brain models', 'Seizures']","['Electroencephalograms (EEG)', 'epilepsy', 'discrete wavelet transform (DWT)', 'naïve Bayes (NB)', 'k-nearest neighbor (k-NN)']"
"Index modulation has become a promising technique in the context of orthogonal frequency division multiplexing (OFDM), whereby the specific activation of the frequency domain subcarriers is used for implicitly conveying extra information, hence improving the achievable throughput at a given bit error ratio (BER) performance. In this paper, a dual-mode OFDM technique (DM-OFDM) is proposed, which is combined with index modulation and enhances the attainable throughput of conventional index-modulation-based OFDM. In particular, the subcarriers are divided into several subblocks, and in each subblock, all the subcarriers are partitioned into two groups, modulated by a pair of distinguishable modem-mode constellations, respectively. Hence, the information bits are conveyed not only by the classic constellation symbols, but also implicitly by the specific activated subcarrier indices, representing the subcarriers' constellation mode. At the receiver, a maximum likelihood (ML) detector and a reduced-complexity near optimal log-likelihood ratio-based detector are invoked for demodulation. The minimum distance between the different legitimate realizations of the OFDM subblocks is calculated for characterizing the performance of DM-OFDM. Then, the associated theoretical analysis based on the pairwise error probability is carried out for estimating the BER of DM-OFDM. Furthermore, the simulation results confirm that at a given throughput, DM-OFDM achieves a considerably better BER performance than other OFDM systems using index modulation, while imposing the same or lower computational complexity. The results also demonstrate that the performance of the proposed low-complexity detector is indistinguishable from that of the ML detector, provided that the system's signal to noise ratio is sufficiently high.","['Indexes', 'OFDM', 'Detectors', 'Phase shift keying', 'Throughput', 'Signal to noise ratio']","['Orthogonal frequency division multiplexing (OFDM)', 'index modulation', 'index pattern', 'constellation', 'maximum likelihood detection', 'log-likelihood ratio based detection']"
"Heart disease is one of the complex diseases and globally many people suffered from this disease. On time and efficient identification of heart disease plays a key role in healthcare, particularly in the field of cardiology. In this article, we proposed an efficient and accurate system to diagnosis heart disease and the system is based on machine learning techniques. The system is developed based on classification algorithms includes Support vector machine, Logistic regression, Artificial neural network, K-nearest neighbor, Naïve bays, and Decision tree while standard features selection algorithms have been used such as Relief, Minimal redundancy maximal relevance, Least absolute shrinkage selection operator and Local learning for removing irrelevant and redundant features. We also proposed novel fast conditional mutual information feature selection algorithm to solve feature selection problem. The features selection algorithms are used for features selection to increase the classification accuracy and reduce the execution time of classification system. Furthermore, the leave one subject out cross-validation method has been used for learning the best practices of model assessment and for hyperparameter tuning. The performance measuring metrics are used for assessment of the performances of the classifiers. The performances of the classifiers have been checked on the selected features as selected by features selection algorithms. The experimental results show that the proposed feature selection algorithm (FCMIM) is feasible with classifier support vector machine for designing a high-level intelligent system to identify heart disease. The suggested diagnosis system (FCMIM-SVM) achieved good accuracy as compared to previously proposed methods. Additionally, the proposed system can easily be implemented in healthcare for the identification of heart disease.","['Feature extraction', 'Diseases', 'Machine learning', 'Heart', 'Prediction algorithms', 'Machine learning algorithms', 'Support vector machines']","['Heart disease classification', 'features selection', 'disease diagnosis', 'intelligent system', 'medical data analytics']"
"With the sharp increase in the number of intelligent devices, the Internet of Things (IoT) has gained more and more attention and rapid development in recent years. It effectively integrates the physical world with the Internet over existing network infrastructure to facilitate sharing data among intelligent devices. However, its complex and large-scale network structure brings new security risks and challenges to IoT systems. To ensure the security of data, traditional access control technologies are not suitable to be directly used for implementing access control in IoT systems because of their complicated access management and the lack of credibility due to centralization. In this paper, we proposed a novel attribute-based access control scheme for IoT systems, which simplifies greatly the access management. We use blockchain technology to record the distribution of attributes in order to avoid single point failure and data tampering. The access control process has also been optimized to meet the need for high efficiency and lightweight calculation for IoT devices. The security and performance analysis show that our scheme could effectively resist multiple attacks and be efficiently implemented in IoT systems.","['Blockchain', 'Internet of Things', 'Authorization', 'Authentication']","['Access control', 'attribute-based access control', 'blockchain', 'consortium blockchain', 'Internet of Things']"
This paper introduces a novel algorithm that increases the efficiency of the current cloud-based smart-parking system and develops a network architecture based on the Internet-of-Things technology. This paper proposed a system that helps users automatically find a free parking space at the least cost based on new performance metrics to calculate the user parking cost by considering the distance and the total number of free places in each car park. This cost will be used to offer a solution of finding an available parking space upon a request by the user and a solution of suggesting a new car park if the current car park is full. The simulation results show that the algorithm helps improve the probability of successful parking and minimizes the user waiting time. We also successfully implemented the proposed system in the real world.,"['Smart systems', 'Performance evaluation', 'Algorithm design and analysis', 'Cloud computing', 'Internet of things']","['Smart-parking system', 'performance metrics']"
"Twitter sentiment analysis offers organizations ability to monitor public feeling towards the products and events related to them in real time. The first step of the sentiment analysis is the text pre-processing of Twitter data. Most existing researches about Twitter sentiment analysis are focused on the extraction of new sentiment features. However, to select the pre-processing method is ignored. This paper discussed the effects of text pre-processing method on sentiment classification performance in two types of classification tasks, and summed up the classification performances of six pre-processing methods using two feature models and four classifiers on five Twitter datasets. The experiments show that the accuracy and F1-measure of Twitter sentiment classification classifier are improved when using the pre-processing methods of expanding acronyms and replacing negation, but barely changes when removing URLs, removing numbers or stop words. The Naive Bayes and Random Forest classifiers are more sensitive than Logistic Regression and support vector machine classifiers when various pre-processing methods were applied.",[],[]
"Crowdsensing applications utilize the pervasive smartphone users to collect large-scale sensing data efficiently. The quality of sensing data depends on the participation of highly skilled users. To motivate these skilled users to participate, they should receive enough rewards for compensating their resource consumption. Available incentive mechanisms mainly consider the truthfulness of the mechanism, but mostly ignore the issues of security and privacy caused by a “trustful” center. In this paper, we propose a privacy-preserving blockchain incentive mechanism in crowdsensing applications, in which a cryptocurrency built on blockchains is used as a secure incentive way. High quality contributors will get their payments that are recorded in transaction blocks. The miners will verify the transaction according to the sensing data assessment criteria published by the server. As the transaction information can disclose users’ privacy, a node cooperation verification approach is proposed to achieve k -anonymity privacy protection. Through theoretical analysis and simulation experiments, we show the feasibility and security of our incentive mechanism.","['Sensors', 'Servers', 'Task analysis', 'Data privacy', 'Privacy']","['Blockchain', 'crowdsensing', 'incentive mechanism', 'node cooperation', 'privacy-preserving', 'signcryption']"
"Diabetes, also known as chronic illness, is a group of metabolic diseases due to a high level of sugar in the blood over a long period. The risk factor and severity of diabetes can be reduced significantly if the precise early prediction is possible. The robust and accurate prediction of diabetes is highly challenging due to the limited number of labeled data and also the presence of outliers (or missing values) in the diabetes datasets. In this literature, we are proposing a robust framework for diabetes prediction where the outlier rejection, filling the missing values, data standardization, feature selection, K-fold cross-validation, and different Machine Learning (ML) classifiers (k-nearest Neighbour, Decision Trees, Random Forest, AdaBoost, Naive Bayes, and XGBoost) and Multilayer Perceptron (MLP) were employed. The weighted ensembling of different ML models is also proposed, in this literature, to improve the prediction of diabetes where the weights are estimated from the corresponding Area Under ROC Curve (AUC) of the ML model. AUC is chosen as the performance metric, which is then maximized during hyperparameter tuning using the grid search technique. All the experiments, in this literature, were conducted under the same experimental conditions using the Pima Indian Diabetes Dataset. From all the extensive experiments, our proposed ensembling classifier is the best performing classifier with the sensitivity, specificity, false omission rate, diagnostic odds ratio, and AUC as 0.789, 0.934, 0.092, 66.234, and 0.950 respectively which outperforms the state-of-the-art results by 2.00 % in AUC. Our proposed framework for the diabetes prediction outperforms the other methods discussed in the article. It can also provide better results on the same dataset which can lead to better performance in diabetes prediction. Our source code for diabetes prediction is made publicly available.","['Diabetes', 'Measurement', 'Maximum likelihood estimation', 'Machine learning', 'Standardization', 'Feature extraction', 'Predictive models']","['Diabetes prediction', 'ensembling classifier', 'machine learning', 'multilayer perceptron', 'missing values and outliers', 'Pima Indian Diabetic dataset']"
"The strengthening of electric energy security and the reduction of greenhouse gas emissions have gained enormous momentum in previous decades. The integration of large-scale intermittent renewable energy resources (RER) like wind energy into the existing electricity grids has increased significantly in the last decade. However, this integration poses many operational and control challenges that hamper the reliable and stable operation of the grids. This article aims to review the reported challenges caused by the integration of wind energy and the proposed solutions methodologies. Among the various challenges, the generation uncertainty, power quality issues, angular and voltage stability, reactive power support, and fault ride-through capability are reviewed and discussed. Besides, socioeconomic, environmental, and electricity market challenges due to the grid integration of wind power are also investigated. Many of the solutions used and proposed to mitigate the impact of these challenges, such as energy storage systems, wind energy policy, and grid codes, are also reviewed and discussed. This paper will assist the enthusiastic readers in seeing the full picture of wind energy integration challenges. It also puts in the hands of policymakers all aspects of the challenges so that they can adopt sustainable policies that support and overcome the difficulties facing the integration of wind energy into electricity grids.","['Power system stability', 'Wind turbines', 'Wind energy', 'Wind power generation', 'Reactive power', 'Wind forecasting', 'Mathematical model']","['Angular stability', 'energy storage system', 'fault ride-through capability', 'frequency response', 'grid codes', 'reactive power support', 'voltage stability', 'wind intermittency']"
"Healthcare is undergoing a rapid transformation from traditional hospital and specialist focused approach to a distributed patient-centric approach. Advances in several technologies fuel this rapid transformation of healthcare vertical. Among various technologies, communication technologies have enabled to deliver personalized and remote healthcare services. At present, healthcare widely uses the existing 4G network and other communication technologies for smart healthcare applications and are continually evolving to accommodate the needs of future intelligent healthcare applications. As the smart healthcare market expands the number of applications connecting to the network will generate data that will vary in size and formats. This will place complex demands on the network in terms of bandwidth, data rate, and latency, among other factors. As this smart healthcare market matures, the connectivity needs for a large number of devices and machines with sensor-based applications in hospitals will necessitate the need to implement Massive-Machine Type Communication. Further use cases such as remote surgeries and Tactile Internet will spur the need for Ultra Reliability and Low Latency Communications or Critical Machine Type Communication. The existing communication technologies are unable to fulfill the complex and dynamic need that is put on the communication networks by the diverse smart healthcare applications. Therefore, the emerging 5G network is expected to support smart healthcare applications, which can fulfill most of the requirements such as ultra-low latency, high bandwidth, ultra-high reliability, high density, and high energy efficiency. The future smart healthcare networks are expected to be a combination of the 5G and IoT devices which are expected to increase cellular coverage, network performance and address security-related concerns. This paper provides a state-of-the-art review of the 5G and IoT enabled smart healthcare, Taxonomy, research trends, challenges, and future research directions.","['Medical services', '5G mobile communication', 'Computer architecture', 'Internet of Things', 'Monitoring', 'Wireless communication', 'Biomedical monitoring']","['5G', 'smart healthcare', 'software-defined network', 'network function virtualization', 'the Internet of Things (IoT)', 'device-to-device (D2D)', 'ultra reliability and low latency communications']"
"Network slicing is born as an emerging business to operators by allowing them to sell the customized slices to various tenants at different prices. In order to provide better-performing and costefficient services, network slicing involves challenging technical issues and urgently looks forward to intelligent innovations to make the resource management consistent with users' activities per slice. In that regard, deep reinforcement learning (DRL), which focuses on how to interact with the environment by trying alternative actions and reinforcing the tendency actions producing more rewarding consequences, is assumed to be a promising solution. In this paper, after briefly reviewing the fundamental concepts of DRL, we investigate the application of DRL in solving some typical resource management for network slicing scenarios, which include radio resource slicing and priority-based core network slicing, and demonstrate the advantage of DRL over several competing schemes through extensive simulations. Finally, we also discuss the possible challenges to apply DRL in network slicing from a general perspective.","['Network slicing', 'Resource management', 'Neural networks', '5G mobile communication', 'Quality of experience']","['Deep reinforcement learning', 'network slicing', 'neural networks', 'Q-learning', 'resource management']"
"Synthetic aperture radar (SAR) images have been widely used for ship monitoring. The traditional methods of SAR ship detection are difficult to detect small scale ships and avoid the interference of inshore complex background. Deep learning detection methods have shown great performance on various object detection tasks recently but using deep learning methods for SAR ship detection does not show an excellent performance it should have. One of the important reasons is that there is no effective model to handle the detection of multiscale ships in multiresolution SAR images. Another important reason is it is difficult to handle multiscene SAR ship detection including offshore and inshore, especially it cannot effectively distinguish between inshore complex background and ships. In this paper, we propose a densely connected multiscale neural network based on faster-RCNN framework to solve multiscale and multiscene SAR ship detection. Instead of using a single feature map to generate proposals, we densely connect one feature map to every other feature maps from top to down and generate proposals from each fused feature map. In addition, we propose a training strategy to reduce the weight of easy examples in the loss function, so that the training process more focus on the hard examples to reduce false alarm. Experiments on expanded public SAR ship detection dataset, verify the proposed method can achieve an excellent performance on multiscale SAR ship detection in multiscene.","['Marine vehicles', 'Feature extraction', 'Synthetic aperture radar', 'Proposals', 'Object detection', 'Machine learning', 'Image resolution']","['Ship detection', 'multiscale', 'neural network', 'synthetic aperture radar (SAR)']"
"Nowadays, in the international scientific community of machine learning, there exists an enormous discussion about the use of black-box models or explainable models; especially in practical problems. On the one hand, a part of the community defends that black-box models are more accurate than explainable models in some contexts, like image preprocessing. On the other hand, there exist another part of the community alleging that explainable models are better than black-box models because they can obtain comparable results and also they can explain these results in a language close to a human expert by using patterns. In this paper, advantages and weaknesses for each approach are shown; taking into account a state-of-the-art review for both approaches, their practical applications, trends, and future challenges. This paper shows that both approaches are suitable for solving practical problems, but experts in machine learning need to understand the input data, the problem to solve, and the best way for showing the output data before applying a machine learning model. Also, we propose some ideas for fusing both, explainable and black-box, approaches to provide better solutions to experts in real-world domains. Additionally, we show one way to measure the effectiveness of the applied machine learning model by using expert opinions jointly with statistical methods. Throughout this paper, we show the impact of using explainable and black-box models on the security and medical applications.","['Machine learning', 'Mathematical model', 'Biological system modeling', 'Gallium nitride', 'Biological neural networks', 'Statistical analysis', 'Computational modeling']","['Black-box', 'white-box', 'explainable artificial intelligence', 'deep learning']"
"Credit card fraud is a serious problem in financial services. Billions of dollars are lost due to credit card fraud every year. There is a lack of research studies on analyzing real-world credit card data owing to confidentiality issues. In this paper, machine learning algorithms are used to detect credit card fraud. Standard models are first used. Then, hybrid methods which use AdaBoost and majority voting methods are applied. To evaluate the model efficacy, a publicly available credit card data set is used. Then, a real-world credit card data set from a financial institution is analyzed. In addition, noise is added to the data samples to further assess the robustness of the algorithms. The experimental results positively indicate that the majority voting method achieves good accuracy rates in detecting fraud cases in credit cards.","['Credit cards', 'Machine learning algorithms', 'Support vector machines', 'Radio frequency', 'Vegetation', 'Classification algorithms', 'Self-organizing feature maps']","['AdaBoost', 'classification', 'credit card', 'fraud detection', 'predictive modelling', 'voting']"
"With the rapid growth of social networks and microblogging websites, communication between people from different cultural and psychological backgrounds has become more direct, resulting in more and more “cyber”conflicts between these people. Consequently, hate speech is used more and more, to the point where it has become a serious problem invading these open spaces. Hate speech refers to the use of aggressive, violent or offensive language, targeting a specific group of people sharing a common property, whether this property is their gender (i.e., sexism), their ethnic group or race (i.e., racism) or their believes and religion. While most of the online social networks and microblogging websites forbid the use of hate speech, the size of these networks and websites makes it almost impossible to control all of their content. Therefore, arises the necessity to detect such speech automatically and filter any content that presents hateful language or language inciting to hatred. In this paper, we propose an approach to detect hate expressions on Twitter. Our approach is based on unigrams and patterns that are automatically collected from the training set. These patterns and unigrams are later used, among others, as features to train a machine learning algorithm. Our experiments on a test set composed of 2010 tweets show that our approach reaches an accuracy equal to 87.4% on detecting whether a tweet is offensive or not (binary classification), and an accuracy equal to 78.4% on detecting whether a tweet is hateful, offensive, or clean (ternary classification).","['Speech', 'Feature extraction', 'Twitter', 'Voice activity detection', 'Task analysis', 'Sentiment analysis']","['Twitter', 'hate speech', 'machine learning', 'sentiment analysis']"
"Iris recognition refers to the automated process of recognizing individuals based on their iris patterns. The seemingly stochastic nature of the iris stroma makes it a distinctive cue for biometric recognition. The textural nuances of an individual's iris pattern can be effectively extracted and encoded by projecting them onto Gabor wavelets and transforming the ensuing phasor response into a binary code - a technique pioneered by Daugman. This textural descriptor has been observed to be a robust feature descriptor with very low false match rates and low computational complexity. However, recent advancements in deep learning and computer vision indicate that generic descriptors extracted using convolutional neural networks (CNNs) are able to represent complex image characteristics. Given the superior performance of CNNs on the ImageNet large scale visual recognition challenge and a large number of other computer vision tasks, in this paper, we explore the performance of state-of-the-art pre-trained CNNs on iris recognition. We show that the off-the-shelf CNN features, while originally trained for classifying generic objects, are also extremely good at representing iris images, effectively extracting discriminative visual features and achieving promising recognition results on two iris datasets: ND-CrossSensor-2013 and CASIA-IrisThousand. We also discuss the challenges and future research directions in leveraging deep learning methods for the problem of iris recognition.","['Iris recognition', 'Machine learning', 'Computer architecture', 'Visualization', 'Computer vision', 'Feature extraction']","['Iris recognition', 'biometrics', 'deep learning', 'convolutional neural network']"
"The growing interest and recent breakthroughs in artificial intelligence and machine learning (ML) have actively contributed to an increase in research and development of new methods to estimate the states of electrified vehicle batteries. Data-driven approaches, such as ML, are becoming more popular for estimating the state of charge (SOC) and state of health (SOH) due to greater availability of battery data and improved computing power capabilities. This paper provides a survey of battery state estimation methods based on ML approaches such as feedforward neural networks (FNNs), recurrent neural networks (RNNs), support vector machines (SVM), radial basis functions (RBF), and Hamming networks. Comparisons between methods are shown in terms of data quality, inputs and outputs, test conditions, battery types, and stated accuracy to give readers a bigger picture view of the ML landscape for SOC and SOH estimation. Additionally, to provide insight into how to best approach with the comparison of different neural network structures, an FNN and long short-term memory (LSTM) RNN are trained fifty times each for 3000 epochs. The error is somewhat different for each training repetition due to the random initial values of the trainable parameters, demonstrating that it is important to train networks multiple times to achieve the best result. Furthermore, it is recommended that when performing a comparison among estimation techniques such as those presented in this review paper, the compared networks should have a similar number of learnable parameters and be trained and tested with identical data. Otherwise, it is difficult to make a general conclusion regarding the quality of a given estimation technique.","['Batteries', 'State of charge', 'Machine learning', 'Maximum likelihood estimation', 'Training', 'Temperature measurement']","['Machine learning', 'artificial intelligence', 'deep learning', 'battery management systems (BMS)', 'electric vehicles', 'state of charge', 'state of health']"
"This paper discusses the power quality issues for distributed generation systems based on renewable energy sources, such as solar and wind energy. A thorough discussion about the power quality issues is conducted here. This paper starts with the power quality issues, followed by discussions of basic standards. A comprehensive study of power quality in power systems, including the systems with dc and renewable sources is done in this paper. Power quality monitoring techniques and possible solutions of the power quality issues for the power systems are elaborately studied. Then, we analyze the methods of mitigation of these problems using custom power devices, such as D-STATCOM, UPQC, UPS, TVSS, DVR, etc., for micro grid systems. For renewable energy systems, STATCOM can be a potential choice due to its several advantages, whereas spinning reserve can enhance the power quality in traditional systems. At Last, we study the power quality in dc systems. Simpler arrangement and higher reliability are two main advantages of the dc systems though it faces other power quality issues, such as instability and poor detection of faults.","['Power quality', 'Voltage fluctuations', 'Harmonic analysis', 'Power system harmonics', 'Renewable energy sources', 'Voltage control']","['DC system', 'mitigation', 'monitor', 'power quality', 'renewable energy', 'spinning reserve', 'standards']"
"Fungal diseases not only influence the economic importance of the plants and its products but also abate their ecological prominence. Mango tree, specifically the fruits and the leaves are highly affected by the fungal disease named as Anthracnose. The main aim of this paper is to develop an appropriate and effective method for diagnosis of the disease and its symptoms, therefore espousing a suitable system for an early and cost-effective solution of this problem. Over the last few years, due to their higher performance capability in terms of computation and accuracy, computer vision, and deep learning methodologies have gained popularity in assorted fungal diseases classification. Therefore, for this paper, a multilayer convolutional neural network (MCNN) is proposed for the classification of the Mango leaves infected by the Anthracnose fungal disease. This paper is validated on a real-time dataset captured at the Shri Mata Vaishno Devi University, Katra, J&K, India consists of 1070 images of the Mango tree leaves. The dataset contains both healthy and infected leaf images. The results envisage the higher classification accuracy of the proposed MCNN model when compared to the other state-of-the-art approaches.","['Diseases', 'Deep learning', 'Convolutional neural networks', 'Agriculture', 'Training', 'Real-time systems', 'Biological system modeling']","['Convolutional neural network', 'image classification', 'plant pathology', 'precision agriculture']"
"The role-based access control (RBAC) framework is a mechanism that describes the access control principle. As a common interaction, an organization provides a service to a user who owns a certain role that was issued by a different organization. Such trans-organizational RBAC is common in face-toface communication but not in a computer network, because it is difficult to establish both the security that prohibits the malicious impersonation of roles and the flexibility that allows small organizations to participate and users to fully control their own roles. In this paper, we present an RBAC using smart contract (RBAC-SC), a platform that makes use of Ethereum's smart contract technology to realize a trans organizational utilization of roles. Ethereum is an open blockchain platform that is designed to be secure, adaptable, and flexible. It pioneered smart contracts, which are decentralized applications that serve as “autonomous agents”running exactly as programmed and are deployed on a blockchain. The RBAC-SC uses smart contracts and blockchain technology as versatile infrastructures to represent the trust and endorsement relationship that are essential in the RBAC and to realize a challenge-response authentication protocol that verifies a user's ownership of roles. We describe the RBAC-SC framework, which is composed of two main parts, namely, the smart contract and the challenge-response protocol, and present a performance analysis. A prototype of the smart contract is created and deployed on Ethereum's Testnet blockchain, and the source code is publicly available.","['Organizations', 'Contracts', 'Access control', 'Protocols', 'Standards organizations', 'Computer networks']","['Blockchain technology', 'role-based access control', 'smart contracts']"
"In the last century, the automotive industry has arguably transformed society, being one of the most complex, sophisticated, and technologically advanced industries, with innovations ranging from the hybrid, electric, and self-driving smart cars to the development of IoT-connected cars. Due to its complexity, it requires the involvement of many Industry 4.0 technologies, like robotics, advanced manufacturing systems, cyber-physical systems, or augmented reality. One of the latest technologies that can benefit the automotive industry is blockchain, which can enhance its data security, privacy, anonymity, traceability, accountability, integrity, robustness, transparency, trustworthiness, and authentication, as well as provide long-term sustainability and a higher operational efficiency to the whole industry. This review analyzes the great potential of applying blockchain technologies to the automotive industry emphasizing its cybersecurity features. Thus, the applicability of blockchain is evaluated after examining the state-of-the-art and devising the main stakeholders' current challenges. Furthermore, the article describes the most relevant use cases, since the broad adoption of blockchain unlocks a wide area of short- and medium-term promising automotive applications that can create new business models and even disrupt the car-sharing economy as we know it. Finally, after strengths, weaknesses, opportunities, and threats analysis, some recommendations are enumerated with the aim of guiding researchers and companies in future cyber-resilient automotive industry developments.","['Blockchain', 'Industries', 'Automotive engineering', 'Computer security', 'Biological system modeling', 'Automobiles']","['Blockchain', 'distributed ledger technology (DLT)', 'Industry 4.0', 'IIoT', 'cyber-physical system', 'cryptography', 'cybersecurity', 'tamper-proof data', 'privacy', 'traceability']"
"Sarcasm identification on text documents is one of the most challenging tasks in natural language processing (NLP), has become an essential research direction, due to its prevalence on social media data. The purpose of our research is to present an effective sarcasm identification framework on social media data by pursuing the paradigms of neural language models and deep neural networks. To represent text documents, we introduce inverse gravity moment based term weighted word embedding model with trigrams. In this way, critical words/terms have higher values by keeping the word-ordering information. In our model, we present a three-layer stacked bidirectional long short-term memory architecture to identify sarcastic text documents. For the evaluation task, the presented framework has been evaluated on three-sarcasm identification corpus. In the empirical analysis, three neural language models (i.e., word2vec, fastText and GloVe), two unsupervised term weighting functions (i.e., term-frequency, and TF-IDF) and eight supervised term weighting functions (i.e., odds ratio, relevance frequency, balanced distributional concentration, inverse question frequency-question frequency-inverse category frequency, short text weighting, inverse gravity moment, regularized entropy and inverse false negative-true positive-inverse category frequency) have been evaluated. For sarcasm identification task, the presented model yields promising results with a classification accuracy of 95.30%.","['Task analysis', 'Social networking (online)', 'Blogs', 'Long short term memory', 'Predictive models', 'Gravity', 'Analytical models']","['Sarcasm identification', 'term weighting', 'neural language model', 'bidirectional long shortterm memory']"
"Mobile edge computing (MEC) providing information technology and cloud-computing capabilities within the radio access network is an emerging technique in fifth-generation networks. MEC can extend the computational capacity of smart mobile devices (SMDs) and economize SMDs' energy consumption by migrating the computation-intensive task to the MEC server. In this paper, we consider a multi-mobile-users MEC system, where multiple SMDs ask for computation offloading to a MEC server. In order to minimize the energy consumption on SMDs, we jointly optimize the offloading selection, radio resource allocation, and computational resource allocation coordinately. We formulate the energy consumption minimization problem as a mixed interger nonlinear programming (MINLP) problem, which is subject to specific application latency constraints. In order to solve the problem, we propose a reformulation-linearization-technique-based Branch-and-Bound (RLTBB) method, which can obtain the optimal result or a suboptimal result by setting the solving accuracy. Considering the complexity of RTLBB cannot be guaranteed, we further design a Gini coefficient-based greedy heuristic (GCGH) to solve the MINLP problem in polynomial complexity by degrading the MINLP problem into the convex problem. Many simulation results demonstrate the energy saving enhancements of RLTBB and GCGH.","['Energy consumption', 'Servers', 'Resource management', 'Mobile communication', 'Computational modeling', 'Minimization', 'Radio spectrum management']","['Mobile edge computing', 'computation offloading', 'energy minimization', 'branch-and-bound method', 'reformulation-linearization-technique', 'Gini coefficient']"
"Herein, we focus on convergent 6G communication, localization and sensing systems by identifying key technology enablers, discussing their underlying challenges, implementation issues, and recommending potential solutions. Moreover, we discuss exciting new opportunities for integrated localization and sensing applications, which will disrupt traditional design principles and revolutionize the way we live, interact with our environment, and do business. Regarding potential enabling technologies, 6G will continue to develop towards even higher frequency ranges, wider bandwidths, and massive antenna arrays. In turn, this will enable sensing solutions with very fine range, Doppler, and angular resolutions, as well as localization to cm-level degree of accuracy. Besides, new materials, device types, and reconfigurable surfaces will allow network operators to reshape and control the electromagnetic response of the environment. At the same time, machine learning and artificial intelligence will leverage the unprecedented availability of data and computing resources to tackle the biggest and hardest problems in wireless communication systems. As a result, 6G will be truly intelligent wireless systems that will provide not only ubiquitous communication but also empower high accuracy localization and high-resolution sensing services. They will become the catalyst for this revolution by bringing about a unique new set of features and service capabilities, where localization and sensing will coexist with communication, continuously sharing the available resources in time, frequency, and space. This work concludes by highlighting foundational research challenges, as well as implications and opportunities related to privacy, security, and trust.","['Sensors', 'Location awareness', '6G mobile communication', '5G mobile communication', 'Robot sensing systems', 'Frequency measurement', 'Protocols']","['6G', 'beamforming', 'cmWave', 'context-aware', 'IRS', 'ML/AI', 'mmWave', 'radar', 'security', 'sensing', 'SLAM', 'THz']"
"While augment reality applications are becoming popular, more and more data-hungry and computation-intensive tasks are delay-sensitive. Mobile edge computing is expected to an effective solution to meet the low latency demand. In contrast to previous work on mobile edge computing, which mainly focus on computation offloading, this paper introduces a new concept of task caching. Task caching refers to the caching of completed task application and their related data in edge cloud. Then, we investigate the problem of joint optimization of task caching and offloading on edge cloud with the computing and storage resource constraint. We formulate this problem as mixed integer programming which is hard to solve. To solve the problem, we propose efficient algorithm, called task caching and offloading (TCO), based on alternating iterative algorithm. Finally, the simulation experimental results show that our proposed TCO algorithm outperforms others in terms of less energy cost.","['Task analysis', 'Cloud computing', 'Mobile handsets', 'Edge computing', 'Delays', 'Solid modeling', 'Streaming media']","['Caching', 'computation offloading', 'mobile edge computing', 'energy efficient']"
"Recently, multilevel inverters (MLIs) have gained lots of interest in industry and academia, as they are changing into a viable technology for numerous applications, such as renewable power conversion system and drives. For these high power and high/medium voltage applications, MLIs are widely used as one of the advanced power converter topologies. To produce high-quality output without the need for a large number of switches, development of reduced switch MLI (RS MLI) topologies has been a major focus of current research. Therefore, this review paper focuses on a number of recently developed MLIs used in various applications. To assist with advanced current research in this field and in the selection of suitable inverter for various applications, significant understanding on these topologies is clearly summarized based on the three categories, i.e., symmetrical, asymmetrical, and modified topologies. This review paper also includes a comparison based on important performance parameters, detailed technical challenges, current focus, and future development trends. By a suitable combination of switches, the MLI produces a staircase output with low harmonic distortion. For a better understanding of the working principle, a single-phase RS MLI topology is experimentally illustrated for different level generation using both fundamental and high switching frequency techniques which will help the readers to gain the utmost knowledge for advance research.","['Topology', 'Switches', 'Pulse width modulation', 'Inverters', 'Harmonic analysis', 'Power harmonic filters', 'Renewable energy sources']","['Control techniques', 'drives application', 'fundamental switching frequency', 'high switching frequency', 'multilevel inverter (MLI)', 'performance parameters', 'photovoltaic (PV) systems', 'reduced component count', 'renewable energy application']"
"In recent years, unmanned aerial vehicles (UAVs) have received considerable attention from regulators, industry and research community, due to rapid growth in a broad range of applications. Particularly, UAVs are being used to provide a promising solution to reliable and cost-effective wireless communications from the sky. The deployment of UAVs has been regarded as an alternative complement of existing cellular systems, to achieve higher transmission efficiency with enhanced coverage and capacity. However, heavily utilized microwave spectrum bands below 6 GHz utilized by legacy wireless systems are insufficient to attain remarkable data rate enhancement for numerous emerging applications. To resolve the spectrum crunch crisis and satisfy the requirements of 5G and beyond mobile communications, one potential solution is to use the abundance of unoccupied bandwidth available at millimeter wave (mmWave) frequencies. Inspired by the technique potentials, mmWave communications have also paved the way into the widespread use of UAVs to assist wireless networks for future 5G and beyond wireless applications. In this paper, we provide a comprehensive survey on current achievements in the integration of 5G mmWave communications into UAV-assisted wireless networks. More precisely, a taxonomy to classify the existing research issues is presented, by considering seven cutting-edge solutions. Subsequently, we provide a brief overview of 5G mmWave communications for UAV-assisted wireless networks from two aspects, i.e., key technical advantages and challenges as well as potential applications. Based on the proposed taxonomy, we further discuss in detail the state-of-the-art issues, solutions, and open challenges for this newly emerging area. Lastly, we complete this survey by pointing out open issues and shedding new light on future directions for further research on this area.","['Drones', '5G mobile communication', 'Wireless networks', 'Wireless sensor networks', 'Ad hoc networks', 'Millimeter wave communication']","['Millimeter wave (mmWave) communications', 'unmanned aerial vehicle (UAV)', 'mmWave UAV communications', 'UAV-assisted wireless networks', '5G and beyond']"
"Prosumer concept and digitilization offer the exciting potential of microgrid transactive energy systems at distribution level for reducing transmission losses, decreasing electric infrastructure expenditure, improving reliability, enhancing local energy use, and minimizing customers' electricity bills. Distributed energy resources, demand response, distributed ledger technologies, and local energy markets are integral parts of transaction energy system for emergence of decentralized smart grid system. Hence, this paper discusses transactive energy concept and proposes seven functional layers architecture for designing transactive energy system. The proposed architecture is compared with practical case study of Brooklyn microgrid. Moreover, this paper reviews the existing architectures and explains the widely known distributed ledger technologies (blockchain, directed acyclic graph, hashgraph, holochain, and tempo) alongwith their advantages and challenges. The local energy market concept is presented and critically analyzed for energy trade within a transactive energy system. This paper also reviews the potential and challenges of peer-to-peer and community-based energy markets. Proposed architecture and analytical review of distributed ledger technologies and local energy markets pave the way for advanced research and industrialization of transactive energy systems.","['Transactive energy', 'Microgrids', 'Distributed ledger', 'Load management', 'Blockchain', 'Computer architecture']","['Blockchain', 'decentralization', 'demand response', 'distributed ledger technologies', 'energy trading', 'local energy market', 'microgrid', 'peer-to-peer market', 'prosumer', 'renewable energy sources', 'smart grid', 'system architectures', 'transactive energy']"
"Advances in technology are not only changing the world around us but also driving the wireless industry to develop the next generation of network technology. There is a lot of buzz building over the advent of 5G that will facilitate the entire planet through continuous and ubiquitous communication connecting anybody to anything, anywhere, anytime, and anyhow regardless of the device, service, network, or geographical existence. 5G will also prove to be a paradigm shift including high carrier frequencies with massive bandwidths, having a large number of antennas, and with an extreme base station and device densities. In this paper, we investigate the potential beneficiaries of 5G and identify the use-cases, where 5G can make an impact. In particular, we consider three main use-cases: vehicle-to-everything (V2X) communication, drones, and healthcare. We explore and highlight the problems and deficiencies of current cellular technologies with respect to these use-cases and identify how 5G will overcome those deficiencies. We also identified the open research problems and provide possible future directions to cope with those issues.","['5G mobile communication', 'Vehicle-to-everything', 'Medical services', 'Drones', 'Reliability', 'Vehicular ad hoc networks', 'Wireless communication']","['5G', 'V2X communication', 'drones', 'healthcare', 'ultra-low-latency', 'ultra-high-reliability']"
This paper presents a complete approach to a successful utilization of a high-performance extreme learning machines (ELMs) Toolbox for Big Data. It summarizes recent advantages in algorithmic performance; gives a fresh view on the ELM solution in relation to the traditional linear algebraic performance; and reaps the latest software and hardware performance achievements. The results are applicable to a wide range of machine learning problems and thus provide a solid ground for tackling numerous Big Data challenges. The included toolbox is targeted at enabling the full potential of ELMs to the widest range of users.,"['Learning systems', 'Performance evaluation', 'Machine learning']","['Learning systems', 'Supervised learning', 'Machine learning', 'Prediction methods', 'Predictive models', 'Neural networks', 'Artificial neural networks', 'Feedforward neural networks', 'Radial basis function networks', 'Computer applications', 'Scientific computing', 'Performance analysis', 'High performance computing Software', 'Open source software', 'Utility programs']"
"Smart grid technology increases reliability, security, and efficiency of the electrical grids. However, its strong dependencies on digital communication technology bring up new vulnerabilities that need to be considered for efficient and reliable power distribution. In this paper, an unsupervised anomaly detection based on statistical correlation between measurements is proposed. The goal is to design a scalable anomaly detection engine suitable for large-scale smart grids, which can differentiate an actual fault from a disturbance and an intelligent cyber-attack. The proposed method applies feature extraction utilizing symbolic dynamic filtering (SDF) to reduce computational burden while discovering causal interactions between the subsystems. The simulation results on IEEE 39, 118, and 2848 bus systems verify the performance of the proposed method under different operation conditions. The results show an accuracy of 99%, true positive rate of 98%, and false positive rate of less than 2%","['Smart grids', 'Generators', 'Power system reliability', 'Reliability', 'Feature extraction', 'Security']","['Anomaly', 'cyber-attack', 'smart grid', 'statistical property', 'machine learning', 'unsupervised learning']"
"Unmanned aerial vehicles (UAVs) have stroke great interested both by the academic community and the industrial community due to their diverse military applications and civilian applications. Furthermore, UAVs are also envisioned to be part of future airspace traffic. The application functions delivery relies on information exchange among UAVs as well as between UAVs and ground stations (GSs), which further closely depends on aeronautical channels. However, there is a paucity of comprehensive surveys on aeronautical channel modeling in line with the specific aeronautical characteristics and scenarios. To fill this gap, this paper focuses on reviewing the air-to-ground (A2G), ground-to-ground (G2G), and air-to-air (A2A) channel measurements and modeling for UAV communications and aeronautical communications under various scenarios. We also provide the design guideline for managing the link budget of UAV communications taking account of link losses and channel fading effects. Moreover, we also analyze the receive/transmit diversity gain and spatial multiplexing gain achieved by multiple-antenna-aided UAV communications. Finally, we discuss the remaining challenge and open issues for the future development of UAV communication channel modeling.","['Atmospheric modeling', 'Solid modeling', 'Analytical models', 'Unmanned aerial vehicles', 'Airports', 'Sea measurements', 'Shadow mapping']","['UAV communication', 'aeronautical communication', 'channel characterization', 'statistical channel', 'air-to-ground', 'cellular networks', 'evaporation duct', 'shadowing', 'MIMO']"
"Massive multiple-input multiple-output is a promising physical layer technology for 5G wireless communications due to its capability of high spectrum and energy efficiency, high spatial resolution, and simple transceiver design. To embrace its potential gains, the acquisition of channel state information is crucial, which unfortunately faces a number of challenges, such as the uplink pilot contamination, the overhead of downlink training and feedback, and the computational complexity. In order to reduce the effective channel dimensions, researchers have been investigating the low-rank (sparse) properties of channel environments from different viewpoints. This paper then provides a general overview of the current low-rank channel estimation approaches, including their basic assumptions, key results, as well as pros and cons on addressing the aforementioned tricky challenges. Comparisons among all these methods are provided for better understanding and some future research prospects for these low-rank approaches are also forecasted.","['MIMO', 'Physical layer', '5G mobile communication', 'Energy efficiency', 'Spatial resolution', 'Transceivers']","['Massive MIMO', 'channel estimation', 'low-rank property', 'channel sparsity', 'angle reciprocity']"
"This paper focuses on bearing fault diagnosis with limited training data. A major challenge in fault diagnosis is the infeasibility of obtaining sufficient training samples for every fault type under all working conditions. Recently deep learning based fault diagnosis methods have achieved promising results. However, most of these methods require large amount of training data. In this study, we propose a deep neural network based few-shot learning approach for rolling bearing fault diagnosis with limited data. Our model is based on the siamese neural network, which learns by exploiting sample pairs of the same or different categories. Experimental results over the standard Case Western Reserve University (CWRU) bearing fault diagnosis benchmark dataset showed that our few-shot learning approach is more effective in fault diagnosis with limited data availability. When tested over different noise environments with minimal amount of training data, the performance of our few-shot learning model surpasses the one of the baseline with reasonable noise level. When evaluated over test sets with new fault types or new working conditions, few-shot models work better than the baseline trained with all fault types. All our models and datasets in this study are open sourced and can be downloaded from https://mekhub.cn/as/fault_diagnosis_with_few-shot_learning/.","['Fault diagnosis', 'Neural networks', 'Training', 'Employee welfare', 'Kernel', 'Mathematical model', 'Deep learning']","['Deep learning', 'few-shot learning', 'bearing fault diagnosis', 'limited data']"
"This study aims to explore the current status, potential applications, and future directions of blockchain technology in supply chain management. A literature survey, along with an analytical review, of blockchain-based supply chain research was conducted to better understand the trajectory of related research and shed light on the benefits, issues, and challenges in the blockchain-supply-chain paradigm. A selected corpus comprising 106 review articles was analyzed to provide an overview of the use of blockchain and smart contracts in supply chain management. The diverse industrial applications of these technologies in various sectors have increasingly received attention by researchers, engineers, and practitioners. Four major issues: traceability and transparency, stakeholder involvement and collaboration, supply chain integration and digitalization, and common frameworks on blockchain-based platforms, are critical for future orientation. Traditional supply chain activities involve several intermediaries, trust, and performance issues. The potential of blockchain can be leveraged to disrupt supply chain operations for better performance, distributed governance, and process automation. This study contributes to the comprehension of blockchain applications in supply chain management and provides a blueprint for these applications from the perspective of literature analysis. Future efforts regarding technical adoption/diffusion, block-supply chain integration, and their social impacts were highlighted to enrich the research scope.","['Blockchain', 'Smart contracts', 'Supply chains', 'Peer-to-peer computing', 'Distributed ledger', 'Systematics', 'Bibliographies']","['Blockchain', 'digital ledger', 'distributed ledger technology', 'logistics', 'shared ledger', 'smart contract', 'supply chain management', 'systematic literature review', 'value chain']"
"The proliferation of smartphones has significantly facilitated people's daily life, and diverse and powerful embedded sensors make smartphone a ubiquitous platform to acquire and analyze data, which may also provide great potential for efficient human activity recognition. This paper presents a systematic performance analysis of motion-sensor behavior for human activity recognition via smartphones. Sensory data sequences are collected via smartphones, when participants perform typical and daily human activities. A cycle detection algorithm is applied to segment the data sequence for obtaining the activity unit, which is then characterized by time-, frequency-, and wavelet-domain features. Then both personalized and generalized model using diverse classification algorithms are developed and implemented to perform activity recognition. Analyses are conducted using 27 681 sensory samples from 10 subjects, and the performance is measured in the form of F-score under various placement settings, and in terms of sensitivity to user space, stability to combination of motion sensors, and impact of data imbalance. Extensive results show that each individual has its own specific and discriminative movement patterns, and the F-score for personalized model and generalized model can reach 95.95% and 96.26%, respectively, which indicates our approach is accurate and efficient for practical implementation.","['Activity recognition', 'Smart phones', 'Accelerometers', 'Feature extraction', 'Legged locomotion', 'Gyroscopes']","['Smartphone', 'motion sensor', 'behavior analysis', 'human activity recognition', 'performance analysis']"
"Phasor measurement units (PMUs) are rapidly being deployed in electric power networks across the globe. Wide-area measurement system (WAMS), which builds upon PMUs and fast communication links, is consequently emerging as an advanced monitoring and control infrastructure. Rapid adaptation of such devices and technologies has led the researchers to investigate multitude of challenges and pursue opportunities in synchrophasor measurement technology, PMU structural design, PMU placement, miscellaneous applications of PMU from local perspectives, and various WAMS functionalities from the system perspective. Relevant research articles appeared in the IEEE and IET publications from 1983 through 2014 are rigorously surveyed in this paper to represent a panorama of research progress lines. This bibliography will aid academic researchers and practicing engineers in adopting appropriate topics and will stimulate utilities toward development and implementation of software packages.","['Phasor measurement', 'Synchronization', 'Bibliographies', 'Wide area measurement', 'Measurement']","['Phasor measurement unit (PMU)', 'synchrophasor measurement technology (SMT)', 'wide-area measurement system (WAMS)']"
"The network intrusion detection system is an important tool for protecting computer networks against threats and malicious attacks. Many techniques have recently been proposed; however, these face significant challenges due to the continuous emergence of new threats that are not recognized by existing systems. In this paper, we propose a novel two-stage deep learning (TSDL) model, based on a stacked auto-encoder with a soft-max classifier, for efficient network intrusion detection. The model comprises two decision stages: an initial stage responsible for classifying network traffic as normal or abnormal, using a probability score value. This is then used in the final decision stage as an additional feature, for detecting the normal state and other classes of attacks. The proposed model is able to learn useful feature representations from large amounts of unlabeled data and classifies them automatically and efficiently. To evaluate its effectiveness, several experiments are conducted on two public datasets, specifically the benchmark KDD99 and UNSW-NB15 datasets. Comparative simulation results demonstrate that our proposed model significantly outperforms existing approaches, achieving high recognition rates, up to 99.996% and 89.134%, for the KDD99 and UNSW-NB15 datasets respectively. We conclude that our model has the potential to serve as a future benchmark for the deep learning and network security research communities.","['Intrusion detection', 'Deep learning', 'Feature extraction', 'Computational modeling', 'Benchmark testing', 'Neural networks', 'Tools']","['Computational intelligence', 'two-stage deep learning model', 'feature representation', 'network intrusion detection', 'stacked auto-encoder']"
"Blockchain and other Distributed Ledger Technologies (DLTs) have evolved significantly in the last years and their use has been suggested for numerous applications due to their ability to provide transparency, redundancy and accountability. In the case of blockchain, such characteristics are provided through public-key cryptography and hash functions. However, the fast progress of quantum computing has opened the possibility of performing attacks based on Grover's and Shor's algorithms in the near future. Such algorithms threaten both public-key cryptography and hash functions, forcing to redesign blockchains to make use of cryptosystems that withstand quantum attacks, thus creating which are known as post-quantum, quantum-proof, quantum-safe or quantum-resistant cryptosystems. For such a purpose, this article first studies current state of the art on post-quantum cryptosystems and how they can be applied to blockchains and DLTs. Moreover, the most relevant post-quantum blockchain systems are studied, as well as their main challenges. Furthermore, extensive comparisons are provided on the characteristics and performance of the most promising post-quantum public-key encryption and digital signature schemes for blockchains. Thus, this article seeks to provide a broad view and useful guidelines on post-quantum blockchain security to future blockchain researchers and developers.","['Blockchain', 'Hash functions', 'Elliptic curve cryptography', 'Quantum computing']","['Blockchain', 'blockchain security', 'DLT', 'post-quantum', 'quantum-safe', 'quantum-resistant', 'quantum computing', 'cryptography', 'cryptosystem', 'cybersecurity']"
"According to the recent studies, malicious software (malware) is increasing at an alarming rate, and some malware can hide in the system by using different obfuscation techniques. In order to protect computer systems and the Internet from the malware, the malware needs to be detected before it affects a large number of systems. Recently, there have been made several studies on malware detection approaches. However, the detection of malware still remains problematic. Signature-based and heuristic-based detection approaches are fast and efficient to detect known malware, but especially signature-based detection approach has failed to detect unknown malware. On the other hand, behavior-based, model checking-based, and cloud-based approaches perform well for unknown and complicated malware; and deep learning-based, mobile devices-based, and IoT-based approaches also emerge to detect some portion of known and unknown malware. However, no approach can detect all malware in the wild. This shows that to build an effective method to detect malware is a very challenging task, and there is a huge gap for new studies and methods. This paper presents a detailed review on malware detection approaches and recent detection methods which use these approaches. Paper goal is to help researchers to have a general idea of the malware detection approaches, pros and cons of each detection approach, and methods that are used in these approaches.","['Computer viruses', 'Feature extraction', 'Encryption', 'Internet']","['Cyber security', 'malware classification', 'malware detection approaches', 'malware features']"
"Big data is considered to be the key to unlocking the next great waves of growth in productivity. The amount of collected data in our world has been exploding due to a number of new applications and technologies that permeate our daily lives, including mobile and social networking applications, and Internet of Thing-based smart-world systems (smart grid, smart transportation, smart cities, and so on). With the exponential growth of data, how to efficiently utilize the data becomes a critical issue. This calls for the development of a big data market that enables efficient data trading. Via pushing data as a kind of commodity into a digital market, the data owners and consumers are able to connect with each other, sharing and further increasing the utility of data. Nonetheless, to enable such an effective market for data trading, several challenges need to be addressed, such as determining proper pricing for the data to be sold or purchased, designing a trading platform and schemes to enable the maximization of social welfare of trading participants with efficiency and privacy preservation, and protecting the traded data from being resold to maintain the value of the data. In this paper, we conduct a comprehensive survey on the lifecycle of data and data trading. To be specific, we first study a variety of data pricing models, categorize them into different groups, and conduct a comprehensive comparison of the pros and cons of these models. Then, we focus on the design of data trading platforms and schemes, supporting efficient, secure, and privacy-preserving data trading. Finally, we review digital copyright protection mechanisms, including digital copyright identifier, digital rights management, digital encryption, watermarking, and others, and outline challenges in data protection in the data trading lifecycle.","['Pricing', 'Copyright protection', 'Data models', 'Data mining', 'Data analysis', 'Big Data applications']","['Big data', 'data pricing', 'privacy and digital copyright protection', 'data trading', 'data utilization', 'Internet of Things']"
"With the advent of the Internet of Things (IoT), the security of the network layer in the IoT is getting more and more attention. The traditional intrusion detection technologies cannot be well adapted in the complex Internet environment of IoT. For the deep learning algorithm of intrusion detection, a neural network structure may have fine detection accuracy for one kind of attack, but it may not have a good detection effect when facing other attacks. Therefore, it is urgent to design a self-adaptive model to change the network structure for different attack types. This paper presents an intrusion detection model based on improved genetic algorithm (GA) and deep belief network (DBN). Facing different types of attacks, through multiple iterations of the GA, the optimal number of hidden layers and number of neurons in each layer are generated adaptively, so that the intrusion detection model based on the DBN achieves a high detection rate with a compact structure. Finally, the NSL-KDD dataset was used to simulate and evaluate the model and algorithms. The experimental results show that the improved intrusion detection model combined with DBN can effectively improve the recognition rate of intrusion attacks and reduce the complexity of the neural network structure.","['Intrusion detection', 'Neurons', 'Genetic algorithms', 'Neural networks', 'Internet of Things', 'Deep learning']","['Internet of Things security', 'intrusion detection', 'deep belief network', 'genetic algorithm']"
"Tuberculosis (TB) is a chronic lung disease that occurs due to bacterial infection and is one of the top 10 leading causes of death. Accurate and early detection of TB is very important, otherwise, it could be life-threatening. In this work, we have detected TB reliably from the chest X-ray images using image pre-processing, data augmentation, image segmentation, and deep-learning classification techniques. Several public databases were used to create a database of 3500 TB infected and 3500 normal chest X-ray images for this study. Nine different deep CNNs (ResNet18, ResNet50, ResNet101, ChexNet, InceptionV3, Vgg19, DenseNet201, SqueezeNet, and MobileNet) were used for transfer learning from their pre-trained initial weights and were trained, validated and tested for classifying TB and non-TB normal cases. Three different experiments were carried out in this work: segmentation of X-ray images using two different U-net models, classification using X-ray images and that using segmented lung images. The accuracy, precision, sensitivity, F1-score and specificity of best performing model, ChexNet in the detection of tuberculosis using X-ray images were 96.47%, 96.62%, 96.47%, 96.47%, and 96.51% respectively. However, classification using segmented lung images outperformed that with whole X-ray images; the accuracy, precision, sensitivity, F1-score and specificity of DenseNet201 were 98.6%, 98.57%, 98.56%, 98.56%, and 98.54% respectively for the segmented lung images. The paper also used a visualization technique to confirm that CNN learns dominantly from the segmented lung regions that resulted in higher detection accuracy. The proposed method with state-of-the-art performance can be useful in the computer-aided faster diagnosis of tuberculosis.","['X-ray imaging', 'Lung', 'Image segmentation', 'Deep learning', 'Diseases', 'Medical diagnostic imaging']","['Tuberculosis detection', 'TB screening', 'deep learning', 'transfer learning', 'lungs segmentation', 'image processing']"
"Supply chains are evolving into automated and highly complex networks and are becoming an important source of potential benefits in the modern world. At the same time, consumers are now more interested in food product quality. However, it is challenging to track the provenance of data and maintain its traceability throughout the supply chain network. The traditional supply chains are centralized and they depend on a third party for trading. These centralized systems lack transparency, accountability and auditability. In our proposed solution, we have presented a complete solution for blockchain-based Agriculture and Food (Agri-Food) supply chain. It leverages the key features of blockchain and smart contracts, deployed over ethereum blockchain network. Although blockchain provides immutability of data and records in the network, it still fails to solve some major problems in supply chain management like credibility of the involved entities, accountability of the trading process and traceability of the products. Therefore, there is a need of a reliable system that ensures traceability, trust and delivery mechanism in Agri-Food supply chain. In the proposed system, all transactions are written to blockchain which ultimately uploads the data to Interplanetary File Storage System (IPFS). The storage system returns a hash of the data which is stored on blockchain and ensures efficient, secure and reliable solution. Our system provides smart contracts along with their algorithms to show interaction of entities in the system. Furthermore, simulations and evaluation of smart contracts along with the security and vulnerability analyses are also presented in this work.","['Supply chains', 'Blockchain', 'Supply chain management', 'Agricultural products', 'Storage management']","['Accountability', 'blockchain', 'credibility', 'reputation', 'supply chain', 'traceability', 'trust']"
"With the continued development of artificial intelligence (AI) technology, research on interaction technology has become more popular. Facial expression recognition (FER) is an important type of visual information that can be used to understand a human's emotional situation. In particular, the importance of AI systems has recently increased due to advancements in research on AI systems applied to AI robots. In this paper, we propose a new scheme for FER system based on hierarchical deep learning. The feature extracted from the appearance feature-based network is fused with the geometric feature in a hierarchical structure. The appearance feature-based network extracts holistic features of the face using the preprocessed LBP image, whereas the geometric feature-based network learns the coordinate change of action units (AUs) landmark, which is a muscle that moves mainly when making facial expressions. The proposed method combines the result of the softmax function of two features by considering the error associated with the second highest emotion (Top-2) prediction result. In addition, we propose a technique to generate facial images with neutral emotion using the autoencoder technique. By this technique, we can extract the dynamic facial features between the neutral and emotional images without sequence data. We compare the proposed algorithm with the other recent algorithms for CK+ and JAFFE dataset, which are typically considered to be verified datasets in the facial expression recognition. The ten-fold cross validation results show 96.46% of accuracy in the CK+ dataset and 91.27% of accuracy in the JAFFE dataset. When comparing with other methods, the result of the proposed hierarchical deep network structure shows up to about 3% of the accuracy improvement and 1.3% of average improvement in CK+ dataset, respectively. In JAFFE datasets, up to about 7% of the accuracy is enhanced, and the average improvement is verified by about 1.5%.","['Feature extraction', 'Face recognition', 'Face', 'Deep learning', 'Emotion recognition', 'Data mining']","['Artificial intelligence (AI)', 'facial expression recognition (FER)', 'emotion recognition', 'deep learning', 'LBP feature', 'geometric feature', 'convolutional neural network (CNN)']"
"Attention deficit hyperactivity disorder (ADHD) is one of the most common mental health disorders. As a neuro development disorder, neuroimaging technologies, such as magnetic resonance imaging (MRI), coupled with machine learning algorithms, are being increasingly explored as biomarkers in ADHD. Among various machine learning methods, deep learning has demonstrated excellent performance on many imaging tasks. With the availability of publically-available, large neuroimaging data sets for training purposes, deep learning-based automatic diagnosis of psychiatric disorders can become feasible. In this paper, we develop a deep learning-based ADHD classification method via 3-D convolutional neural networks (CNNs) applied to MRI scans. Since deep neural networks may utilize millions of parameters, even the large number of MRI samples in pooled data sets is still relatively limited if one is to learn discriminative features from the raw data. Instead, here we propose to first extract meaningful 3-D low-level features from functional MRI (fMRI) and structural MRI (sMRI) data. Furthermore, inspired by radiologists' typical approach for examining brain images, we design a 3-D CNN model to investigate the local spatial patterns of MRI features. Finally, we discover that brain functional and structural information are complementary, and design a multi-modality CNN architecture to combine fMRI and sMRI features. Evaluations on the hold-out testing data of the ADHD-200 global competition shows that the proposed multi-modality 3-D CNN approach achieves the state-of-the-art accuracy of 69.15% and outperforms reported classifiers in the literature, even with fewer training samples. We suggest that multi-modality classification will be a promising direction to find potential neuroimaging biomarkers of neuro development disorders.","['Feature extraction', 'Three-dimensional displays', 'Testing', 'Training', 'Neuroimaging', 'Biological neural networks']","['Attention deficit hyperactive disorder', '3D CNN', 'magnetic resonance imaging', 'multi-modality analysis']"
"With the popularity and development of network technology and the Internet, intrusion detection systems (IDSs), which can identify attacks, have been developed. Traditional intrusion detection algorithms typically employ mining association rules to identify intrusion behaviors. However, they fail to fully extract the characteristic information of user behaviors and encounter various problems, such as high false alarm rate (FAR), poor generalization capability, and poor timeliness. In this paper, we propose a network intrusion detection model based on a convolutional neural network-IDS (CNN-IDS). Redundant and irrelevant features in the network traffic data are first removed using different dimensionality reduction methods. Features of the dimensionality reduction data are automatically extracted using the CNN, and more effective information for identifying intrusion is extracted by supervised learning. To reduce the computational cost, we convert the original traffic vector format into an image format and use a standard KDD-CUP99 dataset to evaluate the performance of the proposed CNN model. The experimental results indicate that the AC, FAR, and timeliness of the CNN-IDS model are higher than those of traditional algorithms. Therefore, the model we propose has not only research significance but also practical value.","['Intrusion detection', 'Feature extraction', 'Convolution', 'Data models', 'Machine learning', 'Principal component analysis', 'Data mining']","['Communication technology', 'convolutional neural network', 'data dimensionality reduction', 'intrusion detection']"
"A smart factory is a highly digitized and connected production facility that relies on smart manufacturing. Additionally, artificial intelligence is the core technology of smart factories. The use of machine learning and deep learning algorithms has produced fruitful results in many fields like image processing, speech recognition, fault detection, object detection, or medical sciences. With the increment in the use of smart machinery, the faults in the machinery equipment are expected to increase. Machinery fault detection and diagnosis through various deep learning algorithms has increased day by day. Many types of research have been done and published using both open-source and closed-source datasets, implementing the deep learning algorithms. Out of many publicly available datasets, Case Western Reserve University (CWRU) bearing dataset has been widely used to detect and diagnose machinery bearing fault and is accepted as a standard reference for validating the models. This paper summarizes the recent works which use the CWRU bearing dataset in machinery fault detection and diagnosis employing deep learning algorithms. We have reviewed the published works and presented the working algorithm, result, and other necessary details in this paper. This paper, we believe, can be of good help for future researchers to start their work on machinery fault detection and diagnosis using the CWRU dataset.","['Fault detection', 'Machinery', 'Deep learning', 'Vibrations', 'Frequency-domain analysis', 'Wavelet transforms']","['Bearing', 'deep learning', 'machine learning', 'machinery fault detection and diagnosis', 'CWRU dataset']"
"In this paper, we propose a Blockchain-based infrastructure to support security- and privacy-oriented spatio-temporal smart contract services for the sustainable Internet of Things (IoT)-enabled sharing economy in mega smart cities. The infrastructure leverages cognitive fog nodes at the edge to host and process off loaded geo-tagged multimedia payload and transactions from a mobile edge and IoT nodes, uses AI for processing and extracting significant event information, produces semantic digital analytics, and saves results in Blockchain and decentralized cloud repositories to facilitate sharing economy services. The framework offers a sustainable incentive mechanism, which can potentially support secure smart city services, such as sharing economy, smart contracts, and cyber-physical interaction with Blockchain and IoT. Our unique contribution is justified by detailed system design and implementation of the framework.","['Sharing economy', 'Blockchain', 'Smart cities', 'Cognitive systems', 'Security', 'Business']","['Sharing economy', 'cognitive processing at the edge', 'mobile edge computing', 'Blockchain', 'smart city']"
"A feature of the Internet of Things (IoT) is that some users in the system need to be served quickly for small packet transmission. To address this requirement, a new multiple-input multiple-output non-orthogonal multiple access (MIMO-NOMA) scheme is designed in this paper, where one user is served with its quality of service requirement strictly met, and the other user is served opportunistically by using the NOMA concept. The novelty of this new scheme is that it confronts the challenge that the existing MIMO-NOMA schemes rely on the assumption that users' channel conditions are different, a strong assumption which may not be valid in practice. The developed precoding and detection strategies can effectively create a significant difference between the users' effective channel gains, and therefore, the potential of NOMA can be realized even if the users' original channel conditions are similar. Analytical and numerical results are provided to demonstrate the performance of the proposed MIMO-NOMA scheme.","['Internet of Things', 'MIMO', 'Precoding', 'Quality of service', 'Packet switching', 'NOMA']","['Non-orthogonal multiple access (NOMA)', 'multiple-input multiple-output (MIMO)', 'QR decomposition', 'MIMO precoding', 'power allocation']"
"The prognostic and health management (PHM) of lithium-ion batteries has received increasing attention in recent years. The remaining useful life (RUL) prediction and state of health (SOH) monitoring are two important parts in PHM of the lithium-ion battery. Nowadays, the development of signal processing technology and neural network technology introduces new data-driven methods to RUL prediction and SOH monitoring of the lithium-ion battery. This paper presents a neural-network-based method that combines long short-term memory (LSTM) network with particle swarm optimization and attention mechanism for RUL prediction and SOH monitoring of the lithium-ion battery. Before predicting RUL of the lithium-ion battery, the Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) is utilized for the raw data denoising, which can improve the accuracy of prediction. A real-life cycle dataset of lithium-ion batteries from NASA is used to evaluate the proposed method, and the experiment results show that when compared with traditional methods, the proposed method has higher accuracy.","['Lithium-ion batteries', 'Monitoring', 'Prognostics and health management', 'Support vector machines', 'Data models', 'Load modeling']","['Lithium-ion battery', 'prognostic and health management (PHM)', 'long short-term memory (LSTM)', 'attention mechanism']"
"Optimization of deep learning is no longer an imminent problem, due to various gradient descent methods and the improvements of network structure, including activation functions, the connectivity style, and so on. Then the actual application depends on the generalization ability, which determines whether a network is effective. Regularization is an efficient way to improve the generalization ability of deep CNN, because it makes it possible to train more complex models while maintaining a lower overfitting. In this paper, we propose to optimize the feature boundary of deep CNN through a two-stage training method (pre-training process and implicit regularization training process) to reduce the overfitting problem. In the pre-training stage, we train a network model to extract the image representation for anomaly detection. In the implicit regularization training stage, we re-train the network based on the anomaly detection results to regularize the feature boundary and make it converge in the proper position. Experimental results on five image classification benchmarks show that the two-stage training method achieves a state-of-the-art performance and that it, in conjunction with more complicated anomaly detection algorithm, obtains better results. Finally, we use a variety of strategies to explore and analyze how implicit regularization plays a role in the two-stage training process. Furthermore, we explain how implicit regularization can be interpreted as data augmentation and model ensemble.","['Training', 'Anomaly detection', 'Feature extraction', 'Image representation', 'Production', 'Principal component analysis', 'Machine learning']","['Deep CNN', 'image classification', 'overfitting', 'generalization', 'anomaly detection', 'implicit regularization']"
"Feature selection is a critical and prominent task in machine learning. To reduce the dimension of the feature set while maintaining the accuracy of the performance is the main aim of the feature selection problem. Various methods have been developed to classify the datasets. However, metaheuristic algorithms have achieved great attention in solving numerous optimization problem. Therefore, this paper presents an extensive literature review on solving feature selection problem using metaheuristic algorithms which are developed in the ten years (2009-2019). Further, metaheuristic algorithms have been classified into four categories based on their behaviour. Moreover, a categorical list of more than a hundred metaheuristic algorithms is presented. To solve the feature selection problem, only binary variants of metaheuristic algorithms have been reviewed and corresponding to their categories, a detailed description of them explained. The metaheuristic algorithms in solving feature selection problem are given with their binary classification, name of the classifier used, datasets and the evaluation metrics. After reviewing the papers, challenges and issues are also identified in obtaining the best feature subset using different metaheuristic algorithms. Finally, some research gaps are also highlighted for the researchers who want to pursue their research in developing or modifying metaheuristic algorithms for classification. For an application, a case study is presented in which datasets are adopted from the UCI repository and numerous metaheuristic algorithms are employed to obtain the optimal feature subset.","['Feature extraction', 'Search problems', 'Biomedical imaging', 'Task analysis', 'Particle swarm optimization', 'Measurement', 'Machine learning algorithms']","['Binary variants', 'classification', 'feature selection', 'literature review', 'metaheuristic algorithms']"
"This paper presents research challenges on security and privacy issues in the field of green IoT-based agriculture. We start by describing a four-tier green IoT-based agriculture architecture and summarizing the existing surveys that deal with smart agriculture. Then, we provide a classification of threat models against green IoT-based agriculture into five categories, including, attacks against privacy, authentication, confidentiality, availability, and integrity properties. Moreover, we provide a taxonomy and a side-by-side comparison of the state-of-the-art methods toward secure and privacy-preserving technologies for IoT applications and how they will be adapted for green IoT-based agriculture. In addition, we analyze the privacy-oriented blockchain-based solutions as well as consensus algorithms for IoT applications and how they will be adapted for green IoT-based agriculture. Based on the current survey, we highlight open research challenges and discuss possible future research directions in the security and privacy of green IoT-based agriculture.","['Internet of Things', 'Security', 'Privacy', 'Green products', 'Production', 'Agricultural products']","['Security', 'privacy', 'authentication', 'blockchain', 'smart agriculture', 'greenhouse']"
"Industry 4.0 is a concept devised for improving the way modern factories operate through the use of some of the latest technologies, like the ones used for creating the Industrial Internet of Things (IIoT), robotics, or Big Data applications. One of such technologies is blockchain, which is able to add trust, security, and decentralization to different industrial fields. This article focuses on analyzing the benefits and challenges that arise when using blockchain and smart contracts to develop Industry 4.0 applications. In addition, this paper presents a thorough review of the most relevant blockchain-based applications for Industry 4.0 technologies. Thus, its aim is to provide a detailed guide for the future Industry 4.0 developers that allows for determining how the blockchain can enhance the next generation of cybersecure industrial applications.","['Blockchain', 'Industries', 'Smart manufacturing', 'Production facilities', 'Next generation networking', 'Internet', 'Computer crime']","['Blockchain', 'Industry 4.0', 'cybersecurity', 'IIoT', 'smart factory', 'industrial augmented reality', 'cyber-physical system', 'fog and edge computing', 'cloud computing', 'Big Data']"
"Cloud Computing provides an effective platform for executing large-scale and complex workflow applications with a pay-as-you-go model. Nevertheless, various challenges, especially its optimal scheduling for multiple conflicting objectives, are yet to be addressed properly. The existing multi-objective workflow scheduling approaches are still limited in many ways, e.g., encoding is restricted by prior experts’ knowledge when handling a dynamic real-time problem, which strongly influences the performance of scheduling. In this paper, we apply a deep-Q-network model in a multi-agent reinforcement learning setting to guide the scheduling of multi-workflows over infrastructure-as-a-service clouds. To optimize multi-workflow completion time and user’s cost, we consider a Markov game model, which takes the number of workflow applications and heterogeneous virtual machines as state input and the maximum completion time and cost as rewards. The game model is capable of seeking for correlated equilibrium between make-span and cost criteria without prior experts’ knowledge and converges to the correlated equilibrium policy in a dynamic real-time environment. To validate our proposed approach, we conduct extensive case studies based on multiple well-known scientific workflow templates and Amazon EC2 cloud. The experimental results clearly suggest that our proposed approach outperforms traditional ones, e.g., non-dominated sorting genetic algorithm-II, multi-objective particle swarm optimization, and game-theoretic-based greedy algorithms, in terms of optimality of scheduling plans generated.","['Optimal scheduling', 'Games', 'Task analysis', 'Cloud computing', 'Reinforcement learning', 'Markov processes', 'Scheduling']","['Multi-objective workflow scheduling', 'deep-Q-network (DQN)', 'multi-agent reinforcement learning (MARL)', 'infrastructure-as-a-service (IaaS) cloud', 'quality-of-service (QoS)']"
"The volatility and uncertainty of wind power often affect the quality of electric energy, the security of the power grid, the stability of the power system, and the fluctuation of the power market. In this case, the research on wind power forecasting is of great significance for ensuring the better development of wind power grids and the higher quality of electric energy. Therefore, a lot of new forecasting methods have been put forward. In this paper, a new forecasting model based on a convolution neural network and LightGBM is constructed. The procedure is shown as follows. First, we construct new feature sets by analyzing the characteristics of the raw data on the time series from the wind field and adjacent wind field. Second, the convolutional neural network (CNN) is proposed to extract information from input data, and the network parameters are adjusted by comparing the actual results. Third, in consideration of the limitations of the single-convolution model in predicting wind power, we innovatively integrated the LightGBM classification algorithm at the model to improve the forecasting accuracy and robustness. Finally, compared with the existing support vector machines, LightGBM, and CNN, the fusion model has better performance in accuracy and efficiency.","['Forecasting', 'Wind power generation', 'Convolution', 'Feature extraction', 'Predictive models', 'Data mining', 'Kernel']","['Convolutional neural network', 'fusion model', 'LightGBM', 'ultra-short-term wind power forecasting', 'wind energy']"
"More and more network traffic data have brought great challenge to traditional intrusion detection system. The detection performance is tightly related to selected features and classifiers, but traditional feature selection algorithms and classification algorithms can't perform well in massive data environment. Also the raw traffic data are imbalanced, which has a serious impact on the classification results. In this paper, we propose a novel network intrusion detection model utilizing convolutional neural networks (CNNs). We use CNN to select traffic features from raw data set automatically, and we set the cost function weight coefficient of each class based on its numbers to solve the imbalanced data set problem. The model not only reduces the false alarm rate (FAR) but also improves the accuracy of the class with small numbers. To reduce the calculation cost further, we convert the raw traffic vector format into image format. We use the standard NSL-KDD data set to evaluate the performance of the proposed CNN model. The experimental results show that the accuracy, FAR, and calculation cost of the proposed model perform better than traditional standard algorithms. It is an effective and reliable solution for the intrusion detection of a massive network.","['Intrusion detection', 'Feature extraction', 'Training', 'Data models', 'Data preprocessing', 'Convolutional neural networks']","['Network intrusion detection', 'convolutional neural networks', 'image data format conversion', 'cost function weight', 'imbalanced dataset']"
"Nowadays, learning-based modeling system is adopted to establish an accurate prediction model for renewable energy resources. Computational Intelligence (CI) methods have become significant tools in production and optimization of renewable energies. The complexity of this type of energy lies in its coverage of large volumes of data and variables which have to be analyzed carefully. The present study discusses different types of Deep Learning (DL) algorithms applied in the field of solar and wind energy resources and evaluates their performance through a novel taxonomy. It also presents a comprehensive state-of-the-art of the literature leading to an assessment and performance evaluation of DL techniques as well as a discussion about major challenges and opportunities for comprehensive research. Based on results, differences on accuracy, robustness, precision values as well as the generalization ability are the most common challenges for the employment of DL techniques. In case of big dataset, the performance of DL techniques is significantly higher than that for other CI techniques. However, using and developing hybrid DL techniques with other optimization techniques in order to improve and optimize the structure of the techniques is preferably emphasized. In all cases, hybrid networks have better performance compared with single networks, because hybrid techniques take the advantages of two or more methods for preparing an accurate prediction. It is recommended to use hybrid methods in DL techniques.","['Wind energy', 'Solar energy', 'Renewable energy sources', 'Wind', 'Neural networks', 'Mathematical model', 'Computational modeling']","['Big dataset', 'deep learning', 'modeling', 'optimizing', 'solar energy', 'wind energy']"
"As the technique that determines the position of a target device based on wireless measurements, Wi-Fi localization is attracting increasing attention due to its numerous applications and the widespread deployment of Wi-Fi infrastructure. In this paper, we propose ConFi, the first convolutional neural network (CNN)-based Wi-Fi localization algorithm. Channel state information (CSI), which contains more position related information than traditional received signal strength, is organized into a time-frequency matrix that resembles image and utilized as the feature for localization. The ConFi models localization as a classification problem and addresses it with a five layer CNN that consists of three convolutional layers and two fully connected layers. The ConFi has a training stage and a localization stage. In the training stage, the CSI is collected at a number of reference points (RPs) and used to train the CNN via stochastic gradient descent algorithm. In the localization stage, the CSI of the target device is fed to the CNN and the localization result is calculated as the weighted centroid of the RPs with high output value. Extensive experiments are conducted to select appropriate parameters for the CNN and demonstrate the superior performance of the ConFi over existing methods.","['Antennas', 'Wireless fidelity', 'Artificial neural networks', 'Training', 'Antenna measurements', 'Feature extraction', 'Fading channels']","['Wi-Fi localization', 'channel state information', 'convolutional neural network', 'pattern recognition']"
"5G is the next generation cellular network that aspires to achieve substantial improvement on quality of service, such as higher throughput and lower latency. Edge computing is an emerging technology that enables the evolution to 5G by bringing cloud capabilities near to the end users (or user equipment, UEs) in order to overcome the intrinsic problems of the traditional cloud, such as high latency and the lack of security. In this paper, we establish a taxonomy of edge computing in 5G, which gives an overview of existing state-of-the-art solutions of edge computing in 5G on the basis of objectives, computational platforms, attributes, 5G functions, performance measures, and roles. We also present other important aspects, including the key requirements for its successful deployment in 5G and the applications of edge computing in 5G. Then, we explore, highlight, and categorize recent advancements in edge computing for 5G. By doing so, we reveal the salient features of different edge computing paradigms for 5G. Finally, open research issues are outlined.","['Edge computing', '5G mobile communication', 'Cloud computing', 'Real-time systems', 'Servers', 'Quality of service', 'Throughput']","['5G', 'cloud computing', 'edge computing', 'fog computing']"
"Emotion recognition represents the position and motion of facial muscles. It contributes significantly in many fields. Current approaches have not obtained good results. This paper aimed to propose a new emotion recognition system based on facial expression images. We enrolled 20 subjects and let each subject pose seven different emotions: happy, sadness, surprise, anger, disgust, fear, and neutral. Afterward, we employed biorthogonal wavelet entropy to extract multiscale features, and used fuzzy multiclass support vector machine to be the classifier. The stratified cross validation was employed as a strict validation model. The statistical analysis showed our method achieved an overall accuracy of 96.77±0.10%. Besides, our method is superior to three state-of-the-art methods. In all, this proposed method is efficient.","['Support vector machines', 'Wavelet transforms', 'Entropy', 'Feature extraction', 'Face recognition', 'Low-pass filters', 'Fuzzy logic', 'Emotion recognition']","['Facial emotion recognition', 'facial expression', 'biorthogonal wavelet entropy', 'support vector machine', 'fuzzy logic']"
"In the last few years, Internet of Things, Cloud computing, Edge computing, and Fog computing have gained a lot of attention in both industry and academia. However, a clear and neat definition of these computing paradigms and their correlation is hard to find in the literature. This makes it difficult for researchers new to this area to get a concrete picture of these paradigms. This work tackles this deficiency, representing a helpful resource for those who will start next. First, we show the evolution of modern computing paradigms and related research interest. Then, we address each paradigm, neatly delineating its key points and its relation with the others. Thereafter, we extensively address Fog computing, remarking its outstanding role as the glue between IoT, Cloud, and Edge computing. In the end, we briefly present open challenges and future research directions for IoT, Cloud, Edge, and Fog computing.","['Cloud computing', 'Edge computing', 'Internet of Things', 'Market research', 'Computer architecture', 'Libraries']","['Fog computing', 'cloud computing', 'edge computing', 'Internet of Things', 'mobile cloud computing', 'mobile edge computing']"
"The advancement of technologies over years has poised Internet of Things (IoT) to scoop out untapped information and communication technology opportunities. It is anticipated that IoT will handle the gigantic network of billions of devices to deliver plenty of smart services to the users. Undoubtedly, this will make our life more resourceful but at the cost of high energy consumption and carbon footprint. Consequently, there is a high demand for green communication to reduce energy consumption, which requires optimal resource availability and controlled power levels. In contrast to this, IoT devices are constrained in terms of resources-memory, power, and computation. Low power wide area (LPWA) technology is a response to the need for efficient utilization of power resource, as it evinces characteristics such as the capability to proffer low power connectivity to a huge number of devices spread over wide geographical areas at low cost. Various LPWA technologies, such as LoRa and SigFox, exist in the market, offering a proficient solution to the users. However, in order to abstain the need of new infrastructure (like base station) that is required for proprietary technologies, a new cellular-based licensed technology, narrowband IoT (NBIoT), is introduced by 3GPP in Rel-13. This technology presents a good candidature to handle LPWA market because of its characteristics like enhanced indoor coverage, low power consumption, latency insensitivity, and massive connection support towards NBIoT. This survey presents a profound view of IoT and NBIoT, subsuming their technical features, resource allocation, and energy-efficiency techniques and applications. The challenges that hinder the NBIoT path to success are also identified and discussed. In this paper, two novel energy-efficient techniques “zonal thermal pattern analysis” and energy-efficient adaptive health monitoring system have been proposed towards green IoT.","['Energy efficiency', 'Internet of Things', 'Power demand', 'Resource management', 'Agriculture', 'Computer architecture', 'Monitoring']","['Internet of Things (IoT)', 'narrowband Internet of Things (NBIoT)', 'low power wide area network (LPWAN)', 'green communication', 'smart agriculture', 'smart health']"
"Today, internet and device ubiquity are paramount in individual, formal and societal considerations. Next generation communication technologies, such as Blockchains (BC), Internet of Things (IoT), cloud computing, etc. offer limitless capabilities for different applications and scenarios including industries, cities, healthcare systems, etc. Sustainable integration of healthcare nodes (i.e. devices, users, providers, etc.) resulting in healthcare IoT (or simply IoHT) provides a platform for efficient service delivery for the benefit of care givers (doctors, nurses, etc.) and patients. Whereas confidentiality, accessibility and reliability of medical data are accorded high premium in IoHT, semantic gaps and lack of appropriate assets or properties remain impediments to reliable information exchange in federated trust management frameworks. Consequently, We propose a Blockchain Decentralised Interoperable Trust framework (DIT) for IoT zones where a smart contract guarantees authentication of budgets and Indirect Trust Inference System (ITIS) reduces semantic gaps and enhances trustworthy factor (TF) estimation via the network nodes and edges. Our DIT IoHT makes use of a private Blockchain ripple chain to establish trustworthy communication by validating nodes based on their inter-operable structure so that controlled communication required to solve fusion and integration issues are facilitated via different zones of the IoHT infrastructure. Further, C# implementation using Ethereum and ripple Blockchain are introduced as frameworks to associate and aggregate requests over trusted zones.","['Semantics', 'Ontologies', 'Internet of Things', 'Interoperability', 'Data models', 'Sensors', 'Medical services']","['Trustworthness', 'blockchain', 'security', 'interoperability', 'sustainable healthcare IoT systems']"
"In this paper, an edge computing system for IoT-based (Internet of Things) smart grids is proposed to overcome the drawbacks in the current cloud computing paradigm in power systems, where many problems have yet to be addressed such as fully realizing the requirements of high bandwidth with low latency. The new system mainly introduces edge computing in the traditional cloud-based power system and establishes a new hardware and software architecture. Therefore, a considerable amount of data generated in the electrical grid will be analyzed, processed, and stored at the edge of the network. Aided with edge computing paradigm, the IoT-based smart grids will realize the connection and management of substantial terminals, provide the real-time analysis and processing of massive data, and foster the digitalization of smart grids. In addition, we propose a privacy protection strategy via edge computing, data prediction strategy, and preprocessing strategy of hierarchical decision-making based on task grading (HDTG) for the IoT-based smart girds. The effectiveness of our proposed approaches has been demonstrated via the numerical simulations.","['Smart grids', 'Edge computing', 'Cloud computing', 'Industries', 'Sensors', 'Real-time systems']","['Edge computing', 'IoT-based smart grids', 'data prediction', 'artificial intelligence', 'data privacy protection', 'cloud computing']"
"Gesture recognition aims to recognize meaningful movements of human bodies, and is of utmost importance in intelligent human-computer/robot interactions. In this paper, we present a multimodal gesture recognition method based on 3-D convolution and convolutional long-short-term-memory (LSTM) networks. The proposed method first learns short-term spatiotemporal features of gestures through the 3-D convolutional neural network, and then learns long-term spatiotemporal features by convolutional LSTM networks based on the extracted short-term spatiotemporal features. In addition, fine-tuning among multimodal data is evaluated, and we find that it can be considered as an optional skill to prevent overfitting when no pre-trained models exist. The proposed method is verified on the ChaLearn LAP large-scale isolated gesture data set (IsoGD) and the Sheffield Kinect gesture (SKIG) data set. The results show that our proposed method can obtain the state-of-the-art recognition accuracy (51.02% on the validation set of IsoGD and 98.89% on SKIG).","['Gesture recognition', 'Three-dimensional displays', 'Spatiotemporal phenomena', 'Feature extraction', 'Neural networks', 'Convolution', 'Solid modeling']","['3-D convolution', 'convolutional LSTM', 'gesture recognition', 'multimodal']"
"Blockchains offer a decentralized, immutable and verifiable ledger that can record transactions of digital assets, provoking a radical change in several innovative scenarios, such as smart cities, eHealth or eGovernment. However, blockchains are subject to different scalability, security and potential privacy issues, such as transaction linkability, crypto-keys management (e.g. recovery), on-chain data privacy, or compliance with privacy regulations (e.g. GDPR). To deal with these challenges, novel privacy-preserving solutions for blockchain based on crypto-privacy techniques are emerging to empower users with mechanisms to become anonymous and take control of their personal data during their digital transactions of any kind in the ledger, following a Self-Sovereign Identity (SSI) model. In this sense, this paper performs a systematic review of the current state of the art on privacy-preserving research solutions and mechanisms in blockchain, as well as the main associated privacy challenges in this promising and disrupting technology. The survey covers privacy techniques in public and permissionless blockchains, e.g. Bitcoin and Ethereum, as well as privacy-preserving research proposals and solutions in permissioned and private blockchains. Diverse blockchain scenarios are analyzed, encompassing, eGovernment, eHealth, cryptocurrencies, Smart cities, and Cooperative ITS.","['Blockchain', 'Privacy', 'Data privacy', 'Proposals', 'Bitcoin']","['Blockchain', 'privacy', 'security', 'survey', 'bitcoin']"
"In recent years, multiple-input-multiple-output (MIMO) antennas with the ability to radiate waves in more than one pattern and polarization play a great role in modern telecommunication systems. This paper provides a theoretical review of different mutual coupling reduction techniques in MIMO antenna systems. The increase in the mutual coupling can affect the antenna characteristics drastically and therefore degrades the performance of the MIMO systems. It is possible to improve the performance partially by calibrating the mutual coupling in the digital domain. However, the simple and effective approach is to use the techniques, such as defected ground structure, parasitic or slot element, complementary split ring resonator, and decoupling networks which can overcome the mutual coupling effects by means of physical implementation. An extensive discussion on the basis of different mutual coupling reduction techniques, their examples, and comparative study is still rare in the literature. Therefore, in this paper, different MIMO antenna design techniques and all of their mutual coupling reduction techniques through various structures and mechanisms are presented with multiple examples and characteristics comparison.","['Mutual coupling', 'MIMO communication', 'Antenna radiation patterns', 'Correlation', 'Correlation coefficient', 'Topology']","['Diversity gain', 'ECC', 'MIMO', 'mutual coupling', 'PCB', 'UWB', 'WLAN']"
"The inherent flexibility of hierarchical structure scheme with main-servo loop control structure is proposed to the problem of integrated chassis control system for the vehicle. It includes both main loop, which calculates and allocates the aim force using the optimal robust control algorithm and servo loop control systems, which track and achieve the target force using the onboard independent brake actuators. In fact, for the brake actuator, the aim friction is obtained by tracking the corresponding slip ratio of target force. For the coefficient of tire-road friction varying with different road surface, to get the nonlinear time-varying target slip ratio, the most famous quasi-static magic formula is proposed to estimate and predict real-time coefficient of different road surface and the constrained hybrid genetic algorithm (GA) is used to identify the key parameters of the magic formula on-line. Then, a self-tuning longitudinal slip ratio controller (LSC) based on the nonsingular and fast terminal sliding mode (NFTSM) control method is designed to improve the tracking accuracy and response speed of the actuators. At last, the proposed integrated chassis control strategies and the self-tuning control strategies are verified by computer simulations.","['Friction', 'Force', 'Tires', 'Brakes', 'Genetic algorithms', 'Vehicle dynamics']","['Main-servo loop control structure', 'parameters identification', 'tire-road friction control', 'constrained hybrid genetic algorithm', 'nonlinear sliding mode control']"
"Industrial cyber-physical systems (ICPSs) are the backbones of Industry 4.0 and as such, have become a core transdisciplinary area of research, both in industry and academia. New challenges brought about by the growing scale and complexity of systems, insufficient information exchange, and the exploitation of knowledge available have started threatening the overall system safety and stability. This work is motivated by these challenges and the strategic and practical demands of developing ICPSs for safety critical systems such as the intelligent factory and the smart grid. It investigates the current status of research in ICPS monitoring and control, and reviews the recent advances in monitoring, fault diagnosis, and control approaches based on data-driven realization, which can take full advantage of the abundant data available from past observations and those collected online in realtime. The practical requirements in the typical ICPS applications are summarized as the major issues to be addressed for the monitoring and the safety control tasks. The key challenges and the research directions are proposed as references to the future work.","['Monitoring', 'Sensors', 'Iterative closest point algorithm', 'Observers', 'Smart grids', 'Fault diagnosis', 'Cyber-physical systems']","['Cyber-physical system (CPS)', 'data-driven', 'system monitoring', 'fault diagnosis', 'smart grid', 'plug-and-play control']"
"Q-learning is arguably one of the most applied representative reinforcement learning approaches and one of the off-policy strategies. Since the emergence of Q-learning, many studies have described its uses in reinforcement learning and artificial intelligence problems. However, there is an information gap as to how these powerful algorithms can be leveraged and incorporated into general artificial intelligence workflow. Early Q-learning algorithms were unsatisfactory in several aspects and covered a narrow range of applications. It has also been observed that sometimes, this rather powerful algorithm learns unrealistically and overestimates the action values hence abating the overall performance. Recently with the general advances of machine learning, more variants of Q-learning like Deep Q-learning which combines basic Q learning with deep neural networks have been discovered and applied extensively. In this paper, we thoroughly explain how Q-learning evolved by unraveling the mathematical complexities behind it as well its flow from reinforcement learning family of algorithms. Improved variants are fully described, and we categorize Q-learning algorithms into single-agent and multi-agent approaches. Finally, we thoroughly investigate up-to-date research trends and key applications that leverage Q-learning algorithms.","['Reinforcement learning', 'Mathematical model', 'Classification algorithms', 'Machine learning algorithms', 'Market research']","['Reinforcement learning', 'Q-learning', 'single-agent', 'multi-agent']"
"Intrusion detection systems (IDSs) play a pivotal role in computer security by discovering and repealing malicious activities in computer networks. Anomaly-based IDS, in particular, rely on classification models trained using historical data to discover such malicious activities. In this paper, an improved IDS based on hybrid feature selection and two-level classifier ensembles are proposed. A hybrid feature selection technique comprising three methods, i.e., particle swarm optimization, ant colony algorithm, and genetic algorithm, is utilized to reduce the feature size of the training datasets (NSL-KDD and UNSW-NB15 are considered in this paper). Features are selected based on the classification performance of a reduced error pruning tree (REPT) classifier. Then, a two-level classifier ensemble based on two meta learners, i.e., rotation forest and bagging, is proposed. On the NSL-KDD dataset, the proposed classifier shows 85.8% accuracy, 86.8% sensitivity, and 88.0% detection rate, which remarkably outperform other classification techniques recently proposed in the literature. The results regarding the UNSW-NB15 dataset also improve the ones achieved by several state-of-the-art techniques. Finally, to verify the results, a two-step statistical significance test is conducted. This is not usually considered by the IDS research thus far and, therefore, adds value to the experimental results achieved by the proposed classifier.","['Feature extraction', 'Internet of Things', 'Training', 'Intrusion detection', 'Bagging', 'Anomaly detection']","['Two-stage meta classifier', 'network anomaly detection', 'hybrid feature selection', 'intrusion detection system', 'statistical significance test']"
"Rotating machines have been widely used in industrial engineering. The fault diagnosis of rotating machines plays a vital important role to reduce the catastrophic failures and heavy economic loss. However, the measured vibration signal of rotating machinery often represents non-linear and non-stationary characteristics, resulting in difficulty in the fault feature extraction. As a statistical measure, entropy can quantify the complexity and detect dynamic change through taking into account the non-linear behavior of time series. Therefore, entropy can be served as a promising tool to extract the dynamic characteristics of rotating machines. Recently, many studies have applied entropy in fault diagnosis of rotating machinery. This paper aims to investigate the applications of entropy for the fault characteristics extraction of rotating machines. First, various entropy methods are briefly introduced. Its foundation, application, and some improvements are described and discussed. The review is divided into eight parts: Shannon entropy, Rényi entropy, approximate entropy, sample entropy, fuzzy entropy, permutation entropy, and other entropy methods. In each part, we will review the applications using the original entropy method and the improved entropy methods, respectively. In the end, a summary and some research prospects are given.","['Entropy', 'Time series analysis', 'Fault diagnosis', 'Complexity theory', 'Rotating machines', 'Vibrations']","['Entropy', 'fault diagnosis', 'fault feature extraction', 'rotating machinery', 'condition-based maintenance']"
"Federated learning is a newly emerged distributed machine learning paradigm, where the clients are allowed to individually train local deep neural network (DNN) models with local data and then jointly aggregate a global DNN model at the central server. Vehicular edge computing (VEC) aims at exploiting the computation and communication resources at the edge of vehicular networks. Federated learning in VEC is promising to meet the ever-increasing demands of artificial intelligence (AI) applications in intelligent connected vehicles (ICV). Considering image classification as a typical AI application in VEC, the diversity of image quality and computation capability in vehicular clients potentially affects the accuracy and efficiency of federated learning. Accordingly, we propose a selective model aggregation approach, where “fine” local DNN models are selected and sent to the central server by evaluating the local image quality and computation capability. Regarding the implementation of model selection, the central server is not aware of the image quality and computation capability in the vehicular clients, whose privacy is protected under such a federated learning framework. To overcome this information asymmetry, we employ two-dimension contract theory as a distributed framework to facilitate the interactions between the central server and vehicular clients. The formulated problem is then transformed into a tractable problem through successively relaxing and simplifying the constraints, and eventually solved by a greedy algorithm. Using two datasets, i.e., MNIST and BelgiumTSC, our selective model aggregation approach is demonstrated to outperform the original federated averaging (FedAvg) approach in terms of accuracy and efficiency. Meanwhile, our approach also achieves higher utility at the central server compared with the baseline approaches.","['Computational modeling', 'Servers', 'Image quality', 'Training', 'Contracts', 'Artificial intelligence', 'Data models']","['Federated learning', 'vehicular edge computing', 'model aggregation', 'contract theory']"
"With the significant development of practicability in deep learning and the ultra-high-speed information transmission rate of 5G communication technology will overcome the barrier of data transmission on the Internet of Vehicles, automated driving is becoming a pivotal technology affecting the future industry. Sensors are the key to the perception of the outside world in the automated driving system and whose cooperation performance directly determines the safety of automated driving vehicles. In this survey, we mainly discuss the different strategies of multi-sensor fusion in automated driving in recent years. The performance of conventional sensors and the necessity of multi-sensor fusion are analyzed, including radar, LiDAR, camera, ultrasonic, GPS, IMU, and V2X. According to the differences in the latest studies, we divide the fusion strategies into four categories and point out some shortcomings. Sensor fusion is mainly applied for multi-target tracking and environment reconstruction. We discuss the method of establishing a motion model and data association in multi-target tracking. At the end of the paper, we analyzed the deficiencies in the current studies and put forward some suggestions for further improvement in the future. Through this investigation, we hope to analyze the current situation of multi-sensor fusion in the automated driving process and provide more efficient and reliable fusion strategies.","['Sensor fusion', 'Sensor systems', 'Sensor phenomena and characterization', 'Cameras', 'Laser radar']","['Automated driving', 'multi-sensor fusion strategy', 'multi-target tracking', 'environmental reconstruction', 'data association', 'intent analysis', 'deep learning']"
"Underwater communication remains a challenging technology via communication cables and the cost of underwater sensor network (UWSN) deployment is still very high. As an alternative, underwater wireless communication has been proposed and have received more attention in the last decade. Preliminary research indicated that the Radio Frequency (RF) and Magneto-Inductive (MI) communication achieve higher data rate in the near field communication. The optical communication achieves good performance when limited to the line-of-sight positioning. The acoustic communication allows long transmission range. However, it suffers from transmission losses and time-varying signal distortion due to its dependency on environmental properties. These latter are salinity, temperature, pressure, depth of transceivers, and the environment geometry. This paper is focused on both the acoustic and magneto-inductive communications, which are the most used technologies for underwater networking. Such as acoustic communication is employed for applications requiring long communication range while the MI is used for real-time communication. Moreover, this paper highlights the trade-off between underwater properties, wireless communication technologies, and communication quality. This can help the researcher community by providing clear insight for further research.","['Wireless communication', 'Absorption', 'Wireless sensor networks', 'Propagation losses', 'Ocean temperature', 'Acoustic communication (telecommunication)']","['Underwater wireless sensor networks', 'underwater wireless communications', 'magneto-inductive communications', 'acoustic communications', 'simultaneous wireless power', 'information transfer', 'Internet of Underwater Things']"
"Multimodal representation learning, which aims to narrow the heterogeneity gap among different modalities, plays an indispensable role in the utilization of ubiquitous multimodal data. Due to the powerful representation ability with multiple levels of abstraction, deep learning-based multimodal representation learning has attracted much attention in recent years. In this paper, we provided a comprehensive survey on deep multimodal representation learning which has never been concentrated entirely. To facilitate the discussion on how the heterogeneity gap is narrowed, according to the underlying structures in which different modalities are integrated, we category deep multimodal representation learning methods into three frameworks: joint representation, coordinated representation, and encoder-decoder. Additionally, we review some typical models in this area ranging from conventional models to newly developed technologies. This paper highlights on the key issues of newly developed technologies, such as encoder-decoder model, generative adversarial networks, and attention mechanism in a multimodal representation learning perspective, which, to the best of our knowledge, have never been reviewed previously, even though they have become the major focuses of much contemporary research. For each framework or model, we discuss its basic structure, learning objective, application scenes, key issues, advantages, and disadvantages, such that both novel and experienced researchers can benefit from this survey. Finally, we suggest some important directions for future work.","['Semantics', 'Feature extraction', 'Deep learning', 'Task analysis', 'Speech recognition', 'Data mining', 'Decoding']","['Multimodal representation learning', 'multimodal deep learning', 'deep multimodal fusion', 'multimodal translation', 'multimodal adversarial learning']"
"Although an Internet-of-Things-based smart home solution can provide an improved and better approach to healthcare management, yet its end user adoption is very low. With elderly people as the main target, these conservative users pose a serious challenge to the successful implementation of smart home healthcare services. The objective of this research was to develop and test a theoretical framework empirically for determining the core factors that can affect the elderly users' acceptance of smart home services for healthcare. Accordingly, an online survey was conducted with 254 elderly people aged 55 years and above across four Asian countries. Partial least square structural equation modeling was applied to analyze the effect of eight hypothesized predicting constructs. The user perceptions were measured on a conceptual level rather than the actual usage intention toward a specific service. Performance expectancy, effort expectancy, expert advice, and perceived trust have a positive impact on the behavioral intention. The same association is negative for technology anxiety and perceived cost. Facilitating conditions and social influence do not have any effect on the behavioral intention. The model could explain 81.4% of the total variance in the dependent variable i.e., behavioral intention. Effort expectancy is the leading predictor of smart homes for healthcare acceptance among the elderly. Together with expert advice, perceived trust, and perceived cost, these four factors represent the key influence of the elderly peoples' acceptance behavior. This paper provides the groundwork to explore the process of the actual adoption of smart home services for healthcare by the elderly people with potential future research areas.","['Senior citizens', 'Smart homes', 'Medical services', 'Wearable sensors', 'Bibliographies', 'Sensor systems']","['Elderly', 'healthcare', 'smart homes']"
"The imminent arrival of the Internet of Things (IoT), which consists of a vast number of devices with heterogeneous characteristics, means that future networks need a new architecture to accommodate the expected increase in data generation. Software defined networking (SDN) and network virtualization (NV) are two technologies that promise to cost-effectively provide the scale and versatility necessary for IoT services. In this paper, we survey the state of the art on the application of SDN and NV to IoT. To the best of our knowledge, we are the first to provide a comprehensive description of every possible IoT implementation aspect for the two technologies. We start by outlining the ways of combining SDN and NV. Subsequently, we present how the two technologies can be used in the mobile and cellular context, with emphasis on forthcoming 5G networks. Afterward, we move to the study of wireless sensor networks, arguably the current foremost example of an IoT network. Finally, we review some general SDN-NV-enabled IoT architectures, along with real-life deployments and use-cases. We conclude by giving directions for future research on this topic.","['Virtualization', 'Internet of things', 'Wireless sensor networks', 'Computer architecture', 'Software defined network', 'Wireless communication', 'Mobile computing']","['Internet of things (IoT)', 'software defined networking (SDN)', 'network virtualization (NV)', 'network functions virtualization (NFV)', '5G', 'wireless sensor network (WSN)']"
"In order to meet the intense user demands, the 5G networks are evolving, and will be available by 2020. The unfolding cellular technology has raised the energy consumption in mobile networks with the carbon footprint surging to alarming rates. This is causing an adverse effect on the environment and human health. Addressing these aspects, this paper presents a survey on techniques for making the next generation cellular networks GREEN. A number of technologies form a part of the 5G networks, in order to support the drastic user demands, and are receiving substantial attention from the perspective of green communication. These include device-to-device communication, spectrum sharing, ultra dense networks, massive MIMO, and the Internet of Things. Also, a prime concern in the current scenario is the battery life of the mobile terminals. For enhancing the battery life of the user terminals, a proposal is given in this paper, with spectrum sharing as its basis, to overcome the energy crunch. Major research challenges have been discussed, and the ongoing projects and standardization activities also stated in this paper.","['Energy efficiency', 'Green products', '5G mobile communication', 'Batteries', 'Air pollution', 'Cellular networks']","['Carbon footprint', 'D2D communication', 'ultra dense networks (UDNs)', 'massive MIMO', 'spectrum sharing', 'Internet of Things (IoT)', 'small cell access point (SCA)']"
"A multi-band 10-antenna array working at the sub-6-GHz spectrum (LTE bands 42/43 and LTE band 46) for massive multiple-input multiple-output (MIMO) applications in future 5G smartphones is proposed. To realize $10\times 10$ MIMO applications in three LTE bands, 10 T-shaped coupled-fed slot antenna elements that can excite dual resonant modes are integrated into a system circuit board. Spatial and polarization diversity techniques are implemented on these elements so that the improved isolation and mitigated coupling effects can be achieved. The proposed antenna array was manufactured and experimentally measured. Desirable antenna efficiencies of higher than 42% and 62% were measured in the low band and high band, respectively. Vital results, such as the envelope correlation coefficient, channel capacity, and mean effective gain ratio, have also been computed and analyzed. The calculated ergodic channel capacities of the $10\times 10$ MIMO system working in the LTE bands 42/43 and LTE band 46 reached up to 48 and 51.4 b/s/Hz, respectively.","['MIMO communication', 'Slot antennas', '5G mobile communication', 'Smart phones', 'Microstrip antenna arrays']","['Sub-6 GHz', '5G', 'MIMO antenna', 'massive MIMO']"
"While machine learning and artificial intelligence have long been applied in networking research, the bulk of such works has focused on supervised learning. Recently, there has been a rising trend of employing unsupervised machine learning using unstructured raw network data to improve network performance and provide services, such as traffic engineering, anomaly detection, Internet traffic classification, and quality of service optimization. The growing interest in applying unsupervised learning techniques in networking stems from their great success in other fields, such as computer vision, natural language processing, speech recognition, and optimal control (e.g., for developing autonomous self-driving cars). In addition, unsupervised learning can unconstrain us from the need for labeled data and manual handcrafted feature engineering, thereby facilitating flexible, general, and automated methods of machine learning. The focus of this survey paper is to provide an overview of applications of unsupervised learning in the domain of networking. We provide a comprehensive survey highlighting recent advancements in unsupervised learning techniques, and describe their applications in various learning tasks, in the context of networking. We also provide a discussion on future directions and open research issues, while identifying potential pitfalls. While a few survey papers focusing on applications of machine learning in networking have previously been published, a survey of similar scope and breadth is missing in the literature. Through this timely review, we aim to advance the current state of knowledge, by carefully synthesizing insights from previous survey papers, while providing contemporary coverage of the recent advances and innovations.","['Unsupervised learning', 'Deep learning', 'Anomaly detection', 'Internet of Things', 'Quality of service']","['Machine learning', 'deep learning', 'unsupervised learning', 'computer networks']"
"Magnetic resonance imaging (MRI), which assists doctors in determining clinical staging and expected surgical range, has high medical value. A large number of MRI images require a large amount of storage space and the transmission bandwidth of the PACS system in offline storage and remote diagnosis. Therefore, high-quality compression of MRI images is very research-oriented. Current compression methods for MRI images with high compression ratio cause loss of information on lesions, leading to misdiagnosis; compression methods for MRI images with low compression ratio does not achieve the desired effect. Therefore, a fast fractal-based compression algorithm for MRI images is proposed in this paper. First, three-dimensional (3D) MRI images are converted into a two-dimensional (2D) image sequence, which facilitates the image sequence based on the fractal compression method. Then, range and domain blocks are classified according to the inherent spatiotemporal similarity of 3D objects. By using self-similarity, the number of blocks in the matching pool is reduced to improve the matching speed of the proposed method. Finally, a residual compensation mechanism is introduced to achieve compression of MRI images with high decompression quality. The experimental results show that compression speed is improved by 2-3 times, and the PSNR is improved by nearly 10. It indicates the proposed algorithm is effective and solves the contradiction between high compression ratio and high quality of MRI medical images.","['Image coding', 'Magnetic resonance imaging', 'Medical diagnostic imaging', 'Fractals', 'Compression algorithms', 'Classification algorithms']","['MRI', 'image compression', 'fractal compression', 'spatiotemporal similarity', 'lossy compression']"
"Intelligent systems are wanting for cities to cope with limited spaces and resources across the world. As a result, smart cities emerged mainly as a result of highly innovative ICT industries and markets, and additionally, they have started to use novel solutions taking advantage of the Internet of Things (IoT), big data and cloud computing technologies to establish a profound connection between each component and layer of a city. Several key technologies congregate to build a working smart city considering human requirements. Even though the smart city concept is an advanced solution for today's cities, recently, more living spaces should be discovered, and the concept of a smart city could be moved to these alternative living spaces, namely floating cities. The concept of a floating city emerged as a novel solution due to rising sea levels and land scarcity in order to provide alternative living spaces for humanity. In this article, our main research question is to raise awareness on the current state of smart city concepts across the world by understanding the key future trends, including floating cities, by motivating researchers and scientists through new IoT technologies and applications. Therefore, we present a survey of smart city initiatives and analyze their key concepts and different data management techniques. We performed a detailed literature survey and review by applying a complex literature matrix including terms, like smart people, smart economy, smart governance, smart mobility, smart environment, and smart living. We also discuss multiple perspectives of smart floating cities in detail. With the proposed approach, recent advances and practical future opportunities for smart cities can be revealed.","['Smart cities', 'Internet of Things', 'Sea level', 'Market research', 'Big Data', 'Cloud computing']","['Smart city', 'floating cities', 'IoT', 'survey']"
"Encoder-decoder networks are state-of-the-art approaches to biomedical image segmentation, but have two problems: i.e., the widely used pooling operations may discard spatial information, and therefore low-level semantics are lost. Feature fusion methods can mitigate these problems but feature maps of different scales cannot be easily fused because downand upsampling change the spatial resolution of feature map. To address these issues, we propose INet, which enlarges receptive fields by increasing the kernel sizes of convolutional layers in steps (e.g., from 3 × 3 to 7 × 7 and then 15 × 15) instead of downsampling. Inspired by an Inception module, INet extracts features by kernels of different sizes through concatenating the output feature maps of all preceding convolutional layers. We also find that the large kernel makes the network feasible for biomedical image segmentation. In addition, INet uses two overlapping max-poolings, i.e., max-poolings with stride 1, to extract the sharpest features. Fixed-size and fixed-channel feature maps enable INet to concatenate feature maps and add multiple shortcuts across layers. In this way, INet can recover low-level semantics by concatenating the feature maps of all preceding layers and expedite the training by adding multiple shortcuts. Because INet has additional residual shortcuts, we compare INet with a UNet system that also has residual shortcuts (ResUNet). To confirm INet as a backbone architecture for biomedical image segmentation, we implement dense connections on INet (called DenseINet) and compare it to a DenseUNet system with residual shortcuts (ResDenseUNet). INet and DenseINet require 16.9% and 37.6% fewer parameters than ResUNet and ResDenseUNet, respectively. In comparison with six encoder- decoder approaches using nine public datasets, INet and DenseINet demonstrate efficient improvements in biomedical image segmentation. INet outperforms DeepLabV3, which implementing atrous convolution instead of downsampling to increase receptive fields. INet also outperforms two recent methods (named HRNet and MS-NAS) that maintain high-resolution representations and repeatedly exchange the information across resolutions.","['Image segmentation', 'Biomedical imaging', 'Semantics', 'Feature extraction', 'Kernel', 'Convolution', 'Tumors']","['Biomedical image', 'convolutional networks', 'encoder–decoder networks', 'semantic segmentation']"
"Intrusion detection system (IDS) plays an important role in network security by discovering and preventing malicious activities. Due to the complex and time-varying network environment, the network intrusion samples are submerged into a large number of normal samples, which leads to insufficient samples for model training and detection results with a high false detection rate. According to the problem of data imbalance, we propose a network intrusion detection algorithm combined hybrid sampling with deep hierarchical network. Firstly, we use the one-side selection (OSS) to reduce the noise samples in majority category, and then increase the minority samples by Synthetic Minority Over-sampling Technique (SMOTE). In this way, a balanced dataset can be established to make the model fully learn the features of minority samples and greatly reduce the model training time. Secondly, we use convolution neural network (CNN) to extract spatial features and Bi-directional long short-term memory (BiLSTM) to extract temporal features, which forms a deep hierarchical network model. The proposed network intrusion detection algorithm was verified by experiments on the NSL-KDD and UNSW-NB15 dataset, and the classification accuracy can achieve 83.58% and 77.16%, respectively.","['Intrusion detection', 'Feature extraction', 'Data models', 'Support vector machines', 'Telecommunication traffic', 'Training', 'Classification algorithms']","['Network intrusion detection', 'hybrid sampling', 'deep hierarchical network', 'convolution neural network', 'bi-directional long short-term memory']"
"Multilevel inverters (MLIs) are a great development for industrial and renewable energy applications due to their dominance over conventional two-level inverter with respect to size, rating of switches, filter requirement, and efficiency. A new single-phase cascaded MLI topology is suggested in this paper. The proposed MLI topology is designed with the aim of reducing the number of switches and the number of dc voltage sources with modularity while having a higher number of levels at the output. For the determination of the magnitude of dc voltage sources and a number of levels in the cascade connection, three different algorithms are proposed. The optimization of the proposed topology is aimed at achieving a higher number of levels while minimizing other parameters. A detailed comparison is made with other comparable MLI topologies to prove the superiority of the proposed structure. A selective harmonic elimination pulse width modulation technique is used to produce the pulses for the switches to achieve high-quality voltage at the output. Finally, the experimental results are provided for the basic unit with 11 levels and for cascading of two such units to achieve 71 levels at the output.","['Topology', 'Switches', 'Inverters', 'Through-silicon vias', 'Power harmonic filters', 'Optimization', 'Harmonic analysis']","['Basic unit', 'cascaded inverter', 'multilevel inverter (MLI)', 'selective harmonic elimination', 'SHEPWM', 'optimization', 'reduce switch count']"
"The advanced features of 5G mobile wireless network systems yield new security requirements and challenges. This paper presents a comprehensive study on the security of 5G wireless network systems compared with the traditional cellular networks. The paper starts with a review on 5G wireless networks particularities as well as on the new requirements and motivations of 5G wireless security. The potential attacks and security services are summarized with the consideration of new service requirements and new use cases in 5G wireless networks. The recent development and the existing schemes for the 5G wireless security are presented based on the corresponding security services, including authentication, availability, data confidentiality, key management, and privacy. This paper further discusses the new security features involving different technologies applied to 5G, such as heterogeneous networks, device-to-device communications, massive multiple-input multiple-output, software-defined networks, and Internet of Things. Motivated by these security research and development activities, we propose a new 5G wireless security architecture, based on which the analysis of identity management and flexible authentication is provided. As a case study, we explore a handover procedure as well as a signaling load scheme to show the advantages of the proposed security architecture. The challenges and future directions of 5G wireless security are finally summarized.","['5G mobile communication', 'Wireless networks', 'Computer architecture', 'Communication system security', 'Cryptography']","['5G wireless network systems', 'security', 'authentication', 'availability', 'confidentiality', 'key management', 'privacy', 'heterogenous networks', 'device-to-device communications', 'massive multiple-input multiple-output', 'software-defined networks', 'Internet of Things', '5G wireless security architecture']"
"This paper deals with the problem of fault detection and diagnosis in sensors considering erratic, drift, hard-over, spike, and stuck faults. The data set containing samples of the above mentioned fault signals was acquired as follows: normal data signals were obtained from a temperature-to-voltage converter by using an Arduino Uno microcontroller board and MATLAB. Then, faults were simulated in normal data to get 100 samples of each fault, in which one sample is composed of 1000 data elements. A support vector machine (SVM) was used for data classification in a one-versus-rest manner. The statistical time-domain features, extracted from a sample, were used as a single observation for training and testing SVM. The number of features varied from 5 to 10 to examine the effect on accuracy of SVM. Three different kernel functions used to train SVM include linear, polynomial, and radial-basis function kernels. The fault occurrence event in fault samples was chosen randomly in some cases to replicate a practical scenario in industrial systems. The results show that an increase in the number of features from 5 to 10 hardly increases the total accuracy of the classifier. However, using ten features gives the highest accuracy for fault classification in an SVM. An increase in the number of training samples from 40 to 60 caused an overfitting problem. The k-fold cross-validation technique was adopted to overcome this issue. The increase in number of data elements per sample to 2500 increases the efficiency of the classifier. However, an increase in the number of training samples to 400 reduces the capability of SVM to classify stuck fault. The receiver operating characteristics curve comparison shows the efficiency of SVM over a neural network.","['Support vector machines', 'Feature extraction', 'Fault detection', 'Kernel', 'Training', 'Time-domain analysis', 'Fault diagnosis']","['Sensor faults', 'fault detection', 'support vector machine', 'drift fault', 'erratic fault', 'hard-over fault', 'spike fault', 'stuck fault']"
"As a breakthrough in the field of machine fault diagnosis, deep learning has great potential to extract more abstract and discriminative features automatically without much prior knowledge compared with other methods, such as the signal processing and analysis-based methods and machine learning methods with shallow architectures. One of the most important aspects in measuring the extracted features is whether they can explore more information of the inputs and avoid redundancy to be representative. Thus, a stacked sparse autoencoder (SAE)-based machine fault diagnosis method is proposed in this paper. The penalty term of the SAE can help mine essential information and avoid redundancy. To help the constructed diagnosis network further mine more abstract and representative high-level features, the collected non-stationary and transient signals are preprocessed with ensemble empirical mode decomposition and autoregressive (AR) models to obtain AR parameters, which are extracted based on the intrinsic mode functions (IMFs) and regarded as the low-level features for the inputs of the proposed diagnosis network. Only the first four IMFs are considered, because fault information is mainly reflected in high-frequency IMFs. Experiments and comparisons are complemented to validate the superiority of the presented diagnosis network. Results fully demonstrate that the stacked SAE-based diagnosis method can extract more discriminative high-level features and has a better performance in rotating machinery fault diagnosis compared with the traditional machine learning methods with shallow architectures.","['Feature extraction', 'Fault diagnosis', 'Machinery', 'Machine learning', 'Support vector machines', 'Data mining', 'Learning systems']","['Sparse autoencoder', 'ensemble empirical mode decomposition', 'autoregressive model', 'fault diagnosis']"
"A 12-port antenna array operating in the long term evolution (LTE) band 42 (3400-3600 MHz), LTE band 43 (3600-3800 MHz), and LTE band 46 (5150-5925 MHz) for 5G massive multiple-input multiple-output (MIMO) applications in mobile handsets is presented. The proposed MIMO antenna is composed of three different antenna element types, namely, inverted π-shaped antenna, longer inverted L-shaped open slot antenna, and shorter inverted L-shaped open slot antenna. In total, eight antenna elements are used for the 8×8 MIMO in LTE bands 42/43, and six antenna elements are designed for the 6×6 MIMO in LTE band 46. The proposed antenna was simulated, and a prototype was fabricated and tested. The measured results show that the LTE bands 42/43/46 are satisfied with reflection coefficient better than -6 dB, isolation lower than -12 dB, and total efficiencies of higher than 40%. In addition to that, the proposed antenna array has also shown good MIMO performances with an envelope correlation coefficient lower than 0.15, and ergodic channel capacities higher than 34 and 26.5 b/s/Hz in the LTE bands 42/43 and LTE band 46, respectively. The hand phantom effects are also investigated, and the results show that the proposed antenna array can still exhibit good radiation and MIMO performances when operating under data mode and read mode conditions.","['MIMO', 'Long Term Evolution', 'Antenna arrays', '5G mobile communication', 'Slot antennas', 'Mobile antennas']","['Handset antenna', 'MIMO antenna', 'sub-6GHz', 'massive MIMO']"
"The fifth generation (5G) of mobile communication system aims to deliver a ubiquitous mobile service with enhanced quality of service (QoS). It is also expected to enable new use-cases for various vertical industrial applications—such as automobiles, public transportation, medical care, energy, public safety, agriculture, entertainment, manufacturing, and so on. Rapid increases are predicted to occur in user density, traffic volume, and data rate. This calls for novel solutions to the requirements of both mobile users and vertical industries in the next decade. Among various available options, one that appears attractive is to redesign the network architecture—more specifically, to reconstruct the radio access network (RAN). In this paper, we present an inclusive and comprehensive survey on various RAN architectures toward 5G, namely cloud-RAN, heterogeneous cloud-RAN, virtualized cloud-RAN, and fog-RAN. We compare them from various perspectives, such as energy consumption, operations expenditure, resource allocation, spectrum efficiency, system architecture, and network performance. Moreover, we review the key enabling technologies for 5G systems, such as multi-access edge computing, network function virtualization, software-defined networking, and network slicing; and some crucial radio access technologies (RATs), such as millimeter wave, massive multi-input multi-output, device-to-device communication, and massive machine-type communication. Last but not least, we discuss the major research challenges in 5G RAN and 5G RATs and identify several possible directions of future research.","['5G mobile communication', 'Radio access networks', 'Cloud computing', 'Computer architecture', 'Radio access technologies', 'Quality of service', 'Systems architecture']","['5G', 'radio access network', 'network architecture', 'cloud-RAN', 'distributed-RAN', 'fog-RAN', 'heterogeneous-CRAN', 'RATs', 'virtualized-CRAN']"
"Customer retention is a major issue for various service-based organizations particularly telecom industry, wherein predictive models for observing the behavior of customers are one of the great instruments in customer retention process and inferring the future behavior of the customers. However, the performances of predictive models are greatly affected when the real-world data set is highly imbalanced. A data set is called imbalanced if the samples size from one class is very much smaller or larger than the other classes. The most commonly used technique is over/under sampling for handling the class-imbalance problem (CIP) in various domains. In this paper, we survey six well-known sampling techniques and compare the performances of these key techniques, i.e., mega-trend diffusion function (MTDF), synthetic minority oversampling technique, adaptive synthetic sampling approach, couples top-N reverse k-nearest neighbor, majority weighted minority oversampling technique, and immune centroids oversampling technique. Moreover, this paper also reveals the evaluation of four rules-generation algorithms (the learning from example module, version 2 (LEM2), covering, exhaustive, and genetic algorithms) using publicly available data sets. The empirical results demonstrate that the overall predictive performance of MTDF and rules-generation based on genetic algorithms performed the best as compared with the rest of the evaluated oversampling methods and rule-generation algorithms.","['Predictive models', 'Prediction algorithms', 'Genetic algorithms', 'Sampling methods', 'Customer retention', 'Learning systems', 'Customer satisfaction', 'Customer profiles']","['SMOTE', 'ADASYN', 'mega trend diffusion function', 'class imbalance', 'rough set', 'customer churn', 'mRMR. ICOTE', 'MWMOTE', 'TRkNN']"
"Human activity recognition (HAR) based on sensor networks is an important research direction in the fields of pervasive computing and body area network. Existing researches often use statistical machine learning methods to manually extract and construct features of different motions. However, in the face of extremely fast-growing waveform data with no obvious laws, the traditional feature engineering methods are becoming more and more incapable. With the development of deep learning technology, we do not need to manually extract features and can improve the performance in complex human activity recognition problems. By migrating deep neural network experience in image recognition, we propose a deep learning model (InnoHAR) based on the combination of inception neural network and recurrent neural network. The model inputs the waveform data of multi-channel sensors end-to-end. Multi-dimensional features are extracted by inception-like modules by using various kernel-based convolution layers. Combined with GRU, modeling for time series features is realized, making full use of data characteristics to complete classification tasks. Through experimental verification on three most widely used public HAR datasets, our proposed method shows consistent superior performance and has good generalization performance, when compared with the state-of-the-art.","['Feature extraction', 'Activity recognition', 'Deep learning', 'Convolution', 'Wearable sensors', 'Convolutional neural networks']","['Complex human activity', 'inception neural network', 'wearable sensor', 'computational efficiency']"
"Mobile devices are increasingly becoming an indispensable part of people's daily life, facilitating to perform a variety of useful tasks. Mobile cloud computing integrates mobile and cloud computing to expand their capabilities and benefits and overcomes their limitations, such as limited memory, CPU power, and battery life. Big data analytics technologies enable extracting value from data having four Vs: volume, variety, velocity, and veracity. This paper discusses networked healthcare and the role of mobile cloud computing and big data analytics in its enablement. The motivation and development of networked healthcare applications and systems is presented along with the adoption of cloud computing in healthcare. A cloudlet-based mobile cloud-computing infrastructure to be used for healthcare big data applications is described. The techniques, tools, and applications of big data analytics are reviewed. Conclusions are drawn concerning the design of networked healthcare systems using big data and mobile cloud-computing technologies. An outlook on networked healthcare is given.","['Cloud computing', 'Medical services', 'Mobile communication', 'Mobile handsets', 'Big data', 'Mobile computing', 'Computational modeling']","['Healthcare systems', 'big data analytics', 'mobile cloud computing', 'cloudlet infrastructure', 'health applications']"
"The state of charge (SOC) is a critical evaluation index of battery residual capacity. The significance of an accurate SOC estimation is great for a lithium-ion battery to ensure its safe operation and to prevent from over-charging or over-discharging. However, to estimate an accurate capacity of SOC of the lithium-ion battery has become a major concern for the electric vehicle (EV) industry. Therefore, numerous researches are being conducted to address the challenges and to enhance the battery performance. The main objective of this paper is to develop an accurate SOC estimation approach for a lithium-ion battery by improving back-propagation neural network (BPNN) capability using backtracking search algorithm (BSA). BSA optimization is utilized to improve the accuracy and robustness of BPNN model by finding the optimal value of hidden layer neurons and learning rate. In this paper, Dynamic Stress Test and Federal Urban Driving Schedule drive profiles are applied for testing the model at three different temperatures. The obtained results of the BPNN based BSA model are compared with the radial basis function neural network, generalized regression neural network and extreme learning machine model using statistical error values of root mean square error, mean absolute error, mean absolute percentage error, and SOC error to check and validate the model performance. The obtained results show that the BPNN based BSA model outperforms other neural network models in estimating SOC with high accuracy under different EV profiles and temperatures.","['State of charge', 'Lithium-ion batteries', 'Estimation', 'Neurons', 'Temperature measurement', 'Algorithm design and analysis']","['Lithium-ion battery', 'the state of charge', 'back propagation neural network', 'backtracking search algorithm', 'electric vehicle']"
"This paper demonstrates an innovative and simple solution for obstacle detection and collision avoidance of unmanned aerial vehicles (UAVs) optimized for and evaluated with quadrotors. The sensors exploited in this paper are low-cost ultrasonic and infrared range finders, which are much cheaper though noisier than more expensive sensors such as laser scanners. This needs to be taken into consideration for the design, implementation, and parametrization of the signal processing and control algorithm for such a system, which is the topic of this paper. For improved data fusion, inertial and optical flow sensors are used as a distance derivative for reference. As a result, a UAV is capable of distance controlled collision avoidance, which is more complex and powerful than comparable simple solutions. At the same time, the solution remains simple with a low computational burden. Thus, memory and time-consuming simultaneous localization and mapping is not required for collision avoidance.","['Collision avoidance', 'Obstacle detection', 'Infrared detection', 'Quadrotors', 'Unmanned aerial vehicles', 'Sensors']","['collision avoidance', 'obstacle detection', 'ultrasonic', 'infrared', 'autonomous', 'UAV', 'quadrotor', 'quadrocopter']"
"Fault diagnosis of chemical process data becomes one of the most important directions in research and practice. Conventional fault diagnosis and classification methods first extract features from the raw process data. Then certain classifiers are adopted to make diagnosis. However, these conventional methods suffer from the expertise of feature extraction and classifier design. They also lack the adaptive processing of the dynamic information in raw data. This paper proposes a fault diagnosis method based on long short-term memory (LSTM) neural network. The novel method can directly classify the raw process data without specific feature extraction and classifier design. It is also able to adaptively learn the dynamic information in raw data. First, raw process data are used to train the LSTM neural network until the cost function of LSTM converges below certain predefined small positive value. In this step, the dynamic information of raw process data is adaptively learned by LSTM. Then testing data are used to obtain the diagnosis results of the trained LSTM neural network. The application of LSTM to fault identification and analysis is evaluated in the Tennessee Eastman benchmark process. Extensive experimental results show LSTM can better separate different faults and provide more promising fault diagnosis performance.","['Fault diagnosis', 'Feature extraction', 'Recurrent neural networks', 'Monitoring', 'Training', 'Standards']","['Process monitoring', 'fault diagnosis', 'recurrent neural network', 'long short-term memory (LSTM) neural network']"
"We present an approach that combines automatic features learned by convolutional neural networks (CNN) and handcrafted features computed by the bag-of-visual-words (BOVW) model in order to achieve the state-of-the-art results in facial expression recognition (FER). To obtain automatic features, we experiment with multiple CNN architectures, pre-trained models, and training procedures, e.g., Dense-Sparse-Dense. After fusing the two types of features, we employ a local learning framework to predict the class label for each test image. The local learning framework is based on three steps. First, a k-nearest neighbors model is applied in order to select the nearest training samples for an input test image. Second, a one-versus-all support vector machines (SVM) classifier is trained on the selected training samples. Finally, the SVM classifier is used to predict the class label only for the test image it was trained for. Although we have used local learning in combination with handcrafted features in our previous work, to the best of our knowledge, local learning has never been employed in combination with deep features. The experiments on the 2013 FER Challenge data set, the FER+ data set, and the AffectNet data set demonstrate that our approach achieves the state-of-the-art results. With a top accuracy of 75.42% on the FER 2013, 87.76% on the FER+, 59.58% on the AffectNet eight-way classification, and 63.31% on the AffectNet seven-way classification, we surpass the state-of-the-art methods by more than 1% on all data sets.","['Face recognition', 'Training', 'Computational modeling', 'Support vector machines', 'Convolutional neural networks', 'Deep learning', 'Task analysis']","['Facial expression recognition', 'local learning', 'convolutional neural networks', 'bag-of-visual-words', 'dense-sparse-dense training']"
"Particle swarm optimization (PSO) is one of the most well-regarded swarm-based algorithms in the literature. Although the original PSO has shown good optimization performance, it still severely suffers from premature convergence. As a result, many researchers have been modifying it resulting in a large number of PSO variants with either slightly or significantly better performance. Mainly, the standard PSO has been modified by four main strategies: modification of the PSO controlling parameters, hybridizing PSO with other well-known meta-heuristic algorithms such as genetic algorithm (GA) and differential evolution (DE), cooperation and multi-swarm techniques. This paper attempts to provide a comprehensive review of PSO, including the basic concepts of PSO, binary PSO, neighborhood topologies in PSO, recent and historical PSO variants, remarkable engineering applications of PSO, and its drawbacks. Moreover, this paper reviews recent studies that utilize PSO to solve feature selection problems. Finally, eight potential research directions that can help researchers further enhance the performance of PSO are provided.","['Signal processing algorithms', 'Optimization', 'Particle swarm optimization', 'Feature extraction', 'Topology', 'Birds', 'Statistics']","['Applications of PSO', 'binary PSO', 'evolutionary computation', 'feature selection', 'hybrid algorithms', 'meta-heuristic algorithms', 'particle swarm optimization', 'PSO variants']"
"In this review paper, a comprehensive study on the concept, theory, and applications of composite right/left-handed transmission lines (CRLH-TLs) by considering their use in antenna system designs have been provided. It is shown that CRLH-TLs with negative permittivity (ε <; 0) and negative permeability (μ <; 0) have unique properties that do not occur naturally. Therefore, they are referred to as artificial structures called “metamaterials”. These artificial structures include series left-handed (LH) capacitances (C L ), shunt LH inductances (L L ), series right-handed (RH) inductances (LR), and shunt RH capacitances (CR) that are realized by slots or interdigital capacitors, stubs or via-holes, unwanted current flowing on the surface, and gap distance between the surface and ground-plane, respectively. In the most cases, it is also shown that structures based on CRLH metamaterial-TLs are superior than their conventional alternatives, since they have smaller dimensions, lower-profile, wider bandwidth, better radiation patterns, higher gain and efficiency, which make them easier and more cost-effective to manufacture and mass produce. Hence, a broad range of metamaterial-based design possibilities are introduced to highlight the improvement of the performance parameters that are rare and not often discussed in available literature. Therefore, this survey provides a wide overview of key early-stage concepts of metematerial-based designs as a thorough reference for specialist antennas and microwave circuits designers. To analyze the critical features of metamaterial theory and concept, several examples are used. Comparisons on the basis of physical size, bandwidth, materials, gain, efficiency, and radiation patterns are made for all the examples that are based on CRLH metamaterialTLs. As revealed in all the metematerial design examples, foot-print area decrement is an important issue of study that have a strong impact for the enlargement of the next generation wireless communication systems.","['Metamaterials', 'Power transmission lines', 'Dispersion', 'Impedance', 'Integrated circuit modeling', 'Capacitance', 'Permittivity']","['Metamaterials (MTMs)', 'artificial structures', 'antennas', 'negative permittivity (ε < 0)', 'negative permeability (μ < 0)', 'high performances', 'composite right/left-handed transmission lines (CRLH-TLs)', 'next generation wireless communication systems']"
"Brain tumor is a deadly disease and its classification is a challenging task for radiologists because of the heterogeneous nature of the tumor cells. Recently, computer-aided diagnosis-based systems have promised, as an assistive technology, to diagnose the brain tumor, through magnetic resonance imaging (MRI). In recent applications of pre-trained models, normally features are extracted from bottom layers which are different from natural images to medical images. To overcome this problem, this study proposes a method of multi-level features extraction and concatenation for early diagnosis of brain tumor. Two pre-trained deep learning models i.e. Inception-v3 and DensNet201 make this model valid. With the help of these two models, two different scenarios of brain tumor detection and its classification were evaluated. First, the features from different Inception modules were extracted from pre-trained Inception-v3 model and concatenated these features for brain tumor classification. Then, these features were passed to softmax classifier to classify the brain tumor. Second, pre-trained DensNet201 was used to extract features from various DensNet blocks. Then, these features were concatenated and passed to softmax classifier to classify the brain tumor. Both scenarios were evaluated with the help of three-class brain tumor dataset that is available publicly. The proposed method produced 99.34 %, and 99.51% testing accuracies respectively with Inception-v3 and DensNet201 on testing samples and achieved highest performance in the detection of brain tumor. As results indicated, the proposed method based on features concatenation using pre-trained models outperformed as compared to existing state-of-the-art deep learning and machine learning based methods for brain tumor classification.","['Tumors', 'Feature extraction', 'Machine learning', 'Brain modeling', 'Magnetic resonance imaging', 'Biomedical imaging', 'Diseases']","['Deep learning', 'magnetic resonance imaging', 'brain tumor classification', 'pre-trained model', 'dataset']"
"It is a challenging task to recognize smoke from images due to large variance of smoke color, texture, and shapes. There are smoke detection methods that have been proposed, but most of them are based on hand-crafted features. To improve the performance of smoke detection, we propose a novel deep normalization and convolutional neural network (DNCNN) with 14 layers to implement automatic feature extraction and classification. In DNCNN, traditional convolutional layers are replaced with normalization and convolutional layers to accelerate the training process and boost the performance of smoke detection. To reduce overfitting caused by imbalanced and insufficient training samples, we generate more training samples from original training data sets by using a variety of data enhancement techniques. Experimental results show that our method achieved very low false alarm rates below 0.60% with detection rates above 96.37% on our smoke data sets.","['Feature extraction', 'Fires', 'Training', 'Temperature sensors', 'Neurons']","['Deep neural networks', 'deep learning', 'smoke detection', 'image classification']"
"A high performance electrocardiogram (ECG)-based arrhythmic beats classification system is presented in this paper. The classifier was designed based on convolutional neural network (CNN). Single channel ECG signal was segmented into heartbeats in accordance with the changing heartbeat rate. The beats were transformed into dual beat coupling matrix as 2-D inputs to the CNN classifier, which captured both beat morphology and beat-to-beat correlation in ECG. A systematic training beat selection procedure was also proposed which automatically include the most representative beats into the training set to improve classification performance. The classification system was evaluated for the detection of supraventricular ectopic beats (SVEB or S beats) and VEB using the MIT-BIH arrhythmia database. Our proposed method has demonstrated superior performance than several state-of-the-art detectors. In particular, our proposed CNN system has improved sensitivity and positive predictive rate for S beats by more than 12.2% and 11.9%, respectively, over these top performing algorithms. Our proposed CNN classifier with an automatic training beats selection process has shown to outperform the previous methods. The classifier is also a personalized one by combining training set from a common pool and a subject-specific set of ECG data. Our proposed system provides a reliable and fully automatic tool for detection of arrhythmia heartbeat without the need for manual feature extraction or expert assistant. It can potentially be implemented on portable device for the long-term monitoring of cardiac arrhythmia.","['Electrocardiography', 'Heart beat', 'Heart rate variability', 'Training', 'Databases', 'Couplings', 'Morphology']","['Convolutional neural network (CNN)', 'ECG classification', 'arrhythmia', 'patient-specific']"
"A vehicular ad-hoc network (VANET) can improve the flow of traffic to facilitate intelligent transportation and to provide convenient information services, where the goal is to provide self-organizing data transmission capabilities for vehicles on the road to enable applications, such as assisted vehicle driving and safety warnings. VANETs are affected by issues such as identity validity and message reliability when vehicle nodes share data with other nodes. The method used to allow the vehicle nodes to upload sensor data to a trusted center for storage is susceptible to security risks, such as malicious tampering and data leakage. To address these security challenges, we propose a data security sharing and storage system based on the consortium blockchain (DSSCB). This digital signature technique based on the nature of bilinear pairing for elliptic curves is used to ensure the reliability and integrity when transmitting data to a node. The emerging consortium blockchain technology provides a decentralized, secure, and reliable database, which is maintained by the entire network node. In DSSCB, smart contracts are used to limit the triggering conditions for preselected nodes when transmitting and storing data and for allocating data coins to vehicles that participate in the contribution of data. The security analysis and performance evaluations demonstrated that our DSSCB solution is more secure and reliable in terms of data sharing and storage. Compared with the traditional blockchain system, the time required to confirm the data block was reduced by nearly six times and the transmission efficiency was improved by 83.33%.","['Blockchain', 'Elliptic curves', 'Vehicular ad hoc networks', 'Elliptic curve cryptography', 'Safety', 'Reliability']","['Consortium blockchain', 'data sharing', 'data storage', 'signature verification', 'vehicular ad-hoc network (VANET)']"
"Body area networks, including smart sensors, are widely reshaping health applications in the new era of smart cities. To meet increasing security and privacy requirements, physiological signalbased biometric human identification is gaining tremendous attention. This paper focuses on two major impediments: the signal processing technique is usually both complicated and data-dependent and the feature engineering is time-consuming and can fit only specific datasets . To enable a data-independent and highly generalizable signal processing and feature learning process, a novel wavelet domain multiresolution convolutional neural network is proposed. Specifically, it allows for blindly selecting a physiological signal segment for identification purpose, avoiding the complicated signal fiducial characteristics extraction process. To enrich the data representation, the random chosen signal segment is then transformed to the wavelet domain, where multiresolution time-frequency representation is achieved. An auto-correlation operation is applied to the transformed data to remove the phase difference as the result of the blind segmentation operation. Afterward, a multiresolution 1-D-convolutional neural network (1-D-CNN) is introduced to automatically learn the intrinsic hierarchical features from the wavelet domain raw data without datadependent and heavy feature engineering, and perform the user identification task. The effectiveness of the proposed algorithm is thoroughly evaluated on eight electrocardiogram datasets with diverse behaviors, such as with or without severe heart diseases, and with different sensor placement methods. Our evaluation is much more extensive than the state-of-the-art works, and an average identification rate of 93.5% is achieved. The proposed multiresolution 1-D-CNN algorithm can effectively identify human subjects, even from randomly selected signal segments and without heavy feature engineering. This paper is expected to demonstrate the feasibility and effectiveness of applying the blind signal processing and deep learning techniques to biometric human identification, to enable a low algorithm engineering effort and also a high generalization ability.","['Electrocardiography', 'Signal resolution', 'Feature extraction', 'Heart rate variability', 'Convolution', 'Wavelet domain', 'Wavelet transforms']","['ECG', 'wavelet transformation', 'convolutional neural network', 'deep learning', 'machine learning', 'feature learning', 'blind signal processing', 'data representation']"
"This paper presents a comprehensive review of current literature on drone detection and classification using machine learning with different modalities. This research area has emerged in the last few years due to the rapid development of commercial and recreational drones and the associated risk to airspace safety. Addressed technologies encompass radar, visual, acoustic, and radio-frequency sensing systems. The general finding of this study demonstrates that machine learning-based classification of drones seems to be promising with many successful individual contributions. However, most of the performed research is experimental and the outcomes from different papers can hardly be compared. A general requirement-driven specification for the problem of drone detection and classification is still missing as well as reference datasets which would help in evaluating different solutions.","['Drones', 'Radar cross-sections', 'Birds', 'Radar detection', 'Acoustics', 'Data models']","['Drone detection', 'drone classification', 'machine learning', 'radar', 'vision', 'acoustics', 'radio-frequency']"
"Nowadays, 5G is in its initial phase of commercialization. The 5G network will revolutionize the existing wireless network with its enhanced capabilities and novel features. 5G New Radio (5G NR), referred to as the global standardization of 5G, is presently under the 3 rd Generation Partnership Project (3GPP) and can be operable over the wide range of frequency bands from less than 6GHz to mmWave (100GHz). 3GPP mainly focuses on the three major use cases of 5G NR that are comprised of Ultra-Reliable and Low Latency Communication (uRLLC), Massive Machine Type Communication (mMTC), Enhanced Mobile Broadband (eMBB). For meeting the targets of 5G NR, multiple features like scalable numerology, flexible spectrum, forward compatibility, and ultra-lean design are added as compared to the LTE systems. This paper presents a brief overview of the added features and key performance indicators of 5G NR. The issues related to the adaptation of higher modulation schemes and inter-RAT handover synchronization are well addressed in this paper. With the consideration of these challenges, a next-generation wireless communication architecture is proposed. The architecture acts as the platform for migration towards beyond 5G/6G networks. Along with this, various technologies and applications of 6G networks are also overviewed in this paper. 6G network will incorporate Artificial intelligence (AI) based services, edge computing, quantum computing, optical wireless communication, hybrid access, and tactile services. For enabling these diverse services, a virtualized network slicing based architecture of 6G is proposed. Various ongoing projects on 6G and its technologies are also listed in this paper.","['6G mobile communication', 'Computer architecture', 'Ultra reliable low latency communication', '3GPP', 'Next generation networking', 'New Radio']","['5G', '5G NR', 'eMBB', 'mMTC', 'uRLLC', 'EVM', 'inter-RAT', '6G', 'network slicing', 'Tactile Internet']"
"Intrusion detection can identify unknown attacks from network traffics and has been an effective means of network security. Nowadays, existing methods for network anomaly detection are usually based on traditional machine learning models, such as KNN, SVM, etc. Although these methods can obtain some outstanding features, they get a relatively low accuracy and rely heavily on manual design of traffic features, which has been obsolete in the age of big data. To solve the problems of low accuracy and feature engineering in intrusion detection, a traffic anomaly detection model BAT is proposed. The BAT model combines BLSTM (Bidirectional Long Short-term memory) and attention mechanism. Attention mechanism is used to screen the network flow vector composed of packet vectors generated by the BLSTM model, which can obtain the key features for network traffic classification. In addition, we adopt multiple convolutional layers to capture the local features of traffic data. As multiple convolutional layers are used to process data samples, we refer BAT model as BAT-MC. The softmax classifier is used for network traffic classification. The proposed end-to-end model does not use any feature engineering skills and can automatically learn the key features of the hierarchy. It can well describe the network traffic behavior and improve the ability of anomaly detection effectively. We test our model on a public benchmark dataset, and the experimental results demonstrate our model has better performance than other comparison methods.","['Intrusion detection', 'Feature extraction', 'Deep learning', 'Anomaly detection', 'Machine learning algorithms', 'Pattern matching']","['Network traffic', 'intrusion detection', 'deep learning', 'BLSTM', 'attention mechanism']"
"The 6G vision of creating authentic digital twin representations of the physical world calls for new sensing solutions to compose multi-layered maps of our environments. Radio sensing using the mobile communication network as a sensor has the potential to become an essential component of the solution. With the evolution of cellular systems to mmWave bands in 5G and potentially sub-THz bands in 6G, small cell deployments will begin to dominate. Large bandwidth systems deployed in small cell configurations provide an unprecedented opportunity to employ the mobile network for sensing. In this paper, we focus on the major design aspects of such a cellular joint communication and sensing (JCAS) system. We present an analysis of the choice of the waveform that points towards choosing the one that is best suited for communication also for radar sensing. We discuss several techniques for efficiently integrating the sensing capability into the JCAS system, some of which are applicable with NR air-interface for evolved 5G systems. Specifically, methods for reducing sensing overhead by appropriate sensing signal design or by configuring separate numerologies for communications and sensing are presented. Sophisticated use of the sensing signals is shown to reduce the signaling overhead by a factor of 2.67 for an exemplary road traffic monitoring use case. We then present a vision for future advanced JCAS systems building upon distributed massive MIMO and discuss various other research challenges for JCAS that need to be addressed in order to pave the way towards natively integrated JCAS in 6G.","['Sensors', '5G mobile communication', 'Radar', '6G mobile communication', 'Location awareness', 'Industries', 'Bandwidth']","['5G', 'beyond 5G', '6G', 'air interface', 'artificial intelligence', 'cellular', 'communication', 'Industry 40', 'JCAS', 'localization', 'machine learning', 'positioning', 'radar', 'radcom', 'sensing', 'system design', 'vertical industries', 'wireless']"
"The immense increase in multimedia-on-demand traffic that refers to audio, video, and images, has drastically shifted the vision of the Internet of Things (IoT) from scalar to Multimedia Internet of Things (M-IoT). IoT devices are constrained in terms of energy, computing, size, and storage memory. Delay-sensitive and bandwidth-hungry multimedia applications over constrained IoT networks require revision of IoT architecture for M-IoT. This paper provides a comprehensive survey of M-IoT with an emphasis on architecture, protocols, and applications. This article starts by providing a horizontal overview of the IoT. Then, we discuss the issues considering the characteristics of multimedia and provide a summary of related M-IoT architectures. Various multimedia applications supported by IoT are surveyed, and numerous use cases related to road traffic management, security, industry, and health are illustrated to show how different M-IoT applications are revolutionizing human life. We explore the importance of Quality-of-Experience (QoE) and Quality-of-Service (QoS) for multimedia transmission over IoT. Moreover, we explore the limitations of IoT for multimedia computing and present the relationship between the M-IoT and emerging technologies including event processing, feature extraction, cloud computing, Fog/Edge computing and Software-Defined-Networks (SDNs). We also present the need for better routing and Physical-Medium Access Control (PHY-MAC) protocols for M-IoT. Finally, we present a detailed discussion on the open research issues and several potential research areas related to emerging multimedia communication in IoT.","['Internet of Things', 'Computer architecture', 'Multimedia communication', 'Quality of service', 'Streaming media']","['Multimedia Internet of Things (M-IoT)', 'multimedia communication', 'Internet of Multimedia Things (IoMT)', 'multimedia computing', 'Quality-of-Experience (QoE)', 'Quality-of-Service (QoS)', 'multimedia routing', 'medium access control (MAC)']"
"Novel coronavirus (COVID-19) outbreak, has raised a calamitous situation all over the world and has become one of the most acute and severe ailments in the past hundred years. The prevalence rate of COVID-19 is rapidly rising every day throughout the globe. Although no vaccines for this pandemic have been discovered yet, deep learning techniques proved themselves to be a powerful tool in the arsenal used by clinicians for the automatic diagnosis of COVID-19. This paper aims to overview the recently developed systems based on deep learning techniques using different medical imaging modalities like Computer Tomography (CT) and X-ray. This review specifically discusses the systems developed for COVID-19 diagnosis using deep learning techniques and provides insights on well-known data sets used to train these networks. It also highlights the data partitioning techniques and various performance measures developed by researchers in this field. A taxonomy is drawn to categorize the recent works for proper insight. Finally, we conclude by addressing the challenges associated with the use of deep learning methods for COVID-19 detection and probable future trends in this research area. The aim of this paper is to facilitate experts (medical or otherwise) and technicians in understanding the ways deep learning techniques are used in this regard and how they can be potentially further utilized to combat the outbreak of COVID-19.","['COVID-19', 'Deep learning', 'Computed tomography', 'X-ray imaging', 'Transfer learning', 'Feature extraction', 'Taxonomy']","['Coronavirus', 'COVID-19', 'deep learning', 'deep transfer learning', 'diagnosis', 'x-ray', 'computer tomography']"
"Food traceability has been one of the emerging blockchain applications in recent years, for improving the areas of anti-counterfeiting and quality assurance. Existing food traceability systems do not guarantee a high level of system reliability, scalability, and information accuracy. Moreover, the traceability process is time-consuming and complicated in modern supply chain networks. To alleviate these concerns, blockchain technology is promising to create a new ontology for supply chain traceability. However, most consensus mechanisms and data flow in blockchain are developed for cryptocurrency, not for supply chain traceability; hence, simply applying blockchain technology to food traceability is impractical. In this paper, a blockchain-IoT-based food traceability system (BIFTS) is proposed to integrate the novel deployment of blockchain, IoT technology, and fuzzy logic into a total traceability shelf life management system for managing perishable food. To address the needs for food traceability, lightweight and vaporized characteristics are deployed in the blockchain, while an integrated consensus mechanism that considers shipment transit time, stakeholder assessment, and shipment volume is developed. The data flow of blockchain is then aligned to the deployment of IoT technologies according to the level of traceable resource units. Subsequently, the decision support can be established in the food supply chain by using reliable and accurate data for shelf life adjustment, and by using fuzzy logic for quality decay evaluation.","['Supply chains', 'Blockchain', 'Reliability', 'Quality assurance', 'Radiofrequency identification', 'Monitoring', 'Fuzzy logic']","['Food traceability', 'blockchain', 'consensus mechanism', 'Internet of Things', 'shelf life management']"
"How different cultures react and respond given a crisis is predominant in a society’s norms and political will to combat the situation. Often, the decisions made are necessitated by events, social pressure, or the need of the hour, which may not represent the nation’s will. While some are pleased with it, others might show resentment. Coronavirus (COVID-19) brought a mix of similar emotions from the nations towards the decisions taken by their respective governments. Social media was bombarded with posts containing both positive and negative sentiments on the COVID-19, pandemic, lockdown, and hashtags past couple of months. Despite geographically close, many neighboring countries reacted differently to one another. For instance, Denmark and Sweden, which share many similarities, stood poles apart on the decision taken by their respective governments. Yet, their nation’s support was mostly unanimous, unlike the South Asian neighboring countries where people showed a lot of anxiety and resentment. The purpose of this study is to analyze reaction of citizens from different cultures to the novel Coronavirus and people’s sentiment about subsequent actions taken by different countries. Deep long short-term memory (LSTM) models used for estimating the sentiment polarity and emotions from extracted tweets have been trained to achieve state-of-the-art accuracy on the sentiment140 dataset. The use of emoticons showed a unique and novel way of validating the supervised deep learning models on tweets extracted from Twitter.","['Machine learning', 'Twitter', 'Analytical models', 'Cultural differences', 'Training', 'Natural language processing']","['Behaviour analysis', 'COVID-19', 'crisis', 'deep learning', 'emotion detection', 'LSTM', 'natural language processing', 'neural network', 'outbreak', 'opinion mining', 'pandemic', 'polarity assessment', 'sentiment analysis', 'tweets', 'twitter', 'virus']"
"The concept of digital twin (DT) has emerged to enable the benefits of future paradigms such as the industrial Internet of Things and Industry 4.0. The idea is to bring every data source and control interface description related to a product or process available through a single interface, for auto-discovery and automated communication establishment. However, designing the architecture of a DT to serve every future application is an ambitious task. Therefore, the prototyping systems for specific applications are required to design the DT incrementally. We developed a novel DT prototype to analyze the requirements of communication in a mission-critical application such as mobile networks supported remote surgery. Such operations require low latency and high levels of security and reliability and therefore are a perfect subject for analyzing DT communication and cybersecurity. The system comprised of a robotic arm and HTC vive virtual reality (VR) system connected over a 4G mobile network. More than 70 test users were employed to assess the system. To address the cybersecurity of the system, we incorporated a network manipulation module to test the effect of network outages and attacks; we studied state of the art practices and their utilization within DTs. The capability of the system for actual remote surgery is limited by capabilities of the VR system and insufficient feedback from the robot. However, simulations and research of remote surgeries could be conducted with the system. As a result, we propose ideas for communication establishment and necessary cybersecurity technologies that will help in developing the DT architecture. Furthermore, we concluded that developing the DT requires cross-disciplinary development in several different engineering fields. Each field makes use of its own tools and methods, which do not always fit together perfectly. This is a potentially major obstacle in the realization of Industry 4.0 and similar concepts.","['Robots', 'Surgery', 'Real-time systems', 'Computer security', 'Task analysis', 'Reliability']","['Digital twin', 'virtual reality', 'robot control', 'mobile networks', 'network security']"
"Unmanned aerial vehicle (UAV) systems are one of the most rapidly developing, highest level and most practical applied unmanned aerial systems. Collision avoidance and trajectory planning are the core areas of any UAV system. However, there are theoretical and practical problems associated with the existing methods. To manage these problems, this paper presents an optimized artificial potential field (APF) algorithm for multi-UAV operation in 3-D dynamic space. The classic APF algorithm is restricted to single UAV trajectory planning and usually fails to guarantee the avoidance of collisions. To overcome this challenge, a method is proposed with a distance factor and jump strategy to solve common problems, such as unreachable targets, and ensure that the UAV will not collide with any obstacles. The method considers the UAV companions as dynamic obstacles to realize collaborative trajectory planning. Furthermore, the jitter problem is solved using the dynamic step adjustment method. Several resolution scenarios are illustrated. The method has been validated in quantitative test simulation models and satisfactory results were obtained in a simulated urban environment.","['Trajectory', 'Force', 'Planning', 'Unmanned aerial vehicles', 'Collision avoidance', 'Jitter', 'Algorithm design and analysis']","['Multi-UAV', 'trajectory planning', 'collision avoidance', 'artificial potential field', 'jitter problem']"
"Recently, a chaotic image encryption algorithm based on information entropy (IEAIE) was proposed. This paper scrutinizes the security properties of the algorithm and evaluates the validity of the used quantifiable security metrics. When the round number is only one, the equivalent secret key of every basic operation of IEAIE can be recovered with a differential attack separately. Some common insecurity problems in the field of chaotic image encryption are found in IEAIE, e.g., the short orbits of the digital chaotic system and the invalid sensitivity mechanism built on information entropy of the plain image. Even worse, each security metric is questionable, which undermines the security credibility of IEAIE. Hence, IEAIE can only serve as a counterexample for illustrating common pitfalls in designing secure communication method for image data.","['Encryption', 'Chaotic communication', 'Information entropy', 'Sensitivity']","['Chaotic cryptanalysis', 'multimedia cryptography', 'image encryption', 'secure communication', 'privacy protection']"
"The fifth generation of cellular communication systems is foreseen to enable a multitude of new applications and use cases with very different requirements. A new 5G multi-service air interface needs to enhance broadband performance as well as provide new levels of reliability, latency, and supported number of users. In this paper, we focus on the massive Machine Type Communications (mMTC) service within a multi-service air interface. Specifically, we present an overview of different physical and medium access techniques to address the problem of a massive number of access attempts in mMTC and discuss the protocol performance of these solutions in a common evaluation framework.","['5G mobile communication', 'Interference', 'Long Term Evolution', 'Power control', 'Media Access Protocol']","['5G', 'mMTC', 'massive access', 'massive connectivity', 'random access']"
"To improve the performance of network intrusion detection systems (IDS), we applied deep learning theory to intrusion detection and developed a deep network model with automatic feature extraction. In this paper, we consider the characteristics of the time-related intrusion and propose a novel IDS that consists of a recurrent neural network with gated recurrent units (GRU), multilayer perceptron (MLP), and softmax module. Experiments on the well-known KDD 99 and NSL-KDD data sets show that the system has leading performance. The overall detection rate was 99.42% using KDD 99 and 99.31% using NSL-KDD with false positive rates as low as 0.05% and 0.84%, respectively. In particular, for detecting the denial of service attacks, the system achieved detection rates of 99.98% and 99.55%, respectively. Comparative experiments showed that the GRU is more suitable as a memory unit for IDS than LSTM, and proved that it is an effective simplification and improvement of LSTM. Moreover, the bidirectional GRU can reach the best performance compared with the recently published methods.","['Intrusion detection', 'Feature extraction', 'Machine learning', 'Support vector machines', 'Recurrent neural networks', 'Logic gates']","['Intrusion detection', 'deep learning', 'recurrent neural network', 'gated recurrent unit']"
"Depression is viewed as the largest contributor to global disability and a major reason for suicide. It has an impact on the language usage reflected in the written text. The key objective of our study is to examine Reddit users' posts to detect any factors that may reveal the depression attitudes of relevant online users. For such purpose, we employ the Natural Language Processing (NLP) techniques and machine learning approaches to train the data and evaluate the efficiency of our proposed method. We identify a lexicon of terms that are more common among depressed accounts. The results show that our proposed method can significantly improve performance accuracy. The best single feature is bigram with the Support Vector Machine (SVM) classifier to detect depression with 80% accuracy and 0.80 F1 scores. The strength and effectiveness of the combined features (LIWC+LDA+bigram) are most successfully demonstrated with the Multilayer Perceptron (MLP) classifier resulting in the top performance for depression detection reaching 91% accuracy and 0.93 F1 scores. According to our study, better performance improvement can be achieved by proper feature selections and their multiple feature combinations.","['Feature extraction', 'Task analysis', 'Linguistics', 'Twitter', 'Natural language processing', 'Support vector machines']","['Natural language processing', 'machine learning', 'Reddit', 'social networks', 'depression']"
"Vehicular edge computing (VEC) is introduced to extend computing capacity to vehicular network edge recently. With the advent of VEC, service providers directly host services in close proximity of mobile vehicles for great improvements. As a result, a new networking paradigm, vehicular edge networks is emerged along with the development of VEC. However, it is necessary to address security issues for facilitating VEC well. In this paper, we focus on reputation management to ensure security protection and improve network efficiency in the implementation of VEC. A distributed reputation management system (DREAMS) is proposed, wherein VEC servers are adopted to execute local reputation management tasks for vehicles. This system has remarkable features for improving overall performance: 1) distributed reputation maintenance; 2) trusted reputation manifestation; 3) accurate reputation update; and 4) available reputation usage. In particular, we utilize multi-weighted subjective logic for accurate reputation update in DREAMS. To enrich reputation usage in DREAMS, service providers optimize resource allocation in computation offloading by considering reputation of vehicles. Numerical results indicate that DREAMS has great advantages in optimizing misbehavior detection and improving the recognition rate of misbehaving vehicles. Meanwhile, we demonstrate the effectiveness of our reputation-based resource allocation algorithm.","['Edge computing', 'Mobile communication', 'Servers', 'Resource management', 'Optimization', 'Security']","['Edge computing', 'security management', 'intelligent vehicles', 'optimization']"
"Speech emotion recognition is a vital and challenging task that the feature extraction plays a significant role in the SER performance. With the development of deep learning, we put our eyes on the structure of end-to-end and authenticate the algorithm that is extraordinary effective. In this paper, we introduce a novel architecture ADRNN (dilated CNN with residual block and BiLSTM based on the attention mechanism) to apply for the speech emotion recognition which can take advantage of the strengths of diverse networks and overcome the shortcomings of utilizing alone, and are evaluated in the popular IEMOCAP database and Berlin EMODB corpus. Dilated CNN can assist the model to acquire more receptive fields than using the pooling layer. Then, the skip connection can keep more historic info from the shallow layer and BiLSTM layer are adopted to learn long-term dependencies from the learned local features. And we utilize the attention mechanism to enhance further extraction of speech features. Furthermore, we improve the loss function to apply softmax together with the center loss that achieves better classification performance. As emotional dialogues are transformed of the spectrograms, we pick up the values of the 3-D Log-Mel spectrums from raw signals and put them into our proposed algorithm and obtain a notable performance to get the 74.96% unweighted accuracy in the speaker-dependent and the 69.32% unweighted accuracy in the speaker-independent experiment. It is better than the 64.74% from previous state-of-the-art methods in the spontaneous emotional speech of the IEMOCAP database. In addition, we propose the networks that achieve recognition accuracies of 90.78% and 85.39% on Berlin EMODB of speaker-dependent and speaker-independent experiment respectively, which are better than the accuracy of 88.30% and 82.82% obtained by previous work. For validating the robustness and generalization, we also make an experiment for cross-corpus between above databases and get the preferable 63.84% recognition accuracy in final.","['Speech recognition', 'Feature extraction', 'Emotion recognition', 'Convolution', 'Databases', 'Deep learning', 'Three-dimensional displays']","['3-D Log-Mel', 'dilated CNN', 'residual block', 'center loss', 'BiLSTM', 'attention mechanism']"
"Autonomous mobile robots are becoming more prominent in recent time because of their relevance and applications to the world today. Their ability to navigate in an environment without a need for physical or electro-mechanical guidance devices has made it more promising and useful. The use of autonomous mobile robots is emerging in different sectors such as companies, industries, hospital, institutions, agriculture and homes to improve services and daily activities. Due to technology advancement, the demand for mobile robot has increased due to the task they perform and services they render such as carrying heavy objects, monitoring, search and rescue missions, etc. Various studies have been carried out by researchers on the importance of mobile robot, its applications and challenges. This survey paper unravels the current literatures, the challenges mobile robot is being faced with. A comprehensive study on devices/sensors and prevalent sensor fusion techniques developed for tackling issues like localization, estimation and navigation in mobile robot are presented as well in which they are organised according to relevance, strengths and weaknesses. The study therefore gives good direction for further investigation on developing methods to deal with the discrepancies faced with autonomous mobile robot.","['Mobile robots', 'Robot sensing systems', 'Navigation', 'Wheels', 'Task analysis', 'Robot kinematics']","['Autonomous mobile robot', 'sensor fusion', 'devices', 'estimation', 'localization', 'navigation']"
"Due to the real working conditions and data acquisition equipment, the collected working data of bearings are actually limited. Meanwhile, as the rolling bearing works in the normal state at most times, it is easy to raise the imbalance problem of fault types which restricts the diagnosis accuracy and stability. To solve these problems, we present an imbalanced fault diagnosis method based on the generative adversarial network (GAN) and provide a comparative study in detail. The key idea is utilizing GAN, a kind of deep learning technique, to generate synthetic samples for minority fault class and then improve the generalization ability of the fault diagnosis model. First, this method applies fast Fourier transform to pre-process the original vibration signal and then obtains the frequency spectrum of fault samples. Second, it uses the spectrum data as the input of GAN to generate the synthetic minority samples following the data distribution of the real samples. Finally, it puts the synthetic samples into the training set and builds a stacked denoising auto encoder model for fault diagnosis. To testify the effectiveness of the proposed method, a series of comparative experiments is carried out on the CWRU bearing dataset. The results show that the proposed method can provide a better solution for imbalanced fault diagnosis on the basis of generating similar fault samples. As a comparative study, the proposed method is compared to several diagnostic methods with traditional time-frequency domain characteristics. Moreover, we also demonstrate that the proposed method outperforms three widely used sample synthesis techniques, such as random oversampling, synthetic minority oversampling technique, and the principal curve-based oversampling method in terms of diagnosis accuracy and numerical stability.","['Gallium nitride', 'Fault diagnosis', 'Generative adversarial networks', 'Feature extraction', 'Training', 'Rolling bearings', 'Neural networks']","['Generative adversarial network', 'fault diagnosis', 'imbalanced fault', 'SDAE']"
"Droop control is a well-known strategy for the parallel operation of inverters. However, the droop control strategy changes its form for inverters with different types of output impedance, and so far, it is impossible to operate inverters with inductive and capacitive output impedances in parallel. In this paper, it is shown that there exists a universal droop control principle for inverters with output impedance having a phase angle between -(π/2) rad and (π/2) rad. It takes the form of the droop control for inverters with resistive output impedance (R-inverters). Hence, the robust droop controller recently proposed in the literature for R-inverters actually provides one way to implement such a universal droop controller that can be applied to all practical inverters without the need of knowing the impedance angle. The small-signal stability of an inverter equipped with the universal droop controller is analyzed, and it is shown to be stable when the phase angle of the output impedance changes from -(π/2) rad to (π/2) rad. Both real-time simulation results and experimental results from a test rig consisting of an R-inverter, an L-inverter, and a C-inverter operated in parallel are presented to validate the proposed strategy.","['Inverters', 'Impedance', 'Reactive power', 'Robustness', 'Voltage control', 'Stability analysis', 'Frequency control', 'Droop controllers', 'Stability analysis', 'Real-time systems']","['C-inverters', 'L-inverters', 'output impedance', 'parallel operation of inverters', 'R-inverters', 'robust droop controller', 'universal droop controller']"
"The number of used batteries is increasing in quantity as time passes by, and this amount is to expand drastically, as electric vehicles are getting increasingly popular. Proper disposal of the spent batteries has always been a concern, but it has also been discovered that these batteries often retain enough energy perfectly suited for other uses, which can extend the batteries' operational lifetime into a second one. Such use of batteries has been termed as the “second-life,” and it is high time to adopt such usage in large scale to properly exploit the energy and economics that went into battery production and reduce the environmental impacts of battery waste ending up in landfills. This paper aids in that quest by providing a complete picture of the current state of the second-life battery (SLB) technology by reviewing all the prominent work done in this field previously. The second-life background, manufacturing process of energy storage systems using the SLBs, applications, and impacts of this technology, required business strategies and policies, and current barriers of this technology along with potential solutions are discussed in detail in this paper to act as a major stepping stone for future research in this ever-expanding field.","['Batteries', 'Resistance', 'Business', 'Recycling', 'Second Life', 'Discharges (electric)']","['Second life battery', 'battery energy storage system', 'electric vehicle', 'battery management system', 'disposal', 'battery aging', 'economic and environmental values', 'recycling and waste management']"
"Due to the imbalanced distribution of business data, missing user features, and many other reasons, directly using big data techniques on realistic business data tends to deviate from the business goals. It is difficult to model the insurance business data by classification algorithms, such as logistic regression and support vector machine (SVM). In this paper, we exploit a heuristic bootstrap sampling approach combined with the ensemble learning algorithm on the large-scale insurance business data mining, and propose an ensemble random forest algorithm that uses the parallel computing capability and memory-cache mechanism optimized by Spark. We collected the insurance business data from China Life Insurance Company to analyze the potential customers using the proposed algorithm. We use F-Measure and G-mean to evaluate the performance of the algorithm. Experiment result shows that the ensemble random forest algorithm outperformed SVM and other classification algorithms in both performance and accuracy within the imbalanced data, and it is useful for improving the accuracy of product marketing compared to the traditional artificial approach.","['Insurance', 'Big Data', 'Sparks', 'Companies', 'Industries', 'Data models']","['Classification algorithms', 'ensemble learning', 'random forest', 'big data', 'spark']"
"In recent years, due to the wide utilization of direct current (DC) power sources, such as solar photovoltaic (PV), fuel cells, different DC loads, high-level integration of different energy storage systems such as batteries, supercapacitors, DC microgrids have been gaining more importance. Furthermore, unlike conventional AC systems, DC microgrids do not have issues such as synchronization, harmonics, reactive power control, and frequency control. However, the incorporation of different distributed generators, such as PV, wind, fuel cell, loads, and energy storage devices in the common DC bus complicates the control of DC bus voltage as well as the power-sharing. In order to ensure the secure and safe operation of DC microgrids, different control techniques, such as centralized, decentralized, distributed, multilevel, and hierarchical control, are presented. The optimal planning of DC microgrids has an impact on operation and control algorithms; thus, coordination among them is required. A detailed review of the planning, operation, and control of DC microgrids is missing in the existing literature. Thus, this article documents developments in the planning, operation, and control of DC microgrids covered in research in the past 15 years. DC microgrid planning, operation, and control challenges and opportunities are discussed. Different planning, control, and operation methods are well documented with their advantages and disadvantages to provide an excellent foundation for industry personnel and researchers. Power-sharing and energy management operation, control, and planning issues are summarized for both grid-connected and islanded DC microgrids. Also, key research areas in DC microgrid planning, operation, and control are identified to adopt cutting-edge technologies. This review explicitly helps readers understand existing developments on DC microgrid planning, operation, and control as well as identify the need for additional research in order to further contribute to the topic.","['Microgrids', 'Planning', 'Reliability', 'Energy storage', 'Renewable energy sources', 'Supercapacitors', 'Batteries']","['DC microgrids', 'renewable energy sources', 'batteries', 'supercapacitors', 'dc bus voltage', 'power management', 'state of charge', 'microgrid operation', 'planning']"
"In this paper, a novel swarm intelligent algorithm is proposed, known as the fitness dependent optimizer (FDO). The bee swarming the reproductive process and their collective decision-making have inspired this algorithm; it has no algorithmic connection with the honey bee algorithm or the artificial bee colony algorithm. It is worth mentioning that the FDO is considered a particle swarm optimization (PSO)-based algorithm that updates the search agent position by adding velocity (pace). However, the FDO calculates velocity differently; it uses the problem fitness function value to produce weights, and these weights guide the search agents during both the exploration and exploitation phases. Throughout this paper, the FDO algorithm is presented, and the motivation behind the idea is explained. Moreover, the FDO is tested on a group of 19 classical benchmark test functions, and the results are compared with three well-known algorithms: PSO, the genetic algorithm (GA), and the dragonfly algorithm (DA); in addition, the FDO is tested on the IEEE Congress of Evolutionary Computation Benchmark Test Functions (CEC-C06, 2019 Competition) [1]. The results are compared with three modern algorithms: (DA), the whale optimization algorithm (WOA), and the salp swarm algorithm (SSA). The FDO results show better performance in most cases and comparative results in other cases. Furthermore, the results are statistically tested with the Wilcoxon rank-sum test to show the significance of the results. Likewise, the FDO stability in both the exploration and exploitation phases is verified and performance-proofed using different standard measurements. Finally, the FDO is applied to real-world applications as evidence of its feasibility.","['Optimization', 'Heuristic algorithms', 'Classification algorithms', 'Genetic algorithms', 'Artificial bee colony algorithm', 'Particle swarm optimization']","['Optimization', 'swarm intelligence', 'evolutionary computation', 'metaheuristic algorithms', 'fitness dependent optimizer', 'FDO']"
"The nature of stock market movement has always been ambiguous for investors because of various influential factors. This study aims to significantly reduce the risk of trend prediction with machine learning and deep learning algorithms. Four stock market groups, namely diversified financials, petroleum, non-metallic minerals and basic metals from Tehran stock exchange, are chosen for experimental evaluations. This study compares nine machine learning models (Decision Tree, Random Forest, Adaptive Boosting (Adaboost), eXtreme Gradient Boosting (XGBoost), Support Vector Classifier (SVC), Naïve Bayes, K-Nearest Neighbors (KNN), Logistic Regression and Artificial Neural Network (ANN)) and two powerful deep learning methods (Recurrent Neural Network (RNN) and Long short-term memory (LSTM). Ten technical indicators from ten years of historical data are our input values, and two ways are supposed for employing them. Firstly, calculating the indicators by stock trading values as continuous data, and secondly converting indicators to binary data before using. Each prediction model is evaluated by three metrics based on the input ways. The evaluation results indicate that for the continuous data, RNN and LSTM outperform other prediction models with a considerable difference. Also, results show that in the binary data evaluation, those deep learning methods are the best; however, the difference becomes less because of the noticeable improvement of models' performance in the second way.","['Stock markets', 'Machine learning', 'Predictive models', 'Market research', 'Prediction algorithms', 'Support vector machines', 'Indexes']","['Stock market', 'trends prediction', 'classification', 'machine learning', 'deep learning']"
"Near-field magnetic wireless systems have distinct advantages over their conventional farfield counterparts in water-rich environments, such as underwater, underground, and in biological tissues, due to lower power absorption. This paper presents a comprehensive review of near-field magnetic wireless power transfer (WPT) and communication technologies in a variety of applications from general free-space systems, to implantable biomedical devices we find of particular interest. To implement a fully wirelessly-powered implantable system, both high-efficiency power transfer and high-rate data communication are essential. This paper first presents the history and the fundamentals of near-field WPT and communication in free-space systems, followed by technical details for their specific use in implantable biomedical devices. Finally, this paper reviews recent advances in simultaneous wireless information and power transfer and highlights their applications in implantable biomedical systems. The knowledge reviewed in the paper could provide intuition in the design of various wireless and mobile systems such as wireless body area networks, small-cell 5G cellular, as well as in-body biomedical applications, especially for efficient power and data management and higher security.","['Wireless communication', 'Implants', 'Communication system security', 'Radiofrequency identification', 'Couplings', 'Absorption', 'Wireless sensor networks']","['Near-field wireless power', 'near-field wireless communication', 'biomedical applications', 'implantable device']"
"Given the ubiquity of handwritten documents in human transactions, Optical Character Recognition (OCR) of documents have invaluable practical worth. Optical character recognition is a science that enables to translate various types of documents or images into analyzable, editable and searchable data. During last decade, researchers have used artificial intelligence/machine learning tools to automatically analyze handwritten and printed documents in order to convert them into electronic format. The objective of this review paper is to summarize research that has been conducted on character recognition of handwritten documents and to provide research directions. In this Systematic Literature Review (SLR) we collected, synthesized and analyzed research articles on the topic of handwritten OCR (and closely related topics) which were published between year 2000 to 2019. We followed widely used electronic databases by following pre-defined review protocol. Articles were searched using keywords, forward reference searching and backward reference searching in order to search all the articles related to the topic. After carefully following study selection process 176 articles were selected for this SLR. This review article serves the purpose of presenting state of the art results and techniques on OCR and also provide research directions by highlighting research gaps.","['Optical character recognition software', 'Character recognition', 'Databases', 'Optical imaging', 'Bibliographies', 'Protocols', 'Systematics']","['Optical character recognition', 'classification', 'languages', 'feature extraction', 'deep learning']"
"PM2.5 is one of the most important pollutants related to air quality, and the increase of its concentration will aggravate the threat to people's health. Therefore, the prediction of surface PM2.5 concentration is of great significance to human health protection. In this study, A hybrid CNN-LSTM model is developed by combining the convolutional neural network (CNN) with the long short-term memory (LSTM) neural network for forecasting the next 24h PM2.5 concentration in Beijing, which makes full use of their advantages that CNN can effectively extract the features related to air quality and the LSTM can reflect the long term historical process of input time series data. The air quality data of the last 7days and the PM2.5 concentration of the next day are first set as the input and output of the model due to the periodicity, respectively. Subsequently four models namely univariate LSTM model, multivariate LSTM model, univariate CNN-LSTM model and multivariate CNN-LSTM model, are established for PM2.5 concentration prediction. Finally, mean absolute error (MAE) and root mean square error (RMSE) are employed to evaluate the performance of these models and results show that the proposed multivariate CNN-LSTM model performs the best results due to low error and short training time.","['Predictive models', 'Atmospheric modeling', 'Time series analysis', 'Forecasting', 'Data models', 'Deep learning', 'Feature extraction']","['Deep learning', 'CNN', 'LSTM', 'PM2.5 concentration prediction']"
"Lightweight virtualization technologies have revolutionized the world of software development by introducing flexibility and innovation to this domain. Although the benefits introduced by these emerging solutions have been widely acknowledged in cloud computing, recent advances have led to the spread of such technologies in different contexts. As an example, the Internet of Things (IoT) and mobile edge computing benefit from container virtualization by exploiting the possibility of using these technologies not only in data centers but also on devices, which are characterized by fewer computational resources, such as single-board computers. This has led to a growing trend to more efficiently redesign the critical components of IoT/edge scenarios (e.g., gateways) to enable the concept of device virtualization. The possibility for efficiently deploying virtualized instances on single-board computers has already been addressed in recent studies; however, these studies considered only a limited number of devices and omitted important performance metrics from their empirical assessments. This paper seeks to fill this gap and to provide insights for future deployments through a comprehensive performance evaluation that aims to show the strengths and weaknesses of several low-power devices when handling container-virtualized instances.","['Containers', 'Virtualization', 'Logic gates', 'Performance evaluation', 'Cloud computing', 'Hardware']","['Internet of Things', 'edge computing', 'container virtualization', 'docker', 'performance evaluation']"
"Rapid advance of location acquisition technologies boosts the generation of trajectory data, which track the traces of moving objects. A trajectory is typically represented by a sequence of timestamped geographical locations. A wide spectrum of applications can benefit from the trajectory data mining. Bringing unprecedented opportunities, large-scale trajectory data also pose great challenges. In this paper, we survey various applications of trajectory data mining, e.g., path discovery, location prediction, movement behavior analysis, and so on. Furthermore, this paper reviews an extensive collection of existing trajectory data mining techniques and discusses them in a framework of trajectory data mining. This framework and the survey can be used as a guideline for designing future trajectory data mining solutions.","['Data mining', 'Trajectory', 'Big data', 'Data analytics', 'Data processing', 'Location awareness']","['Trajectory data mining', 'big data applications', 'data mining techniques']"
"Due to the advances in computer-based communication and health services over the past decade, the need for image security becomes urgent to address the requirements of both safety and non-safety in medical applications. This paper proposes a new fragile watermarking-based scheme for image authentication and self-recovery for medical applications. The proposed scheme locates image tampering as well as recovers the original image. A host image is broken into 4\times 4 blocks and singular value decomposition (SVD) is applied by inserting the traces of block wise SVD into the least significant bit of the image pixels to figure out the transformation in the original image. Two authentication bits namely block authentication and self-recovery bits are used to survive the vector quantization attack. The insertion of self-recovery bits is determined with Arnold transformation, which recovers the original image even after a high tampering rate. SVD-based watermarking information improves the image authentication and provides a way to detect different attacked area of the watermarked image. The proposed scheme is tested against different types of attacks such as text removal attack, text insertion attack, and copy and paste attack. Compared with the state-of-the art methods, the proposed scheme greatly improves both tamper localization accuracy and the peak signal to noise ratio of self-recovered image.","['Watermarking', 'Matrix decomposition', 'Authentication', 'Transforms', 'Biomedical imaging', 'Symmetric matrices']","['Medical image security', 'tamper localization', 'singular value decomposition', 'fragile watermarking', 'arnold transformation', 'image security', 'authentication']"
"Solid-state transformer (SST) is an emerging technology integrating with a transformer power electronics converters and control circuitry. This paper comprehensively reviews the SST topologies suitable for different voltage levels and with varied stages, their control operation, and different trends in applications. The paper discusses various SST configurations with their design and characteristics to convert the input to output under unipolar and bipolar operation. A comparison between the topologies, control operation and applications are included. Different control models and schemes are explained. Potential benefits of SST in many applications in terms of controllability and the synergy of AC and DC systems are highlighted to appreciate the importance of SST technologies. This review highlights many factors including existing issues and challenges and provides recommendations for the improvement of future SST configuration and development.","['Topology', 'Switches', 'Voltage control', 'Reactive power', 'Bridge circuits', 'Bidirectional control', 'Power conversion']","['Converter control', 'power distribution', 'solid-state transformer', 'transformer topologies', 'power converter']"
"This paper analyzes the notion of resilience in power systems from a fundamental viewpoint and thoroughly examines its practical implications. This paper aims to describe and classify different high-impact rare (HR) events, provide a more technical definition of power system resilience, and discuss linkages between resilience and other well-established concepts, such as security and reliability. Most relevant decisions of system operators in the face of HR events involve a significant level of stress and strain. In order to make informed decisions within this context, it is crucial to have an all-inclusive picture of the state of the system. This paper provides an appropriate framework that not only characterizes the various states of the system but also derives informed decisions from a resilience-oriented perspective. It also describes and analyzes diverse resilience improvement strategies. Comprehensive models and classifications are provided to clearly capture various aspects of power system resilience.","['Resilience', 'Power system stability', 'Uncertainty', 'Meteorology', 'Power system reliability', 'Power grids']","['High-impact rare (HR) events', 'power system restoration', 'proactive management', 'resilience assessment', 'resilience improvement']"
"The state-of-the-art technologies in new generation information technologies (New IT) greatly stimulate the development of smart manufacturing. In a smart manufacturing environment, more and more devices would be connected to the Internet so that a large volume of data can be obtained during all phases of the product lifecycle. Cloud-based smart manufacturing paradigm facilitates a new variety of applications and services to analyze a large volume of data and enable large-scale manufacturing collaboration. However, different factors, such as the network unavailability, overfull bandwidth, and latency time, restrict its availability for high-speed and low-latency real-time applications. Fog computing and edge computing extended the compute, storage, and networking capabilities of the cloud to the edge, which will respond to the above-mentioned issues. Based on cloud computing, fog computing, and edge computing, in this paper, a hierarchy reference architecture is introduced for smart manufacturing. The architecture is expected to be applied in the digital twin shop floor, which opens a bright perspective of new applications within the field of manufacturing.","['Cloud computing', 'Edge computing', 'Smart manufacturing', 'Computer architecture', 'Manufacturing systems', 'Real-time systems']","['Cloud computing', 'digital twin', 'edge computing', 'fog computing', 'hierarchical architecture', 'smart manufacturing']"
"Originally conceived as a mechanism to enable a trustless cryptocurrency-Bitcoin, blockchain has since unbound itself from its original purpose as an increasing number of industries and stakeholders' eye the technology as an attractive alternative to solve existing business solutions as well as disrupt mature industries. This paper presents a systematic literature review of the blockchain technology, tracking its increase in popularity in relation to similar technologies, such as cryptocurrencies and Bitcoin. The objective of this paper is to identify the current standing of the blockchain technology within the literature while also identifying the major fields of study and areas of application for which blockchain offers a valuable solution. This paper finds that unique features to the blockchain, such as privacy, security, anonymity, decentralization, and immutability, provide valuable benefits to various fields and subjects. This paper also finds that exploring the application of blockchain has only begun with some limited studies in areas, such as the Internet of Things, energy, finance, healthcare, and government, that also stand to benefit disproportionately from its implementation.","['Blockchain', 'Industries', 'Bitcoin', 'Bibliographies', 'Public key']","['Blockchain', 'cryptocurrency', 'energy', 'eGovernment', 'finance', 'healthcare', 'Internet of Things', 'applications', 'review']"
"Emotional state recognition of a speaker is a difficult task for machine learning algorithms which plays an important role in the field of speech emotion recognition (SER). SER plays a significant role in many real-time applications such as human behavior assessment, human-robot interaction, virtual reality, and emergency centers to analyze the emotional state of speakers. Previous research in this field is mostly focused on handcrafted features and traditional convolutional neural network (CNN) models used to extract high-level features from speech spectrograms to increase the recognition accuracy and overall model cost complexity. In contrast, we introduce a novel framework for SER using a key sequence segment selection based on redial based function network (RBFN) similarity measurement in clusters. The selected sequence is converted into a spectrogram by applying the STFT algorithm and passed into the CNN model to extract the discriminative and salient features from the speech spectrogram. Furthermore, we normalize the CNN features to ensure precise recognition performance and feed them to the deep bi-directional long short-term memory (BiLSTM) to learn the temporal information for recognizing the final state of emotion. In the proposed technique, we process the key segments instead of the whole utterance to reduce the computational complexity of the overall model and normalize the CNN features before their actual processing, so that it can easily recognize the Spatio-temporal information. The proposed system is evaluated over different standard dataset including IEMOCAP, EMO-DB, and RAVDESS to improve the recognition accuracy and reduce the processing time of the model, respectively. The robustness and effectiveness of the suggested SER model is proved from the experimentations when compared to state-of-the-art SER methods with an achieve up to 72.25%, 85.57%, and 77.02% accuracy over IEMOCAP, EMO-DB, and RAVDESS dataset, respectively.","['Feature extraction', 'Speech recognition', 'Emotion recognition', 'Spectrogram', 'Task analysis', 'Deep learning', 'Data mining']","['Speech emotion recognition', 'deep bidirectional long shot term memory', 'key segment sequence selection', 'normalization of CNN features', 'radial-based function network (RBFN)']"
"In this paper, we propose a distributed joint computation offloading and resource allocation optimization (JCORAO) scheme in heterogeneous networks with mobile edge computing. An optimization problem is formulated to provide the optimal computation offloading strategy policy, uplink subchannel allocation, uplink transmission power allocation, and computation resource scheduling. The optimization problem is decomposed into two sub-problems due to the NP-hard property. In order to analyze the offloading strategy, a sub-algorithm named distributed potential game is built. The existence of Nash equilibrium is proved. To jointly allocate uplink subchannel, uplink transmission power, and computation resource for the offloading mobile terminals, a sub-algorithm named cloud and wireless resource allocation algorithm is designed. The solutions for subchannel allocation consist of uniform zero frequency reuse method without interference and fractional frequency reuse method based on Hungarian and graph coloring with interference. A distributed JCORAO scheme is proposed to solve the optimization problem by the mutual iteration of the two sub-algorithms. Simulation results show that the distributed JCORAO scheme can effectively decrease the energy consumption and task completion time with lower complexity.","['Resource management', 'Task analysis', 'Servers', 'Optimization', 'Uplink', 'Wireless communication', 'Computational modeling']","['Mobile edge computing', 'heterogeneous networks', 'offloading strategy', 'resource allocation', 'game theory']"
"Recently, big data analytics has received important attention in a variety of application domains including business, finance, space science, healthcare, telecommunication and Internet of Things (IoT). Among these areas, IoT is considered as an important platform in bringing people, processes, data and things/objects together in order to enhance the quality of our everyday lives. However, the key challenges are how to effectively extract useful features from the massive amount of heterogeneous data generated by resource-constrained IoT devices in order to provide real-time information and feedback to the end-users, and how to utilize this data-aware intelligence in enhancing the performance of wireless IoT networks. Although there are parallel advances in cloud computing and edge computing for addressing some issues in data analytics, they have their own benefits and limitations. The convergence of these two computing paradigms, i.e., massive virtually shared pool of computing and storage resources from the cloud and real-time data processing by edge computing, could effectively enable live data analytics in wireless IoT networks. In this regard, we propose a novel framework for coordinated processing between edge and cloud computing/processing by integrating advantages from both the platforms. The proposed framework can exploit the network-wide knowledge and historical information available at the cloud center to guide edge computing units towards satisfying various performance requirements of heterogeneous wireless IoT networks. Starting with the main features, key enablers and the challenges of big data analytics, we provide various synergies and distinctions between cloud and edge processing. More importantly, we identify and describe the potential key enablers for the proposed edge-cloud collaborative framework, the associated key challenges and some interesting future research directions.","['Cloud computing', 'Wireless communication', 'Data analysis', 'Big Data', 'Wireless sensor networks', 'Edge computing', 'Distributed databases']","['Big data', 'data analytics', 'internet of things (IoT)', 'cloud computing', 'edge computing', 'fog computing']"
"As a prerequisite for cell detection, cell classification, and cancer grading, nuclei segmentation in histology images has attracted wide attention in recent years. It is quite a challenging task due to the diversity in staining procedure, cell morphology, and cell arrangement between different histopathology images, especially with different color contrasts. In this paper, an Unet-based neural network, RIC-Unet (residual-inception-channel attention-Unet), for nuclei segmentation is proposed. The techniques of residual blocks, multi-scale and channel attention mechanism are applied on RIC-Unet to segment nuclei more accurately. RIC-Unet is compared with two traditional segmentation methods: CP and Fiji, two original CNN methods: CNN2, CNN3, and original U-net on The Cancer Genomic Atlas (TCGA) dataset. Besides, in this paper, we use Dice, F1-score, and aggregated Jaccard index to evaluate these methods. The average of RIC-Unet and U-net on these three indicators are 0.8008 versus 0.7844, 0.8278 versus 0.8155, and 0.5635 versus 0.5462. In addition, our method won the third place in the computational precision medicine nuclei segmentation challenge together with MICCAI 2018.","['Image segmentation', 'Computer architecture', 'Feature extraction', 'Microprocessors', 'Pathology', 'Semantics', 'Task analysis']","['Computational pathology', 'nuclei segmentation', 'residual block', 'deep learning']"
"This paper presents a technical overview for low-noise switched reluctance motor (SRM) drives in electric vehicle (EV) applications. With ever-increasing concerns over environmental and cost issues associated with permanent magnet machines, there is a technical trend to utilize SRMs in some mass production markets. The SRM is gaining much interest for EVs due to its rare-earth-free characteristic and excellent performance. In spite of many advantages compared with conventional adjustable-speed drives, SRMs suffer from torque ripple and radial distortion (and thus noise and vibration) by their nature. Therefore, for high-performance vehicle applications, it is important and urgent to optimize the SRM system to overcome the drawbacks of the noise and vibration. In order to present clear solutions to the acoustic noise in SRMs, this paper starts by analyzing the mechanism of the radial vibration and torque ripples inherent in the motors, and then focuses on the state-of-the-art technologies to mitigate the radial force and torque ripples. It highlights two categories for low-noise SRMs, including the machine topology improvement and control strategy design for radial vibration mitigation and torque ripple reduction. Advanced technologies are reviewed, classified, and compared accordingly. In addition to these methodologies, the schemes that have been developed by authors are also presented and discussed. Finally, the research status on this topic is summarized and forecast research hotspots are presented. It is our intention that this paper provides the guidance on performance improvements for low-noise SRM drives in EV applications.","['Reluctance motors', 'Force', 'Vibrations', 'Stator windings', 'Rotors']","['Switched reluctance motor (SRM)', 'low noise', 'torque ripple', 'radial distortion', 'control', 'motor structure']"
"The diagnosis of blood-related diseases involves the identification and characterization of a patient's blood sample. As such, automated methods for detecting and classifying the types of blood cells have important medical applications in this field. Although deep convolutional neural network (CNN) and the traditional machine learning methods have shown good results in the classification of blood cell images, they are unable to fully exploit the long-term dependence relationship between certain key features of images and image labels. To resolve this problem, we have introduced the recurrent neural networks (RNNs). Specifically, we combined the CNN and RNN in order to propose the CNN-RNN framework that can deepen the understanding of image content and learn the structured features of images and to begin endto-end training of big data in medical image analysis. In particular, we apply the transfer learning method to transfer the weight parameters that were pre-trained on the ImageNet dataset to the CNN section and adopted a custom loss function to allow our network to train and converge faster and with more accurate weight parameters. Experimental results show that compared with the other CNN models such as ResNet and Inception V3, our proposed network model is more accurate and efficient in classifying blood cell images.","['Classification algorithms', 'Feature extraction', 'Convolutional neural networks', 'Support vector machines', 'White blood cells', 'Image classification']","['Artificial intelligence', 'convolutional neural network', 'recurrent neural network', 'transfer learning']"
"The Internet of Things (IoT) has facilitated services without human intervention for a wide range of applications, including continuous remote patient monitoring (RPM). However, the complexity of RPM architectures, the size of data sets generated and limited power capacity of devices make RPM challenging. In this paper, we propose a tier-based End to End architecture for continuous patient monitoring that has a patient centric agent (PCA) as its center piece. The PCA manages a blockchain component to preserve privacy when data streaming from body area sensors needs to be stored securely. The PCA based architecture includes a lightweight communication protocol to enforce security of data through different segments of a continuous, real time patient monitoring architecture. The architecture includes the insertion of data into a personal blockchain to facilitate data sharing amongst healthcare professionals and integration into electronic health records while ensuring privacy is maintained. The blockchain is customized for RPM with modifications that include having the PCA select a Miner to reduce computational effort, enabling the PCA to manage multiple blockchains for the same patient, and the modification of each block with a prefix tree to minimize energy consumption and incorporate secure transaction payments. Simulation results demonstrate that security and privacy can be enhanced in RPM with the PCA based End to End architecture.","['Computer architecture', 'Medical services', 'Principal component analysis', 'Sensors', 'Biomedical monitoring']","['Blockchain', 'body area sensor network', 'healthcare', 'remote patient monitoring', 'patient centric agent', 'proof of work', 'streamed data', 'Internet of Things', 'dynamically generated session key', 'patient record encryption key']"
"Cardiovascular diseases (CVD) are among the most common serious illnesses affecting human health. CVDs may be prevented or mitigated by early diagnosis, and this may reduce mortality rates. Identifying risk factors using machine learning models is a promising approach. We would like to propose a model that incorporates different methods to achieve effective prediction of heart disease. For our proposed model to be successful, we have used efficient Data Collection, Data Pre-processing and Data Transformation methods to create accurate information for the training model. We have used a combined dataset (Cleveland, Long Beach VA, Switzerland, Hungarian and Stat log). Suitable features are selected by using the Relief, and Least Absolute Shrinkage and Selection Operator (LASSO) techniques. New hybrid classifiers like Decision Tree Bagging Method (DTBM), Random Forest Bagging Method (RFBM), K-Nearest Neighbors Bagging Method (KNNBM), AdaBoost Boosting Method (ABBM), and Gradient Boosting Boosting Method (GBBM) are developed by integrating the traditional classifiers with bagging and boosting methods, which are used in the training process. We have also instrumented some machine learning algorithms to calculate the Accuracy (ACC), Sensitivity (SEN), Error Rate, Precision (PRE) and F1 Score (F1) of our model, along with the Negative Predictive Value (NPR), False Positive Rate (FPR), and False Negative Rate (FNR). The results are shown separately to provide comparisons. Based on the result analysis, we can conclude that our proposed model produced the highest accuracy while using RFBM and Relief feature selection methods (99.05%).","['Heart', 'Predictive models', 'Prediction algorithms', 'Boosting', 'Support vector machines', 'Feature extraction', 'Classification algorithms']","['Heart disease', 'machine learning', 'CVD', 'relief feature selection', 'LASSO feature selection', 'decision tree', 'random forest', 'K-nearest neighbors', 'AdaBoost', 'gradient boosting']"
"Bidirectional DC-DC power converters are increasingly employed in diverse applications whereby power flow in both forward and reverse directions are required. These include but not limited to energy storage systems, uninterruptable power supplies, electric vehicles, and renewable energy systems, to name a few. This paper aims to review these converters from the point of view of topology as well as control schemes. From the point of view of topology, these converters are divided into two main categories, namely non-isolated and isolated configurations. Each category is divided into eight groups along with their respective schematics and a table of summary. Furthermore, the common control schemes and switching strategies for these converters are also reviewed. Some of the control schemes are typically applied to all DC-DC power converters such as PID, sliding mode, fuzzy, model predictive, digital control, etc. In this context, it should be noted that some switching strategies were designed specifically for isolated bidirectional DC-DC converters in order to improve their performance such as single phase shift, dual phase shift, triple phase shift, etc. The features of each topology and control scheme along with their typical applications are discussed, in order to provide a ground of comparison for realizing new configurations or finding the appropriate converter for the specific application.","['DC-DC power converters', 'Topology', 'Switches', 'Load flow', 'Inductors', 'Batteries']","['Batteries', 'bidirectional power flow', 'control systems', 'dc-dc power converters']"
"Modern power systems face different challenges such as the ever-increasing electrical energy demand, the massive growth of renewable energy with distributed generations, the large-scale Internet of Things (IoT) devices adaptation, the emerging cyber-physical security threats, and the main goal of maintaining the system's stability and reliability. These challenges pose extreme pressure on finding advanced technologies and sustainable solutions for secure and reliable operations of the power system. The blockchain is one of the recent technologies that have gained lots of attention in different applications including smart grid for its uniqueness and decentralized nature. In the last few years, this technology grew a momentum specifically with the cryptocurrencies' industry such as the Bitcoin and Etherium. The Blockchain's applications in the smart grids could offer many innovative and affordable solutions to some of the challenges that the future and the current smart grids will be facing. This paper reviews different prospects, advantages, approaches, and technical challenges of utilizing the blockchain technology in the smart grid, and presents frameworks for key smart grid blockchain-based applications; more specifically, it is shown that how the blockchain can be used as the smart grid's cyber-physical layer.","['Blockchain', 'Smart grids', 'Consensus algorithm', 'Cryptography', 'Industries', 'Renewable energy sources']","['Blockchain applications', 'cyber-physical security', 'energy trading', 'electric vehicles', 'microgrid monitoring and control', 'smart grids']"
"Our world and our lives are changing in many ways. Communication, networking, and computing technologies are among the most influential enablers that shape our lives today. Digital data and connected worlds of physical objects, people, and devices are rapidly changing the way we work, travel, socialize, and interact with our surroundings, and they have a profound impact on different domains, such as healthcare, environmental monitoring, urban systems, and control and management applications, among several other areas. Cities currently face an increasing demand for providing services that can have an impact on people's everyday lives. The CityPulse framework supports smart city service creation by means of a distributed system for semantic discovery, data analytics, and interpretation of large-scale (near-)real-time Internet of Things data and social media data streams. To goal is to break away from silo applications and enable cross-domain data integration. The CityPulse framework integrates multimodal, mixed quality, uncertain and incomplete data to create reliable, dependable information and continuously adapts data processing techniques to meet the quality of information requirements from end users. Different than existing solutions that mainly offer unified views of the data, the CityPulse framework is also equipped with powerful data analytics modules that perform intelligent data aggregation, event detection, quality assessment, contextual filtering, and decision support. This paper presents the framework, describes its components, and demonstrates how they interact to support easy development of custom-made applications for citizens. The benefits and the effectiveness of the framework are demonstrated in a use-case scenario implementation presented in this paper.","['Smart cities', 'Data analytics', 'Distributed databases', 'Semantics', 'Control systems', 'Environmental monitoring', 'Telecommunication services', 'Medical services', 'Digital systems', 'Knowledge discovery']","['Data analytics framework', 'smart cities']"
"The collection and analysis of data are continuously growing due to the pervasiveness of computing devices. The analysis of such information is fostering businesses and contributing beneficially to the society in many different fields. However, this storage and flow of possibly sensitive data poses serious privacy concerns. Methods that allow the knowledge extraction from data, while preserving privacy, are known as privacy-preserving data mining (PPDM) techniques. This paper surveys the most relevant PPDM techniques from the literature and the metrics used to evaluate such techniques and presents typical applications of PPDM methods in relevant fields. Furthermore, the current challenges and open issues in PPDM are discussed.",[],[]
"The upcoming fifth-generation (5G) mobile technology, which includes advanced communication features, is posing new challenges on cybersecurity defense systems. Although innovative approaches have evolved in the last few years, 5G will make existing intrusion detection and defense procedures become obsolete, in case they are not adapted accordingly. In this sense, this paper proposes a novel 5G-oriented cyberdefense architecture to identify cyberthreats in 5G mobile networks efficient and quickly enough. For this, our architecture uses deep learning techniques to analyze network traffic by extracting features from network flows. Moreover, our proposal allows adapting, automatically, the configuration of the cyberdefense architecture in order to manage traffic fluctuation, aiming both to optimize the computing resources needed in each particular moment and to fine tune the behavior and the performance of analysis and detection processes. Experiments using a well-known botnet data set depict how a neural network model reaches a sufficient classification accuracy in our anomaly detection system. Extended experiments using diverse deep learning solutions analyze and determine their suitability and performance for different network traffic loads. The experimental results show how our architecture can self-adapt the anomaly detection system based on the volume of network flows gathered from 5G subscribers' user equipments in real-time and optimizing the resource consumption.","['Anomaly detection', '5G mobile communication', 'Machine learning', 'Botnet', 'Computer architecture', 'Feature extraction']","['5G', 'anomaly detection', 'botnets', 'deep learning', 'performance evaluation']"
"Deep learning (DL) algorithms are considered as a methodology of choice for remote-sensing image analysis over the past few years. Due to its effective applications, deep learning has also been introduced for automatic change detection and achieved great success. The present study attempts to provide a comprehensive review and a meta-analysis of the recent progress in this subfield. Specifically, we first introduce the fundamentals of deep learning methods which are frequently adopted for change detection. Secondly, we present the details of the meta-analysis conducted to examine the status of change detection DL studies. Then, we focus on deep learning-based change detection methodologies for remote sensing images by giving a general overview of the existing methods. Specifically, these deep learning-based methods were classified into three groups; fully supervised learning-based methods, fully unsupervised learning-based methods and transfer learning-based techniques. As a result of these investigations, promising new directions were identified for future research. This study will contribute in several ways to our understanding of deep learning for change detection and will provide a basis for further research. Some source codes of the methods discussed in this paper are available from: https://github.com/lazharkhelifi/deeplearning_changedetection_remotesensing_review.","['Deep learning', 'Remote sensing', 'Task analysis', 'Learning systems', 'Biological neural networks', 'Computer vision', 'Change detection algorithms']","['Change detection', 'remote sensing images', 'deep learning', 'feature learning', 'weakly supervised learning', 'review']"
"Smart city advancements are driving massive transformations of healthcare, the largest global industry. The drivers include increasing demands for ubiquitous, preventive, and personalized healthcare, to be provided to the public at reduced risks and costs. Mobile cloud computing could potentially meet the future healthcare demands by enabling anytime, anywhere capture and analyses of patients' data. However, network latency, bandwidth, and reliability are among the many challenges hindering the realization of next-generation healthcare. This paper proposes a ubiquitous healthcare framework, UbeHealth, that leverages edge computing, deep learning, big data, high-performance computing (HPC), and the Internet of Things (IoT) to address the aforementioned challenges. The framework enables an enhanced network quality of service using its three main components and four layers. Deep learning, big data, and HPC are used to predict network traffic, which in turn are used by the Cloudlet and network layers to optimize data rates, data caching, and routing decisions. Application protocols of the traffic flows are classified, enabling the network layer to meet applications' communication requirements better and to detect malicious traffic and anomalous data. Clustering is used to identify the different kinds of data originating from the same application protocols. A proof of concept UbeHealth system has been developed based on the framework. A detailed literature review is used to capture the design requirements for the proposed system. The system is described in detail including the algorithmic implementation of the three components and four layers. Three widely used data sets are used to evaluate the UbeHealth system.","['Medical services', 'Cloud computing', 'Edge computing', 'Quality of service', 'Machine learning', 'Smart cities', 'Internet of Things']","['Cloudlets', 'deep learning', 'Internet of Things (IoT)', 'mobile edge computing', 'mobile healthcare', 'preventive healthcare', 'traffic classification', 'traffic prediction', 'survey', 'fog computing', 'cloud computing', 'multimedia applications', 'smart cities']"
"Wearable antennas have gained much attention in recent years due to their attractive features and possibilities in enabling lightweight, flexible, low cost, and portable wireless communication and sensing. Such antennas need to be conformal when used on different parts of the human body, thus need to be implemented using flexible materials and designed in a low profile structure. Ultimately, these antennas need to be capable of operating with minimum degradation in proximity to the human body. Such requirements render the design of wearable antennas challenging, especially when considering aspects such as their size compactness, effects of structural deformation and coupling to the body, and fabrication complexity and accuracy. Despite slight variations in severity according to applications, most of these issues exist in the context of body-worn implementation. This review aims to present different challenges and issues in designing wearable antennas, their material selection, and fabrication techniques. More importantly, recent innovative methods in back radiations reduction techniques, circular polarization (CP) generation methods, dual polarization techniques, and providing additional robustness against environmental effects are first presented. This is followed by a discussion of innovative features and their respective methods in alleviating these issues recently proposed by the scientific community researching in this field.","['Antennas', 'Biomedical monitoring', 'Substrates', 'Fabrics', 'Polymers', 'Fabrication', 'Conductivity']","['Wearable devices', 'Internet of Things (IoT)', 'wearable antennas', 'flexible', 'reconfigurable antennas', 'energy harvesting for wearable devices', 'specific absorption rate (SAR)']"
"The fifth generation (5G) mobile networks are envisaged to enable a plethora of breakthrough advancements in wireless technologies, providing support of a diverse set of services over a single platform. While the deployment of 5G systems is scaling up globally, it is time to look ahead for beyond 5G systems. This is mainly driven by the emerging societal trends, calling for fully automated systems and intelligent services supported by extended reality and haptics communications. To accommodate the stringent requirements of their prospective applications, which are data-driven and defined by extremely low-latency, ultra-reliable, fast and seamless wireless connectivity, research initiatives are currently focusing on a progressive roadmap towards the sixth generation (6G) networks, which are expected to bring transformative changes to this premise. In this article, we shed light on some of the major enabling technologies for 6G, which are expected to revolutionize the fundamental architectures of cellular networks and provide multiple homogeneous artificial intelligence-empowered services, including distributed communications, control, computing, sensing, and energy, from its core to its end nodes. In particular, the present paper aims to answer several 6G framework related questions: What are the driving forces for the development of 6G? How will the enabling technologies of 6G differ from those in 5G? What kind of applications and interactions will they support which would not be supported by 5G? We address these questions by presenting a comprehensive study of the 6G vision and outlining seven of its disruptive technologies, i.e., mmWave communications, terahertz communications, optical wireless communications, programmable metasurfaces, drone-based communications, backscatter communications and tactile internet, as well as their potential applications. Then, by leveraging the state-of-the-art literature surveyed for each technology, we discuss the associated requirements, key challenges, and open research problems. These discussions are thereafter used to open up the horizon for future research directions.","['5G mobile communication', 'Wireless communication', 'Wireless sensor networks', 'Market research', 'Artificial intelligence', 'Reliability', 'Haptic interfaces']","['6G', 'backscatter communications', 'drone-based communications', 'terahertz communications', 'metasurfaces', 'mm-wave', 'optical wireless communications', 'tactile internet']"
"In vehicular ad hoc networks (VANETs), trust establishment among vehicles is important to secure integrity and reliability of applications. In general, trust and reliability help vehicles to collect correct and credible information from surrounding vehicles. On top of that, a secure trust model can deal with uncertainties and risk taking from unreliable information in vehicular environments. However, inaccurate, incomplete, and imprecise information collected by vehicles as well as movable/immovable obstacles have interrupting effects on VANET. In this paper, a fuzzy trust model based on experience and plausibility is proposed to secure the vehicular network. The proposed trust model executes a series of security checks to ensure the correctness of the information received from authorized vehicles. Moreover, fog nodes are adopted as a facility to evaluate the level of accuracy of event's location. The analyses show that the proposed solution not only detects malicious attackers and faulty nodes, but also overcomes the uncertainty and imprecision of data in vehicular networks in both line of sight and non-line of sight environments.","['Security', 'Vehicular ad hoc networks', 'Reliability', 'Computational modeling', 'Edge computing', 'Fuzzy logic', 'Data models']","['Trust', 'plausibility', 'experience', 'fog node', 'fuzzy logic', 'VANET']"
"RGB-D (red, green, blue, and depth) salient object detection aims to identify the most visually distinctive objects in a pair of color and depth images. Based upon an observation that most of the salient objects may stand out at least in one modality, this paper proposes an adaptive fusion scheme to fuse saliency predictions generated from two modalities. Specifically, we design two-streamed convolutional neural networks (CNN), each of which extracts features and predicts a saliency map from either RGB or depth modality. Then, a saliency fusion module learns a switch map that is used to adaptively fuse the predicted saliency maps. A loss function composed of saliency supervision, switch map supervision, and edge-preserving constraints are designed to make full supervision, and the entire network is trained in an end-to-end manner. Benefited from the adaptive fusion strategy and the edge-preserving constraint, our approach outperforms state-of-the-art methods on three publicly available datasets.","['Switches', 'Feature extraction', 'Fuses', 'Image color analysis', 'Saliency detection', 'Streaming media', 'Object detection']","['RGB-D salient object detection', 'switch map', 'edge-preserving']"
"Air pollution forecasting can provide reliable information about the future pollution situation, which is useful for an efficient operation of air pollution control and helps to plan for prevention. Dynamics of air pollution are usually reflected by various factors, such as the temperature, humidity, wind direction, wind speed, snowfall, rainfall, and so on, which increase the difficulty in understanding the change of air pollutant concentration. In this paper, a short-term forecasting model based on deep learning is proposed for PM2.5 (particulate matter with an aerodynamic diameter less than or equal to 2.5~\mu \text{m} ) concentration, and the convolutional-based bidirectional gated recurrent unit (CBGRU) method is presented, which combines 1D convnets (convolutional neural networks) and bidirectional GRU (gated recurrent unit) neural networks. The case is carried out by using the Beijing PM2.5 data set in UCI Machine Learning Repository. Comparing the prediction results with the traditional ones, it is proved that the error of the CBGRU model is lower and the prediction performance is better.","['Predictive models', 'Forecasting', 'Wind speed', 'Atmospheric modeling', 'Time series analysis', 'Air pollution', 'Correlation']","['Air pollution forecasting', 'deep learning', '1D convolutional neural networks', 'bidirectional gated recurrent unit']"
"Disastrous events are cordially involved with the momentum of nature. As such mishaps have been showing off own mastery, situations have gone beyond the control of human resistive mechanisms far ago. Fortunately, several technologies are in service to gain affirmative knowledge and analysis of a disaster's occurrence. Recently, Internet of Things (IoT) paradigm has opened a promising door toward catering of multitude problems related to agriculture, industry, security, and medicine due to its attractive features, such as heterogeneity, interoperability, light-weight, and flexibility. This paper surveys existing approaches to encounter the relevant issues with disasters, such as early warning, notification, data analytics, knowledge aggregation, remote monitoring, real-time analytics, and victim localization. Simultaneous interventions with IoT are also given utmost importance while presenting these facts. A comprehensive discussion on the state-of-the-art scenarios to handle disastrous events is presented. Furthermore, IoT-supported protocols and market-ready deployable products are summarized to address these issues. Finally, this survey highlights open challenges and research trends in IoT-enabled disaster management systems.","['Protocols', 'Disaster management', 'Wireless sensor networks', 'Security', 'Real-time systems', 'Earthquakes', 'Nanoscale devices']","['Internet of Things', 'disaster management', 'cloud-assisted services']"
"The frequency of extreme events (e.g., hurricanes, earthquakes, and floods) and man-made attacks (cyber and physical attacks) has increased dramatically in recent years. These events have severely impacted power systems ranging from long outage times to major equipment (e.g., substations, transmission lines, and power plants) destructions. This calls for developing control and operation methods and planning strategies to improve grid resilience against such events. The first step toward this goal is to develop resilience metrics and evaluation methods to compare planning and operation alternatives and to provide techno-economic justifications for resilience enhancement. Although several power system resilience definitions, metrics, and evaluation methods have been proposed in the literature, they have not been universally accepted or standardized. This paper provides a comprehensive and critical review of current practices of power system resilience metrics and evaluation methods and discusses future directions and recommendations to contribute to the development of universally accepted and standardized definitions, metrics, evaluation methods, and enhancement strategies. This paper thoroughly examines the consensus on the power system resilience concept provided by different organizations and scholars and existing and currently practiced resilience enhancement methods. Research gaps, associated challenges, and potential solutions to existing limitations are also provided.","['Resilience', 'Measurement', 'Power system reliability', 'Meteorology', 'Hurricanes', 'Reliability']","['Critical review', 'extreme events', 'power system resilience', 'resilience definitions', 'metrics', 'enhancement strategies']"
"Technology and the rapid growth in the area of brain imaging technologies have forever made for a pivotal role in analyzing and focusing the new views of brain anatomy and functions. The mechanism of image processing has widespread usage in the area of medical science for improving the early detection and treatment phases. Deep neural networks (DNN), till date, have demonstrated wonderful performance in classification and segmentation task. Carrying this idea into consideration, in this paper, a technique for image compression using a deep wavelet autoencoder (DWA), which blends the basic feature reduction property of autoencoder along with the image decomposition property of wavelet transform is proposed. The combination of both has a tremendous effect on sinking the size of the feature set for enduring further classification task by using DNN. A brain image dataset was taken and the proposed DWA-DNN image classifier was considered. The performance criterion for the DWA-DNN classifier was compared with other existing classifiers such as autoencoder-DNN or DNN, and it was noted that the proposed method outshines the existing methods.","['Image segmentation', 'Magnetic resonance imaging', 'Tumors', 'Biological neural networks', 'Biomedical imaging', 'Task analysis']","['Neural network (NN)', 'deep neural network (DNN)', 'autoencoder (AE)', 'image classification']"
"As the amount of unstructured text data that humanity produces overall and on the Internet grows, so does the need to intelligently to process it and extract different types of knowledge from it. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been applied to natural language processing systems with comparative, remarkable results. The CNN is a noble approach to extract higher level features that are invariant to local translation. However, it requires stacking multiple convolutional layers in order to capture long-term dependencies, due to the locality of the convolutional and pooling layers. In this paper, we describe a joint CNN and RNN framework to overcome this problem. Briefly, we use an unsupervised neural language model to train initial word embeddings that are further tuned by our deep learning network, then, the pre-trained parameters of the network are used to initialize the model. At a final stage, the proposed framework combines former information with a set of feature maps learned by a convolutional layer with long-term dependencies learned via long-short-term memory. Empirically, we show that our approach, with slight hyperparameter tuning and static vectors, achieves outstanding results on multiple sentiment analysis benchmarks. Our approach outperforms several existing approaches in term of accuracy; our results are also competitive with the state-of-the-art results on the Stanford Large Movie Review data set with 93.3% accuracy, and the Stanford Sentiment Treebank data set with 48.8% fine-grained and 89.2% binary accuracy, respectively. Our approach has a significant role in reducing the number of parameters and constructing the convolutional layer followed by the recurrent layer as a substitute for the pooling layer. Our results show that we were able to reduce the loss of detailed, local information and capture long-term dependencies with an efficient framework that has fewer parameters and a high level of performance.","['Feature extraction', 'Task analysis', 'Machine learning', 'Computational modeling', 'Convolutional neural networks', 'Sentiment analysis']","['Convolutional neural network', 'recurrent neural network', 'natural language processing', 'deep learning', 'sentiment analysis', 'long-term dependencies']"
"The parameters of physical layer radio frame for 5th generation (5G) mobile cellular systems are expected to be flexibly configured to cope with diverse requirements of different scenarios and services. This paper presents a frame structure and design, which is specifically targeting Internet of Things (IoT) provision in 5G wireless communication systems. We design a suitable radio numerology to support the typical characteristics, that is, massive connection density and small and bursty packet transmissions with the constraint of low-cost and low complexity operation of IoT devices. We also elaborate on the design of parameters for random access channel enabling massive connection requests by IoT devices to support the required connection density. The proposed design is validated by link level simulation results to show that the proposed numerology can cope with transceiver imperfections and channel impairments. Furthermore, the results are also presented to show the impact of different values of guard band on system performance using different subcarrier spacing sizes for data and random access channels, which show the effectiveness of the selected waveform and guard bandwidth. Finally, we present system-level simulation results that validate the proposed design under realistic cell deployments and inter-cell interference conditions.","['Physical layer', '5G mobile communication', 'Cellular networks', 'Mobile communication', 'Internet of things', 'Complexity theory', 'Wireless communication']","['5G', 'frame structure', 'Internet of Things', 'random access channel']"
"As a significant application of energy, smart grid is a complicated interconnected power grid that involves sensors, deployment strategies, smart meters, and real-time data processing. It continuously generates data with large volume, high velocity, and diverse variety. In this paper, we first give a brief introduction on big data, smart grid, and big data application in the smart grid scenario. Then, recent studies and developments are summarized in the context of integrated architecture and key enabling technologies. Meanwhile, security issues are specifically addressed. Finally, we introduce several typical big data applications and point out future challenges in the energy domain.","['Big data', 'Smart grids', 'Data mining', 'Phasor measurement units', 'Real-time systems']","['Big data', 'energy', 'smart grids', 'data processing', 'data mining', 'energy internet', 'survey']"
"This paper presents robust virtual inertia control of an islanded microgrid considering high penetration of renewable energy sources (RESs). In such microgrids, the lack of system inertia due to the replacement of traditional generating units with a large amount of RESs causes undesirable influence to microgrid frequency stability, leading to weakening of the microgrid. In order to handle this challenge, the H robust control method is implemented to the virtual inertial control loop, taking into account the high penetration of RESs, thus enhancing the robust performance and stability of the microgrid during contingencies. The controller's robustness and performance are determined along with numerous disturbances and parametric uncertainties. The comparative study between H and optimal proportionalintegral (PI)-based virtual inertia controller is also presented. The results show the superior robustness and control effect of the proposed H controller in terms of precise reference frequency tracking and disturbance attenuation over the optimal PI controller. It is validated that the proposed H -based virtual inertia controller successfully provides desired robust frequency support to a low-inertia islanded microgrid against high RESs penetration.","['Microgrids', 'Frequency control', 'Power system stability', 'Robustness', 'Stability analysis', 'Uncertainty', 'Robust control']","['Frequency control', 'H∞', 'islanded microgrid', 'renewable energy', 'robust control', 'virtual inertia control', 'virtual synchronous generator']"
"Smart cities are expected to improve the quality of daily life, promote sustainable development, and improve the functionality of urban systems. Now that many smart systems have been implemented, security and privacy issues have become a major challenge that requires effective countermeasures. However, traditional cybersecurity protection strategies cannot be applied directly to these intelligent applications because of the heterogeneity, scalability, and dynamic characteristics of smart cities. Furthermore, it is necessary to be aware of security and privacy threats when designing and implementing new mechanisms or systems. Motivated by these factors, we survey the current situations of smart cities with respect to security and privacy to provide an overview of both the academic and industrial fields and to pave the way for further exploration. Specifically, this survey begins with an overview of smart cities to provide an integrated context for readers. Then, we discuss the privacy and security issues in current smart applications along with the corresponding requirements","['Smart cities', 'Security', 'Privacy', 'Computer architecture', 'Sensors']","['Smart city', 'Internet of Things', 'security', 'privacy']"
"The explosive popularity of small-cell and Internet of Everything devices has tremendously increased traffic loads. This increase has revolutionised the current network into 5G technology, which demands increased capacity, high data rate and ultra-low latency. Two of the research focus areas for meeting these demands are exploring the spectrum resource and maximising the utilisation of its bands. However, the scarcity of the spectrum resource creates a serious challenge in achieving an efficient management scheme. This work aims to conduct an in-depth survey on recent spectrum sharing (SS) technologies towards 5G development and recent 5G-enabling technologies. SS techniques are classified, and SS surveys and related studies on SS techniques relevant to 5G networks are reviewed. The surveys and studies are categorised into one of the main SS techniques on the basis of network architecture, spectrum allocation behaviour and spectrum access method. Moreover, a detailed survey on cognitive radio (CR) technology in SS related to 5G implementation is performed. For a complete survey, discussions are conducted on the issues and challenges in the current implementation of SS and CR, and the means to support efficient 5G advancement are provided.","['5G mobile communication', 'Wireless communication', 'Massive MIMO', 'Wireless sensor networks', 'Internet of Things', 'Computer architecture', 'Resource management']","['5G', 'new radio', 'spectrum sharing', 'spectrum efficiency', 'cognitive radio', 'enabling technologies']"
"In recent years, the deep learning is applied to the field of traffic sign detection methods which achieves excellent performance. However, there are two main challenges in traffic sign detection to be solve urgently. For one thing, some traffic signs of small size are more difficult to detect than those of large size so that the small traffic signs are undetected. For another, some false signs are always detected because of interferences caused by the illumination variation, bad weather and some signs similar to the true traffic signs. Therefore, to solve the undetection and false detection, we first propose a cascaded R-CNN to obtain the multiscale features in pyramids. Each layer of the cascaded network except the first layer fuses the output bounding box of the previous one layer for joint training. This method contributes to the traffic sign detection. Then, we propose a multiscale attention method to obtain the weighted multiscale features by dot-product and softmax, which is summed to fine the features to highlight the traffic sign features and improve the accuracy of the traffic sign detection. Finally, we increase the number of difficult negative samples for dataset balance and data augmentation in the training to relieve the interference by complex environment and similar false traffic signs. The data augment method expands the German traffic sign training dataset by simulation of complex environment changes. We conduct numerous experiments to verify the effectiveness of our proposed algorithm. The accuracy and recall rate of our method are 98.7% and 90.5% in GTSDB, 99.7% and 83.62% in CCTSDB and 98.9% and 85.6% in Lisa dataset respectively.","['Feature extraction', 'Object detection', 'Image color analysis', 'Deep learning', 'Detection algorithms', 'Training', 'Shape']","['Traffic sign detection', 'convolutional neural network', 'attention', 'object detection', 'Multiscale']"
"Electric energy forecasting domain attracts researchers due to its key role in saving energy resources, where mainstream existing models are based on Gradient Boosting Regression (GBR), Artificial Neural Networks (ANNs), Extreme Learning Machine (ELM) and Support Vector Machine (SVM). These models encounter high-level of non-linearity between input data and output predictions and limited adoptability in real-world scenarios. Meanwhile, energy forecasting domain demands more robustness, higher prediction accuracy and generalization ability for real-world implementation. In this paper, we achieve the mentioned tasks by developing a hybrid sequential learning-based energy forecasting model that employs Convolution Neural Network (CNN) and Gated Recurrent Units (GRU) into a unified framework for accurate energy consumption prediction. The proposed framework has two major phases: (1) data refinement and (2) training, where the data refinement phase applies preprocessing strategies over raw data. In the training phase, CNN features are extracted from input dataset and fed in to GRU, that is selected as optimal and observed to have enhanced sequence learning abilities after extensive experiments. The proposed model is an effective alternative to the previous hybrid models in terms of computational complexity as well prediction accuracy, due to the representative features' extraction potentials of CNNs and effectual gated structure of multi-layered GRU. The experimental evaluation over existing energy forecasting datasets reveal the better performance of our method in terms of preciseness and efficiency. The proposed method achieved the smallest error rate on Appliances Energy Prediction (AEP) and Individual Household Electric Power Consumption (IHEPC) datasets, when compared to other baseline models.","['Predictive models', 'Machine learning', 'Load modeling', 'Data models', 'Forecasting', 'Energy consumption', 'Support vector machines']","['CNN', 'CNN-GRU', 'deep learning', 'energy forecasting', 'electricity consumption prediction', 'GRU', 'LSTM', 'short-term load forecasting']"
"The vision of the Internet of Things (IoT) is to enable systems across the globe to share data using advanced communication technologies. With the recent technological advancements, IoT-based solutions are no longer a challenging vision. IoT will offer numerous and potentially revolutionary benefits to today’s digital world. Future personalized and connected healthcare is one of the promising areas to see the benefits of IoT. This paper surveys emerging healthcare applications, including detailed technical aspects required for the realization of a complete end-to-end solution for each application. The survey explores the key application-specific requirements from the perspective of communication technologies. Furthermore, a detailed exploration from the existing to the emerging technologies and standards that would enable such applications is presented, highlighting the critical consideration of short-range and long-range communications. Finally, the survey highlights important open research challenges and issues specifically related to IoT-based future healthcare systems.","['Medical services', 'Communications technology', 'Biomedical monitoring', 'Standards', 'Monitoring', 'Security', 'Temperature measurement']","['Internet of Things', 'network communication technologies', 'personalized healthcare', 'wearable sensors', 'standards', 'challenges']"
"The very first infected novel coronavirus case (COVID-19) was found in Hubei, China in Dec. 2019. The COVID-19 pandemic has spread over 214 countries and areas in the world, and has significantly affected every aspect of our daily lives. At the time of writing this article, the numbers of infected cases and deaths still increase significantly and have no sign of a well-controlled situation, e.g., as of 13 July 2020, from a total number of around 13.1 million positive cases, 571,527 deaths were reported in the world. Motivated by recent advances and applications of artificial intelligence (AI) and big data in various areas, this paper aims at emphasizing their importance in responding to the COVID-19 outbreak and preventing the severe effects of the COVID-19 pandemic. We firstly present an overview of AI and big data, then identify the applications aimed at fighting against COVID-19, next highlight challenges and issues associated with state-of-the-art solutions, and finally come up with recommendations for the communications to effectively control the COVID-19 situation. It is expected that this paper provides researchers and communities with new insights into the ways AI and big data improve the COVID-19 situation, and drives further studies in stopping the COVID-19 outbreak.","['Artificial intelligence', 'Big Data', 'Drugs', 'Government', 'Viruses (medical)', 'Computational modeling', 'COVID-19']","['Artificial intelligence (AI)', 'big data', 'COVID-19', 'coronavirus', 'epidemic outbreak', 'deep learning', 'data analytics', 'machine learning']"
"The modern intelligent transportation system brings not only new opportunities for vehicular Internet of Things (IoT) services but also new challenges for vehicular ad-hoc networks (VANETs). Apart from enhanced network performance, a practical and reliable security scheme is needed to handle the trust management while preserving user privacy at the same time. The emerging 5G mobile communication system is viewed as a prominent technology for ultra-reliable, low-latency wireless communication services. Furthermore, incorporating software-defined network (SDN) architecture into the 5G-VANET enables global information gathering and network control. Hence, real-time IoT services on transportation monitoring and reporting can be well supported. Both pave the way for an innovative vehicular security scheme. This paper investigates the security and privacy issue in the transportation system and the vehicular IoT environment in SDN-enabled 5G-VANET. Due to the decentralized and immutable characteristics of blockchain, a blockchain-based security framework is designed to support the vehicular IoT services, i.e., real-time cloud-based video report and trust management on vehicular messages. This paper explicitly illustrates the SDN-enabled 5G-VANET model and the scheduling procedures of the blockchain-based framework. The numerical simulation results also show that malicious vehicular nodes or messages can be well detected while the overhead and impact on the network performance are acceptable for large-scale scenarios. Through case studies and theoretical analysis, we demonstrate our design substantially guarantees a secure and trustworthy vehicular IoT environment with user privacy preserved.","['Roads', 'Internet of Things', 'Blockchain', 'Trust management', 'Vehicular ad hoc networks', 'Privacy']","['Blockchain', '5G-VANET', 'IoT', 'security and privacy', 'SDN', 'trust']"
"Internet of everything (IoE)-based smart services are expected to gain immense popularity in the future, which raises the need for next-generation wireless networks. Although fifth-generation (5G) networks can support various IoE services, they might not be able to completely fulfill the requirements of novel applications. Sixth-generation (6G) wireless systems are envisioned to overcome 5G network limitations. In this article, we explore recent advances made toward enabling 6G systems. We devise a taxonomy based on key enabling technologies, use cases, emerging machine learning schemes, communication technologies, networking technologies, and computing technologies. Furthermore, we identify and discuss open research challenges, such as artificial-intelligence-based adaptive transceivers, intelligent wireless energy harvesting, decentralized and secure business models, intelligent cell-less architecture, and distributed security models. We propose practical guidelines including deep Q-learning and federated learning-based transceivers, blockchain-based secure business models, homomorphic encryption, and distributed-ledger-based authentication schemes to cope with these challenges. Finally, we outline and recommend several future directions.","['5G mobile communication', 'Tutorials', 'Wireless networks', 'Machine learning', 'Computational modeling']","['6G', '5G', 'Internet of Things', 'Internet of Everything', 'federated learning', 'meta learning', 'blockchain']"
"With the spread of Internet of Things' (IoT) applications, security has become extremely important. A recent distributed denial-of-service (DDoS) attack revealed the ubiquity of vulnerabilities in IoT, and many IoT devices unwittingly contributed to the DDoS attack. The emerging software-defined anything (SDx) paradigm provides a way to safely manage IoT devices. In this paper, we first present a general framework for software-defined Internet of Things (SD-IoT) based on the SDx paradigm. The proposed framework consists of a controller pool containing SD-IoT controllers, SD-IoT switches integrated with an IoT gateway, and IoT devices. We then propose an algorithm for detecting and mitigating DDoS attacks using the proposed SD-IoT framework, and in the proposed algorithm, the cosine similarity of the vectors of the packet-in message rate at boundary SD-IoT switch ports is used to determine whether DDoS attacks occur in the IoT. Finally, experimental results show that the proposed algorithm has good performance, and the proposed framework adapts to strengthen the security of the IoT with heterogeneous and vulnerable devices.","['Computer crime', 'Computer architecture', 'Internet of Things', 'Control systems', 'Logic gates', 'Cloud computing']","['Software-defined Internet of Things (SD-IoT)', 'distributed denial of service (DDoS)', 'attack detection', 'attack mitigation', 'cosine similarity']"
"The health condition of a wheelset bearing, the key component of a railway bogie, has a considerable impact on the safety of a train. Traditional bearing fault diagnosis techniques generally extract signals manually and then diagnose the bearing health conditions through the classifier. However, high-speed trains (HSTs) are usually faced with variable loads, variable speeds, and strong environmental noise, which pose a huge challenge to the application of the traditional bearing fault diagnosis methods in wheelset bearing fault diagnosis. Therefore, this paper proposes a 1D residual block, and based on the block, a novel deeper 1D convolutional neural network (Der-1DCNN) is proposed. The framework includes the idea of residual learning and can effectively learn high-level and abstract features while effectively alleviating the problem of training difficulty and the performance degradation of a deeper network. Additionally, for the first time, we fully use the wide convolution kernel and dropout technology to improve the model's ability to learn low-frequency signal features related to the fault components and to enhance the network's generalization performance. By constructing a deep residual learning network, Der-1DCNN can adaptively learn the deep fault features of the original vibration signal. This method not only achieves very high diagnostic accuracy for the fault diagnosis task of wheelset bearings in HSTs under strong noise environment, but also its performance is quite superior when the train's working load changes without any domain adaptation algorithm processing. The proposed Der-1DCNN is evaluated on the dataset of the multi-operating conditions of the wheelset bearings of HSTs. Experiments show that this method shows a better diagnostic performance compared with the state-of-the-art deep learning methods of bearing fault diagnosis, which proves the method's effectiveness and superiority.","['Fault diagnosis', 'Feature extraction', 'Convolution', 'Training', 'Kernel', 'Vibrations', 'Safety']","['High-speed trains', 'wheelset bearings fault diagnosis', 'deep learning', 'one-dimensional residual block', 'wide convolutional kernel']"
"The advent of the Industry 4.0 initiative has made it so that manufacturing environments are becoming more and more dynamic, connected but also inherently more complex, with additional inter-dependencies, uncertainties and large volumes of data being generated. Recent advances in Industrial Artificial Intelligence have showcased the potential of this technology to assist manufacturers in tackling the challenges associated with this digital transformation of Cyber-Physical Systems, through its data-driven predictive analytics and capacity to assist decision-making in highly complex, non-linear and often multistage environments. However, the industrial adoption of such solutions is still relatively low beyond the experimental pilot stage, as real environments provide unique and difficult challenges for which organizations are still unprepared. The aim of this paper is thus two-fold. First, a systematic review of current Industrial Artificial Intelligence literature is presented, focusing on its application in real manufacturing environments to identify the main enabling technologies and core design principles. Then, a set of key challenges and opportunities to be addressed by future research efforts are formulated along with a conceptual framework to bridge the gap between research in this field and the manufacturing industry, with the goal of promoting industrial adoption through a successful transition towards a digitized and data-driven company-wide culture. This paper is among the first to provide a clear definition and holistic view of Industrial Artificial Intelligence in the Industry 4.0 landscape, identifying and analysing its fundamental building blocks and ongoing trends. Its findings are expected to assist and empower researchers and manufacturers alike to better understand the requirements and steps necessary for a successful transition into Industry 4.0 supported by AI, as well as the challenges that may arise during this process.","['Artificial intelligence', 'Industries', 'Robots', 'Systematics', 'Manufacturing', 'Decision making', 'Service robots']","['Artificial intelligence', 'Industry 4.0', 'digital transformation', 'guidelines', 'systematic review', 'framework', 'manufacturing']"
"Electric load forecasting has always been a key component of power grids. Many countries have opened up electricity markets and facilitated the participation of multiple agents, which create a competitive environment and reduce costs to consumers. In the electricity market, multi-step short-term load forecasting becomes increasingly significant for electricity market bidding and spot price calculation, but the performances of traditional algorithms are not robust and unacceptable enough. In recent years, the rise of deep learning gives us the opportunity to improve the accuracy of multi-step forecasting further. In this paper, we propose a novel model multi-scale convolutional neural network with time-cognition (TCMS-CNN). At first, a deep convolutional neural network model based on multi-scale convolutions (MS-CNN) extracts different level features that are fused into our network. In addition, we design an innovative time coding strategy called the periodic coding strengthening the ability of the sequential model for time cognition effectively. At last, we integrate MS-CNN and periodic coding into the proposed TCMS-CNN model with an end-to-end training and inference process. With ablation experiments, the MS-CNN and periodic coding methods had better performances obviously than the most popular methods at present. Specifically, for 48-step point load forecasting, the TCMS-CNN had been improved by 34.73%, 14.22%, and 19.05% on MAPE than the state-of-the-art methods recursive multi-step LSTM (RM-LSTM), direct multi-step MS-CNN (DM-MS-CNN), and the direct multi-step GCNN (DM-GCNN), respectively. For 48-step probabilistic load forecasting, the TCMS-CNN had been improved by 3.54% and 6.77% on average pinball score than the DM-MS-CNN and the DM-GCNN. These results show a great promising potential applied in practice.","['Load forecasting', 'Load modeling', 'Forecasting', 'Feature extraction', 'Predictive models', 'Training', 'Neural networks']","['Short-term load forecasting', 'probabilistic load forecasting', 'multi-step', 'multi-scale convolution', 'time cognition', 'deep learning']"
"The ultra-reliable low latency communications (uRLLC) in the fifth generation mobile communication system aims to support diverse emerging applications with strict requirements of latency and reliability. Mobile edge computing (MEC) is considered as a promising solution to reduce the latency of computation-intensive tasks leveraging powerful computing units at short distance. The state-of-art work on task offloading to MEC mainly focuses on the tradeoff between latency and energy consumption, rather than reliability. In this paper, the tradeoff between the latency and reliability in task offloading to MEC is studied. A framework is provided, where user equipment partitions a task into sub-tasks and offloads them to multiple nearby edge nodes (ENs) in sequence. In this framework, we formulate an optimization problem to jointly minimize the latency and offloading failure probability. Since the formulated problem is nonconvex, we design three algorithms based on heuristic search, reformulation linearization technique and semi-definite relaxation, respectively, and solve the problem through optimizing EN candidates selection, offloading ordering and task allocation. Compared with the previous work, the numerical simulation results show that the proposed algorithms strike a good balance between the latency and reliability in uRLLC. Among them, the Heuristic Algorithm achieves the best performance in terms of the latency and reliability with the minimal complexity.","['Task analysis', 'Reliability', 'Algorithm design and analysis', 'Mobile communication', 'Energy consumption', 'Cloud computing', 'Optimization']","['5G', 'ultra-reliable low latency communications', 'mobile edge computing', 'computation offloading']"
"When will automated vehicles come onto the market? This question has puzzled the automotive industry and society for years. The technology and its implementation have made rapid progress over the last decade, but the challenge of how to prove the safety of these systems has not yet been solved. Since a market launch without proof of safety would neither be accepted by society nor by legislators, much time and many resources have been invested into safety assessment in recent years in order to develop new approaches for an efficient assessment. This paper therefore provides an overview of various approaches, and gives a comprehensive survey of the so-called scenario-based approach. The scenario-based approach is a promising method, in which individual traffic situations are typically tested by means of virtual simulation. Since an infinite number of different scenarios can theoretically occur in real-world traffic, even the scenario-based approach leaves the question unanswered as to how to break these down into a finite set of scenarios, and find those which are representative in order to render testing more manageable. This paper provides a comprehensive literature review of related safety-assessment publications that deal precisely with this question. Therefore, this paper develops a novel taxonomy for the scenario-based approach, and classifies all literature sources. Based on this, the existing methods will be compared with each other and, as one conclusion, the alternative concept of formal verification will be combined with the scenario-based approach. Finally, future research priorities are derived.","['Safety', 'Testing', 'Bibliographies', 'Vehicles', 'Microscopy', 'Taxonomy', 'Automation']","['Automated vehicles', 'autonomous vehicles', 'data analysis', 'formal verification', 'intelligent vehicles', 'key performance indicators', 'simulation', 'vehicle safety']"
"Detection of cyber attacks against vehicles is of growing interest. As vehicles typically afford limited processing resources, proposed solutions are rule-based or lightweight machine learning techniques. We argue that this limitation can be lifted with computational offloading commonly used for resource-constrained mobile devices. The increased processing resources available in this manner allow access to more advanced techniques. Using as case study a small four-wheel robotic land vehicle, we demonstrate the practicality and benefits of offloading the continuous task of intrusion detection that is based on deep learning. This approach achieves high accuracy much more consistently than with standard machine learning techniques and is not limited to a single type of attack or the in-vehicle CAN bus as previous work. As input, it uses data captured in real-time that relate to both cyber and physical processes, which it feeds as time series data to a neural network architecture. We use both a deep multilayer perceptron and recurrent neural network architecture, with the latter benefitting from a long-short term memory hidden layer, which proves very useful for learning the temporal context of different attacks. We employ denial of service, command injection and malware as examples of cyber attacks that are meaningful for a robotic vehicle. The practicality of computation offloading depends on the resources afforded onboard and remotely, and the reliability of the communication means between them. Using detection latency as the criterion, we have developed a mathematical model to determine when computation offloading is beneficial given parameters related to the operation of the network and the processing demands of the deep learning model. The more reliable the network and the greater the processing demands, the greater the reduction in detection latency achieved through offloading.","['Robot sensing systems', 'Intrusion detection', 'Monitoring', 'Aircraft', 'Machine learning']","['Intrusion detection', 'machine learning', 'autonomous vehicles']"
"This paper aims to explore and investigate the potential factors influencing students' behavioral intentions to use the e-learning system. This paper proposes an extended technology acceptance model (TAM) that has been tested and examined through the use of both innovation diffusion theory (IDT) and integrating TAM. This paper was conducted on 1286 students utilizing systems of e-learning in Malaysia. The findings were obtained via a quantitative research method. The findings illustrate that six perceptions of innovation characteristics, in particular, have impacts on students' e-learning system behavioral intention. The influences of the relative advantages, observability, trialability, perceived compatibility, complexity, and perceived enjoyment on the perceived ease of use is noteworthy. Moreover, the effects of the relative advantages, complexity, trialability, observability, perceived compatibility, and perceived enjoyment on the perceived usefulness have a strong impact. Therefore, the empirical results provide strong backing to the integrative approach between TAM and IDT. The findings suggest an extended model of TAM with IDT for the acceptance of the e-learning system used to improve the students' learning performance, which can help decision makers in higher education, universities, as well as colleges to evaluate, plan and execute the use of e-learning systems.","['Electronic learning', 'Technological innovation', 'Observability', 'Complexity theory', 'Learning systems', 'Training']","['Technology acceptance model (TAM)', 'innovation diffusion', 'theory (IDT)', 'E-learning system', 'structural equation modeling', 'system adoption', 'end-students’ perception']"
"In this paper, we provide the theoretical framework for the performance comparison of reconfigurable intelligent surfaces (RISs) and amplify-and-forward (AF) relaying wireless systems. In particular, after statistically characterizing the end-to-end (e2e) wireless channel coefficient of the RIS-assisted wireless system, in terms of probability density function (PDF) and cumulative density function (CDF), we extract novel closed-form expressions for the instantaneous and average e2e signal-to-noise ratio (SNR) for both the RIS-assisted and AF-relaying wireless systems. Building upon these expressions, we derive the diversity gain of the RIS-assisted wireless system as well as the outage probability (OP) and symbol error rate (SER) for a large variety of Gray-mapped modulation schemes of both systems under investigation. Additionally, the diversity order of the RIS-assisted wireless system is presented as well as the ergodic capacity (EC) of both the RIS-assisted and AF-relaying wireless systems. Likewise, high-SNR and high-number of metasurfaces (MS) approximations for the SER and EC for the RIS-assisted wireless system are reported. Finally, for the sake of completeness, the special case in which the RIS is equipped with only one MS is also investigated. For this case, the instantaneous and average e2e SNR are derived, as well as the OP, SER and EC. Our analysis is verified through respective Monte Carlo simulations, which reveal the accuracy of the presented theoretical framework. Moreover, our results highlight that, in general, RIS-assisted wireless systems outperform the corresponding AF-relaying ones in terms of average SNR, OP, SER and EC.","['Wireless communication', 'Signal to noise ratio', 'Closed-form solutions', 'Probability density function', 'Power system reliability', 'Probability', 'Error analysis']","['Amplify-and-forward', 'average signal-to-noise-ratio', 'beyond 5G systems', 'ergodic capacity', 'high-signal-to-noise-ratio approximation', 'meta-surfaces', 'multipath fading', 'outage probability', 'performance analysis', 'reconfigurable intelligent surfaces', 'symbol error rate', 'theoretical framework']"
"Topic extraction is an essential task in bibliometric data analysis, data mining and knowledge discovery, which seeks to identify significant topics from text collections. The conventional topic extraction schemes require human intervention and involve also comprehensive pre-processing tasks to represent text collections in an appropriate way. In this paper, we present a two-stage framework for topic extraction from scientific literature. The presented scheme employs a two-staged procedure, where word embedding schemes have been utilized in conjunction with cluster analysis. To extract significant topics from text collections, we propose an improved word embedding scheme, which incorporates word vectors obtained by word2vec, POS2vec, word-position2vec and LDA2vec schemes. In the clustering phase, an improved clustering ensemble framework, which incorporates conventional clustering methods (i.e., k-means, k-modes, k-means++, self-organizing maps and DIANA algorithm) by means of the iterative voting consensus, has been presented. In the empirical analysis, we analyze a corpus containing 160,424 abstracts of articles from various disciplines, including agricultural engineering, economics, engineering and computer science. In the experimental analysis, performance of the proposed scheme has been compared to conventional baseline clustering methods (such as, k-means, k-modes, and k-means++), LDA-based topic modelling and conventional word embedding schemes. The empirical analysis reveals that ensemble word embedding scheme yields better predictive performance compared to the baseline word vectors for topic extraction. Ensemble clustering framework outperforms the baseline clustering methods. The results obtained by the proposed framework show an improvement in Jaccard coefficient, Folkes & Mallows measure and F1 score.","['Task analysis', 'Clustering algorithms', 'Text categorization', 'Data analysis', 'Clustering methods', 'Text mining']","['topic extraction', 'machine learning', 'Cluster analysis', 'text mining']"
"The ever-increasing advancement in communication technologies of modern smart objects brings with it a newera of application development for Internet of Things (IoT)-based networks. In particular, owing to the contactless-ness nature and efficiency of the data retrieval of mobile smart objects, such as wearable equipment or tailored bio-sensors, several innovative types of healthcare systems with body sensor networks (BSN) have been proposed. In this paper, we introduce a secure IoT-based healthcare system, which operates through the BSN architecture. To simultaneously achieve system efficiency and robustness of transmission within public IoT-based communication networks, we utilize robust crypto-primitives to construct two communication mechanisms for ensuring transmission confidentiality and providing entity authentication among smart objects, the local processing unit and the backend BSN server. Moreover, we realize the implementation of the proposed healthcare system with the Raspberry PI platform to demonstrate the practicability and feasibility of the presented mechanisms.","['Smart devices', 'Authentication', 'Body sensor networks', 'Network security', 'Internet of things']","['Authentication', 'body sensor networks', 'internet of things (IoT)', 'security']"
"Drowsiness or fatigue is a major cause of road accidents and has significant implications for road safety. Several deadly accidents can be prevented if the drowsy drivers are warned in time. A variety of drowsiness detection methods exist that monitor the drivers' drowsiness state while driving and alarm the drivers if they are not concentrating on driving. The relevant features can be extracted from facial expressions such as yawning, eye closure, and head movements for inferring the level of drowsiness. The biological condition of the drivers' body, as well as vehicle behavior, is analyzed for driver drowsiness detection. This paper presents a comprehensive analysis of the existing methods of driver drowsiness detection and presents a detailed analysis of widely used classification techniques in this regard. First, in this paper, we classify the existing techniques into three categories: behavioral, vehicular, and physiological parameters-based techniques. Second, top supervised learning techniques used for drowsiness detection are reviewed. Third, the pros and cons and comparative study of the diverse method are discussed. In addition, the research frameworks are elaborated in diagrams for better understanding. In the end, overall research findings based on the extensive survey are concluded which will help young researchers for finding potential future work in the relevant field.","['Vehicles', 'Fatigue', 'Support vector machines', 'Mouth', 'Road accidents', 'Systematics']","['Digital image processing', 'driver drowsiness', 'sensors', 'fatigue detection', 'supervised learning', 'classification', 'support vector machine (SVM)']"
"With the rise of artificial intelligence (AI) and deep learning techniques, fake digital contents have proliferated in recent years. Fake footage, images, audios, and videos (known as deepfakes) can be a scary and dangerous phenomenon and can have the potential of altering the truth and eroding trust by giving false reality. Proof of authenticity (PoA) of digital media is critical to help eradicate the epidemic of forged content. Current solutions lack the ability to provide history tracking and provenance of digital media. In this paper, we provide a solution and a general framework using Ethereum smart contracts to trace and track the provenance and history of digital content to its original source even if the digital content is copied multiple times. The smart contract utilizes the hashes of the interplanetary file system (IPFS) used to store digital content and its metadata. Our solution focuses on video content, but the solution framework provided in this paper is generic enough and can be applied to any other form of digital content. Our solution relies on the principle that if the content can be credibly traced to a trusted or reputable source, the content can then be real and authentic. The full code of the smart contract has been made publicly available at Github.","['Blockchain', 'Smart contracts', 'History', 'Metadata', 'Artificial intelligence']","['AI', 'deepfake', 'blockchain', 'Ethereum', 'smart contracts']"
"The new development trends including Internet of Things (IoT), smart city, enterprises digital transformation and world’s digital economy are at the top of the tide. The continuous growth of data storage pressure drives the rapid development of the entire storage market on account of massive data generated. By providing data storage and management, cloud storage system becomes an indispensable part of the new era. Currently, the governments, enterprises and individual users are actively migrating their data to the cloud. Such a huge amount of data can create magnanimous wealth. However, this increases the possible risk, for instance, unauthorized access, data leakage, sensitive information disclosure and privacy disclosure. Although there are some studies on data security and privacy protection, there is still a lack of systematic surveys on the subject in cloud storage system. In this paper, we make a comprehensive review of the literatures on data security and privacy issues, data encryption technology, and applicable countermeasures in cloud storage system. Specifically, we first make an overview of cloud storage, including definition, classification, architecture and applications. Secondly, we give a detailed analysis on challenges and requirements of data security and privacy protection in cloud storage system. Thirdly, data encryption technologies and protection methods are summarized. Finally, we discuss several open research topics of data security for cloud storage.","['Cloud computing', 'Encryption', 'Data privacy', 'Secure storage', 'Memory']","['Cloud storage', 'data security', 'cryptography', 'access control', 'privacy protection']"
"In the last decades, fiber Bragg gratings (FBGs) have become increasingly attractive to medical applications due to their unique properties such as small size, biocompatibility, immunity to electromagnetic interferences, high sensitivity and multiplexing capability. FBGs have been employed in the development of surgical tools, assistive devices, wearables, and biosensors, showing great potentialities for medical uses. This paper reviews the FBG-based measuring systems, their principle of work, and their applications in medicine and healthcare. Particular attention is given to sensing solutions for biomechanics, minimally invasive surgery, physiological monitoring, and medical biosensing. Strengths, weaknesses, open challenges, and future trends are also discussed to highlight how FBGs can meet the demands of next-generation medical devices and healthcare system.","['Fiber gratings', 'Strain', 'Optical fiber sensors', 'Medical services', 'Temperature measurement']","['Biomechanics', 'biosensing', 'fiber Bragg grating sensors', 'minimally invasive surgery', 'physiological monitoring']"
"A number of impedance-based fault location algorithms have been developed for estimating the distance to faults in a transmission network. Each algorithm has specific input data requirements and makes certain assumptions that may or may not hold true in a particular fault location scenario. Without a detailed understanding of the principle of each fault-locating method, choosing the most suitable fault location algorithm can be a challenging task. This paper, therefore, presents the theory of one-ended (simple reactance, Takagi, modified Takagi, Eriksson, and Novosel et al.) and two-ended (synchronized, unsynchronized, and current-only) impedance-based fault location algorithms and demonstrates their application in locating real-world faults. The theory details the formulation and input data requirement of each fault-locating algorithm and evaluates the sensitivity of each to the following error sources: 1) load; 2) remote infeed; 3) fault resistance; 4) mutual coupling; 5) inaccurate line impedances; 6) DC offset and CT saturation; 7) three-terminal lines; and 8) tapped radial lines. From the theoretical analysis and field data testing, the following criteria are recommended for choosing the most suitable fault-locating algorithm: 1) data availability and 2) fault location application scenario. Another objective of this paper is to assess what additional information can be gleaned from waveforms recorded by intelligent electronic devices (IEDs) during a fault. Actual fault event data captured in utility networks is exploited to gain valuable feedback about the transmission network upstream from the IED device, and estimate the value of fault resistance.","['Fault location', 'Algorithm design and analysis', 'Resistance', 'Impedance', 'Synchronization', 'Mutual coupling', 'Fault currents', 'Estimation', 'Transmission lines']","['Fault location', 'impedance-measurement', 'intelligent electronic devices (IED)', 'power system faults', 'power system reliability', 'transmission line measurements']"
"The electrocardiogram (ECG) is an efficient and noninvasive indicator for arrhythmia detection and prevention. In real-world scenarios, ECG signals are prone to be contaminated with various noises, which may lead to wrong interpretation. Therefore, significant attention has been paid on denoising of ECG for accurate diagnosis and analysis. A denoising autoencoder (DAE) can be applied to reconstruct the clean data from its noisy version. In this paper, a DAE using the fully convolutional network (FCN) is proposed for ECG signal denoising. Meanwhile, the proposed FCN-based DAE can perform compression with regard to the DAE architecture. The proposed approach is applied to ECG signals from the MIT-BIH Arrhythmia database and the added noise signals are obtained from the MIT-BIH Noise Stress Test database. The denoising performance is evaluated using the root-mean-square error (RMSE), percentage-root-mean-square difference (PRD), and improvement in signal-to-noise ratio (SNR imp ). The results of the experiments conducted on noisy ECG signals of different levels of input SNR show that the FCN acquires better performance as compared to the deep fully connected neural network- and convolutional neural network-based denoising models. Moreover, the proposed FCN-based DAE reduces the size of the input ECG signals, where the compressed data is 32 times smaller than the original. The results of the study demonstrate the superiority of FCN in denoising, with lower RMSE and PRD, as well as higher SNR imp . According to the results, we believe that the proposed FCN-based DAE has a good application prospect in clinical practice.","['Electrocardiography', 'Noise reduction', 'Convolution', 'Decoding', 'Noise measurement', 'Databases', 'Signal to noise ratio']","['Electrocardiography', 'signal denoising', 'artificial neural networks', 'denoising autoencoders', 'fully convolutional network']"
"Optimal allocation of distributed generation units is essential to ensure power loss minimization, while meeting the real and reactive power demands in a distribution network. This paper proposes a solution to this non-convex, discrete problem by using the hybrid grey wolf optimizer, a new metaheuristic algorithm. This algorithm is applied to IEEE 33-, IEEE 69-, and Indian 85-bus radial distribution systems to minimize the power loss. The results show that there is a considerable reduction in the power loss and an enhancement of the voltage profile of the buses across the network. Comparisons show that the proposed method outperforms all other metaheuristic methods, and matches the best results by other methods, including exhaustive search, suggesting that the solution obtained is a global optimum. Furthermore, unlike for most other metaheuristic methods, this is achieved with no tuning of the algorithm on the part of the user, except for the specification of the population size.","['Xenon', 'Resource management', 'Reactive power', 'Distributed power generation', 'Hybrid power systems', 'Tuning', 'Particle swarm optimization']","['Distributed generation (DG)', 'optimal DG location', 'optimal DG size', 'loss minimization', 'radial distribution system', 'metaheuristic algorithm']"
"Blockchain is the promising technology of recent years, which has attracted remarkable attention in both academic studies and practical industrial applications. The smart contract is a programmable transaction that can perform a sophisticated task, execute automatically, and store on the blockchain. The smart contract is the key component of the blockchain, which has made blockchain a technology beyond the scope of the cryptocurrencies and applicable for a variety of applications such as healthcare, IoT, supply chain, digital identity, business process management, and more. Although in recent years the progress toward improving blockchain technology with the focus on the smart contract has been impressive, there is a lack of reviewing the smart contract topic. This paper systematically reviews the key concepts and proposes the direction of recent studies and developments regarding the smart contract. The research studies are presented in three main categories: 1) security methods and tools; 2) performance improvement approaches; and 3) decentralized applications based on smart contracts.","['Smart contracts', 'Blockchain', 'Peer-to-peer computing', 'Systematics', 'Bitcoin']","['Smart contract', 'blockchain', 'review', 'security', 'performance', 'application']"
"In recent years, computation offloading has become an effective way to overcome the constraints of mobile devices (MDs) by offloading delay-sensitive and computation-intensive mobile application tasks to remote cloud-based data centers. Smart cities can benefit from offloading to edge points in the framework of the so-called cyber-physical-social systems (CPSS), as for example in traffic violation tracking cameras. We assume that there are mobile edge computing networks (MECNs) in more than one region, and they consist of multiple access points, multi-edge servers, and N MDs, where each MD has M independent real-time massive tasks. The MDs can connect to a MECN through the access points or the mobile network. Each task be can processed locally by the MD itself or remotely. There are three offloading options: nearest edge server, adjacent edge server, and remote cloud. We propose a reinforcement-learning-based state-action-reward-state-action (RL-SARSA) algorithm to resolve the resource management problem in the edge server, and make the optimal offloading decision for minimizing system cost, including energy consumption and computing time delay. We call this method OD-SARSA (offloading decision-based SARSA). We compared our proposed method with reinforcement learning based Q learning (RL-QL), and it is concluded that the performance of the former is superior to that of the latter.",[],[]
"As an increasing attention towards sustainable development of energy and environment, the power electronics (PEs) are gaining more and more attraction on various energy systems. The insulated gate bipolar transistor (IGBT), as one of the PEs with numerous advantages and potentials for development of higher voltage and current ratings, has been used in a board range of applications. However, the continuing miniaturization and rapid increasing power ratings of IGBTs have remarkable high heat flux, which requires complex thermal management. In this paper, studies of the thermal management on IGBTs are generally reviewed including analyzing, comparing, and classifying the results originating from these researches. The thermal models to accurately calculate the dynamic heat dissipation are divided into analytical models, numerical models, and thermal network models, respectively. The thermal resistances of current IGBT modules are also studied. According to the current products on a number of IGBTs, we observe that the junction-to-case thermal resistance generally decreases inversely in terms of the total thermal power. In addition, the cooling solutions of IGBTs are reviewed and the performance of the various solutions are studied and compared. At last, we have proposed a quick and efficient evaluation judgment for the thermal management of the IGBTs depended on the requirements on the junction-to-case thermal resistance and equivalent heat transfer coefficient of the test samples.","['Insulated gate bipolar transistors', 'Thermal analysis', 'Thermal management', 'Thermal resistance', 'Thermal management of electronics', 'Heating systems', 'Stress']","['Power electronics', 'IGBT', 'thermal management', 'cooling', 'qualifications']"
"In the downlink transmission scenario, power allocation and beamforming design at the transmitter are essential when using multiple antenna arrays. This paper considers a multiple input–multiple output broadcast channel to maximize the weighted sum-rate under the total power constraint. The classical weighted minimum mean-square error (WMMSE) algorithm can obtain suboptimal solutions but involves high computational complexity. To reduce this complexity, we propose a fast beamforming design method using unsupervised learning, which trains the deep neural network (DNN) offline and provides real-time service online only with simple neural network operations. The training process is based on an end-to-end method without labeled samples avoiding the complicated process of obtaining labels. Moreover, we use the “APoZ”-based pruning algorithm to compress the network volume, which further reduces the computational complexity and volume of the DNN, making it more suitable for low computation-capacity devices. Finally, the experimental results demonstrate that the proposed method improves computational speed significantly with performance close to the WMMSE algorithm.","['Array signal processing', 'MIMO communication', 'Downlink', 'Neurons', 'Biological neural networks']","['MIMO', 'beamforming', 'deep learning', 'unsupervised learning', 'network pruning']"
"Research and development on the next generation wireless systems, namely 5G, has experienced explosive growth in recent years. In the physical layer, the massive multiple-input-multiple output (MIMO) technique and the use of high GHz frequency bands are two promising trends for adoption. Millimeter-wave (mmWave) bands, such as 28, 38, 64, and 71 GHz, which were previously considered not suitable for commercial cellular networks, will play an important role in 5G. Currently, most 5G research deals with the algorithms and implementations of modulation and coding schemes, new spatial signal processing technologies, new spectrum opportunities, channel modeling, 5G proof of concept systems, and other system-level enabling technologies. In this paper, we first investigate the contemporary wireless user equipment (UE) hardware design, and unveil the critical 5G UE hardware design constraints on circuits and systems. On top of the said investigation and design tradeoff analysis, a new, highly reconfigurable system architecture for 5G cellular user equipment, namely distributed phased arrays based MIMO (DPA-MIMO) is proposed. Finally, the link budget calculation and data throughput numerical results are presented for the evaluation of the proposed architecture.","['5G mobile communication', 'Wireless communication', 'MIMO', 'Batteries', 'Hardware', 'Radio frequency', 'Mobile handsets']","['5G', 'massive multiple-input-multiple-output (MIMO)', 'millimeter-wave (mmWave)', 'beamforming', 'distributed phased array', 'user equipment (UE)', 'hardware', 'system-on-chip (SoC)', 'spectral efficiency']"
"The intelligent transportation system (ITS) concept was introduced to increase road safety, manage traffic efficiently, and preserve our green environment. Nowadays, ITS applications are becoming more data-intensive and their data are described using the “5Vs of Big Data”. Thus, to fully utilize such data, big data analytics need to be applied. The Internet of vehicles (IoV) connects the ITS devices to cloud computing centres, where data processing is performed. However, transferring huge amount of data from geographically distributed devices creates network overhead and bottlenecks, and it consumes the network resources. In addition, following the centralized approach to process the ITS big data results in high latency which cannot be tolerated by the delay-sensitive ITS applications. Fog computing is considered a promising technology for real-time big data analytics. Basically, the fog technology complements the role of cloud computing and distributes the data processing at the edge of the network, which provides faster responses to ITS application queries and saves the network resources. However, implementing fog computing and the lambda architecture for real-time big data processing is challenging in the IoV dynamic environment. In this regard, a novel architecture for real-time ITS big data analytics in the IoV environment is proposed in this paper. The proposed architecture merges three dimensions including intelligent computing (i.e. cloud and fog computing) dimension, real-time big data analytics dimension, and IoV dimension. Moreover, this paper gives a comprehensive description of the IoV environment, the ITS big data characteristics, the lambda architecture for real-time big data analytics, several intelligent computing technologies. More importantly, this paper discusses the opportunities and challenges that face the implementation of fog computing and real-time big data analytics in the IoV environment. Finally, the critical issues and future research directions section discusses some issues that should be considered in order to efficiently implement the proposed architecture.","['Big Data', 'Real-time systems', 'Edge computing', 'Computer architecture', 'Transportation', 'Cloud computing']","['Vehicular and wireless technologies', 'intelligent transportation systems', 'data preprocessing', 'real time systems', 'ubiquitous computing']"
"In this paper, we propose a new chaos-based encryption scheme for medical images. It is based on a combination of chaos and DNA computing under the scenario of two encryption rounds, preceded by a key generation layer, and follows the permutation-substitution-diffusion structure. The SHA-256 hash function alongside the initial secret keys is employed to produce the secret keys of the chaotic systems. Each round of the proposed algorithm involves six steps, i.e., block-based permutation, pixel-based substitution, DNA encoding, bit-level substitution (i.e., DNA complementing), DNA decoding, and bit-level diffusion. A thorough search of the relevant literature yielded only this time the pixel-based substitution and the bit-level substitution are used in cascade for image encryption. The key-streams in the bit-level substitution are based on the logistic-Chebyshev map, while the sine-Chebyshev map allows producing the key-streams in the bit-level diffusion. The final encrypted image is obtained by repeating once the previous steps using new secret keys. Security analyses and computer simulations both confirm that the proposed scheme is robust enough against all kinds of attacks. Its low complexity indicates its high potential for real-time and secure image applications.","['DNA', 'Encryption', 'Biomedical imaging', 'Encoding', 'Chaos']","['Image encryption', 'medical images', 'permutation and diffusion', 'S-box', 'chaos', 'DNA encoding', 'SHA-256 hash function']"
"With ongoing large-scale smart energy metering deployments worldwide, disaggregation of a household's total energy consumption down to individual appliances using analytical tools, also known as non-intrusive appliance load monitoring (NALM), has generated increased research interest lately. NALM can deepen energy feedback, support appliance retrofit advice, and support home automation. However, despite the fact that NALM was proposed over 30 years ago, there are still many open challenges with respect to its practicality and effectiveness at low sampling rates. Indeed, the majority of NALM approaches, supervised or unsupervised, require training to build appliance models, and are sensitive to appliance changes in the house, thus requiring regular re-training. In this paper, we tackle this challenge by proposing an NALM approach that does not require any training. The main idea is to build upon the emerging field of graph signal processing to perform adaptive thresholding, signal clustering, and pattern matching. We determine the performance limits of our approach and demonstrate its usefulness in practice. Using two open access datasets-the US REDD data set with active power measurements downsampled to 1 min resolution and the UK REFIT data set with 8-s resolution, we demonstrate the effectiveness of the proposed method for typical smart meter sampling rate, with the state-of-the-art supervised and unsupervised NALM approaches as benchmarks.","['Home appliances', 'Training', 'Signal resolution', 'Energy consumption', 'Home automation', 'Load management', 'Adaptation models', 'Power system measurements']","['Non-intrusive appliance load monitoring', 'load disaggregation', 'graph signal processing']"
"Integration of the Internet into the entities of the different domains of human society (such as smart homes, health care, smart grids, manufacturing processes, product supply chains, and environmental monitoring) is emerging as a new paradigm called the Internet of Things (IoT). However, the ubiquitous and wide-range IoT networks make them prone to cyberattacks. One of the main types of attack is a denial of service (DoS), where the attacker floods the network with a large volume of data to prevent nodes from using the services. An intrusion detection mechanism is considered a chief source of protection for information and communications technology. However, conventional intrusion detection methods need to be modified and improved for application to the IoT owing to certain limitations, such as resource-constrained devices, the limited memory and battery capacity of nodes, and specific protocol stacks. In this paper, we develop a lightweight attack detection strategy utilizing a supervised machine learning-based support vector machine (SVM) to detect an adversary attempting to inject unnecessary data into the IoT network. The simulation results show that the proposed SVM-based classifier, aided by a combination of two or three incomplex features, can perform satisfactorily in terms of classification accuracy and detection time.","['Internet of Things', 'Intrusion detection', 'Feature extraction', 'Support vector machines', 'Protocols', 'Sensor phenomena and characterization']","['Intrusion detection system', 'anomaly detection', 'Internet of Things', 'support vector machine']"
"Nowadays, telemedicine is an emerging healthcare service where the healthcare professionals can diagnose, evaluate, and treat a patient using telecommunication technology. To diagnose and evaluate a patient, the healthcare professionals need to access the electronic medical record (EMR) of the patient, which might contain huge multimedia big data including X-rays, ultrasounds, CT scans, and MRI reports. For efficient access and supporting mobility for both the healthcare professionals as well as the patients, the EMR needs to be kept in big data storage in the healthcare cloud. In spite of the popularity of the healthcare cloud, it faces different security issues; for instance, data theft attacks are considered to be one of the most serious security breaches of healthcare data in the cloud. In this paper, the main focus has been given to secure healthcare private data in the cloud using a fog computing facility. To this end, a tri-party one-round authenticated key agreement protocol has been proposed based on the bilinear pairing cryptography that can generate a session key among the participants and communicate among them securely. Finally, the private healthcare data are accessed and stored securely by implementing a decoy technique.","['Cloud computing', 'Medical services', 'Edge computing', 'Big Data', 'Cryptography']","['Key management', 'security and privacy', 'medical big data', 'fog computing', 'pairing-based cryptography', 'decoy technique']"
"In the era of big data, recommender system (RS) has become an effective information filtering tool that alleviates information overload for Web users. Collaborative filtering (CF), as one of the most successful recommendation techniques, has been widely studied by various research institutions and industries and has been applied in practice. CF makes recommendations for the current active user using lots of users’ historical rating information without analyzing the content of the information resource. However, in recent years, data sparsity and high dimensionality brought by big data have negatively affected the efficiency of the traditional CF-based recommendation approaches. In CF, the context information, such as time information and trust relationships among the friends, is introduced into RS to construct a training model to further improve the recommendation accuracy and user’s satisfaction, and therefore, a variety of hybrid CF-based recommendation algorithms have emerged. In this paper, we mainly review and summarize the traditional CF-based approaches and techniques used in RS and study some recent hybrid CF-based recommendation approaches and techniques, including the latest hybrid memory-based and model-based CF recommendation algorithms. Finally, we discuss the potential impact that may improve the RS and future direction. In this paper, we aim at introducing the recent hybrid CF-based recommendation techniques fusing social networks to solve data sparsity and high dimensionality and provide a novel point of view to improve the performance of RS, thereby presenting a useful resource in the state-of-the-art research result for future researchers.","['Recommender systems', 'Collaboration', 'Social network services', 'Prediction algorithms', 'Big Data', 'Predictive models']","['Recommender systems', 'collaborative filtering', 'matrix factorization', 'singular value decomposition', 'trust-aware collaborative filtering', 'social networks']"
"There is an increasing interest and research effort focused on the analysis, design and implementation of distributed control systems for AC, DC and hybrid AC / DC microgrids. It is claimed that distributed controllers have several advantages over centralised control schemes, e.g., improved reliability, flexibility, controllability, black start operation, robustness to failure in the communication links, etc. In this work, an overview of the state-of-the-art of distributed cooperative control systems for isolated microgrids is presented. Protocols for cooperative control such as linear consensus, heterogeneous consensus and finite-time consensus are discussed and reviewed in this paper. Distributed cooperative algorithms for primary and secondary control systems, including (among others issues) virtual impedance, synthetic inertia, droop-free control, stability analysis, imbalance sharing, total harmonic distortion regulation, are also reviewed and discussed in this survey. Tertiary control systems, e.g., for economic dispatch of electric energy, based on cooperative control approaches, are also addressed in this work. This review also highlights existing issues, research challenges and future trends in distributed cooperative control of microgrids and their future applications.","['Decentralized control', 'Stability analysis', 'Topology', 'Power system stability', 'Microgrids', 'Asymptotic stability']","['AC-microgrid', 'consensus', 'DC-microgrid', 'distributed control', 'hierarchical control', 'hybrid-microgrid', 'microgrids']"
"In the last decade, Human Activity Recognition (HAR) has become a vibrant research area, especially due to the spread of electronic devices such as smartphones, smartwatches and video cameras present in our daily lives. In addition, the advance of deep learning and other machine learning algorithms has allowed researchers to use HAR in various domains including sports, health and well-being applications. For example, HAR is considered as one of the most promising assistive technology tools to support elderly's daily life by monitoring their cognitive and physical function through daily activities. This survey focuses on critical role of machine learning in developing HAR applications based on inertial sensors in conjunction with physiological and environmental sensors.","['Deep learning', 'Computational modeling', 'Activity recognition', 'Data models', 'Sensors', 'Biomedical monitoring', 'Smart phones']","['Human activity recognition (HAR)', 'deep learning (DL)', 'machine learning (ML)', 'available datasets', 'sensors', 'accelerometer']"
"Prognostics and systems health management (PHM) is an enabling discipline that uses sensors to assess the health of systems, diagnoses anomalous behavior, and predicts the remaining useful performance over the life of the asset. The advent of the Internet of Things (IoT) enables PHM to be applied to all types of assets across all sectors, thereby creating a paradigm shift that is opening up significant new business opportunities. This paper introduces the concepts of PHM and discusses the opportunities provided by the IoT. Developments are illustrated with examples of innovations from manufacturing, consumer products, and infrastructure. From this review, a number of challenges that result from the rapid adoption of IoT-based PHM are identified. These include appropriate analytics, security, IoT platforms, sensor energy harvesting, IoT business models, and licensing approaches.","['Prognostics and health management', 'Sensors', 'Internet of things', 'Maintenance engineering', 'Reliability', 'STEM', 'Degradation']","['Internet of things', 'maintenance', 'prognostics and systems health management', 'reliability', 'remaining useful life']"
"Cyber-physical-social system (CPSS) has drawn tremendous attention in industrial applications such as industrial Internet of Things (IIoT). As the fundamental component of IIoT, bearings play an increasingly important role in CPSS for IIoT. Better understanding of bearing working conditions and degradation patterns so as to more accurately predict the remaining useful life (RUL), becomes an urgent demand for industrial prognostics in IIoT. The data-driven approach has indicated good potential, but the prediction accuracy is still not satisfactory. This paper proposes a new method for the prediction of bearing RUL based on deep convolution neural network (CNN). A new feature extraction method is presented to obtain the eigenvector, named the spectrum-principal-energy-vector. The eigenvector is suitable for deep CNN. In the prediction phase, we propose a smoothing method to deal with the discontinuity problem found in the prediction results. To the best of our knowledge, we are the first to propose such a smoothing method for bearing RUL prediction. Experiments show that our method can significantly improve the prediction accuracy of bearing RUL.","['Vibrations', 'Feature extraction', 'Frequency-domain analysis', 'Neural networks', 'Predictive models', 'Convolution', 'Time-domain analysis']","['Cyber-physical-social system', 'industrial big data', 'deep learning', 'RUL prediction', 'deep convolution neural network']"
"In this paper, atom search optimization (ASO) algorithm and a novel chaotic version of it [chaotic ASO (ChASO)] are proposed to determine the optimal parameters of the fractional-order proportional+integral+derivative (FOPID) controller for dc motor speed control. The ASO algorithm is simple and easy to implement, which mathematically models and mimics the atomic motion model in nature, and is developed to address a diverse set of optimization problems. The proposed ChASO algorithm, on the other hand, is based on logistic map chaotic sequences, which makes the original algorithm be able to escape from local minima stagnation and improve its convergence rate and resulting precision. First, the proposed ChASO algorithm is applied to six unimodal and multimodal benchmark optimization problems and the results are compared with other algorithms. Second, the proposed ChASO-FOPID, ASO-FOPID, and ASO-PID controllers are compared with GWO-FOPID, GWO-PID, IWO-PID, and SFS-PID controllers using the integral of time multiplied absolute error (ITAE) objective function for a fair comparison. Comparisons were also made for the integral of time multiplied squared error (ITSE) and Zwe-Lee Gaing's (ZLG) objective function as the most commonly used objective functions in the literature. Transient response analysis, frequency response (Bode) analysis, and robustness analysis were all carried out. The simulation results are promising and validate the effectiveness of the proposed approaches. The numerical simulations of the proposed ChASO-FOPID and ASO-FOPID controllers for the dc motor speed control system demonstrated the superior performance of both the chaotic ASO and the original ASO, respectively.","['Optimization', 'Heuristic algorithms', 'Logistics', 'Tuning', 'Linear programming', 'DC motors', 'Velocity control']","['DC motor speed control', 'fractional order PID controller', 'chaotic atom search optimization algorithm', 'robustness analysis', 'transient response']"
"The achievable performance of subcarrier-index modulation (SIM) is analyzed in terms of its minimum Euclidean distance, constrained and unconstrained average mutual information, as well as its peak-to-average power ratio (PAPR). Our performance investigations identify the beneficial operating region of the SIM scheme over its conventional orthogonal frequency-division multiplexing (OFDM) counterpart, hence providing general design guidelines for the SIM parameters. More specifically, an SIM scheme is shown to be beneficial for the scenario of a relatively low transmission rate below 2 b/s/Hz. In addition, we demonstrate that the PAPR of the SIM scheme is comparable with that of its OFDM counterpart under the idealized simplifying assumption of having Gaussian input symbols.","['OFDM', 'Subcarrier indexes', 'Parallel processing', 'Peak to average power ratio', 'Capacity planning', 'Spatial modulation']","['Capacity', 'EXIT chart', 'index modulation', 'mutual information', 'OFDM', 'parallel combinatory', 'peak-to-average power ratio', 'spatial modulation', 'subcarrier-index modulation']"
"This paper provides an extensive review of the popular multi-objective optimization algorithm NSGA-II for selected combinatorial optimization problems viz. assignment problem, allocation problem, travelling salesman problem, vehicle routing problem, scheduling problem, and knapsack problem. It is identified that based on the manner in which NSGA-II has been implemented for solving the aforementioned group of problems, there can be three categories: Conventional NSGA-II, where the authors have implemented the basic version of NSGA-II, without making any changes in the operators; the second one is Modified NSGA-II, where the researchers have implemented NSGA-II after making some changes into it and finally, Hybrid NSGA-II variants, where the researchers have hybridized the conventional and modified NSGA-II with some other technique. The article analyses the modifications in NSGA-II and also discusses the various performance assessment techniques used by the researchers, i.e., test instances, performance metrics, statistical tests, case studies, benchmarking with other state-of-the-art algorithms. Additionally, the paper also provides a brief bibliometric analysis based on the work done in this study.","['Optimization', 'Statistics', 'Sociology', 'Sorting', 'Resource management', 'Linear programming', 'Genetic algorithms']","['NSGA-II', 'combinatorial optimization', 'multi-objective optimization', 'genetic algorithms']"
"Power electronic converter (PEC)-interfaced renewable energy generators (REGs) are increasingly being integrated to the power grid. With the high renewable power penetration levels, one of the key power system parameters, namely reactive power, is affected, provoking steady-state voltage and dynamic/transient stability issues. Therefore, it is imperative to maintain and manage adequate reactive power reserve to ensure a stable and reliable power grid. This paper presents a comprehensive literature review on the reactive power management in renewable rich power grids. Reactive power requirements stipulated in different grid codes for REGs are summarized to assess their adequacy for future network requirements. The PEC-interfaced REGs are discussed with a special emphasis on their reactive power compensation capability and control schemes. Along with REGs, conventional reactive power support devices (e.g., capacitor banks) and PEC-interfaced reactive power support devices (e.g., static synchronous compensators) play an indispensable role in the reactive power management of renewable rich power grids, and thus their reactive power control capabilities and limitations are thoroughly reviewed in this paper. Then, various reactive power control strategies are reviewed with a special emphasis on their advantages/disadvantages. Reactive power coordination between support devices and their optimal capacity are vital for an efficient and stable management of the power grid. Accordingly, the prominent reactive power coordination and optimization algorithms are critically examined and discussed in this paper. Finally, the key issues pertinent to the reactive power management in renewable rich power grids are enlisted with some important technical recommendations for the power industry, policymakers, and academic researchers.","['Reactive power', 'Power grids', 'Power system stability', 'Generators', 'Renewable energy sources', 'Wind power generation', 'Voltage control']","['Control strategies', 'grid codes', 'optimization algorithms', 'renewable energy generators (REGs)', 'reactive power', 'solar photovoltaic (PV)', 'wind generation']"
"Seismology is a data rich and data-driven science. Application of machine learning for gaining new insights from seismic data is a rapidly evolving sub-field of seismology. The availability of a large amount of seismic data and computational resources, together with the development of advanced techniques can foster more robust models and algorithms to process and analyze seismic signals. Known examples or labeled data sets, are the essential requisite for building supervised models. Seismology has labeled data, but the reliability of those labels is highly variable, and the lack of high-quality labeled data sets to serve as ground truth as well as the lack of standard benchmarks are obstacles to more rapid progress. In this paper we present a high-quality, large-scale, and global data set of local earthquake and non-earthquake signals recorded by seismic instruments. The data set in its current state contains two categories: (1) local earthquake waveforms (recorded at “local” distances within 350 km of earthquakes) and (2) seismic noise waveforms that are free of earthquake signals. Together these data comprise ~1.2 million time series or more than 19,000 hours of seismic signal recordings. Constructing such a large-scale database with reliable labels is a challenging task. Here, we present the properties of the data set, describe the data collection, quality control procedures, and processing steps we undertook to insure accurate labeling, and discuss potential applications. We hope that the scale and accuracy of STEAD presents new and unparalleled opportunities to researchers in the seismological community and beyond.","['Earthquakes', 'Instruments', 'Seismic waves', 'Seismic measurements', 'Seismology', 'Benchmark testing', 'Artificial intelligence']","['Earthquakes', 'seismic waveform data', 'machine learning', 'seismic measurements', 'artificial intelligence', 'benchmark testing']"
"Early detection of skin cancer, particularly melanoma, is crucial to enable advanced treatment. Due to the rapid growth in the number of skin cancers, there is a growing need of computerised analysis for skin lesions. The state-of-the-art public available datasets for skin lesions are often accompanied with a very limited amount of segmentation ground truth labeling. Also, the available segmentation datasets consist of noisy expert annotations reflecting the fact that precise annotations to represent the boundary of skin lesions are laborious and expensive. The lesion boundary segmentation is vital to locate the lesion accurately in dermoscopic images and lesion diagnosis of different skin lesion types. In this work, we propose the fully automated deep learning ensemble methods to achieve high sensitivity and high specificity in lesion boundary segmentation. We trained the ensemble methods based on Mask R-CNN and DeeplabV3+ methods on ISIC-2017 segmentation training set and evaluate the performance of the ensemble networks on ISIC-2017 testing set and PH2 dataset. Our results showed that the proposed ensemble methods segmented the skin lesions with Sensitivity of 89.93% and Specificity of 97.94% for the ISIC-2017 testing set. The proposed ensemble method Ensemble-A outperformed FrCN, FCNs, U-Net, and SegNet in Sensitivity by 4.4%, 8.8%, 22.7%, and 9.8% respectively. Furthermore, the proposed ensemble method Ensemble-S achieved a specificity score of 97.98% for clinically benign cases, 97.30% for the melanoma cases, and 98.58% for the seborrhoeic keratosis cases on ISIC-2017 testing set, exhibiting better performance than FrCN, FCNs, U-Net, and SegNet.","['Lesions', 'Skin', 'Image segmentation', 'Melanoma', 'Deep learning', 'Testing', 'Annotations']","['Skin cancer', 'skin lesion segmentation', 'ensemble segmentation methods', 'deep learning', 'melanoma', 'instance segmentation', 'semantic segmentation']"
"Urban air pollutant concentration prediction is dealing with a surge of massive environmental monitoring data and complex changes in air pollutants. This requires effective prediction methods to improve prediction accuracy and to prevent serious pollution incidents, thereby enhancing environmental management decision-making capacity. In this paper, a new pollutant concentration prediction method is proposed based on the vast amounts of environmental data and deep learning techniques. The proposed method integrates big data by using two kinds of deep networks. This method is based on the design that uses a convolutional neural network as the base layer, automatically extracting features of input data. A long short-term memory network is used for the output layer to consider the time dependence of pollutants. Our model consists of these two deep networks. With performance optimization, the model can predict future particulate matter (PM2.5) concentrations as a time series. Finally, the prediction results are compared with the results of numerical models. The applicability and advantages of the model are also analyzed. The experimental results show that it improves prediction performance compared with classic models.","['Feature extraction', 'Predictive models', 'Air pollution', 'Mathematical model', 'Atmospheric modeling', 'Data models', 'Data mining']","['Air pollution', 'machine learning', 'neural network', 'numerical analysis', 'prediction method']"
"In this paper, a review is presented for the research on eye gaze estimation techniques and applications, which has progressed in diverse ways over the past two decades. Several generic eye gaze use-cases are identified: desktop, TV, head-mounted, automotive, and handheld devices. Analysis of the literature leads to the identification of several platform specific factors that influence gaze tracking accuracy. A key outcome from this review is the realization of a need to develop standardized methodologies for the performance evaluation of gaze tracking systems and achieve consistency in their specification and comparative evaluation. To address this need, the concept of a methodological framework for practical evaluation of different gaze tracking systems is proposed.","['Gaze tracking', 'Estimation', 'Visualization', 'Performance evaluation', 'Computers', 'Calibration', 'Imaging']","['Eye gaze', 'gaze estimation', 'accuracy', 'error sources', 'performance evaluation', 'user platforms']"
"A building energy management system (BEMS) is a sophisticated method used for monitoring and controlling a building's energy requirements. A number of potential studies were conducted in nearly or net zero energy buildings (nZEBs) for the optimization of building energy consumption through efficient and sustainable ways. Moreover, policy makers are approving measures to improve building energy efficiency in order to foster sustainable energy usages. However, the intelligence of existing BEMSs or nZEBs is inadequate, because of the static set points for heating, cooling, and lighting, the complexity of large amounts of BEMS data, data loss, and network problems. To solve these issues, a BEMS or nZEB solution based on the Internet of energy (IoE) provides disruptive opportunities for revolutionizing sustainable building energy management. This paper presents a critical review of the potential of an IoE-based BEMS for enhancing the performance of future generation building energy utilization. The detailed studies of the IoE architecture, typical nZEB configuration, different generations of nZEB, and smart building energy systems for future BEMS are investigated. The operations, advantages, and limitations of the existing BEMSs or nZEBs are illustrated. A comprehensive review of the different types of IoE-based BEMS technologies, such as energy routers, storage systems and materials, renewable sources, and plug-and-play interfaces, is then presented. The rigorous review indicates that existing BEMSs require advanced controllers integrated with IoE-based technologies for sustainable building energy usage. The main objective of this review is to highlight several issues and challenges of the conventional controllers and IoE applications of BEMSs or nZEBs. Accordingly, the review provides several suggestions for the research and development of the advanced optimized controller and IoE of future BEMSs. All the highlighted insights and recommendations of this review will hopefully lead to increasing efforts toward the development of the future BEMS applications.","['Buildings', 'Energy consumption', 'Internet', 'Energy efficiency', 'Monitoring', 'Cooling']","['Internet of energy (IoE)', 'building energy management system', 'nearly or net zero energy building', 'sustainable energy']"
"This paper presents an efficient and secure chaotic S-Box based image encryption algorithm. Firstly, by cryptanalyzing a multiple chaotic S-Boxes based image encryption algorithm, we successfully cracked the cryptosystem by using chosen-plaintext attack (CPA). Secondly, we put forward a new image encryption scheme based on a novel compound chaotic map and single S-Box. In the new scheme, a novel discrete compound chaotic system, Logistic-Sine system (LSS), is proposed, which has wider chaotic range and better chaotic properties. And a new S-Box is constructed by using LSS, which has satisfactory cryptographic performance. Based on the new S-Box and the chaotic key stream, the new image encryption algorithm is designed, which consist of a round of permutation and two rounds of substitution process. The permutation and substitution key sequences are related to the plaintext image content, this strategy enables the cryptosystem to resist CPA. The simulation results and security analysis verified the effectiveness of the proposed image encryption scheme. Especially, the new scheme has obvious efficiency advantages, showing that it has better application potential in real-time image encryption.","['Encryption', 'Chaotic communication', 'Ciphers', 'Heuristic algorithms']","['Image encryption', 'chaos', 'S-box', 'Logistic-Sine system', 'substitution', 'chosen-plaintext attack']"
"The world is witnessing an unprecedented growth of cyber-physical systems (CPS), which are foreseen to revolutionize our world via creating new services and applications in a variety of sectors, such as environmental monitoring, mobile-health systems, intelligent transportation systems, and so on. The information and communication technology sector is experiencing a significant growth in data traffic, driven by the widespread usage of smartphones, tablets, and video streaming, along with the significant growth of sensors deployments that are anticipated in the near future. It is expected to outstandingly increase the growth rate of raw sensed data. In this paper, we present the CPS taxonomy via providing a broad overview of data collection, storage, access, processing, and analysis. Compared with other survey papers, this is the first panoramic survey on big data for CPS, where our objective is to provide a panoramic summary of different CPS aspects. Furthermore, CPS requires cybersecurity to protect them against malicious attacks and unauthorized intrusion, which become a challenge with the enormous amount of data that are continuously being generated in the network. Thus, we also provide an overview of the different security solutions proposed for CPS big data storage, access, and analytics. We also discuss big data meeting green challenges in the contexts of CPS.","['Big Data', 'Sensors', 'Cloud computing', 'Security', 'Green products', 'Cyber-physical systems', 'Tools']","['Cyber-physical systems (CPS)', 'Internet of Things (IoT)', 'context-awareness', 'social computing', 'cloud computing', 'big data', 'clustering', 'data mining', 'data analytics', 'machine learning', 'real-time analytics', 'space-time analytics', 'cybersecurity', 'green', 'energy', 'sustainability']"
"Intrusion detection system (IDS) provides an important basis for the network defense. Due to the development of the cloud computing and social network, massive amounts of data are generated, which inevitably brings much pressure to IDS. And therefore, it becomes crucial to efficiently divide the data into different classes over big data according to data features. Moreover, we can further determine whether one is normal behavior or not based on the classes information. Although the clustering approach based on K-means for IDS has been well studied, unfortunately directly using it in big data environment may suffer from inappropriateness. On the one hand, the efficiency of data clustering needs to be improved. On the other hand, differ from the classification, there is no unified evaluation indicator for clustering issue, and thus, it is necessary to study which indicator is more suitable for evaluating the clustering results of IDS. In this paper, we propose a clustering method for IDS based on Mini Batch K-means combined with principal component analysis. First, a preprocessing method is proposed to digitize the strings and then the data set is normalized so as to improve the clustering efficiency. Second, the principal component analysis method is used to reduce the dimension of the processed data set aiming to further improve the clustering efficiency, and then mini batch K-means method is used for data clustering. More specifically, we use K-means++ to initialize the centers of cluster in order to avoid the algorithm getting into the local optimum, in addition, we choose the Calsski Harabasz indicator so that the clustering result is more easily determined. Compared with the other methods, the experimental results and the time complexity analysis show that our proposed method is effective and efficient. Above all, our proposed clustering method can be used for IDS over big data environment.","['Principal component analysis', 'Clustering algorithms', 'Big Data', 'Clustering methods', 'Data mining', 'Intrusion detection', 'Classification algorithms']","['IDS', 'big data', 'clustering', 'principal component analysis', 'mini batch Kmeans']"
"Fault diagnosis of rotating machinery plays a significant role in the industrial production and engineering field. Owing to the drawbacks of traditional fault diagnosis methods, such as heavily dependence on human knowledge and professional experience, intelligent fault diagnosis based on deep learning (DL) has aroused the interest of researchers. DL achieves the desirable automatic feature learning and fault classification. Therefore, in this review, DL and DL-based intelligent fault diagnosis techniques are overviewed. DL-based fault diagnosis approaches for rotating machinery are summarized and discussed, primarily including bearing, gear/gearbox and pumps. Finally, with respect to modern intelligent fault diagnosis, the existing challenges and possible future research orientations are prospected and analyzed.","['Fault diagnosis', 'Artificial intelligence', 'Feature extraction', 'Neural networks', 'Pumps', 'Gears']","['Deep learning', 'deep neural network', 'intelligent fault diagnosis', 'rotating machinery']"
"In recent years, big data have become a hot research topic. The increasing amount of big data also increases the chance of breaching the privacy of individuals. Since big data require high computational power and large storage, distributed systems are used. As multiple parties are involved in these systems, the risk of privacy violation is increased. There have been a number of privacy-preserving mechanisms developed for privacy protection at different stages (e.g., data generation, data storage, and data processing) of a big data life cycle. The goal of this paper is to provide a comprehensive overview of the privacy preservation mechanisms in big data and present the challenges for existing mechanisms. In particular, in this paper, we illustrate the infrastructure of big data and the state-of-the-art privacy-preserving mechanisms in each stage of the big data life cycle. Furthermore, we discuss the challenges and future research directions related to privacy preservation in big data.","['Big data', 'Cloud computing', 'Data privacy', 'Memory management', 'Privacy']","['Big data', 'privacy', 'data auditing', 'big data storage', 'big data processing']"
"Air pollution has become an extremely serious problem, with particulate matter having a significantly greater impact on human health than other contaminants. The small diameter of fine particulate matter (PM2.5) allows it to penetrate deep into the alveoli as far as the bronchioles, interfering with a gas exchange within the lungs. Long-term exposure to particulate matter has been shown to cause the cardiovascular disease, respiratory disease, and increase the risk of lung cancers. Therefore, forecasting air quality has also become important to help guide individual actions. This paper aims to forecast air quality for up to 48 h using a combination of multiple neural networks, including an artificial neural network, a convolutional neural network, and a long-short-term memory to extract spatial-temporal relations. The proposed predictive model considers various meteorology data from the previous few hours as well as information related to the elevation space to extract terrain impact on air quality. The model includes trends from multiple locations, extracted from correlations between adjacent locations, and among similar locations in the temporal domain. Experiments employing Taiwan and Beijing data sets show that the proposed model achieves excellent performance and outperforms current state-of-the-art methods.","['Air quality', 'Atmospheric modeling', 'Predictive models', 'Neural networks', 'Wind forecasting', 'Lung', 'Data mining']","['Dynamic time warping(DTW)', 'convolutional neural network(CNN)', 'long-short-term memory(LSTM)', 'spatio-temporal analysis', 'big data', 'air quality forecast']"
"Internet of Things allow massive number of uniquely addressable “things” to communicate with each other and transfer data over existing internet or compatible network protocols. This paper proposes a new concept which tackles the issues for supporting control and monitoring activities at deployment sites and industrial automations, where intelligent things can monitor peripheral events, induce sensor data acquired from a variety of sources, use ad hoc, local, and distributed “machine intelligence” to determine appropriate course of actions, and then act to control or disseminate static or dynamic position aware robotic things in the physical world through a seamless manner by providing a means for utilizing them as Internet of robotic things (IoRT). Although progressive advancements can be seen in multi-robotic systems, robots are constantly getting enriched by easier developmental functionalities, such vertical robotic service centric silos are not enough for continuously and seamlessly supporting for which they are meant. In this paper, a novel concept-IoRT is presented that highlights architectural principles, vital characteristics, as well as research challenges. The aim of this paper is to provide a better understanding of the architectural assimilation of IoRT and identify important research directions on this term.","['Internet of things', 'Monitoring', 'Robot sensing systems', 'Intelligent systems', 'Automation']","['Internet of things', 'IoRT', 'robotics', 'cloud']"
"Grey wolf optimizer (GWO) is a very efficient metaheuristic inspired by the hierarchy of the Canis lupus wolves. It has been extensively employed to a variety of practical applications. Crow search algorithm (CSA) is a recently proposed metaheuristic algorithm, which mimics the intellectual conduct of crows. In this paper, a hybrid GWO with CSA, namely GWOCSA is proposed, which combines the strengths of both the algorithms effectively with the aim to generate promising candidate solutions in order to achieve global optima efficiently. In order to validate the competence of the proposed hybrid GWOCSA, a widely utilized set of 23 benchmark test functions having a wide range of dimensions and varied complexities is used in this paper. The results obtained by the proposed algorithm are compared to 10 other algorithms in this paper for verification. The statistical results demonstrate that the GWOCSA outperforms other algorithms, including the recent variants of GWO called, enhanced grey wolf optimizer (EGWO) and augmented grey wolf optimizer (AGWO) in terms of high local optima avoidance ability and fast convergence speed. Furthermore, in order to demonstrate the applicability of the proposed algorithm at solving complex real-world problems, the GWOCSA is also employed to solve the feature selection problem as well. The GWOCSA as a feature selection approach is tested on 21 widely employed data sets acquired from the University of California at Irvine repository. The experimental results are compared to the state-of-the-art feature selection techniques, including the native GWO, the EGWO, and the AGWO. The results reveal that the GWOCSA has comprehensive superiority in solving the feature selection problem, which proves the capability of the proposed algorithm in solving real-world complex problems.","['Optimization', 'Feature extraction', 'Heuristic algorithms', 'Prediction algorithms', 'Search problems', 'Genetic algorithms', 'Computer science']","['Grey wolf optimizer', 'crow search algorithm', 'hybrid algorithm', 'function optimization', 'feature selection']"
"A new method is presented to denoise 1-D experimental signals using wavelet transforms. Although the state-of-the-art wavelet denoising methods perform better than other denoising methods, they are not very effective for experimental signals. Unlike images and other signals, experimental signals in chemical and biophysical applications, for example, are less tolerant to signal distortion and under-denoising caused by the standard wavelet denoising methods. The new method: 1) provides a method to select the number of decomposition levels to denoise; 2) uses a new formula to calculate noise thresholds that does not require noise estimation; 3) uses separate noise thresholds for positive and negative wavelet coefficients; 4) applies denoising to the approximation component; and 5) allows the flexibility to adjust the noise thresholds. The new method is applied to continuous wave electron spin resonance spectra and it is found that it increases the signal-to-noise ratio (SNR) by more than 32 dB without distorting the signal, whereas standard denoising methods improve the SNR by less than 10 dB and with some distortion. In addition, its computation time is more than six times faster.","['Wavelet transforms', 'Noise measurement', 'Threshold current', 'Spectroscopy', 'Magnetic resonance', 'Noise reduction']","['Wavelet transform', 'wavelet denoising', 'noise thresholding', 'noise reduction', 'magnetic resonance spectroscopy']"
"Diabetic retinopathy (DR) is an important cause of blindness worldwide. However, DR is hard to be detected in the early stages, and the diagnostic procedure can be time-consuming even for the experienced experts. Therefore, a computer-aided diagnosis method based on deep learning algorithms is proposed to automatedly diagnose the referable diabetic retinopathy by classifying color retinal fundus photographs into two grades. In this paper, a novel convolutional neural network model with the Siamese-like architecture is trained with a transfer learning technique. Different from the previous works, the proposed model accepts binocular fundus images as inputs and learns their correlation to help to make a prediction. In the case with a training set of only 28 104 images and a test set of 7024 images, an area under the receiver operating curve of 0.951 is obtained by the proposed binocular model, which is 0.011 higher than that obtained by the existing monocular model. To further verify the effectiveness of the binocular design, a binocular model for five-class DR detection is also trained and evaluated on a 10% validation set. The result shows that it achieves a kappa score of 0.829 which is higher than that of the existing non-ensemble model.","['Diabetes', 'Retinopathy', 'Deep learning', 'Lesions', 'Convolutional neural networks', 'Blindness', 'Retina']","['Biomedical imaging processing', 'diabetic retinopathy', 'fundus photograph', 'convolutional neural network', 'deep learning', 'Siamese-like network']"
"The contribution of a plant is highly important for both human life and environment. Plants do suffer from diseases, like human beings and animals. There is the number of plant diseases that occur and affects the normal growth of a plant. These diseases affect complete plant including leaf, stem, fruit, root, and flower. Most of the time when the disease of a plant has not been taken care of, the plant dies or may cause leaves drop, flowers, and fruits drop. Appropriate diagnosis of such diseases is required for accurate identification and treatment of plant diseases. Plant pathology is the study of plant diseases, their causes, procedures for controlling and managing them. But, the existing method encompasses human involvement for classification and identification of diseases. This procedure is time-consuming and costly. Automatic segmentation of diseases from plant leaf images using soft computing approach can be reasonably useful than the existing one. In this paper, we have introduced a method named as bacterial foraging optimization based radial basis function neural network (BRBFNN) for identification and classification of plant leaf diseases automatically. For assigning optimal weight to radial basis function neural network we use bacterial foraging optimization that further increases the speed and accuracy of the network to identify and classify the regions infected of different diseases on the plant leafs. The region growing algorithm increases the efficiency of the network by searching and grouping of seed points having common attributes for feature extraction process. We worked on fungal diseases like common rust, cedar apple rust, late blight, leaf curl, leaf spot, and early blight. The proposed method attains higher accuracy in identification and classification of diseases.","['Diseases', 'Support vector machines', 'Microorganisms', 'Task analysis', 'Radial basis function networks', 'Optimization']","['Bacteria foraging algorithm', 'image segmentation', 'plant diseases', 'radial basis function neural network', 'soft computing']"
"Power system faults are significant problems in power transmission and distribution. Methods based on relay protection actions and electrical component actions have been put forward in recent years. However, they have deficiencies dealing with power system fault. In this paper, a method for data-based line trip fault prediction in power systems using long short-term memory (LSTM) networks and support vector machine (SVM) is proposed. The temporal features of multisourced data are captured with LSTM networks, which perform well in extracting the features of time series for a long-time span. The strong learning and mining ability of LSTM networks is suitable for a large quantity of time series in power transmission and distribution. SVM, with a strong generalization ability and robustness, is introduced for classification to get the final prediction results. Considering the overfitting problem in fault prediction, layer of dropout and batch normalization are added into the network. The complete network architecture is shown in this paper in detail. The parameters are adjusted to fit the specific situation of the actual power system. The data for experiments are obtained from the Wanjiang substation in the China Southern Power Grid. The real experiments prove the proposed method's improvements compared with current data mining methods. Concrete analyses of results are elaborated in this paper. A discussion of practical applications is presented to demonstrate the feasibility in real scenarios.","['Circuit faults', 'Support vector machines', 'Power system stability', 'Data mining', 'Protective relaying', 'Power system faults', 'Feature extraction']","['Data mining', 'power system faults', 'recurrent neural networks', 'support vector machines']"
"Cardiovascular disease is a substantial cause of mortality and morbidity in the world. In clinical data analytics, it is a great challenge to predict heart disease survivor. Data mining transforms huge amounts of raw data generated by the health industry into useful information that can help in making informed decisions. Various studies proved that significant features play a key role in improving performance of machine learning models. This study analyzes the heart failure survivors from the dataset of 299 patients admitted in hospital. The aim is to find significant features and effective data mining techniques that can boost the accuracy of cardiovascular patient’s survivor prediction. To predict patient’s survival, this study employs nine classification models: Decision Tree (DT), Adaptive boosting classifier (AdaBoost), Logistic Regression (LR), Stochastic Gradient classifier (SGD), Random Forest (RF), Gradient Boosting classifier (GBM), Extra Tree Classifier (ETC), Gaussian Naive Bayes classifier (G-NB) and Support Vector Machine (SVM). The imbalance class problem is handled by Synthetic Minority Oversampling Technique (SMOTE). Furthermore, machine learning models are trained on the highest ranked features selected by RF. The results are compared with those provided by machine learning algorithms using full set of features. Experimental results demonstrate that ETC outperforms other models and achieves 0.9262 accuracy value with SMOTE in prediction of heart patient’s survival.","['Heart', 'Data mining', 'Predictive models', 'Machine learning algorithms', 'Support vector machines', 'Medical diagnostic imaging', 'Boosting']","['Data mining', 'heart disease classification', 'machine learning', 'cardiovascular disease', 'feature selection', 'SMOTE']"
"Battery packs with a large number of battery cells are becoming more and more widely adopted in electronic systems, such as robotics, renewable energy systems, energy storage in smart grids, and electronic vehicles. Therefore, a well-designed battery pack is essential for battery applications. In the literature, the majority of research in battery pack design focuses on battery management system, safety circuit, and cell-balancing strategies. Recently, the reconfigurable battery pack design has gained increasing attentions as a promising solution to solve the problems existing in the conventional battery packs and associated battery management systems, such as low energy efficiency, short pack lifespan, safety issues, and low reliability. One of the most prominent features of reconfigurable battery packs is that the battery cell topology can be dynamically reconfigured in the real-time fashion based on the current condition (in terms of the state of charge and the state of health) of battery cells. So far, there are several reconfigurable battery schemes having been proposed and validated in the literature, all sharing the advantage of cell topology reconfiguration that ensures balanced cell states during charging and discharging, meanwhile providing strong fault tolerance ability. This survey is undertaken with the intent of identifying the state-of-the-art technologies of reconfigurable battery as well as providing review on related technologies and insight on future research in this emerging area.","['Energy storage', 'Battery management system', 'Safety', 'Battery cells', 'Reconfigurable batteries']","['Reconfigurable battery pack', 'energy storage system', 'SOC', 'battery management system']"
"Smart wearables collect and analyze data, and in some scenarios make a smart decision and provide a response to the user and are finding more and more applications in our daily life. In this paper, we comprehensively survey the most recent and important research works conducted in the area of wearable Internet of Things (IoT) and classify the wearables into four major clusters: (i) health, (ii) sports and daily activity, (iii) tracking and localization, and (iv) safety. The fundamental differences of the algorithms associated within each cluster are grouped and analyzed and the research challenges and open issues in each cluster are discussed. This survey reveals that although Cellular IoT (CIoT) has many advantages and can bring enormous applications to IoT wearables, it has been rarely studied by the researchers. This article also addresses the opportunities and challenges related to implementing CIoT-enabled wearables.","['Biomedical monitoring', 'Temperature sensors', 'Monitoring', 'Internet of Things', 'Wearable sensors', 'Temperature measurement']","['Smart wearables', 'Internet of Things', 'cellular IoT']"
"An important application in the growing field of unmanned aerial vehicles (UAVs) is in monitoring and inspection of high voltage power lines and electrical networks. The UAV-based monitoring method will save energy, simplify access to impassable or remote areas, reduce inspection cost, and automate the inspection process. However, the battery capacity of a medium scale drone limits their travel distance and mission duration. It is crucial to improve the energy feeding system of drones and overcome the battery capacity problem to foster the use of drones for routine monitoring operations. In this study, we focus on presenting wireless techniques available for drone mission duration improvement as well as discuss and practically examine the most feasible and reliable technique to charge UAV using power lines.","['Drones', 'Batteries', 'Laser beams', 'Monitoring', 'Wind', 'Wireless communication']","['Unmanned aerial vehicles', 'wireless charging', 'wireless energy transfer', 'inductive power transfer']"
"The Internet of Things (IoT) is rapidly becoming an integral part of our life and also multiple industries. We expect to see the number of IoT connected devices explosively grows and will reach hundreds of billions during the next few years. To support such a massive connectivity, various wireless technologies are investigated. In this survey, we provide a broad view of the existing wireless IoT connectivity technologies and discuss several new emerging technologies and solutions that can be effectively used to enable massive connectivity for IoT. In particular, we categorize the existing wireless IoT connectivity technologies based on coverage range and review diverse types of connectivity technologies with different specifications. We also point out key technical challenges of the existing connectivity technologies for enabling massive IoT connectivity. To address the challenges, we further review and discuss some examples of promising technologies such as compressive sensing (CS) random access, non-orthogonal multiple access (NOMA), and massive multiple input multiple output (mMIMO) based random access that could be employed in future standards for supporting IoT connectivity. Finally, a classification of IoT applications is considered in terms of various service requirements. For each group of classified applications, we outline its suitable IoT connectivity options.","['Wireless communication', 'Bluetooth', 'Internet of Things', 'Wireless sensor networks', 'NOMA', '5G mobile communication', 'OFDM']","['IoT connectivity technologies', '5G', 'massive MTC', 'massive connectivity', 'compressive sensing', 'NOMA', 'massive MIMO', 'machine learning', 'IoT applications']"
"Lithium-ion batteries (LIBs) are being intensively studied and universally used as power sources for electric vehicle applications. Despite the staggering growth in sales of LIBs worldwide, thermal safety issues still turn out to be the most intolerable pain point, and remain the focus of research for technological improvements. This paper presents a comprehensive overview on thermal safety issues of LIBs, in terms of thermal behavior and thermal runaway modeling and tests for battery cells, and safety management strategies for battery packs. Considering heat generation mechanism and thermal characteristics of LIBs, heat generation, dissipation and accumulation inside a cell are elaborated. The triggering factors leading to thermal runaway are also summarized. Finally, thermal runaway detection and prevention strategies for both cell- and pack-levels are introduced. Different engineering approaches from material refinement and additive adoption to thermal, electrical, and mechanical design are presented for thermal runaway prevention.","['Heating systems', 'Batteries', 'Mathematical model', 'Temperature measurement', 'Safety', 'Electrodes', 'Calorimetry']","['Electric vehicles', 'batteries', 'modeling', 'calorimetry', 'safety', 'thermal runaway', 'thermal management']"
"Automated digital contact tracing is effective and efficient, and one of the non-pharmaceutical complementary approaches to mitigate and manage epidemics like Coronavirus disease 2019 (COVID-19). Despite the advantages of digital contact tracing, it is not widely used in the western world, including the US and Europe, due to strict privacy regulations and patient rights. We categorized the current approaches for contact tracing, namely: mobile service-provider-application, mobile network operators’ call detail, citizen-application, and IoT-based. Current measures for infection control and tracing do not include animals and moving objects like cars despite evidence that these moving objects can be infection carriers. In this article, we designed and presented a novel privacy anonymous IoT model. We presented an RFID proof-of-concept for this model. Our model leverages blockchain’s trust-oriented decentralization for on-chain data logging and retrieval. Our model solution will allow moving objects to receive or send notifications when they are close to a flagged, probable, or confirmed diseased case, or flagged place or object. We implemented and presented three prototype blockchain smart contracts for our model. We then simulated contract deployments and execution of functions. We presented the cost differentials. Our simulation results show less than one-second deployment and call time for smart contracts, though, in real life, it can be up to 25 seconds on Ethereum public blockchain. Our simulation results also show that it costs an average of $1.95 to deploy our prototype smart contracts, and an average of $0.34 to call our functions. Our model will make it easy to identify clusters of infection contacts and help deliver a notification for mass isolation while preserving individual privacy. Furthermore, it can be used to understand better human connectivity, model similar other infection spread network, and develop public policies to control the spread of COVID-19 while preparing for future epidemics.","['Privacy', 'Wireless fidelity', 'Bluetooth', 'Diseases', 'Mobile handsets', 'Animals']","['Contact tracing', 'RFID', 'IoT', 'blockchain', 'hospitals', 'telemedicine', 'digital health', 'privacy', 'COVID-19']"
"Registration of multi-temporal remote sensing images has been widely applied in military and civilian fields, such as ground target identification, urban development assessment, and geographic change assessment. Ground surface change challenges feature point detection in amount and quality, which is a common dilemma faced by feature-based registration algorithms. Under severe appearance variation, detected feature points may contain a large proportion of outliers, whereas inliers may be inadequate and unevenly distributed. This paper presents a convolutional neural network (CNN) feature-based multitemporal remote sensing image registration method with two key contributions: (i) we use a CNN to generate robust multi-scale feature descriptors and (ii) we design a gradually increasing selection of inliers to improve the robustness of feature point registration. Extensive experiments on feature matching and image registration are performed over a multi-temporal satellite image data set and a multi-temporal unmanned aerial vehicle image dataset. Our method outperforms four state-of-the-art methods in most scenarios.","['Feature extraction', 'Image registration', 'Remote sensing', 'Robustness', 'Convolutional neural networks', 'Optimization', 'Indexes']","['Remote sensing', 'feature matching', 'image registration', 'convolutional feature']"
"Scientific advances build on reproducible researches which need publicly available benchmark data sets. The computer vision and speech recognition communities have led the way in establishing benchmark data sets. There are much less data sets available in mobile computing, especially for rich locomotion and transportation analytics. This paper presents a highly versatile and precisely annotated large-scale data set of smartphone sensor data for multimodal locomotion and transportation analytics of mobile users. The data set comprises seven months of measurements, collected from all sensors of four smartphones carried at typical body locations, including the images of a body-worn camera, while three participants used eight different modes of transportation in the south-east of the U.K., including in London. In total, 28 context labels were annotated, including transportation mode, participant's posture, inside/outside location, road conditions, traffic conditions, presence in tunnels, social interactions, and having meals. The total amount of collected data exceed 950 GB of sensor data, which corresponds to 2812 h of labeled data and 17 562 km of traveled distance. We present how we set up the data collection, including the equipment used and the experimental protocol. We discuss the data set, including the data curation process, the analysis of the annotations, and of the sensor data. We discuss the challenges encountered and present the lessons learned and some of the best practices we developed to ensure high quality data collection and annotation. We discuss the potential applications which can be developed using this large-scale data set. In particular, we present how a machine-learning system can use this data set to automatically recognize modes of transportations. Many other research questions related to transportation analytics, activity recognition, radio signal propagation and mobility modeling can be addressed through this data set. The full data set is being made available to the community, and a thorough preview is already published.","['Global Positioning System', 'Data collection', 'Automobiles', 'Cameras', 'Public transportation', 'Mobile handsets']","['Activity recognition', 'context awareness', 'camera', 'intelligent transportation systems', 'GPS', 'GSM', 'locomotion dataset', 'multimodal sensors', 'pattern analysis', 'sensor fusion', 'supervised learning', 'transportation dataset', 'Wi-Fi']"
"Sarcasm is a sophisticated form of irony widely used in social networks and microblogging websites. It is usually used to convey implicit information within the message a person transmits. Sarcasm might be used for different purposes, such as criticism or mockery. However, it is hard even for humans to recognize. Therefore, recognizing sarcastic statements can be very useful to improve automatic sentiment analysis of data collected from microblogging websites or social networks. Sentiment Analysis refers to the identification and aggregation of attitudes and opinions expressed by Internet users toward a specific topic. In this paper, we propose a pattern-based approach to detect sarcasm on Twitter. We propose four sets of features that cover the different types of sarcasm we defined. We use those to classify tweets as sarcastic and non-sarcastic. Our proposed approach reaches an accuracy of 83.1% with a precision equal to 91.1%. We also study the importance of each of the proposed sets of features and evaluate its added value to the classification. In particular, we emphasize the importance of pattern-based features for the detection of sarcastic statements.","['Sentiment analysis', 'Twitter', 'Machine learning', 'Feature extraction']","['Twitter', 'sentiment analysis', 'sarcasm detection', 'machine learning']"
"This paper describes work that has been done on design and development of a water quality monitoring system, with the objective of notifying the user of the real-time water quality parameters. The system is able to measure the physiochemical parameters of water quality, such as flow, temperature, pH, conductivity, and the oxidation reduction potential. These physiochemical parameters are used to detect water contaminants. The sensors, which are designed from first principles and implemented with signal conditioning circuits, are connected to a microcontroller-based measuring node, which processes and analyzes the data. In this design, ZigBee receiver and transmitter modules are used for communication between the measuring and notification nodes. The notification node presents the reading of the sensors and outputs an audio alert when water quality parameters reach unsafe levels. Various qualification tests are run to validate each aspect of the monitoring system. The sensors are shown to work within their intended accuracy ranges. The measurement node is able to transmit data by ZigBee to the notification node for audio and visual display. The results demonstrate that the system is capable of reading physiochemical parameters, and can successfully process, transmit, and display the readings.","['Pollution measurement', 'Water contamination', 'Flow control', 'ZigBee', 'Real-time systems', 'Wireless sensor networks', 'Conductivity measurement', 'pH measurement']","['Water quality monitoring', 'flow sensor', 'pH sensor', 'conductivity sensor', 'temperature sensor', 'ORP sensor', 'ZigBee', 'wireless sensor networks']"
"IoT devices have some special characteristics, such as mobility, limited performance, and distributed deployment, which makes it difficult for traditional centralized access control methods to support access control in current large-scale IoT environment. To address these challenges, this paper proposes an access control system in IoT named fabric-iot, which is based on Hyperledger Fabric blockchain framework and attributed based access control (ABAC). The system contains three kinds of smart contracts, which are Device Contract (DC), Policy Contract (PC), and Access Contract (AC). DC provides a method to store the URL of resource data produced by devices, and a method to query it. PC provides functions to manage ABAC policies for admin users. AC is the core program to implement an access control method for normal users. Combined with ABAC and blockchain technology, fabric-iot can provide decentralized, fine-grained and dynamic access control management in IoT. To verify the performance of this system, two groups of simulation experiments are designed. The results show that fabric-iot can maintain high throughput in large-scale request environment and reach consensus efficiently in a distributed system to ensure data consistency.","['Access control', 'Blockchain', 'Peer-to-peer computing', 'Fabrics', 'Distributed ledger', 'Smart contracts']","['Blockchain', 'IoT', 'ABAC', 'hyperledger fabric', 'distributed system']"
"In this paper, we consider the use of a team of multiple unmanned aerial vehicles (UAVs) to accomplish a search and rescue (SAR) mission in the minimum time possible while saving the maximum number of people. A novel technique for the SAR problem is proposed and referred to as the layered search and rescue (LSAR) algorithm. The novelty of LSAR involves simulating real disasters to distribute SAR tasks among UAVs. The performance of LSAR is compared, in terms of percentage of rescued survivors and rescue and execution times, with the max-sum, auction-based, and locust-inspired approaches for multi UAV task allocation (LIAM) and opportunistic task allocation (OTA) schemes. The simulation results show that the UAVs running the LSAR algorithm on average rescue approximately 74% of the survivors, which is 8% higher than the next best algorithm (LIAM). Moreover, this percentage increases with the number of UAVs, almost linearly with the least slope, which means more scalability and coverage is obtained in comparison to other algorithms. In addition, the empirical cumulative distribution function of LSAR results shows that the percentages of rescued survivors clustered around the [78%-100%] range under an exponential curve, meaning most results are above 50%. In comparison, all the other algorithms have almost equal distributions of their percentage of rescued survivor results. Furthermore, because the LSAR algorithm focuses on the center of the disaster, it finds more survivors and rescues them faster than the other algorithms, with an average of 55%~77%. Moreover, most registered times to rescue survivors by LSAR are bounded by a time of 04:50:02 with 95% confidence for a one-month mission time.","['Robots', 'Task analysis', 'Resource management', 'Search problems', 'Unmanned aerial vehicles', 'Approximation algorithms', 'Clustering algorithms']","['Autonomous agents', 'drones', 'search and rescue', 'unmanned aerial vehicles']"
"As the secondary widely used battery, lithium-ion batteries (LIBs) have become the core component of the energy supply for most devices. Accurately predicting the current cycle time of LIBs is of great importance to ensure the reliability and safety of the equipment. In this paper, considering the nonlinear and non-Gaussian capacity degradation characteristics of LIBs, a remaining useful life (RUL) prediction method based on the exponential model and the particle filter is proposed. The cycle life test data of LIBs published by prognostics center of excellence in national aeronautics and space administration were exponentially experiencing the rule of degradation. And then the extrapolation method was used to get the quantitative expression of the uncertainty of life expectancy of LIBs, i.e. the prediction mean and the probability distribution histogram. The prognostic horizon index and the new specific accuracy index were applied to evaluate the prediction performance. Moreover, the prediction error under different prediction starting points is given. Compared with other methods such as the auto-regressive integrated moving average model, the fusion nonlinear degradation autoregressive model and the regularized particle filter algorithm, the proposed algorithm has a better prediction performance. According to the accuracy index, the proposed prediction method has better prediction accuracy and convergence. The RUL prediction for LIBs can provide a better decision support for the maintenance and support systems to optimize maintenance strategies, and reduce maintenance costs.","['Prediction algorithms', 'Degradation', 'Predictive models', 'Mathematical model', 'Batteries', 'Particle filters', 'Analytical models']","['Lithium-ion batteries (LIBs)', 'exponential model', 'particle filter (PF)', 'remaining useful life (RUL) prediction']"
"Accurate and timely prediction of remaining useful life (RUL) of a machine enables the machine to have an appropriate operation and maintenance decision. Data-driven RUL prediction methods are more attractive to researchers because they can be deployed quicker and cheaper compared to other approaches. The existing deep neural network (DNN) models proposed for the applications of RUL prediction are mostly single-path and top-down propagation. In order to improve the prognostic accuracy of the network, this paper proposes a directed acyclic graph (DAG) network that combines long short term memory (LSTM) and a convolutional neural network (CNN) to predict the RUL. Different from the existing prediction models combined with CNN and LSTM, the method proposed in this paper combines CNN and LSTM organically instead of just using CNN for feature extraction. Moreover, when a single timestamp is used as an input, padding the signals in the same training batch would affect the prediction ability of the developed model. To overcome this drawback, the proposed method generates a short-term sequence by sliding the time window (TW) with one step size. In addition, based on the degradation mechanism, the piece-wise RUL function is used instead of the traditional linear function. In the experimental test, the turbofan engine degradation simulation dataset provided by NASA is used to validate the proposed RUL prediction model. By comparing with the existing methods using the same dataset, it can be concluded that the prediction method proposed in this paper has better prediction capability.","['Predictive models', 'Feature extraction', 'Degradation', 'Data models', 'Engines', 'Neural networks']","['Remaining useful life prediction', 'long-short-term memory network', 'convolutional neural networks', 'turbofan engine']"
"In order to improve the accuracy of emotional recognition by end-to-end automatic learning of emotional features in spatial and temporal dimensions of electroencephalogram (EEG), an EEG emotional feature learning and classification method using deep convolution neural network (CNN) was proposed based on temporal features, frequential features, and their combinations of EEG signals in DEAP dataset. The shallow machine learning models including bagging tree (BT), support vector machine (SVM), linear discriminant analysis (LDA), and Bayesian linear discriminant analysis (BLDA) models and deep CNN models were used to make emotional binary classification experiments on DEAP datasets in valence and arousal dimensions. The experimental results showed that the deep CNN models which require no feature engineering achieved the best recognition performance on temporal and frequency combined features in both valence and arousal dimensions, which is 3.58% higher than the performance of the best traditional BT classifier in valence dimension and 3.29% higher than that of BT classifier in arousal dimension.","['Electroencephalography', 'Feature extraction', 'Brain modeling', 'Emotion recognition', 'Frequency-domain analysis', 'Videos', 'Convolutional neural networks']","['EEG', 'emotion recognition', 'convolution neural network', 'combined features', 'deep learning']"
"Different automated decision support systems based on artificial neural network (ANN) have been widely proposed for the detection of heart disease in previous studies. However, most of these techniques focus on the preprocessing of features only. In this paper, we focus on both, i.e., refinement of features and elimination of the problems posed by the predictive model, i.e., the problems of underfitting and overfitting. By avoiding the model from overfitting and underfitting, it can show good performance on both the datasets, i.e., training data and testing data. Inappropriate network configuration and irrelevant features often result in overfitting the training data. To eliminate irrelevant features, we propose to use $\chi ^{2}$ statistical model while the optimally configured deep neural network (DNN) is searched by using exhaustive search strategy. The strength of the proposed hybrid model named $\chi ^{2}$ -DNN is evaluated by comparing its performance with conventional ANN and DNN models, another state of the art machine learning models and previously reported methods for heart disease prediction. The proposed model achieves the prediction accuracy of 93.33%. The obtained results are promising compared to the previously reported methods. The findings of the study suggest that the proposed diagnostic system can be used by physicians to accurately predict heart disease.","['Heart', 'Diseases', 'Neural networks', 'Training data', 'Neurons', 'Predictive models', 'Data models']","['Deep neural network', 'heart disease', 'hyperparameters optimization', 'overfitting', 'underfitting']"
"Bayesian optimisation is a statistical method that efficiently models and optimises expensive “black-box” functions. This review considers the application of Bayesian optimisation to experimental design, in comparison to existing Design of Experiments (DOE) methods. Solutions are surveyed for a range of core issues in experimental design including: the incorporation of prior knowledge, high dimensional optimisation, constraints, batch evaluation, multiple objectives, multi-fidelity data, and mixed variable types.","['Optimization', 'Adaptation models', 'Bayes methods', 'Metals', 'Gaussian processes', 'Computational modeling', 'Uncertainty']","['Bayesian methods', 'design for experiments', 'design optimization', 'machine learning algorithms']"
"With the development of network-enabled sensors and artificial intelligence algorithms, various human-centered smart systems are proposed to provide services with higher quality, such as smart healthcare, affective interaction, and autonomous driving. Considering cognitive computing is an indispensable technology to develop these smart systems, this paper proposes human-centered computing assisted by cognitive computing and cloud computing. First, we provide a comprehensive investigation of cognitive computing, including its evolution from knowledge discovery, cognitive science, and big data. Then, the system architecture of cognitive computing is proposed, which consists of three critical technologies, i.e., networking (e.g., Internet of Things), analytics (e.g., reinforcement learning and deep learning), and cloud computing. Finally, it describes the representative applications of human-centered cognitive computing, including robot technology, emotional communication system, and medical cognitive system.","['Cognitive systems', 'Big Data', 'Cognition', 'Cloud computing', 'Cognitive science', 'Cyberspace', 'Human computer interaction']","['Cognitive computing', 'big data analysis', 'Internet of Things', 'cloud computing']"
"Blockchain technology has a wide range of applications in the fields of finance, credit reporting and intellectual property, etc. As the core of blockchain, consensus algorithm affects the security and performance of blockchain system directly. In the past 10 years, there have been about 30 consensus algorithms such as Proof of Work (PoW), Proof of Stake (PoS), Delegated Proof of Stake (DPoS), Ripple Protocol Consensus Algorithm (RPCA) and AlgoRand. But their security, stability and operating efficiency still lag far behind our actual needs. This paper introduces the computing power competition of PoW into DPoS to design an improved consensus algorithm named Delegated Proof of Stake with Downgrade (DDPoS). Through the further modification, the impact of both computing resources and stakes on generating blocks is reduced to achieve higher efficiency, fairness, and decentralization in consensus process. Then a downgrade mechanism is proposed to quickly replace the malicious nodes to improve the security. The simulation experiments in blockchain system show that the proposed consensus algorithm is significantly more efficient than PoW and PoS, but slightly lower than DPoS. However, its degree of centralization remains far below that of DPoS. And through the downgrade mechanism, the proposed consensus algorithm can detect and downgrade the malicious nodes timely to ensure the security and good operation of system.","['Blockchain', 'Consensus algorithm', 'Peer-to-peer computing', 'Protocols', 'Contracts', 'Bitcoin']","['Blockchain', 'consensus algorithm', 'delegated proof of stake with downgrade', 'downgrade mechanism', 'efficiency', 'fairness', 'decentralization']"
"Medical expert systems are part of the portable and smart healthcare monitoring devices used in day-to-day life. Arrhythmic beat classification is mainly used in electrocardiogram (ECG) abnormality detection for identifying heart related problems. In this paper, ECG signal preprocessing and support vector machine-based arrhythmic beat classification are performed to categorize into normal and abnormal subjects. In ECG signal preprocessing, a delayed error normalized LMS adaptive filter is used to achieve high speed and low latency design with less computational elements. Since the signal processing technique is developed for remote healthcare systems, white noise removal is mainly focused. Discrete wavelet transform is applied on the preprocessed signal for HRV feature extraction and machine learning techniques are used for performing arrhythmic beat classification. In this paper, SVM classifier and other popular classifiers have been used on noise removed feature extracted signal for beat classification. Results indicate that the performance of SVM classifier is better than other machine learning-based classifiers.","['Electrocardiography', 'Feature extraction', 'Adaptive filters', 'Heart rate variability', 'IIR filters', 'Support vector machines']","['Adaptive filter', 'arrhythmic beat classification', 'ECG preprocessing', 'SVM classifier', 'machine learning']"
"The 2019 novel coronavirus (2019-nCoV) outbreak has been treated as a Public Health Emergency of International Concern by the World Health Organization. This work made an early prediction of the 2019-nCoV outbreak in China based on a simple mathematical model and limited epidemiological data. Combing characteristics of the historical epidemic, we found part of the released data is unreasonable. Through ruling out the unreasonable data, the model predictions exhibit that the number of the cumulative 2019-nCoV cases may reach 76,000 to 230,000, with a peak of the unrecovered infectives (22,000-74,000) occurring in late February to early March. After that, the infected cases will rapidly monotonically decrease until early May to late June, when the 2019-nCoV outbreak will fade out. Strong anti-epidemic may reduce the cumulative infected cases by 40%-49%. The improvement of medical care can also lead to about one-half transmission decrease and effectively shorten the duration of the 2019-nCoV.","['Mathematical model', 'Statistics', 'Predictive models', 'Viruses (medical)', 'Urban areas', 'Diseases', 'China']","['Epidemic transmission', 'infection rate', 'mathematical model', 'novel coronavirus', 'prediction', 'removal rate']"
"Emotion is intrinsic to humans and consequently, emotion understanding is a key part of human-like artificial intelligence (AI). Emotion recognition in conversation (ERC) is becoming increasingly popular as a new research frontier in natural language processing (NLP) due to its ability to mine opinions from the plethora of publicly available conversational data on platforms such as Facebook, Youtube, Reddit, Twitter, and others. Moreover, it has potential applications in health-care systems (as a tool for psychological analysis), education (understanding student frustration), and more. In Addition, ERC is also extremely important for generating emotion-aware dialogues that require an understanding of the user’s emotions. Catering to these needs calls for effective and scalable conversational emotion-recognition algorithms. However, it is a difficult problem to solve because of several research challenges. In this paper, we discuss these challenges and shed light on recent research in this field. We also describe the drawbacks of these approaches and discuss the reasons why they fail to successfully overcome the research challenges in ERC.","['Emotion recognition', 'Task analysis', 'Context modeling', 'Taxonomy', 'Natural language processing', 'Pragmatics']","['Emotion recognition', 'sentiment analysis', 'dialogue systems', 'natural language processing']"
"Electricity is the commonest commodity for most businesses in our world today. The use of electricity has been a breakthrough for the discovery of new technologies and has become the main driving force behind several innovations. With the introduction of smart grid systems, there have been improvements in how utility companies interact with their customers with regards to electricity use. However, since the readings are done via the Internet, there is the tendency for the data to be compromised when it gets into the hands of the wrong people. Moreover, customers mostly do not know why they pay huge amounts and which appliances use more electricity, since they are not privy to the readings. The sovereign blockchain technology, which provides transparency and provenance, is utilized in this paper to mitigate these above mentioned problems. A smart contract, which executes laid down procedures to provide a trust-based system between participants on the network is also implemented. Our system proves very efficient as the user can monitor how the electricity is used, and it also provides a platform where there is no manipulation from either party.","['Smart grids', 'Contracts', 'Companies', 'Public key', 'Monitoring']","['Sovereign blockchain', 'smart meter', 'smart grid network', 'electricity', 'event']"
"Mental stress has become a social issue and could become a cause of functional disability during routine work. In addition, chronic stress could implicate several psychophysiological disorders. For example, stress increases the likelihood of depression, stroke, heart attack, and cardiac arrest. The latest neuroscience reveals that the human brain is the primary target of mental stress, because the perception of the human brain determines a situation that is threatening and stressful. In this context, an objective measure for identifying the levels of stress while considering the human brain could considerably improve the associated harmful effects. Therefore, in this paper, a machine learning (ML) framework involving electroencephalogram (EEG) signal analysis of stressed participants is proposed. In the experimental setting, stress was induced by adopting a well-known experimental paradigm based on the montreal imaging stress task. The induction of stress was validated by the task performance and subjective feedback. The proposed ML framework involved EEG feature extraction, feature selection (receiver operating characteristic curve, t-test and the Bhattacharya distance), classification (logistic regression, support vector machine and naïve Bayes classifiers) and tenfold cross validation. The results showed that the proposed framework produced 94.6% accuracy for two-level identification of stress and 83.4% accuracy for multiple level identification. In conclusion, the proposed EEG-based ML framework has the potential to quantify stress objectively into multiple levels. The proposed method could help in developing a computer-aided diagnostic tool for stress detection.","['Stress', 'Electroencephalography', 'Feature extraction', 'Support vector machines', 'Cardiac arrest', 'Object recognition', 'Psychology']","['Absolute power', 'amplitude asymmetry', 'coherence', 'EEG', 'machine learning', 'mental stress levels', 'phase lag', 'relative power', 'support vector machine', 't-test']"
"Methods to overcome metal artifacts in computed tomography (CT) images have been researched and developed for nearly 40 years. When X-rays pass through a metal object, depending on its size and density, different physical effects will negatively affect the measurements, most notably beam hardening, scatter, noise, and the non-linear partial volume effect. These phenomena severely degrade image quality and hinder the diagnostic power and treatment outcomes in many clinical applications. In this paper, we first review the fundamental causes of metal artifacts, categorize metal object types, and present recent trends in the CT metal artifact reduction (MAR) literature. To improve image quality and recover information about underlying structures, many methods and correction algorithms have been proposed and tested. We comprehensively review and categorize these methods into six different classes of MAR: metal implant optimization, improvements to the data acquisition process, data correction based on physics models, modifications to the reconstruction algorithm (projection completion and iterative reconstruction), and image-based post-processing. The primary goals of this paper are to identify the strengths and limitations of individual MAR methods and overall classes, and establish a relationship between types of metal objects and the classes that most effectively overcome their artifacts. The main challenges for the field of MAR continue to be cases with large, dense metal implants, as well as cases with multiple metal objects in the field of view. Severe photon starvation is difficult to compensate for with only software corrections. Hence, the future of MAR seems to be headed toward a combined approach of improving the acquisition process with dual-energy CT, higher energy X-rays, or photon-counting detectors, along with advanced reconstruction approaches. Additional outlooks are addressed, including the need for a standardized evaluation system to compare MAR methods.","['Metal objects', 'Computed tomography', 'X-rays', 'Image quality', 'Image reconstruction']","['Biomedical imaging', 'computed tomography', 'radiation therapy', 'reconstruction algorithms', 'metal artifact reduction']"
"State-of-charge (SOC), which indicates the remaining capacity at the current cycle, is the key to the driving range prediction of electric vehicles and optimal charge control of rechargeable batteries. In this paper, we propose a combined convolutional neural network (CNN) - long short-term memory (LSTM) network to infer battery SOC from measurable data, such as current, voltage, and temperature. The proposed network shares the merits of both CNN and LSTM networks and can extract both spatial and temporal features from input data. The proposed network is trained using data collected from different discharge profiles, including a dynamic stress test, federal urban driving schedule, and US06 test. The performance of the proposed network is evaluated using data collected from a new combined dynamic loading profile in terms of estimation accuracy and robustness against the unknown initial state. The experimental results show that the proposed CNN-LSTM network well captures the nonlinear relationships between SOC and measurable variables and presents better tracking performance than the LSTM and CNN networks. In case of unknown initial SOCs, the proposed network fast converges to true SOC and, then, presents smooth and accurate results, with maximum mean average error under 1% and maximum root mean square error under 2%. Moreover, the proposed network well learns the influence of ambient temperature and can estimate battery SOC under varying temperatures with maximum mean average error under 1.5% and maximum root mean square error under 2%.","['Batteries', 'State of charge', 'Voltage measurement', 'Estimation', 'Temperature measurement', 'Current measurement', 'Battery charge measurement']","['State-of-charge estimation', 'long short-term memory', 'convolutional neural network', 'lithium-ion batteries']"
"With the exponential rise in the number of devices, the Internet of Things (IoT) is geared toward edge-centric computing to offer high bandwidth, low latency, and improved connectivity. In contrast, legacy cloud-centric platforms offer deteriorated bandwidth and connectivity that affect the quality of service. Edge-centric Internet of Things-based technologies, such as fog and mist computing, offer distributed and decentralized solutions to resolve the drawbacks of cloud-centric models. However, to foster distributed edge-centric models, a decentralized consensus system is necessary to incentivize all participants to share their edge resources. This paper is motivated by the shortage of comprehensive reviews on decentralized consensus systems for edge-centric Internet of Things that elucidates myriad of consensus facets, such as data structure, scalable consensus ledgers, and transaction models. Decentralized consensus systems adopt either blockchain or blockchainless directed acyclic graph technologies, which serve as immutable public ledgers for transactions. This paper scrutinizes the pros and cons of state-of-the-art decentralized consensus systems. With an extensive literature review and categorization based on existing decentralized consensus systems, we propose a thematic taxonomy. The pivotal features and characteristics associated with existing decentralized consensus systems are analyzed via a comprehensive qualitative investigation. The commonalities and variances among these systems are analyzed using key criteria derived from the presented literature. Finally, several open research issues on decentralized consensus for edge-centric IoT are presented, which should be highlighted regarding centralization risk and deficiencies in blockchain/blockchainless solutions.","['Cloud computing', 'Edge computing', 'Taxonomy', 'Data structures', 'Internet of Things', 'Electronic mail']","['Blockchain', 'decentralized consensus systems', 'directed acyclic graph', 'edge-centric Internet of Things']"
"In this paper, we propose an eight-port/four-resonator slot antenna array with a dual-polarized function for multiple-input-multiple-output (MIMO) 5G mobile terminals. The design is composed of four dual-polarized square-ring slot radiators fed by pairs of microstrip-line structures. The radiation elements are designed to operate at 3.6 GHz and are located on the corners of the smartphone PCB. The square-ring slot radiators provide good dual-polarization characteristic with similar performances in terms of fundamental radiation characteristics. In order to improve the isolation and also reduce the mutual coupling characteristic between the adjunct microstrip-line feeding ports of the dual-polarized radiators, a pair of circular-ring/open-ended parasitic structures is embedded across each square-ring slot radiator. The -10-dB impedance bandwidth of each antenna-element is 3.4-3.8 GHz. However, for -6-dB impedance bandwidth, this value is 600 MHz (3.3-3.9 GHz). The proposed MIMO antenna offers good S-parameters, high-gain radiation patterns, and sufficient total efficiencies, even though it is arranged on a high-loss FR-4 dielectric. The SAR function and the radiation characteristics of the proposed design in the vicinity of user-hand/user-head are studied. A prototype of the proposed smartphone antenna is fabricated, and good measurements are provided. The antenna provides good features with a potential application for use in the 5G mobile terminals.","['Slot antennas', 'Microstrip antennas', 'Antenna measurements', 'MIMO communication', 'Antenna radiation patterns', '5G mobile communication']","['5G', 'dual-polarized antenna', 'MIMO system', 'mobile terminal', 'ring slot antenna']"
"Traditional feature extraction methods are used to extract the features of signal to construct the fault feature matrix, which exists the complex structure, higher correlation, and redundancy. This will increase the complex fault classification and seriously affect the accuracy and efficiency of fault identification. In order to solve these problems, a new fault diagnosis (PABSFD) method based on the principal component analysis (PCA) and the broad learning system (BLS) is proposed for rotor system in this paper. In the proposed PABSFD method, the PCA with revealing the signal essence is used to reduce the dimension of the constructed feature matrix and decrease the linear feature correlation between data and eliminate the redundant attributes in order to obtain the low-dimensional feature matrix with retaining the essential features for the classification model. Then, the BLS with low time complexity and high classification accuracy is regarded as a classification model to realize the fault identification; it can efficiently accomplish the fault classification of rotor system. Finally, the actual vibration data of rotor system are selected to test and verify the effectiveness of the PABSFD method. The experimental results show that the PCA method can effectively eliminate the feature correlation and realize the dimension reduction of the feature matrix, the BLS can take on better adaptability, faster computation speed, and higher classification accuracy, and the PABSFD method can efficiently and accurately obtain the fault diagnosis results.","['Fault diagnosis', 'Principal component analysis', 'Feature extraction', 'Rotors', 'Dimensionality reduction', 'Covariance matrices', 'Correlation']","['Rotor system', 'fault diagnosis', 'principal component analysis (PCA)', 'broad learning system (BLS)', 'dimension reduction']"
"A systematic and comprehensive review of security and privacy-preserving challenges in e-health solutions indicates various privacy preserving approaches to ensure privacy and security of electronic health records (EHRs) in the cloud. This paper highlights the research challenges and directions concerning cyber security to build a comprehensive security model for EHR. We carry an intensive study in the IEEE, Science Direct, Google Scholar, PubMed, and ACM for papers on EHR approach published between 2000 and 2018 and summarized them in terms of the architecture types as well as evaluation strategies. We surveyed, investigated, and reviewed various aspects of several articles and identified the following tasks: 1) EHR security and privacy; 2) security and privacy requirements of e-health data in the cloud; 3) EHR cloud architecture, and; 4) diverse EHR cryptographic and non-cryptographic approaches. We also discuss some crucial issues and the ample opportunities for advanced research related to security and privacy of EHRs. Since big data provide a great mine of information and knowledge in e-Health applications, serious privacy and security challenges that require immediate attention exist. Studies must focus on efficient comprehensive security mechanisms for EHR and also explore techniques to maintain the integrity and confidentiality of patients' information.","['Security', 'Medical services', 'Data privacy', 'Privacy', 'Cloud computing', 'Servers', 'Electronic medical records']","['e-health', 'electronic health record', 'EHR cryptographic and non-cryptographic', 'security and privacy', 'systematic review']"
"The applications of compressive sensing (CS) in the field of information security have captured a great deal of researchers' attention in the past decade. To supply guidance for researchers from a comprehensive perspective, this paper, for the first time, reviews CS in information security field from two aspects: theoretical security and application security. Moreover, the CS applied in image cipher is one of the most widespread applications, as its characteristics of dimensional reduction and random projection can be utilized and integrated into image cryptosystems, which can achieve simultaneous compression and encryption of an image or multiple images. With respect to this application, the basic framework designs and the corresponding analyses are investigated. Specifically, the investigation proceeds from three aspects, namely, image ciphers based on chaos and CS, image ciphers based on optics and CS, and image ciphers based on chaos, optics, and CS. A total of six frameworks are put forward. Meanwhile, their analyses in terms of security, advantages, disadvantages, and so on are presented. At last, we attempt to indicate some other possible application research topics in future.","['Chaos theory', 'Information security', 'Compressed sensing', 'Ciphers', 'Optical fiber communication', 'Image processing', 'Computer security', 'Cryptography']","['Compressive sensing', 'theoretical security', 'application security', 'image cipher', 'chaos', 'optics']"
"Pervasive growth and usage of the Internet and mobile applications have expanded cyberspace. The cyberspace has become more vulnerable to automated and prolonged cyberattacks. Cyber security techniques provide enhancements in security measures to detect and react against cyberattacks. The previously used security systems are no longer sufficient because cybercriminals are smart enough to evade conventional security systems. Conventional security systems lack efficiency in detecting previously unseen and polymorphic security attacks. Machine learning (ML) techniques are playing a vital role in numerous applications of cyber security. However, despite the ongoing success, there are significant challenges in ensuring the trustworthiness of ML systems. There are incentivized malicious adversaries present in the cyberspace that are willing to game and exploit such ML vulnerabilities. This paper aims to provide a comprehensive overview of the challenges that ML techniques face in protecting cyberspace against attacks, by presenting a literature on ML techniques for cyber security including intrusion detection, spam detection, and malware detection on computer networks and mobile networks in the last decade. It also provides brief descriptions of each ML method, frequently used security datasets, essential ML tools, and evaluation metrics to evaluate a classification model. It finally discusses the challenges of using ML techniques in cyber security. This paper provides the latest extensive bibliography and the current trends of ML in cyber security.","['Computer crime', 'Machine learning', 'Internet', 'Deep learning', 'Market research', 'Malware', 'Intrusion detection']","['Cyber security', 'deep learning', 'intrusion detection', 'malware', 'machine learning', 'spam']"
"In this paper, online deep learning (DL)-based channel estimation algorithm for doubly selective fading channels is proposed by employing the deep neural network (DNN). With properly selected inputs, the DNN can not only exploit the features of channel variation from previous channel estimates but also extract additional features from pilots and received signals. Moreover, the DNN can take the advantages of the least squares estimation to further improve the performance of channel estimation. The DNN is first trained with simulated data in an off-line manner and then it could track the dynamic channel in an online manner. To reduce the performance degradation from random initialization, a pre-training approach is designed to refine the initial parameters of the DNN with several epochs of training. The proposed algorithm benefits from the excellent learning and generalization capability of DL and requires no prior knowledge about the channel statistics. Hence, it is more suitable for communication systems with modeling errors or non-stationary channels, such as high-mobility vehicular systems, underwater acoustic systems, and molecular communication systems. The numerical results show that the proposed DL-based algorithm outperforms the existing estimator in terms of both efficiency and robustness, especially when the channel statistics are time-varying.","['Channel estimation', 'Fading channels', 'OFDM', 'Indexes', 'Channel models', 'Training', 'Deep learning']","['Deep learning', 'neural networks', 'channel estimation', 'doubly selective channel', 'LS oriented input', 'pre-training']"
"Dempster-Shafer evidence theory is efficient to deal with uncertain information. One assumption of evidence theory is that the source of information should be independent when combined by Dempster's rule for evidence combination. However, the assumption does not coincide with the reality. A lot of works are done to solve the problem about the independence. The existing method based on the statistical parameter Pearson correlation coefficient discount is one of the feasible methods. However, the Pearson correlation coefficient is only used to characterize the linear correlation between the attributes of the normal distribution. In this paper, a new method is proposed, the Pearson correlation coefficient and Shearman correlation coefficient to generate the discounting factor. Taking the parametric statistic and nonparametric statistic into consideration, the proposed method is more efficient. The experiments on wine data set are illustrated to show the efficiency of our proposed method.","['Correlation', 'Magnesium', 'Gaussian distribution', 'Image color analysis', 'Manganese', 'Matrix converters', 'Parametric statistics']","['Dempster-Shafer theory', 'dependent evidence combination', 'Pearson coefficient', 'Shearman coefficient', 'total coefficient']"
"Fifth-generation (5G) telecommunication systems are expected to meet the world market demands of accessing and delivering services anywhere and anytime. The Non-Terrestrial Network (NTN) systems are able to satisfy the requests of anywhere and anytime connections by offering wide-area coverage and ensuring service availability, continuity, and scalability. In this work, we review the 3GPP NTN features and their potential for satisfying the user expectations in 5G & beyond networks. The state of the art, current 3GPP research activities, and open issues are summarized to highlight the importance of NTN over the wireless communication landscape. Future research directions are also identified to assess the role of NTN in 5G and beyond systems.","['5G mobile communication', 'Satellite broadcasting', 'Satellites', 'Earth', '3GPP', 'Planetary orbits']","['Non-terrestrial network', 'satellite communication', 'new radio', '5G system and beyond']"
"Applications of Blockchain (BC) technology and Cyber-Physical Systems (CPS) are increasing exponentially. However, framing resilient and correct smart contracts (SCs) for these smart application is a quite challenging task because of the complexity associated with them. SC is modernizing the traditional industrial, technical, and business processes. It is self-executable, self-verifiable, and embedded into the BC that eliminates the need for trusted third-party systems, which ultimately saves administration as well as service costs. It also improves system efficiency and reduces the associated security risks. However, SCs are well encouraging the new technological reforms in Industry 4.0, but still, various security and privacy challenges need to be addressed. In this paper, a survey on SC security vulnerabilities in the software code that can be easily hacked by a malicious user or may compromise the entire BC network is presented. As per the literature, the challenges related to SC security and privacy are not explored much by the authors around the world. From the existing proposals, it has been observed that designing a complex SCs cannot mitigate its privacy and security issues. So, this paper investigates various Artificial Intelligence (AI) techniques and tools for SC privacy protection. Then, open issues and challenges for AI-based SC are analyzed. Finally, a case study of retail marketing is presented, which uses AI and SC to preserve its security and privacy.","['Artificial intelligence', 'Privacy', 'Security', 'Smart contracts', 'Tools', 'Blockchain']","['Cyber-physical system', 'blockchain', 'smart contract', 'artificial intelligence', 'security', 'privacy']"
"Waveguide slot array antennas can have high gain, since the power distribution network (waveguide) has low loss, enabling a large aperture size. Rapid response to requirements for frequency, gain, and skew angle inspired a search for an alternative to conventional, commercial slot array acquisition. The solution to this requirement is high-resolution 3-D printing of modified slot arrays combined with metal plating. Modification of the structure included removal of wall material in regions where these openings would not cause radiation, opening the structure to enable metal plating while not affecting gain. A 3-D-printed slot array requires no further assembly, unlike conventional waveguide-based arrays that require soldering or brazing, block machining, or plate assembled, brazed structures. Stereolithography 3-D printing of plastic slot array antennas, modified to enable metal plating, has been used to produce a 30 dBi realized gain slot array at 21 GHz for under $1000 USD in a few weeks. Metal plated, 3-D printed plastic slot arrays are also very lightweight in comparison with conventional metal structures. Performance of 3-D printed, metal plated slot arrays has been shown to be identical to conventional metal structures at frequencies up to 22 GHz.","['Slat arrays', 'Waveguide arrays', 'Slot antennas', 'Power distribution', 'Antenna arrays', 'Three-dimensional printing', 'Metal structures']","['Slot arrays', 'waveguide arrays']"
"The security of image steganography is an important basis for evaluating steganography algorithms. Steganography has recently made great progress in the long-term confrontation with steganalysis. To improve the security of image steganography, steganography must have the ability to resist detection by steganalysis algorithms. Traditional embedding-based steganography embeds the secret information into the content of an image, which unavoidably leaves a trace of the modification that can be detected by increasingly advanced machine-learning-based steganalysis algorithms. The concept of steganography without embedding (SWE), which does not need to modify the data of the carrier image, appeared to overcome the detection of machine-learning-based steganalysis algorithms. In this paper, we propose a novel image SWE method based on deep convolutional generative adversarial networks. We map the secret information into a noise vector and use the trained generator neural network model to generate the carrier image based on the noise vector. No modification or embedding operations are required during the process of image generation, and the information contained in the image can be extracted successfully by another neural network, called the extractor, after training. The experimental results show that this method has the advantages of highly accurate information extraction and a strong ability to resist detection by state-of-the-art image steganalysis algorithms.","['Gallium nitride', 'Distortion', 'Brain modeling', 'Training', 'Digital images', 'Resists', 'Transform coding']","['Steganography', 'without embedding', 'coverless', 'generative adversarial networks']"
"Nowadays, a large amount of information has to be transmitted or processed. This implies high-power processing, large memory density, and increased energy consumption. In several applications, such as imaging, radar, speech recognition, and data acquisition, the signals involved can be considered sparse or compressive in some domain. The compressive sensing theory could be a proper candidate to deal with these constraints. It can be used to recover sparse or compressive signals with fewer measurements than the traditional methods. Two problems must be addressed by compressive sensing theory: design of the measurement matrix and development of an efficient sparse recovery algorithm. These algorithms are usually classified into three categories: convex relaxation, non-convex optimization techniques, and greedy algorithms. This paper intends to supply a comprehensive study and a state-of-the-art review of these algorithms to researchers who wish to develop and use them. Moreover, a wide range of compressive sensing theory applications is summarized and some open research challenges are presented.","['Matching pursuit algorithms', 'Compressed sensing', 'Sparse matrices', 'Signal processing algorithms', 'Estimation', 'Coherence', 'Sensors']","['Bayesian compressive sensing', 'compressive sensing', 'convex relaxation', 'greedy algorithms', 'sparse recovery algorithms', 'sparse signals']"
"In this study, a new technique is proposed to forecast short-term electrical load. Load forecasting is an integral part of power system planning and operation. Precise forecasting of load is essential for unit commitment, capacity planning, network augmentation and demand side management. Load forecasting can be generally categorized into three classes such as short-term, midterm and long-term. Short-term forecasting is usually done to predict load for next few hours to few weeks. In the literature, various methodologies such as regression analysis, machine learning approaches, deep learning methods and artificial intelligence systems have been used for short-term load forecasting. However, existing techniques may not always provide higher accuracy in short-term load forecasting. To overcome this challenge, a new approach is proposed in this paper for short-term load forecasting. The developed method is based on the integration of convolutional neural network (CNN) and long short-term memory (LSTM) network. The method is applied to Bangladesh power system to provide short-term forecasting of electrical load. Also, the effectiveness of the proposed technique is validated by comparing the forecasting errors with that of some existing approaches such as long short-term memory network, radial basis function network and extreme gradient boosting algorithm. It is found that the proposed strategy results in higher precision and accuracy in short-term load forecasting.","['Forecasting', 'Load forecasting', 'Computer architecture', 'Logic gates', 'Power systems', 'Load modeling', 'Time series analysis']","['Short-term load forecasting', 'convolutional neural network', 'long-short-term memory network', 'Bangladesh power system', 'evaluation metrics']"
"Blockchain is a revolutionary technology that is making a great impact on modern society due to its transparency, decentralization, and security properties. Blockchain gained considerable attention due to its very first application of Cryptocurrencies e.g., Bitcoin. In the near future, Blockchain technology is determined to transform the way we live, interact, and perform businesses. Recently, academics, industrialists, and researchers are aggressively investigating different aspects of Blockchain as an emerging technology. Unlike other Blockchain surveys focusing on either its applications, challenges, characteristics, or security, we present a comprehensive survey of Blockchain technology's evolution, architecture, development frameworks, and security issues. We also present a comparative analysis of frameworks, classification of consensus algorithms, and analysis of security risks & cryptographic primitives that have been used in the Blockchain so far. Finally, this paper elaborates on key future directions, novel use cases and open research challenges, which could be explored by researchers to make further advances in this field.","['Blockchain', 'Security', 'Cryptography', 'Smart contracts', 'Peer-to-peer computing', 'Consensus algorithm', 'Computer architecture']","['Evolution of blockchain', 'blockchain architecture', 'smart contracts', 'blockchain applications', 'development frameworks', 'blockchain security']"
"Having gained momentum from its promise of centralized control over distributed network architectures at bargain costs, software-defined Networking (SDN) is an ever-increasing topic of research. SDN offers a simplified means to dynamically control multiple simple switches via a single controller program, which contrasts with current network infrastructures where individual network operators manage network devices individually. Already, SDN has realized some extraordinary use cases outside of academia with companies, such as Google, AT&T, Microsoft, and many others. However, SDN still presents many research and operational challenges for government, industry, and campus networks. Because of these challenges, many SDN solutions have developed in an ad hoc manner that are not easily adopted by other organizations. Hence, this paper seeks to identify some of the many challenges where new and current researchers can still contribute to the advancement of SDN and further hasten its broadening adoption by network operators.","['Security', 'Government', 'Industries', 'Virtualization', 'Standards organizations', 'Centralized control']","['Software-defined networking (SDN)', 'network virtualization (NV)', 'network functions virtualization (NFV)', 'standards', 'SDN interfaces and APIs', 'data plane', 'middleboxes', 'SDN security', 'hybrid networks', 'software-defined exchange (SDX)', 'software-defined infrastructure (SDI)', 'software-defined wireless networks (SDWN)', 'Internet of Things (IoT)', 'information-centric networking (ICN)', 'cloud', 'software-defined RAN', '5G']"
"Recently, the advancement in communications, intelligent transportation systems, and computational systems has opened up new opportunities for intelligent traffic safety, comfort, and efficiency solutions. Artificial intelligence (AI) has been widely used to optimize traditional data-driven approaches in different areas of the scientific research. Vehicle-to-everything (V2X) system together with AI can acquire the information from diverse sources, can expand the driver's perception, and can predict to avoid potential accidents, thus enhancing the comfort, safety, and efficiency of the driving. This paper presents a comprehensive survey of the research works that have utilized AI to address various research challenges in V2X systems. We have summarized the contribution of these research works and categorized them according to the application domains. Finally, we present open problems and research challenges that need to be addressed for realizing the full potential of AI to advance V2X systems.","['Vehicle-to-everything', 'Particle swarm optimization', 'Long Term Evolution', 'Safety', 'Vehicular ad hoc networks', 'Autonomous vehicles']","['Artificial intelligence', 'machine learning', 'VANETs', 'V2X', 'predictions', 'platoon', 'VEC']"
"Augmented reality smart glasses (ARSG) are increasingly popular and have been identified as a vital technology supporting shop-floor operators in the smart factories of the future. By improving our knowledge of how to efficiently evaluate and select the ARSG for the shop-floor context, this paper aims to facilitate and accelerate the adoption of the ARSG by the manufacturing industry. The market for ARSG has exploded in recent years, and the large variety of products to select from makes it not only difficult but also time consuming to identify the best alternative. To address this problem, this paper presents an efficient step-by-step process for evaluating the ARSG, including concrete guidelines as to what parameters to consider and their recommended minimum values. Using the suggested evaluation process, manufacturing companies can quickly make optimal decisions about what products to implement on their shop floors. This paper demonstrates the evaluation process in practice, presenting a comprehensive review of currently available products along with a recommended best buy. This paper also identifies and discusses topics meriting research attention to ensure that the ARSG are successfully implemented on the industrial shop floor.","['Augmented reality', 'Glass', 'Production facilities', 'Guidelines', 'Companies', 'Optics']","['Augmented reality smart glasses', 'smart factory', 'augmented reality', 'industrial operator support']"
"Parkinson's Disease (PD) is a progressive neurodegenerative disease with multiple motor and non-motor characteristics. PD patients commonly face vocal impairments during the early stages of the disease. So, diagnosis systems based on vocal disorders are at the forefront on recent PD detection studies. Our study proposes two frameworks based on Convolutional Neural Networks to classify Parkinson's Disease (PD) using sets of vocal (speech) features. Although, both frameworks are employed for the combination of various feature sets, they have difference in terms of combining feature sets. While the first framework combines different feature sets before given to 9-layered CNN as inputs, whereas the second framework passes feature sets to the parallel input layers which are directly connected to convolution layers. Thus, deep features from each parallel branch are extracted simultaneously before combining in the merge layer. Proposed models are trained with dataset taken from UCI Machine Learning repository and their performances are validated with Leave-One-Person-Out Cross Validation (LOPO CV). Due to imbalanced class distribution in our data, F-Measure and Matthews Correlation Coefficient metrics are used for the assessment along with accuracy. Experimental results show that the second framework seems to be very promising, since it is able to learn deep features from each feature set via parallel convolution layers. Extracted deep features are not only successful at distinguishing PD patients from healthy individuals but also effective in boosting up the discriminative power of the classifiers.","['Feature extraction', 'Convolution', 'Deep learning', 'Support vector machines', 'Frequency measurement', ""Parkinson's disease""]","['Convolutional neural networks', 'deep learning', 'health informatics', 'Parkinson’s disease classification', 'vocal features']"
"Medical image analysis is currently experiencing a paradigm shift due to deep learning. This technology has recently attracted so much interest of the Medical Imaging Community that it led to a specialized conference in “Medical Imaging with Deep Learning” in the year 2018. This paper surveys the recent developments in this direction and provides a critical review of the related major aspects. We organize the reviewed literature according to the underlying pattern recognition tasks and further sub-categorize it following a taxonomy based on human anatomy. This paper does not assume prior knowledge of deep learning and makes a significant contribution in explaining the core deep learning concepts to the non-experts in the Medical Community. This paper provides a unique computer vision/machine learning perspective taken on the advances of deep learning in medical imaging. This enables us to single out “lack of appropriately annotated large-scale data sets” as the core challenge (among other challenges) in this research direction. We draw on the insights from the sister research fields of computer vision, pattern recognition, and machine learning, where the techniques of dealing with such challenges have already matured, to provide promising directions for the Medical Imaging Community to fully harness deep learning in the future.","['Biomedical imaging', 'Deep learning', 'Computational modeling', 'Task analysis', 'Computer vision', 'Data models']","['Deep learning', 'medical imaging', 'artificial neural networks', 'survey', 'tutorial', 'data sets']"
"Smartphones and smartwatches, which include powerful sensors, provide a readily available platform for implementing and deploying mobile motion-based behavioral biometrics. However, the few studies that utilize these commercial devices for motion-based biometrics are quite limited in terms of the sensors and physical activities that they evaluate. In many such studies, only the smartwatch accelerometer is utilized and only one physical activity, walking, is investigated. In this study we consider the accelerometer and gyroscope sensor on both the smartphone and smartwatch, and determine which combination of sensors performs best. Furthermore, eighteen diverse activities of daily living are evaluated for their biometric efficacy and, unlike most other studies, biometric identification is evaluated in addition to biometric authentication. The results presented in this article show that motion-based biometrics using smartphones and/or smartwatches yield good results, and that these results hold for the eighteen activities. This suggests that zero-effort continuous biometrics based on normal activities of daily living is feasible, and also demonstrates that certain easy-to-perform activities, such as clapping, may be a viable alternative (or supplement) to gait-based biometrics.","['Biometrics (access control)', 'Authentication', 'Accelerometers', 'Biological system modeling', 'Gyroscopes', 'Biosensors']","['Authentication', 'biometrics', 'data mining', 'gait recognition', 'identification', 'sensors', 'smartphone', 'smartwatch', 'ubiquitous computing']"
"Self-governing small regions of power systems, known as “microgrids”, are enabling the integration of small-scale renewable energy sources (RESs) while improving the reliability and energy efficiency of the electricity network. Microgrids can be primarily classified into three types based on their voltage characteristics and system architecture; 1) AC microgrids, 2) DC microgrids, and 3) Hybrid AC/DC microgrids. This paper presents a comprehensive review of stability, control, power management and fault ride-through (FRT) strategies for the AC, DC, and hybrid AC/DC microgrids. This paper also classifies microgrids in terms of their intended application and summarises the operation requirements stipulated in standards (e.g., IEEE Std. 1547-2018). The control strategies for each microgrid architecture are reviewed in terms of their operating principle and performance. In terms of the hybrid AC/DC microgrids, specific control aspects, such as mode transition and coordinated control between multiple interlinking converters (ILCs) and energy storage system (ESS) are analysed. A case study is also presented on the dynamic performance of a hybrid AC/DC microgrid under different control strategies and dynamic loads. Hybrid AC/DC microgrids shown to have more advantages in terms of economy and efficiency compared with the other microgrid architectures. This review shows that hierarchical control schemes, such as primary, secondary, and tertiary control are very popular among all three microgrid types. It is shown that the hybrid AC/DC microgrids require more complex control strategies for power management and control compared to AC or DC microgrids due to their dependency on the ILC controls and the operation mode of the hybrid AC/DC microgrid. Case study illustrated the significant effects of microgrid feeder characteristics on the dynamic performance of the hybrid AC/DC microgrid. It is also revealed that any transient conditions either in the AC or DC microgrids could propagate through the ILC affecting the entire microgrid dynamic performance. Additionally, the critical control issues and the future research challenges of microgrids are also discussed in this paper.","['Microgrids', 'Power system stability', 'Hybrid power systems', 'Voltage control', 'IEEE Standards', 'Reactive power', 'Stability criteria']","['Energy storage system (ESS)', 'hybrid AC/DC microgrid', 'IEEE Std. 1547-2018', 'interlinking converter (ILC)', 'microgrid stability', 'power management', 'renewable energy sources (RESs)']"
"As a major challenge and opportunity for traditional manufacturing, intelligent manufacturing is facing the needs of sustainable development in future. Sustainability assessment undoubtedly plays a pivotal role for future development of intelligent manufacturing. Aiming at this, the paper presents the digital twin driven information architecture of sustainability assessment oriented for dynamic evolution under the whole life cycle based on the classic digital twin mapping system. The sustainability assessment method segment of the architecture includes indicator system building, indicator value determination, indicator importance degree determination and intelligent manufacturing project assessing. A novel approach for treating the ambiguity of expert' judgment in indicator value determination by introducing trapezoidal fuzzy number into analytic hierarchy process is proposed, while the complexity of the influence relationship among the indicators is processed by the integration of complex networks modeling and PROMETHEE II for the indicator importance degree determination. A two-stage evidence combination model based on evidence theory is built for intelligent manufacturing project assessing lastly. The presented digital-twin-driven information architecture and the sustainability assessment method is tested and validated on a study of sustainability assessment of 8 intelligent manufacturing projects of an air conditioning enterprise. The results of the presented method were validated by comparing them with the results of the fuzzy and rough extension of the PROMETHEE II, TOPSIS and VIKOR methods, indicator importance degree determining method by entropy and indicator value determining method by accurate expert scoring.","['Sustainable development', 'Manufacturing', 'Production', 'Decision making', 'Physical layer', 'Technological innovation', 'Information architecture']","['Digital twin', 'sustainability', 'intelligent manufacturing', 'fuzzy number', 'analytic hierarchy process', 'complex networks', 'PROMETHEE II', 'evidence theory']"
"With the rapid evolution of network traffic diversity, the understanding of network traffic has become more pivotal and more formidable. Previously, traffic classification and intrusion detection require a burdensome analyzing of various traffic features and attack-related characteristics by experts, and even, private information might be required. However, due to the outdated features labeling and privacy protocols, the existing approaches may not fit with the characteristics of the changing network environment anymore. In this paper, we present a light-weight framework with the aid of deep learning for encrypted traffic classification and intrusion detection, termed as deep-full-range (DFR). Thanks to deep learning, DFR is able to learn from raw traffic without manual intervention and private information. In such a framework, our proposed algorithms are compared with other state-of-the-art methods using two public datasets. The experimental results show that our framework not only can outperform the state-of-the-art methods by averaging 13.49% on encrypted traffic classification’s F1 score and by averaging 12.15% on intrusion detection’s F1 score but also require much lesser storage resource requirement.","['Cryptography', 'Intrusion detection', 'Deep learning', 'Data preprocessing', 'Feature extraction', 'Privacy', 'Protocols']","['Encrypted traffic classification', 'network intrusion detection', 'deep learning', 'end-to-end']"
"A computer-aided diagnosis (CAD) system based on mammograms enables early breast cancer detection, diagnosis, and treatment. However, the accuracy of the existing CAD systems remains unsatisfactory. This paper explores a breast CAD method based on feature fusion with convolutional neural network (CNN) deep features. First, we propose a mass detection method based on CNN deep features and unsupervised extreme learning machine (ELM) clustering. Second, we build a feature set fusing deep features, morphological features, texture features, and density features. Third, an ELM classifier is developed using the fused feature set to classify benign and malignant breast masses. Extensive experiments demonstrate the accuracy and efficiency of our proposed mass detection and breast cancer classification method.","['Feature extraction', 'Mammography', 'Breast cancer', 'Breast tumors']","['Mass detection', 'computer-aided diagnosis', 'deep learning', 'fusion feature', 'extreme learning machine']"
"Identity-based cryptosystems mean that public keys can be directly derived from user identifiers, such as telephone numbers, email addresses, and social insurance number, and so on. So they can simplify key management procedures of certificate-based public key infrastructures and can be used to realize authentication in blockchain. Linearly homomorphic signature schemes allow to perform linear computations on authenticated data. And the correctness of the computation can be publicly verified. Although a series of homomorphic signature schemes have been designed recently, there are few homomorphic signature schemes designed in identity-based cryptography. In this paper, we construct a new ID-based linear homomorphic signature scheme, which avoids the shortcomings of the use of public-key certificates. The scheme is proved secure against existential forgery on adaptively chosen message and ID attack under the random oracle model. The ID-based linearly homomorphic signature schemes can be applied in e-business and cloud computing. Finally, we show how to apply it to realize authentication in blockchain.",[],[]
"Facial expression recognition (FER) is a significant task for the machines to understand the emotional changes in human beings. However, accurate hand-crafted features that are highly related to changes in expression are difficult to extract because of the influences of individual difference and variations in emotional intensity. Therefore, features that can accurately describe the changes in facial expressions are urgently required. Method: A weighted mixture deep neural network (WMDNN) is proposed to automatically extract the features that are effective for FER tasks. Several pre-processing approaches, such as face detection, rotation rectification, and data augmentation, are implemented to restrict the regions for FER. Two channels of facial images, including facial grayscale images and their corresponding local binary pattern (LBP) facial images, are processed by WMDNN. Expression-related features of facial grayscale images are extracted by fine-tuning a partial VGG16 network, the parameters of which are initialized using VGG16 model trained on ImageNet database. Features of LBP facial images are extracted by a shallow convolutional neural network (CNN) built based on DeepID. The outputs of both channels are fused in a weighted manner. The result of final recognition is calculated using softmax classification. Results: Experimental results indicate that the proposed algorithm can recognize six basic facial expressions (happiness, sadness, anger, disgust, fear, and surprise) with high accuracy. The average recognition accuracies for benchmarking data sets “CK+,”“JAFFE,”and “Oulu-CASIA”are 0.970, 0.922, and 0.923, respectively. Conclusions: The proposed FER method outperforms the state-of-the-art FER methods based on the hand-crafted features or deep networks using one channel. Compared with the deep networks that use multiple channels, our proposed network can achieve comparable performance with easier procedures. Fine-tuning is effective to FER tasks with a well pre-trained model if sufficient samples cannot be collected.","['Feature extraction', 'Face recognition', 'Image recognition', 'Face detection', 'Neural networks', 'Machine learning']","['Facial expression recognition', 'double channel facial images', 'deep neural network', 'weighted mixture', 'softmax classification']"
"To explore human emotions, in this paper, we design and build a multi-modal physiological emotion database, which collects four modal physiological signals, i.e., electroencephalogram (EEG), galvanic skin response, respiration, and electrocardiogram (ECG). To alleviate the influence of culture dependent elicitation materials and evoke desired human emotions, we specifically collect an emotion elicitation material database selected from more than 1500 video clips. By the considerable amount of strict man-made labeling, we elaborately choose 28 videos as standardized elicitation samples, which are assessed by psychological methods. The physiological signals of participants were synchronously recorded when they watched these standardized video clips that described six discrete emotions and neutral emotion. With three types of classification protocols, different feature extraction methods and classifiers (support vector machine and k-NearestNeighbor) were used to recognize the physiological responses of different emotions, which presented the baseline results. Simultaneously, we present a novel attention-long short-term memory (A-LSTM), which strengthens the effectiveness of useful sequences to extract more discriminative features. In addition, correlations between the EEG signals and the participants' ratings are investigated. The database has been made publicly available to encourage other researchers to use it to evaluate their own emotion estimation methods.","['Electroencephalography', 'Brain modeling', 'Physiology', 'Databases', 'Emotion recognition', 'Support vector machines', 'Feature extraction']","['Discrete emotion recognition', 'physiological signals', 'EEG', 'affective computing', 'machine learning', 'video-induced emotion', 'LSTM']"
"Chest X-ray film is the most widely used and common method of clinical examination for pulmonary nodules. However, the number of radiologists obviously cannot keep up with this outburst due to the sharp increase in the number of pulmonary diseases, which increases the rate of missed diagnosis and misdiagnosis. The method based on deep learning is the most appropriate way to deal with such problems so far. The main research in this paper was using inception-v3 transfer learning model to classify pulmonary images, and finally to get a practical and feasible computer-aided diagnostic model. The computer-aided diagnostic model could improve the accuracy and rapidity of doctors in the diagnosis of thoracic diseases. In this experiment, we augmented the data of pulmonary images, then used the fine-tuned Inception-v3 model based on transfer learning to extract features automatically, and used different classifiers (Softmax, Logistic, SVM) to classify the pulmonary images. Finally, it was compared with various models based on the original Deep Convolution Neural Network (DCNN) model. The experiment proved that the experiment based on transfer learning was meaningful for pulmonary image classification. The highest sensitivity and specificity are 95.41% and 80.09% respectively in the experiment, and the better pulmonary image classification performance was obtained than other methods.","['Feature extraction', 'Diseases', 'Medical diagnostic imaging', 'Image classification', 'Computational modeling', 'Convolutional neural networks']","['Pulmonary image', 'data augmentation', 'deep convolutional neural network', 'transfer learning', 'inception-v3']"
"A tri-polarized 12-antenna array working in the 3.5-GHz band (3.4-3.6 GHz) for future 5G (the fifth generation mobile communication) multiple-input multiple-output (MIMO) operations in the smartphone is presented. In order to reduce the mutual couplings and simplify the design process, orthogonal polarization technique is utilized. By combining a quarter mode substrate integrated wave-guide antenna and two open-end slots, a compact 3-antenna tri-polarization block operating in the 3.5-GHz band is achieved within a small volume of 17 × 17 × 6 mm 3 . Thanks to the orthogonal polarization features, the three antennas within the block are able to have good impedance matchings and low mutual couplings between antennas. By integrating four such tri-polarization blocks, a 12-antenna MIMO array is then designed for smartphone applications. It is also due to the tri-polarization feature, the proposed array could attain acceptable isolations and low correlations between antennas with only two additional decoupling structures. The proposed array is fabricated and tested, good antenna performances, such as return loss better than 10 dB, isolation higher than 12.5 dB, and antenna efficiencies higher than 50%, are obtained. The channel capacity of the 12-antenna array is calculated to be about 57 bps/Hz in a 12 × 12 MIMO system with 20-dB signal-to-noise ratio, which indicates the proposed array using tri-polarization technique is a good choice for future 5G terminals.","['Antenna arrays', 'MIMO', '5G mobile communication', 'Slot antennas', 'Substrates', 'Impedance matching']","['Smartphone antennas', 'MIMO antenna array', '5G communication', 'orthogonal polarization']"
"In distributed peer-to-peer (P2P) applications, peers self-organize and cooperate to effectively complete certain tasks such as forwarding files, delivering messages, or uploading data. Nevertheless, users are selfish in nature and they may refuse to cooperate due to their concerns on energy and bandwidth consumption. Thus each user should receive a satisfying reward to compensate its resource consumption for cooperation. However, suitable incentive mechanisms that can meet the diverse requirements of users in dynamic and distributed P2P environments are still missing. On the other hand, we observe that Blockchain is a decentralized secure digital ledger of economic transactions that can be programmed to record not just financial transactions and Blockchain-based cryptocurrencies get more and more market capitalization. Therefore in this paper, we propose a Blockchain based truthful incentive mechanism for distributed P2P applications that applies a cryptocurrency such as Bitcoin to incentivize users for cooperation. In this mechanism, users who help with a successful delivery get rewarded. As users and miners in the Blockchain P2P system may exhibit selfish actions or collude with each other, we propose a secure validation method and a pricing strategy, and integrate them into our incentive mechanism. Through a game theoretical analysis and evaluation study, we demonstrate the effectiveness and security strength of our proposed incentive mechanism.","['Pricing', 'Incentive schemes', 'Bitcoin', 'Data communication']","['Incentive mechanism', 'P2P applications', 'data transmissions', 'Bitcoin System', 'collusion attacks', 'pricing strategy']"
"Increasing use of renewable energy sources, liberalized energy markets and most importantly, the integrations of various monitoring, measuring and communication infrastructures into modern power system network offer the opportunity to build a resilient and efficient grid. However, it also brings about various threats of instabilities and security concerns in form of cyberattack, voltage instability, power quality (PQ) disturbance among others to the complex network. The need for efficient methodologies for quicker identification and detection of these problems have always been a priority to energy stakeholders over the years. In recent times, machine learning techniques (MLTs) have proven to be effective in numerous applications including power system studies. In the literature, various MLTs such as artificial neural networks (ANN), Decision Tree (DT), support vector machines (SVM) have been proposed, resulting in effective decision making and control actions in the secured and stable operations of the power system. Given this growing trend, this paper presents a comprehensive review on the most recent studies whereby MLTs were developed for power system security and stability especially in cyberattack detections, PQ disturbance studies and dynamic security assessment studies. The aim is to highlight the methodologies, achievements and more importantly the limitations in the classifier(s) design, dataset and test systems employed in the reviewed publications. A brief review of reinforcement learning (RL) and deep reinforcement learning (DRL) approaches to transient stability assessment is also presented. Finally, we highlighted some challenges and directions for future studies.","['Power system stability', 'Power system security', 'Stability criteria', 'Machine learning', 'Power quality']","['Classifiers', 'cyberattacks', 'deep reinforcement learning', 'intruder detection system', 'machine learning techniques', 'power quality disturbance', 'power system', 'reinforcement learning', 'test systems', 'transient stability assessment', 'voltage stability']"
"Plug-in hybrid electric vehicles (PHEVs) have emerged as an important tool in reducing greenhouse gas emissions, due to their lower dependency on fossil fuel. Since, for cost efficiency, PHEVs have a limited battery capacity, they must be recharged often and especially after trips. Thus, efficient battery charging plays an important role on the success of PHEVs commercial adoption. This paper surveys the state-of-the-art of existing PHEV battery charging schemes. We classify these schemes into four classes, namely, uncontrolled, indirectly controlled, smart, and bidirectional charging, and review various existing techniques within each class. For uncontrolled charging, existing studies focus on evaluating the impact of adding variable charging load on the smart grid. Various indirectly controlled charging schemes have been proposed to control energy prices, in order to indirectly influence the charging operations. Smart charging schemes can directly control a rich set of charging parameters to achieve various performance objectives, such as minimizing power loss, maximizing operator's profit, ensuring fairness, and so on. Finally, bidirectional charging allows a PHEV to discharge energy into smart grid, such that the vehicle can act as a mobile energy source to further stabilize the grid, which is partially supplied by intermittent renewable energy sources. This survey provides a comprehensive one-stop introductory reference to quickly learn about the key features and technical challenges, addressed by existing PHEV battery charging schemes in smart grid.","['Electric vehicles', 'Batteries', 'Smart grids', 'Plug-in hybrid electric vehicles', 'Global warming', 'Environmental factors', 'Fossill fuels', 'Discharges (electric)', 'Emissions', 'Greenhouse effect']","['Plug-in hybrid electric vehicles', 'uncontrolled charging', 'indirectly-controlled charging', 'smart charging', 'bidirectional charging', 'smart grid']"
"Cognitive radio technology is an important branch in the field of wireless communication, and automatic identification is a major part of cognitive radio technology. Convolutional neural network (CNN) is an advanced neural network, which is the forefront of application in the digital image recognition area. In this paper, we explore CNN in an automatic system to recognize the cognitive radio waveforms. Excitedly, it is a more effective model with high ratio of successful recognition (RSR) under high power background noise. The system can identify eight kinds of signals, including binary phase shift keying (Barker codes modulation) linear frequency modulation, Costas codes, Frank code, and polytime codes (T1, T2, T3, and T4). The recognition part includes a CNN classifier. First, we determine the appropriate architecture to make CNN effective for proposed system. Specifically, we focus on how many convolutional layers are needed, what appropriate number of hidden units is, and what the best pooling strategy is. Second, we research how to obtain the image features into CNN that based on Choi-Williams time-frequency distribution. Finally, by means of the simulations, the results of classification are demonstrated. Simulation results show the overall RSR is 93.7% when the signal-to-noise ratio is -2dB.","['Time-frequency analysis', 'Signal to noise ratio', 'Feature extraction', 'Convolution', 'Neural networks', 'Cognitive radio', 'Image recognition']","['Cognitive radio', 'radar countermeasures', 'waveform recognition', 'time-frequency distribution', 'convolutional neural network']"
"Remaining useful life (RUL) prediction of lithium-ion batteries can reduce the risk of battery failure by predicting the end of life. In this paper, we propose novel RUL prediction techniques based on long short-term memory (LSTM). To estimate RUL even in the presence of capacity regeneration phenomenon, we consider multiple measurable data from battery management system such as voltage, current and temperature charging profiles whose patterns vary as aging. Unlike the traditional LSTM prediction that matches input layer with output layer as one-to-one structure, we leverage many-to-one structure to be flexible for various input types and to substantially reduce the number of parameters for better generalization. Using the NASA lithium-ion battery datasets, we verify the accuracy of the proposed LSTM-based RUL prediction. The experimental results show that the proposed single-channel LSTM model improves the mean absolute percentage error (MAPE) by 39.2% compared to the baseline LSTM model. Furthermore, the proposed multi-channel LSTM model significantly improves the MAPE, e.g., by 63.7% compared to the baseline; the proposed model achieves 0.47-1.88% of MAPE while the state-of-the-art baseline LSTM shows 0.6-6.45% of MAPE.","['Batteries', 'Degradation', 'Logic gates', 'Battery charge measurement', 'Electrodes', 'Aging', 'NASA']","['Lithium-ion battery', 'long short-term memory', 'remaining useful life', 'capacity estimation']"
"With the fast expansion of renewable energy system installed capacity in recent years, the availability, stability, and quality of smart grids have become increasingly important. The renewable energy output forecasting applications have also been developing rapidly in recent years, and such techniques have particularly been applied in the fields of wind and solar photovoltaic (PV). In the case of solar PV output forecasting, many applications have been performed with machine learning and hybrid techniques. In this paper, we propose a high-precision deep neural network model named PVPNet to forecast PV system output power. The methodology behind the proposed model is based on deep neural networks, and the model is able to generate a 24-h probabilistic and deterministic forecasting of PV power output based on meteorological information, such as temperature, solar radiation, and historical PV system output data. The forecasting accuracy of PVPNet is determined by the mean absolute error (MAE) and root mean square error (RMSE) values. The results from the experiments show that the MAE and RMSE of the proposed algorithm are 109.4845 and 163.1513, respectively. The results prove that the prediction accuracy of the PVPNet outperforms other benchmark models, and the algorithm also effectively predicts complex time series with a high degree of volatility and irregularity.","['Forecasting', 'Predictive models', 'Power generation', 'Weather forecasting', 'Data models', 'Numerical models']","['Deep neural network', 'photovoltaic output power forecasting', 'photovoltaic system', 'renewable energy sources']"
"Photovoltaic embedded generation in low-voltage ac networks is quite popular; however, despite its benefits, there are some problems especially when photovoltaic (PV) penetration exceeds certain thresholds. Among others, voltage violation is of prime importance. Our review of the literature focused on PV penetration limits due to voltage violations in low-voltage (LV) networks. The review revealed that voltage violations can occur at a penetration level as low as 2.5% when a large distributed generator (DG) is installed at a single point. Alternatively, a LV network can host a large number of photovoltaic distributed generators (PVDGs), with a penetration level up to 110% if evenly distributed over shorter lengths. However, an LV network has no rules of thumb for safe penetration limits. Penetration-level calculations have been found that they used numerous approaches, which we have analyzed and discussed to adopt a more rational and unified approach. Our literature review revealed that, in LVs, a very high penetration level can be achieved as compared with medium-voltage (MV) networks. However, MV voltage-level control problems impose a limit for PV hosting in LV networks. There is a need to evolve strategies for robust voltage control at the MV level and to develop certain rules of thumb for PV penetration limits in LV networks independent of the MV level, to increase the PV hosting capacity.",[],[]
"Batteries are gaining entry into every home and office for they are widely used because of their variant benefits. However, these batteries are prone to failure caused by charge imbalance in the batteries connected in either series or parallel, which can sometimes be catastrophic and hence they require to be properly monitored in a real-time manner. There exist many battery balancing schemes which are broadly grouped into either passive or active schemes. All these schemes have their own advantages and disadvantages, and hence it is upon the user to decide on which scheme will best work for them. However, research has proven that the hybrid scheme will be the best as it couples the benefits of all schemes. This study will review the various battery cell balancing methodologies and evaluate their relationship with battery performance. At present there are a few studies tackling the mechanical vibration of battery balancing performance. This study shows that battery balancing performance during long-term should be evaluated from various temperature and vibration frequencies.","['Batteries', 'Heating systems', 'Resistance', 'Vibrations', 'Temperature measurement', 'Temperature', 'Integrated circuit modeling']","['Battery cell balancing', 'electric vehicles', 'hybrid schemes', 'and performance optimization']"
"Nowadays, heart disease is the leading cause of death worldwide. Predicting heart disease is a complex task since it requires experience along with advanced knowledge. Internet of Things (IoT) technology has lately been adopted in healthcare systems to collect sensor values for heart disease diagnosis and prediction. Many researchers have focused on the diagnosis of heart disease, yet the accuracy of the diagnosis results is low. To address this issue, an IoT framework is proposed to evaluate heart disease more accurately using a Modified Deep Convolutional Neural Network (MDCNN). The smartwatch and heart monitor device that is attached to the patient monitors the blood pressure and electrocardiogram (ECG). The MDCNN is utilized for classifying the received sensor data into normal and abnormal. The performance of the system is analyzed by comparing the proposed MDCNN with existing deep learning neural networks and logistic regression. The results demonstrate that the proposed MDCNN based heart disease prediction system performs better than other methods. The proposed method shows that for the maximum number of records, the MDCNN achieves an accuracy of 98.2 which is better than existing classifiers.","['Heart', 'Diseases', 'Biomedical monitoring', 'Cloud computing', 'Monitoring', 'Electrocardiography']","['AEHO', 'cuttlefish algorithm', 'MDCNN', 'IoT', 'Cleveland dataset', 'sensors', 'wearable device', 'CAGR', 'electrocardiogram', 'LSTM', 'CNN']"
"We propose automatic contrast-limited adaptive histogram equalization (CLAHE) for image contrast enhancement. We automatically set the clip point for CLAHE based on textureness of a block. Also, we introduce dual gamma correction into CLAHE to achieve contrast enhancement while preserving naturalness. First, we redistribute the histogram of the block in CLAHE based on the dynamic range of each block. Second, we perform dual gamma correction to enhance the luminance, especially in dark regions while reducing over-enhancement artifacts. Since automatic CLAHE adaptively enhances contrast in each block while boosting luminance, it is very effective in enhancing dark images and daylight ones with strong dark shadows. Moreover, automatic CLAHE is computationally efficient, i.e., more than 35 frames/s at 1024 × 682 resolution, due to the independent block processing for contrast enhancement. Experimental results demonstrate that automatic CLAHE with dual gamma correction achieves good performance in contrast enhancement and outperforms state-of-the-art methods in terms of visual quality and quantitative measures.","['Histograms', 'Dynamic range', 'Image enhancement', 'Interpolation', 'Visualization', 'Image segmentation', 'Transfer functions']","['CLAHE', 'luminance enhancement', 'contrast enhancement', 'gamma correction', 'dark image', 'over-enhancement']"
"Narrow body and wide body aircraft are responsible for more than 75% of aviation greenhouse gas (GHG) emission and aviation, itself, was responsible for about 2.5% of all GHG emissions in the United States in 2018. This situation becomes worse when considering a 4-5% annual growth in air travel. Electrified aircraft is clearly a promising solution to combat the GHG challenge; thus, the trend is to eliminate all but electrical forms of energy in aircraft power distribution systems. However, electrification adds tremendously to the complexity of aircraft electric power systems (EPS), which is dramatically changing in our journey from conventional aircraft to more electric aircraft (MEA) and all electric aircraft (AEA). In this article, we provide an in-depth discussion on MEA/AEA EPS: electric propulsion, distributed propulsion systems (DPS), EPS voltage levels, power supplies, and EPS architectures are discussed. Publications on power flow (PF) analysis and management of EPS are reviewed, and an initial schematic of a potential aircraft EPS with electric propulsion is proposed. In this regard, we also briefly review the components required for MEA/AEA EPS, including power electronics (PE) converters, electric machines, electrochemical energy units, circuit breakers (CBs), and wiring harness. A comprehensive review of each of the components mentioned above or other topics except for those related to steady state power flow in MEA/AEA EPS is out of this article's scope and should be found somewhere else. At the close of the paper, some challenges in the path towards AEA are presented. Unless the discussed challenges are satisfactorily addressed and solved, arriving at an AEA that can properly operate over commercial missions will not be possible.","['Aircraft', 'Aircraft propulsion', 'Aerospace electronics', 'Power systems', 'Circuit breakers', 'Atmospheric modeling']","['Aircraft electrification', 'all electric aircraft (AEA)', 'electric power system (EPS)', 'more electric aircraft (MEA)', 'power distribution system', 'steady state power flow analysis']"
"Healthcare data management has been gaining a lot of attention in recent years because of its high potential to provide more accurate and cost-efficient patient care. The traditional client-server and cloud-based healthcare data management systems suffer from the issues of single point of failure, data privacy, centralized data stewardship, and system vulnerability. The replication mechanism, and privacy and security features of blockchain have a promising future in the healthcare domain as they can solve some of the inherent issues of the health management system. However, most of the recent research works on blockchain in the healthcare domain have primarily focused on the permission-less Bitcoin network that suffers from drawbacks such as high energy consumption, limited scalability, and low transaction throughput. Consequently, there is a need for a scalable, fault-tolerant, secure, traceable and private blockchain to suit the requirements of the healthcare domain. We propose a lightweight blockchain architecture for the healthcare data management that reduces the computational and communication overhead compared to the Bitcoin network by dividing the network participants into clusters and maintaining one copy of the ledger per cluster. Our architecture introduces the use of canal, that allows secure and confidential transactions within a group of network participants. Furthermore, we propose a solution to avoid forking which is prevalent in the Bitcoin network. We demonstrate the effectiveness of our proposed architecture in providing security and privacy compared to the Bitcoin network by analyzing different threats and attacks. We also discuss how our proposed architecture addresses the identified threats. Our experimental results demonstrate that our proposed architecture generates 11 times lower network traffic compared to the Bitcoin network as the number of blocks increases. Our ledger update is 1.13 times faster. Our architecture shows a speedup of 67% in ledger update and 10 times lower network traffic when the number of nodes increases.","['Blockchain', 'Computer architecture', 'Medical diagnostic imaging', 'Hospitals', 'Bitcoin', 'Servers']","['Blockchain', 'consensus', 'decentralization', 'health information management', 'privacy', 'scalability']"
"Computed tomography (CT) is a popular medical imaging modality and enjoys wide clinical applications. At the same time, the X-ray radiation dose associated with CT scannings raises a public concern due to its potential risks to the patients. Over the past years, major efforts have been dedicated to the development of low-dose CT (LDCT) methods. However, the radiation dose reduction compromises the signal-to-noise ratio, leading to strong noise and artifacts that down-grade the CT image quality. In this paper, we propose a novel 3-D noise reduction method, called structurally sensitive multi-scale generative adversarial net, to improve the LDCT image quality. Specifically, we incorporate 3-D volumetric information to improve the image quality. Also, different loss functions for training denoising models are investigated. Experiments show that the proposed method can effectively preserve the structural and textural information in reference to the normal-dose CT images and significantly suppress noise and artifacts. Qualitative visual assessments by three experienced radiologists demonstrate that the proposed method retrieves more information and outperforms competing methods.","['Three-dimensional displays', 'Computed tomography', 'Noise reduction', 'Generators', 'Image quality', 'Loss measurement', 'Noise measurement']","['Machine leaning', 'low dose CT', 'image denoising', 'deep learning', 'loss function']"
"In this paper, we propose a home energy management system that employs load shifting strategy of demand side management to optimize the energy consumption patterns of a smart home. It aims to manage the load demand in an efficient way to minimize electricity cost and peak to average ratio while maintaining user comfort through coordination among home appliances. In order to meet the load demand of electricity consumers, we schedule the load in day-ahead and real-time basis. We propose a fitness criterion for proposed hybrid technique, which helps in balancing the load during ON-peak and OFF-peak hours. Moreover, for realtime rescheduling, we present the concept of coordination among home appliances. This helps the scheduler to optimally decide the ON/OFF status of appliances in order to reduce the waiting time of appliance. For this purpose, we formulate our real-time rescheduling problem as knapsack problem and solve it through dynamic programming. This paper also evaluates the behavior of the proposed technique for three pricing schemes including: time of use, real-time pricing, and critical peak pricing. Simulation results illustrate the significance of the proposed optimization technique with 95% confidence interval.","['Home appliances', 'Optimization', 'Real-time systems', 'Peak to average power ratio', 'Microorganisms', 'Schedules', 'Energy management']","['Smart homes', 'home automation', 'energy', 'multi-objective optimization', 'embedded systems', 'day-ahead and real-time scheduling', 'bacterial foraging optimization', 'genetic algorithm', 'coordination', 'hybrid technique']"
"Wireless sensor networks (WSNs) distribute hundreds to thousands of inexpensive microsensor nodes in their regions, and these nodes are important parts of Internet of Things (IoT). In WSN-assisted IoT, the nodes are resource constrained in many ways, such as storage resources, computing resources, energy resources, and so on. Robust routing protocols are required to maintain a long network lifetime and achieve higher energy utilization. In this paper, we propose a new energy-efficient centroid-based routing protocol (EECRP) for WSN-assisted IoT to improve the performance of the network. The proposed EECRP includes three key parts: a new distributed cluster formation technique that enables the self-organization of local nodes, a new series of algorithms for adapting clusters and rotating the cluster head based on the centroid position to evenly distribute the energy load among all sensor nodes, and a new mechanism to reduce the energy consumption for long-distance communications. In particular, the residual energy of nodes is considered in EECRP for calculating the centroid's position. Our simulation results indicate that EECRP performs better than LEACH, LEACH-C, and GEEC. In addition, EECRP is suitable for networks that require a long lifetime and whose base station (BS) is located in the network.","['Energy consumption', 'Wireless sensor networks', 'Routing protocols', 'Clustering algorithms', 'Energy resources', 'Heuristic algorithms']","['Internet of Things', 'wireless sensor networks', 'energy management', 'cluster']"
"Deep neural networks have demonstrated their effectiveness in most machine learning tasks, with intrusion detection included. Unfortunately, recent research found that deep neural networks are vulnerable to adversarial examples in the image classification domain, i.e., they leave some opportunities for an attacker to fool the networks into misclassification by introducing imperceptible changes to the original pixels in an image. The vulnerability raises some concerns in applying deep neural networks in security-critical areas, such as intrusion detection. In this paper, we investigate the performances of the state-of-the-art attack algorithms against deep learning-based intrusion detection on the NSL-KDD data set. The vulnerabilities of neural networks employed by the intrusion detection systems are experimentally validated. The roles of individual features in generating adversarial examples are explored. Based on our findings, the feasibility and applicability of the attack methodologies are discussed.","['Machine learning', 'Neural networks', 'Intrusion detection', 'Feature extraction', 'Perturbation methods', 'Measurement', 'Task analysis']","['Intrusion detection', 'neural networks', 'classification algorithms', 'data security']"
"Power grids are transforming into flexible, smart, and cooperative systems with greater dissemination of distributed energy resources, advanced metering infrastructure, and advanced communication technologies. Short-term electric load forecasting for individual residential customers plays a progressively crucial role in the operation and planning of future grids. Compared to the aggregated electrical load at the community level, the prediction of individual household electric loads is legitimately challenging because of the high uncertainty and volatility involved. Results from previous studies show that prediction using machine learning and deep learning models is far from accurate, and there is still room for improvement. We herein propose a deep learning framework based on a combination of a convolutional neural network (CNN) and long short-term memory (LSTM). The proposed hybrid CNN-LSTM model uses CNN layers for feature extraction from the input data with LSTM layers for sequence learning. The performance of our developed framework is comprehensively compared to state-of-the-art systems currently in use for short-term individual household electric load forecasting. The proposed model achieved significantly better results compared to other competing techniques. We evaluated our proposed model with the recently explored LSTM-based deep learning model on a publicly available electrical load data of individual household customers from the Smart Grid Smart City (SGSC) project. We obtained an average mean absolute percentage error (MAPE) of 40.38% for individual household electric load forecasts in comparison with the LSTM-based model that obtained an average MAPE of 44.06%. Furthermore, we evaluated the effectiveness of the proposed model on different time horizons (up to 3 h ahead). Compared to the recently developed LSTM-based model tested on the same dataset, we obtained 4.01%, 4.76%, and 5.98% improvement for one, two, and six look-forward time steps, respectively (with 2 lookback time steps). Additionally, we have performed clustering analysis based on the power consumption behavior of the energy users, which indicate that prediction accuracy could be improved by grouping and training the representative model using large amount of data. The results indicated that the proposed model outperforms the LSTM-based model for both 1 h ahead and 3 h ahead in forecasting individual household electric loads.","['Load modeling', 'Predictive models', 'Load forecasting', 'Forecasting', 'Machine learning', 'Data models', 'Energy consumption']","['CNN', 'deep learning framework', 'energy consumption', 'energy consumption forecasting', 'individual household', 'LSTM']"
"This paper explores the feasibility of social cooperation between prosumers within an energy network in establishing their sustainable participation in peer-to-peer (P2P) energy trading. In particular, a canonical coalition game (CCG) is utilized to propose a P2P energy trading scheme, in which a set of participating prosumers form a coalition group to trade their energy, if there is any, with one another. By exploring the concept of the core of the designed CCG framework, the mid-market rate is utilized as a pricing mechanism of the proposed P2P trading to confirm the stability of the coalition as well as to guarantee the benefit to the prosumers for forming the social coalition. This paper further introduces the motivational psychology models that are relevant to the proposed P2P scheme and it is shown that the outcomes of the proposed P2P energy trading scheme satisfy the discussed models. Consequently, it is proven that the proposed scheme is consumer-centric and has the potential to corroborate sustainable prosumer participation in P2P energy trading. Finally, some numerical examples are provided to demonstrate the beneficial properties of the proposed scheme.","['Microgrids', 'Peer-to-peer computing', 'Games', 'Biological system modeling', 'Power system stability', 'Australia', 'Psychology']","['Peer-to-peer trading', 'social cooperation', 'game theory', 'consumer-centric', 'motivational psychology']"
"Recent advances in hardware and telecommunications have enabled the development of low cost mobile devices equipped with a variety of sensors. As a result, new functionalities, empowered by emerging mobile platforms, allow millions of applications to take advantage of vast amounts of data. Following this trend, mobile health applications collect users health-related information to help them better comprehend their health status and to promote their overall wellbeing. Nevertheless, health-related information is by nature and by law deemed sensitive and, therefore, its adequate protection is of substantial importance. In this paper we provide an in-depth security and privacy analysis of some of the most popular freeware mobile health applications. We have performed both static and dynamic analysis of selected mobile health applications, along with tailored testing of each application's functionalities. Long term analyses of the life cycle of the reviewed apps and our general data protection regulation compliance auditing procedure are unique features of the present paper. Our findings reveal that the majority of the analyzed applications do not follow well-known practices and guidelines, not even legal restrictions imposed by contemporary data protection regulations, thus jeopardizing the privacy of millions of users.","['Security', 'Privacy', 'Mobile communication', 'Data protection', 'Law']","['Communication system security', 'mobile security', 'application security', 'data privacy']"
"Narrowband Internet of Things (NB-IoT) is a new narrow-band radio technology introduced in the Third Generation Partnership Project release 13 to the 5th generation evolution for providing low-power wide-area IoT. In NB-IoT systems, repeating transmission data or control signals has been considered as a promising approach for enhancing coverage. Considering the new feature of repetition, link adaptation for NB-IoT systems needs to be performed in 2-D, i.e., the modulation and coding scheme (MCS) and the repetition number. Therefore, existing link adaptation schemes without consideration of the repetition number are no longer applicable. In this paper, a novel uplink link adaptation scheme with the repetition number determination is proposed, which is composed of the inner loop link adaptation and the outer loop link adaptation, to guarantee transmission reliability and improve throughput of NB-IoT systems. In particular, the inner loop link adaptation is designed to cope with block error ratio variation by periodically adjusting the repetition number. The outer loop link adaptation coordinates the MCS level selection and the repetition number determination. Besides, key technologies of uplink scheduling, such as power control and transmission gap, are analyzed, and a simple single-tone scheduling scheme is proposed. Link-level simulations are performed to validate the performance of the proposed uplink link adaptation scheme. The results show that our proposed uplink link adaptation scheme for NB-IoT systems outperforms the repetition-dominated method and the straightforward method, particularly for good channel conditions and larger packet sizes. Specifically, it can save more than 14% of the active time and resource consumption compared with the repetition-dominated method and save more than 46% of the active time and resource consumption compared with the straightforward method.","['Uplink', 'Long Term Evolution', 'Narrowband', 'Downlink', 'Internet of Things', 'Wireless communication', '3GPP']","['Narrowband Internet of Things (NB-IoT)', 'coverage enhancement', 'low complexity', 'link adaptation']"
"In this paper, we propose a deep Recurrent Neural Networks (RNNs) based on Gated Recurrent Unit (GRU) in a bidirectional manner (BGRU) for human identification from electrocardiogram (ECG) based biometrics, a classification task which aims to identify a subject from a given time-series sequential data. Despite having a major issue in traditional RNN networks which they learn representations from previous time sequences, bidirectional is designed to learn the representations from future time steps which enables for better understanding of context, and eliminate ambiguity. Moreover, GRU cell in RNNs deploys an update gate and a reset gate in a hidden state layer which is computationally efficient than a usual LSTM network due to the reduction of gates. The experimental results suggest that our proposed BGRU model, the combination of RNN with GRU cell unit in bidirectional manner, achieved a high classification accuracy of 98.55%. Various neural network architectures with different parameters are also evaluated for different approaches, including one-dimensional Convolutional Neural Network (1D-CNN), and traditional RNNs with LSTM and GRU for non-fiducial approach. The proposed models were evaluated with two publicly available datasets: ECG-ID Database (ECGID) and MIT-BIH Arrhythmia Database (MITDB). This paper is expected to demonstrate the feasibility and effectiveness of applying various deep learning approaches to biometric identification and also evaluate the effect of network performance on classification accuracy according to the changes in percentage of training dataset.","['Logic gates', 'Electrocardiography', 'Recurrent neural networks', 'Biological system modeling', 'Feature extraction', 'Heart']","['1D-convolutional neural networks', 'bidriectional recurrent neural networks', 'biometrics classification', 'ECG signals', 'gated recurrent unit', 'user identification', 'signal processing']"
"Supervised intrusion detection system is a system that has the capability of learning from examples about the previous attacks to detect new attacks. Using artificial neural network (ANN)-based intrusion detection is promising for reducing the number of false negative or false positives, because ANN has the capability of learning from actual examples. In this paper, a developed learning model for fast learning network (FLN) based on particle swarm optimization (PSO) has been proposed and named as PSO-FLN. The model has been applied to the problem of intrusion detection and validated based on the famous dataset KDD99. Our developed model has been compared against a wide range of meta-heuristic algorithms for training extreme learning machine and FLN classifier. PSO-FLN has outperformed other learning approaches in the testing accuracy of the learning.","['Intrusion detection', 'Training', 'Computer hacking', 'Artificial intelligence', 'Artificial neural networks']","['Fast learning network', 'KDD Cup 99', 'intrusion detection system', 'particle swarm optimization']"
"Android applications are developing rapidly across the mobile ecosystem, but Android malware is also emerging in an endless stream. Many researchers have studied the problem of Android malware detection and have put forward theories and methods from different perspectives. Existing research suggests that machine learning is an effective and promising way to detect Android malware. Notwithstanding, there exist reviews that have surveyed different issues related to Android malware detection based on machine learning. We believe our work complements the previous reviews by surveying a wider range of aspects of the topic. This paper presents a comprehensive survey of Android malware detection approaches based on machine learning. We briefly introduce some background on Android applications, including the Android system architecture, security mechanisms, and classification of Android malware. Then, taking machine learning as the focus, we analyze and summarize the research status from key perspectives such as sample acquisition, data preprocessing, feature selection, machine learning models, algorithms, and the evaluation of detection effectiveness. Finally, we assess the future prospects for research into Android malware detection based on machine learning. This review will help academics gain a full picture of Android malware detection based on machine learning. It could then serve as a basis for subsequent researchers to start new work and help to guide research in the field more generally.","['Malware', 'Machine learning', 'Security', 'Feature extraction', 'Libraries', 'Kernel', 'Java']","['Android security', 'malware detection', 'machine learning', 'feature extraction', 'classifier evaluation']"
"Blockchain technology has been developed for more than ten years and has become a trend in various industries. As the oil and gas industry is gradually shifting toward intelligence and digitalization, many large oil and gas companies were working on blockchain technology in the past two years because of it can significantly improve the management level, efficiency, and data security of the oil and gas industry. This paper aims to let more people in the oil and gas industry understand the blockchain and lead more thinking about how to apply the blockchain technology. To the best of our knowledge, this is one of the earliest papers on the review of the blockchain system in the oil and gas industry. This paper first presents the relevant theories and core technologies of the blockchain, and then describes how the blockchain is applied to the oil and gas industry from four aspects: trading, management and decision making, supervision, and cyber security. Finally, the application status, the understanding level of the blockchain in the oil and gas industry, opportunities, challenges, and risks and development trends are analyzed. The main conclusions are as follows: 1) at present, Europe and Asia have the fastest pace of developing the application of blockchain in the oil and gas industry, but there are still few oil and gas blockchain projects in operation or testing worldwide; 2) nowadays, the understanding of blockchain in the oil and gas industry is not sufficiently enough, the application is still in the experimental stage, and the investment is not enough; and (3) blockchain can bring many opportunities to the oil and gas industry, such as reducing transaction costs and improving transparency and efficiency. However, since it is still in the early stage of the application, there are still many challenges, primarily technological, and regulatory and system transformation. The development of blockchains in the oil and gas industry will move toward hybrid blockchain architecture, multi-technology combination, cross-chain, hybrid consensus mechanisms, and more interdisciplinary professionals.","['Blockchain', 'Oils', 'Natural gas industry', 'Peer-to-peer computing', 'Natural gas', 'Security', 'Distributed databases']","['Blockchain', 'oil and gas industry', 'smart contract', 'oil and gas trade', 'track equipment', 'supervision']"
"Beyond energy, the growing number of defects in physical substrates is becoming another major constraint that affects the design of computing devices and systems. As the underlying semiconductor technologies are getting less and less reliable, the probability that some components of computing devices fail also increases, preventing designers from realizing the full potential benefits of on-chip exascale integration derived from near atomic scale feature dimensions. As the quest for performance confronts permanent and transient faults, device variation, and thermal issues, major breakthroughs in computing efficiency are expected to benefit from unconventional and new models of computation, such as brain-inspired computing. The challenge is then to find not only high-performance and energy-efficient, but also fault-tolerant computing solutions. Neural computing principles remain elusive, yet as source of a promising fault-tolerant computing paradigm. In the quest to fault tolerance can be translated into scalable and reliable computing systems, hardware design itself and/or to use circuits even with faults has further motivated research on neural networks, which are potentially capable of absorbing some degrees of vulnerability based on their natural properties. This paper presents a survey on fault tolerance in neural networks manly focusing on well-established passive techniques to exploit and improve, by design, such potential but limited intrinsic property in neural models, particularly for feedforward neural networks. First, fundamental concepts and background on fault tolerance are introduced. Then, we review fault types, models, and measures used to evaluate performance and provide a taxonomy of the main techniques to enhance the intrinsic properties of some neural models, based on the principles and mechanisms that they exploit to achieve fault tolerance passively. For completeness, we briefly review some representative works on active fault tolerance in neural networks. We present some key challenges that remain to be overcome and conclude with an outlook for this field.","['Fault tolerance', 'Fault tolerant systems', 'Circuit faults', 'Biological neural networks', 'Computational modeling', 'Transient analysis']","['Fault tolerance', 'neural networks', 'redundancy', 'fault masking', 'fault models', 'taxonomy']"
"Nowadays, there is an ever-increasing migration of people to urban areas. Health care service is one of the most challenging aspects that is greatly affected by the vast influx of people to city centers. Consequently, cities around the world are investing heavily in digital transformation in an effort to provide healthier ecosystems for people. In such a transformation, millions of homes are being equipped with smart devices (e.g., smart meters, sensors, and so on), which generate massive volumes of fine-grained and indexical data that can be analyzed to support smart city services. In this paper, we propose a model that utilizes smart home big data as a means of learning and discovering human activity patterns for health care applications. We propose the use of frequent pattern mining, cluster analysis, and prediction to measure and analyze energy usage changes sparked by occupants' behavior. Since people's habits are mostly identified by everyday routines, discovering these routines allows us to recognize anomalous activities that may indicate people's difficulties in taking care for themselves, such as not preparing food or not using a shower/bath. This paper addresses the need to analyze temporal energy consumption patterns at the appliance level, which is directly related to human activities. For the evaluation of the proposed mechanism, this paper uses the U.K. Domestic Appliance Level Electricity data set-time series data of power consumption collected from 2012 to 2015 with the time resolution of 6 s for five houses with 109 appliances from Southern England. The data from smart meters are recursively mined in the quantum/data slice of 24 h, and the results are maintained across successive mining exercises. The results of identifying human activity patterns from appliance usage are presented in detail in this paper along with the accuracy of shortand long-term predictions.","['Home appliances', 'Smart meters', 'Medical services', 'Data mining', 'Smart homes', 'Monitoring', 'Urban areas']","['Big data', 'smart cities', 'smart homes', 'health care applications', 'behavioral analytics', 'frequent pattern', 'cluster analysis', 'incremental data-mining', 'association rules', 'prediction']"
"In the present scenario, an energy efficiency has become a matter of prime importance for wireless networks. To meet the demands of an increased capacity, an improved data rate, and a better quality of the service of the next-generation networks, there is a need to adopt energy-efficient architectures. Along with these requirements, it is also our social responsibility to reduce the carbon footprint by reducing the power consumption in a wireless network. Hence, a green communication is an urgent need. In this paper, we have surveyed various techniques for the power optimization of the upcoming 5G networks. The primary focus is on the use of relays and small cells to improve the energy efficiency of the network. We have discussed the various scenarios of relaying for the next-generation networks. Along with this, the importance of simultaneous wireless power and information transfer, massive multiple input multiple output, and millimeter waves has been analyzed for 5G networks.","['Energy efficiency', 'Green communication', 'Microcell networks', 'Relays', 'Wireless networks', 'Power demand', 'Quality of service', '5G mobile communication']","['5G', 'C-RAN', 'Energy Efficiency', 'Green Communication', 'Relay', 'Small Cells', 'SWIPT']"
"The Internet of Things (IoT) depicts a bright future, where any devices having sensorial and computing capabilities can interact with each other. Among all existing technologies, the techniques for the fifth generation (5G) systems are the main driving force for the actualization of IoT concept. However, due to the heterogeneous environment in 5G networks and the broadcast nature of radio propagation, the security assurance against eavesdropping is a vital yet challenging task. In this paper, we focus on the transmission design for secure relay communications in IoT networks, where the communication is exposed to eavesdroppers with unknown number and locations. The randomize-and-forward relay strategy specially designed for secure multi-hop communications is employed in our transmission protocol. First, we consider a single-antenna scenario, where all the devices in the network are equipped with the single antenna. We derive the expression for the secrecy outage probability of the two-hop transmission. Following this, a secrecy-rate-maximization problem subject to a secrecy-outage-probability constraint is formulated. The optimal power allocation and codeword rate design are obtained. Furthermore, we generalize the above analyses to a more generic scenario, where the relay and eavesdroppers are equipped with multiple antennas. Numerical results show that the proper use of relay transmission can enhance the secrecy throughput and extend the secure coverage range.","['Radio propagation', 'Internet of things', '5G mobile communication', 'Antenna measurements', 'Sensors', 'Security', 'Resource management']","['Internet of Things (IoT)', 'physical layer security', 'relay transmission', 'resource allocation', 'stochastic geometry']"
"Epilepsy detection from electrical characteristics of EEG signals obtained from the brain of undergone subject is a challenge task for both research and neurologist due to the non-stationary and chaotic nature of EEG signals. As epileptic EEG signals contain huge fluctuating information about the functional behavior of the brain, it is hard to distinguish the fundamental dynamic, complex network of EEG signals without considering the strength among the nodes as they are connected with each other on the basis of these strengths. The prior research on natural visibility graph did not consider this issue in epileptic seizure, although it is a very important key point to have representative information from the signals. Hence, this paper aims to introduce a new idea for epilepsy detection using complex network statistical properties by measuring different strengths of the edges in natural visibility graph theory, which is considered as weight. Thus, the proposed method is named “weighted visibility graph”. In this proposed method, first, the epileptic EEG signals are transformed into complex network and then two important statistical properties of a network such as modularity and average weighted degree used for extracting the imperative characteristics from a network of EEG signals. After that, the extracted features are evaluated by two modern machine-learning classifiers such as, support vector machine with a different kernel function and k-nearest neighbor. The experimental results demonstrate that the combined effect of both features is valuable for network metrics to characterize the EEG time series signals in case of weighted complex network generating up to 100% classification accuracy.","['Epilepsy', 'Electroencephalography', 'Complex networks', 'Feature extraction', 'Support vector machines', 'Electric variables', 'Weight measurement']","['Average weighted degree', 'complex network', 'EEG', 'Epilepsy', 'KNN', 'modularity', 'SVM', 'visibility graph', 'weighted visibility graph']"
"In the era of big data, with the increasing number of audit data features, human-centered smart intrusion detection system performance is decreasing in training time and classification accuracy, and many support vector machine (SVM)-based intrusion detection algorithms have been widely used to identify an intrusion quickly and accurately. This paper proposes the FWP-SVM-genetic algorithm (GA) (feature selection, weight, and parameter optimization of support vector machine based on the genetic algorithm) based on the characteristics of the GA and the SVM algorithm. The algorithm first optimizes the crossover probability and mutation probability of GA according to the population evolution algebra and fitness value; then, it subsequently uses a feature selection method based on the genetic algorithm with an innovation in the fitness function that decreases the SVM error rate and increases the true positive rate. Finally, according to the optimal feature subset, the feature weights and parameters of SVM are simultaneously optimized. The simulation results show that the algorithm accelerates the algorithm convergence, increases the true positive rate, decreases the error rate, and shortens the classification time. Compared with other SVM-based intrusion detection algorithms, the detection rate is higher and the false positive and false negative rates are lower.","['Support vector machines', 'Sociology', 'Statistics', 'Feature extraction', 'Genetic algorithms', 'Intrusion detection', 'Biological cells']","['Genetic algorithm', 'intrusion detection', 'support vector machine']"
"The Internet of Things (IoT) has lately developed into an innovation for developing smart environments. Security and privacy are viewed as main problems in any technology's dependence on the IoT model. Privacy and security issues arise due to the different possible attacks caused by intruders. Thus, there is an essential need to develop an intrusion detection system for attack and anomaly identification in the IoT system. In this work, we have proposed a deep learning-based method Deep Belief Network (DBN) algorithm model for the intrusion detection system. Regarding the attacks and anomaly detection, the CICIDS 2017 dataset is utilized for the performance analysis of the present IDS model. The proposed method produced better results in all the parameters in relation to accuracy, recall, precision, F1-score, and detection rate. The proposed method has achieved 99.37% accuracy for normal class, 97.93% for Botnet class, 97.71% for Brute Force class, 96.67% for Dos/DDoS class, 96.37% for Infiltration class, 97.71% for Ports can class and 98.37% for Web attack, and these results were compared with various classifiers as shown in the results.","['Intrusion detection', 'Data models', 'Mathematical model', 'Training', 'Anomaly detection', 'Internet of Things']","['IoT', 'deep learning', 'anomaly detection', 'intrusion detection', 'DBN']"
"Differential permittivity sensors based on a pair of uncoupled microstrip lines, each one loaded with an open complementary split ring resonator (OCSRR), are proposed in this paper. The sensing principle is based on the measurement of the cross-mode insertion loss, very sensitive to asymmetric loading. Thus, by loading one of the OCSRRs with the reference sample, and the other one with the sample under test (SUT), the difference in the complex permittivity between both samples generates an asymmetry that gives rise to mode conversion. From the measurement of the cross-mode transmission coefficient, the dielectric properties of the SUT can be determined, provided those of the reference sample are well known. It is shown that by adding fluidic channels on top of the OCSRRs, the proposed sensor is useful for the measurement of the complex dielectric constant of liquids, and experimental results in mixtures of ethanol and deionized (DI) water and methanol in DI water, as a function of the ethanol/methanol content, are provided. Due to the high sensitivity of the proposed differential sensor to detect small perturbations (asymmetries), the structure is also of interest for the accurate measurement of solute concentrations in liquid solutions. In this paper, the structure is applied to monitor sodium content in aqueous solutions, and it is found that sodium concentrations as small as 0.25 g/L can be resolved.","['Sensors', 'Liquids', 'Dielectric measurement', 'Transmission line measurements', 'Resonators', 'Dielectrics', 'Permittivity']","['Microwave sensors', 'dielectric characterization', 'permittivity sensors', 'differential sensors', 'split ring resonators', 'microstrip technology']"
"Diabetic retinopathy (DR) is a major reason for the increased visual loss globally, and it became an important cause of visual impairment among people in 25-74 years of age. The DR significantly affects the economic status in society, particularly in healthcare systems. When timely treatment is provided to the DR patients, approximately 90% of patients can be saved from visual loss. Therefore, it becomes highly essential to classify the stages and severity of DR for the recommendation of required treatments. In this view, this paper introduces a new automated Hyperparameter Tuning Inception-v4 (HPTI-v4) model for the detection and classification of DR from color fundus images. At the preprocessing stage, the contrast level of the fundus image will be improved by the use of contrast limited adaptive histogram equalization (CLAHE) model. Then, the segmentation of the preprocessed image takes place utilizing a histogram-based segmentation model. Afterward, the HPTI-v4 model is applied to extract the required features from the segmented image and it subsequently undergoes classification by the use of a multilayer perceptron (MLP). A series of experiments take place on MESSIDOR (Methods to Evaluate Segmentation and Indexing Techniques in the field of Retinal Ophthalmology) DR dataset to guarantee the goodness of the HPTI-v4 approach and the obtained results clearly exhibited the supremacy of the HPTI-v4 model over the compared methods in a significant way.","['Image segmentation', 'Diabetes', 'Tuning', 'Histograms', 'Feature extraction', 'Biomedical imaging', 'Deep learning']","['Diabetic retinopathy', 'image classification', 'hyperparameter', 'deep learning']"
"Conventional sensing methodologies for smart home are known to be labor-intensive and complicated for practical deployment. Thus, researchers are resorting to alternative sensing mechanisms. Wi-Fi is one of the key technologies that enable connectivity for smart home services. Apart from its primary use for communication, Wi-Fi signal has now been widely leveraged for various sensing tasks, such as gesture recognition and fall detection, due to its sensitivity to environmental dynamics. Building smart home based on Wi-Fi sensing is cost-effective, non-invasive, and enjoys convenient deployment. In this paper, we survey the recent advances in the smart home systems based on the Wi-Fi sensing, mainly in such areas as health monitoring, gesture recognition, contextual information acquisition, and authentication.","['Wireless fidelity', 'Smart homes', 'Monitoring', 'Intelligent sensors', 'Gesture recognition', 'Task analysis']","['IoT', 'smart home', 'WiFi sensing']"
"For fifth-generation wireless communication systems, network slicing has emerged as a key concept to meet the diverse requirements of various use cases. By slicing an infrastructure network into multiple dedicated logical networks, wireless networks can support a wide range of services. However, how to fast deploy the end-to-end slices is the main issue in a multi-domain wireless network infrastructure. In this paper, a mathematical model is used to construct network slice requests and map them to the infrastructure network. The mapping process consists of two steps: the placement of virtual network functions and the selection of link paths chaining them. To efficiently utilize the limited physical resources, we pay attention to the service-oriented deployment by offering different deployment policies for three typical slices: eMBB slices, mMTC slices, and uRLLC slices. Furthermore, we adopt complex network theory to obtain the topological information of slices and infrastructure network. With the topological information, we define a node importance metric to rank the nodes in node mapping. To evaluate the performance of deployment policy we proposed, extensive simulations have been conducted. The results have shown that our algorithm performed better in terms of resource efficiency and acceptance ratio. In addition, the average execute time of our algorithm is in a linear growth with the increase of infrastructure network size.","['Mathematical model', 'Network slicing', '5G mobile communication', 'Wireless networks', 'Heuristic algorithms', 'Complex networks']","['5G', 'network slicing', 'complex network theory', 'service-oriented deployment', 'end-to-end slices']"
"A terminal sliding mode control (SMC) method based on nonlinear disturbance observer is investigated to realize the speed and the current tracking control for the permanent magnet synchronous motor (PMSM) drive system in this paper. The proposed method adopts the speed-current single-loop control structure instead of the traditional cascade control in the vector control of the PMSM. First, considering the nonlinear and the coupling characteristic, a single-loop terminal sliding mode controller is designed for PMSM drive system through feedback linearization technology. This method can make the motor speed and current reach the reference value in finite time, which can realize the fast transient response. Although the SMC is less sensitive to parameter uncertainties and external disturbance, it may produce a large switching gain, which may cause the undesired chattering. Meanwhile, the SMC cannot keep the property of invariance in the presence of unmatched uncertainties. Then, a nonlinear disturbance observer is proposed to the estimate the lump disturbance, which is used in the feed-forward compensation control. Thus, a composite control scheme is developed for the PMSM drive system. The results show that the motor control system based on the proposed method has good speed and current tracking performance and strong robustness.","['Sliding mode control', 'Disturbance observers', 'Drives', 'Uncertainty', 'Robustness', 'Torque']","['PMSM drive', 'terminal sliding mode control', 'feedback linearization', 'nonlinear disturbance observer']"
"Multi-access edge computing (MEC) has recently been proposed to aid mobile end devices in providing compute- and data-intensive services with low latency. Growing service demands by the end devices may overwhelm MEC installations, while cost constraints limit the increases of the installed MEC computing and data storage capacities. At the same time, the ever increasing computation capabilities and storage capacities of mobile end devices are valuable resources that can be utilized to enhance the MEC. This article comprehensively surveys the topic area of device-enhanced MEC, i.e., mechanisms that jointly utilize the resources of the community of end devices and the installed MEC to provide services to end devices. We classify the device-enhanced MEC mechanisms into mechanisms for computation offloading and mechanisms for caching. We further subclassify the offloading and caching mechanisms according to the targeted performance goals, which include throughput maximization, latency minimization, energy conservation, utility maximization, and enhanced security. We identify the main limitations of the existing device-enhanced MEC mechanisms and outline future research directions.","['Device-to-device communication', 'Edge computing', 'Servers', 'Cloud computing', 'Wireless communication', 'Security', 'Communication system security']","['Caching', 'computation offloading', 'device-to-device (D2D) communication', 'mobile edge computing (MEC)']"
"To address the vast variety of user requirements, applications, and channel conditions, flexibility support is strongly highlighted for 5G radio access technologies (RATs). For this purpose, usage of multiple orthogonal frequency division multiplexing (OFDM) numerologies, i.e., different parameterization of OFDM-based subframes, within the same frame has been proposed in the third-generation partnership project discussions for 5G new radio. This concept will likely meet the current expectations in multiple service requirements to some extent. However, since the quantity of wireless devices, applications, and heterogeneity of user requirements will keep increasing toward the next decade, the sufficiency of the aforementioned flexibility consideration remains quite disputable for future services. Therefore, novel RATs facilitating much more flexibility are needed to address various technical challenges, e.g., power efficiency, massive connectivity, latency, spectral efficiency, robustness against channel dispersions, and so on. In this paper, we discuss the potential directions to achieve further flexibility in RATs beyond 5G, such as future releases of 5G and 6G. In this context, a framework for developing flexible waveform, numerology, and frame design strategies is proposed along with sample methods. We also discuss their potential role to handle various upper-level system issues, including the ones in orthogonal and nonorthogonal multiple accessing schemes and cellular networks. By doing so, we aim to contribute to the future vision of designing flexible RATs and to point out the possible research gaps in the related fields.","['Radio access technologies', 'OFDM', '5G mobile communication', 'Signal to noise ratio', 'Interference']","['5G', '6G', 'FBMC', 'multi-access communications', 'numerology', 'OFDM', 'radio access networks', 'waveform', 'wireless communications']"
"Mobile edge computing is a new cloud computing paradigm, which makes use of small-sized edge clouds to provide real-time services to users. These mobile edge-clouds (MECs) are located in close proximity to users, thus enabling users to seamlessly access applications running on MECs. Due to the co-existence of the core (centralized) cloud, users, and one or multiple layers of MECs, an important problem is to decide where (on which computational entity) to place different components of an application. This problem, known as the application or workload placement problem, is notoriously hard, and therefore, heuristic algorithms without performance guarantees are generally employed in common practice, which may unknowingly suffer from poor performance as compared with the optimal solution. In this paper, we address the application placement problem and focus on developing algorithms with provable performance bounds. We model the user application as an application graph and the physical computing system as a physical graph, with resource demands/availabilities annotated on these graphs. We first consider the placement of a linear application graph and propose an algorithm for finding its optimal solution. Using this result, we then generalize the formulation and obtain online approximation algorithms with polynomial-logarithmic (poly-log) competitive ratio for tree application graph placement. We jointly consider node and link assignment, and incorporate multiple types of computational resources at nodes.","['Cloud computing', 'Approximation algorithms', 'Databases', 'Mobile communication', 'Mobile handsets', 'Face recognition', 'Streaming media']","['Cloud computing', 'graph mapping', 'mobile edge-cloud (MEC)', 'online approximation algorithm', 'optimization theory']"
"As a crime of employing technical means to steal sensitive information of users, phishing is currently a critical threat facing the Internet, and losses due to phishing are growing steadily. Feature engineering is important in phishing website detection solutions, but the accuracy of detection critically depends on prior knowledge of features. Moreover, although features extracted from different dimensions are more comprehensive, a drawback is that extracting these features requires a large amount of time. To address these limitations, we propose a multidimensional feature phishing detection approach based on a fast detection method by using deep learning. In the first step, character sequence features of the given URL are extracted and used for quick classification by deep learning, and this step does not require third-party assistance or any prior knowledge about phishing. In the second step, we combine URL statistical features, webpage code features, webpage text features, and the quick classification result of deep learning into multidimensional features. The approach can reduce the detection time for setting a threshold. Testing on a dataset containing millions of phishing URLs and legitimate URLs, the accuracy reaches 98.99%, and the false positive rate is only 0.59%. By reasonably adjusting the threshold, the experimental results show that the detection efficiency can be improved.","['Phishing', 'Feature extraction', 'Uniform resource locators', 'Deep learning', 'Blacklisting', 'Internet']","['Phishing website detection', 'convolutional neural network', 'long short-term memory network', 'semantic feature', 'machine learning']"
"The usage and adoption of electric vehicles (EVs) have increased rapidly in the 21st century due to the shifting of the global energy demand away from fossil fuels. The market penetration of EVs brings new challenges to the usual operations of the power system. Uncontrolled EV charging impacts the local distribution grid in terms of its voltage profile, power loss, grid unbalance, and reduction of transformer life, as well as harmonic distortion. Multiple research studies have addressed these problems by proposing various EV charging control methods. This manuscript comprehensively reviews EV control charging strategies using real-world data. This review classifies the EV control charging strategies into scheduling, clustering, and forecasting strategies. The models of EV control charging strategies are highlighted to compare and evaluate the techniques used in EV charging, enabling the identification of the advantages and disadvantages of the different methods applied. A summary of the methods and techniques for these EV charging strategies is presented based on machine learning and probabilities approaches. This research paper indicates many factors and challenges in the development of EV charging control in next-generation smart grid applications and provides potential recommendations. A report on the guidelines for future studies on this research topic is provided to enhance the comparability of the various results and findings. Accordingly, all the highlighted insights of this paper serve to further the increasing effort towards the development of advanced EV charging methods and demand-side management (DSM) for future smart grid applications.","['Electric vehicle charging', 'Forecasting', 'Power system stability', 'Batteries', 'Stability analysis', 'Data models', 'Power grids']","['Electric vehicle charging', 'scheduling', 'clustering', 'forecasting', 'probabilities', 'machine learning']"
"In this paper, a forecasting algorithm is proposed to predict photovoltaic (PV) power generation using a long short term memory (LSTM) neural network (NN). A synthetic weather forecast is created for the targeted PV plant location by integrating the statistical knowledge of historical solar irradiance data with the publicly available type of sky forecast of the host city. To achieve this, a K-means algorithm is used to classify the historical irradiance data into dynamic type of sky groups that vary from hour to hour in the same season. In other words, the types of sky are defined for each hour uniquely using different levels of irradiance based on the hour of the day and the season. This can mitigate the performance limitations of using fixed type of sky categories by translating them into dynamic and numerical irradiance forecast using historical irradiance data. The proposed synthetic weather forecast is proved to embed the statistical features of the historical weather data, which results in a significant improvement in the forecasting accuracy. The performance of the proposed model is investigated using different intraday horizon lengths in different seasons. It is shown that using the synthetic irradiance forecast can achieve up to 33% improvement in accuracy in comparison to that when an hourly categorical type of sky forecast is used, and up to 44.6% in comparison to that when a daily type of sky forecast is used. This highlights the significance of utilizing the proposed synthetic forecast, and promote a more efficient utilization of the publicly available type of sky forecast to achieve a more reliable PV generation prediction. Moreover, the superiority of the LSTM NN with the proposed features is verified by investigating other machine learning engines, namely the recurrent neural network (RNN), the generalized regression neural network (GRNN) and the extreme learning machine (ELM).","['Weather forecasting', 'Forecasting', 'Predictive models', 'Time series analysis', 'Artificial neural networks']","['PV power forecasting', 'machine learning', 'LSTM', 'neural network', 'deep learning', 'synthetic weather forecast']"
"This paper sets up a framework for designing a massive multiple-input multiple-output (MIMO) testbed by investigating hardware (HW) and system-level requirements, such as processing complexity, duplexing mode, and frame structure. Taking these into account, a generic system and processing partitioning is proposed, which allows flexible scaling and processing distribution onto a multitude of physically separated devices. Based on the given HW constraints such as maximum number of links and maximum throughput for peer-to-peer interconnections combined with processing capabilities, the framework allows to evaluate modular HW components. To verify our design approach, we present the Lund University Massive MIMO testbed, which constitutes the first reconfigurable real-time HW platform for prototyping massive MIMO. Utilizing up to 100 base station antennas and more than 50 field programmable gate array, up to 12 user equipment are served on the same time/frequency resource using an LTE-like orthogonal frequency division multiplexing time-division duplex-based transmission scheme. Proof-of-concept tests with this system show that massive MIMO can simultaneously serve a multitude of users in a static indoor and static outdoor environment utilizing the same time/frequency resource.","['MIMO', 'Hardware', 'OFDM', 'Real-time systems', 'Calibration', 'Antennas', 'Channel estimation']","['5G', 'system design', 'testbed', 'outdoor measurement', 'indoor measurement', 'software-defined radio', 'TDD']"
"Cooperative Intelligent Transportation Systems, mainly represented by vehicular ad hoc networks (VANETs), are among the key components contributing to the Smart City and Smart World paradigms. Based on the continuous exchange of both periodic and event triggered messages, smart vehicles can enhance road safety, while also providing support for comfort applications. In addition to the different communication protocols, securing such communications and establishing a certain trustiness among vehicles are among the main challenges to address, since the presence of dishonest peers can lead to unwanted situations. To this end, existing security solutions are typically divided into two main categories, cryptography and trust, where trust appeared as a complement to cryptography on some specific adversary models and environments where the latter was not enough to mitigate all possible attacks. In this paper, we provide an adversary-oriented survey of the existing trust models for VANETs. We also show when trust is preferable to cryptography, and the opposite. In addition, we show how trust models are usually evaluated in VANET contexts, and finally, we point out some critical scenarios that existing trust models cannot handle, together with some possible solutions.","['Cryptography', 'Vehicular ad hoc networks', 'Vehicles', 'Privacy', 'Biological system modeling', 'Safety']","['VANETs', 'trust management', 'attacker models']"
"Recent deep learning based image editing methods have achieved promising results for removing object in an image but fail to generate plausible results for removing large objects of complex nature, especially in facial images. The objective of this work is to remove mask objects in facial images. This problem is challenging because (1) most of the time facial masks cover quite a large region of face that even extends beyond the actual face boundary below chin, and (2) facial image pairs with and without mask object do not exist for training. We break the problem into two stages: mask object detection and image completion of the removed mask region. The first stage of our model automatically produces binary segmentation for the mask region. Then, the second stage removes the mask and synthesizes the affected region with fine details while retaining the global coherency of face structure. For this, we have employed a GAN-based network using two discriminators where one discriminator helps learn the global structure of the face and then another discriminator comes in to focus learning on the deep missing region. To train our model in a supervised manner, we create a paired synthetic dataset using publicly available CelebA dataset and evaluated on real world images collected from the Internet. Our model outperforms others representative state-of-the-art approaches both qualitatively and quantitatively.","['Face', 'Gallium nitride', 'Object detection', 'Glass', 'Image edge detection', 'Deep learning', 'Training']","['Generative adversarial network', 'object removal', 'image editing']"
"Multilevel thresholding has got more attention in recent years with various successful applications. However, the implementation becomes more and more complex and time-consuming when the number of thresholds is high, and color images which contain more information are even worse. Therefore, this paper proposes an alternative hybrid algorithm for color image segmentation, the advantages of which lie in extracting the best features from the high performance of two algorithms and overcoming the limitations of each algorithm to some extent. Two techniques, Otsu's method, and Kapur's entropy, are used as fitness function to determine the segmentation threshold values. Harris hawks optimization (HHO) is a novel and general-purpose algorithm, and the hybridization of HHO is fulfilled by adding another powerful algorithm-differential evolution (DE), which is known as HHO-DE. More specifically, the whole population is divided into two equal subpopulations which will be assigned to HHO and DE algorithms, respectively. Then both algorithms operate in parallel to update the positions of each subpopulation during the iterative process. In order to fully demonstrate the superior performance of HHO-DE, the proposed method is compared with the seven state-of-the-art algorithms by an array of experiments on ten benchmark images. Meanwhile, five measures, including the average fitness values, standard deviation (STD), peak signal to noise ratio (PSNR), structure similarity index (SSIM), and feature similarity index (FSIM), are used to evaluate the performance of each algorithm. In addition, Wilcoxon's rank sum test for statistical analysis and the comparison with the super-pixel method are also conducted to verify the superiority of HHO-DE. The experimental results reveal that the proposed method significantly outperforms other algorithms. Hence, the HHO-DE algorithm is a remarkable and promising tool for multilevel thresholding color image segmentation.","['Image segmentation', 'Entropy', 'Color', 'Optimization', 'Feature extraction', 'Heuristic algorithms', 'Sociology']","['Image segmentation', 'hybrid algorithm', 'Harris hawks optimization', 'differential evolution', 'Kapur’s entropy', 'Otsu’s method']"
"The very last wireless network technology, created to increase the speed and the connections responsiveness, the Fifth-Generation Network (5G) can transmit a great volume of data. It uses wireless broadband connections to support specific end-users and businesses services. It is specifically useful for the Internet of Vehicles (IoV), guaranteeing fast connections and security. The 5G network technology can be used to support Vehicle-to-Everything (V2X) communications and applications on autonomous vehicles. It can enable information exchanges between vehicles and other infrastructures and people. It can also provide a more comfortable and safer environment and accurate traffic knowledge. The traffic ?ow can be improved, reducing pollution and accident rates. The cellular network can be associated with V2X as a communicating base to offer enhanced road safety and autonomous driving, and also to offer the IoV connections. This survey presents the 5G technology evolution, standards, and infrastructure associated with V2X ecosystem by IoV. In other words, it presents the IoV supported by 5G V2X communications, considering its architecture, applications and also the V2X features and protocols, as well as the modes, the evaluation and the technological support in such combination. The contribution of this paper is a systematized study about the interaction among these three contents: IoV, 5G, and V2X. Eighty four works were selected to present concepts, standards and to identify the ways to overcome challenges. This survey aims to guide the development of new 5G-V2X services and technologies dedicated to vehicle communications, and also to indicate future directions.","['5G mobile communication', 'Vehicle-to-everything', 'Sensors', 'Wireless communication', 'Autonomous vehicles', 'Cellular networks']","['IoV', 'connected vehicles', '5G networks', 'V2X communications']"
"In this paper, a random frequency diverse array-based directional modulation with artificial noise (RFDA-DM-AN) scheme is proposed to enhance physical layer security of wireless communications. Specifically, we first design the RFDA-DM-AN scheme by randomly allocating frequencies to transmit antennas, thereby achieving 2-D (i.e., angle and range) secure transmissions, and outperforming the state-of-the-art 1-D (i.e., angle) phase array (PA)-based DM scheme. Then we derive the closed-form expression of a lower bound on the ergodic secrecy capacity (ESC) of our RFDA-DM-AN scheme. Based on the theoretical lower bound derived, we further optimize the transmission power allocation between the useful signal and artificial noise (AN) in order to improve the ESC. Simulation results show that: (1) our RFDA-DM-AN scheme achieves a higher secrecy capacity than that of the PA-based DM scheme; (2) the lower bound derived is shown to approach the ESC as the number of transmit antennas N increases and precisely matches the ESC when N is sufficiently large; and (3) the proposed optimum power allocation achieves the highest ESC of all power allocations schemes in the RFDA-DM-AN.","['Radio spectrum management', 'Frequency diversity', 'Transmitting antennas', 'Network security', 'Physical layer', 'Resource management', 'Antenna arrays', 'Noise measurement', 'Power system reliability']","['Physical layer security', 'directional modulation', 'frequency diverse array', 'power allocation']"
"We propose a cognitive healthcare framework that adopts the Internet of Things (IoT)-cloud technologies. This framework uses smart sensors for communications and deep learning for intelligent decision-making within the smart city perspective. The cognitive and smart framework monitors patients' state in real time and provides accurate, timely, and high-quality healthcare services at low cost. To assess the feasibility of the proposed framework, we present the experimental results of an EEG pathology classification technique that uses deep learning. We employ a range of healthcare smart sensors, including an EEG smart sensor, to record and monitor multimodal healthcare data continuously. The EEG signals from patients are transmitted via smart IoT devices to the cloud, where they are processed and sent to a cognitive module. The system determines the state of the patient by monitoring sensor readings, such as facial expressions, speech, EEG, movements, and gestures. The real-time decision, based on which the future course of action is taken, is made by the cognitive module. When information is transmitted to the deep learning module, the EEG signals are classified as pathologic or normal. The patient state monitoring and the EEG processing results are shared with healthcare providers, who can then assess the patient's condition and provide emergency help if the patient is in a critical state. The proposed deep learning model achieves better accuracy than the state-of-the-art systems.","['Medical services', 'Electroencephalography', 'Monitoring', 'Smart cities', 'Pathology', 'Deep learning', 'Real-time systems']","['Cognitive', 'IoT-cloud', 'deep learning', 'smart healthcare', 'EEG']"
"The concept of smart home is widely favored, as it enhances the lifestyle of the residents involving multiple disciplines, i.e., lighting, security, and much more. As the smart home networks continue to grow in size and complexity, it is essential to address a handful among the myriads of challenges related to data loss due to the interference and efficient energy management. In this paper, we propose a smart home control system using a coordinator-based ZigBee networking. The working of the proposed system is three fold: smart interference control system controls the interference caused due to the co-existence of IEEE 802.11x-based wireless local area networks and wireless sensor networks; smart energy control system is developed to integrate sunlight with light source and optimizes the energy consumption of the household appliances by controlling the unnecessary energy demands; and smart management control system to efficiently control the operating time of the electronic appliances. The performance of the proposed smart home is testified through computer simulation. Simulation results show that the proposed smart home system is less affected by the interference and efficient in reducing the energy consumption of the appliances used in a smart home.","['Smart homes', 'Home automation', 'Energy management', 'Internet of things', 'ZigBee', 'Interference', 'Wireless LAN', 'IEEE 802.11x Standard']","['ZigBee', 'interference', 'IEEE 80211x', 'WLAN', 'smart home']"
"The long short-term memory (LSTM) model is one of the most commonly used vehicle trajectory predicting models. In this paper, we study two problems of the existing LSTM models for long-term trajectory prediction in dense traffic. First, the existing LSTM models cannot simultaneously describe the spatial interactions between different vehicles and the temporal relations between the trajectory time series. Thus, the existing models cannot accurately estimate the influence of the interactions in dense traffic. Second, the basic LSTM models often suffer from vanishing gradient problem and are, thus, hard to train for long time series. These two problems sometimes lead to large prediction errors in vehicle trajectory predicting. In this paper, we proposed a spatio-temporal LSTM-based trajectory prediction model (ST-LSTM) which includes two modifications. We embed spatial interactions into LSTM models to implicitly measure the interactions between neighboring vehicles. We also introduce shortcut connections between the inputs and the outputs of two consecutive LSTM layers to handle gradient vanishment. The proposed new model is evaluated on the I-80 and US-101 datasets. Results show that our new model has a higher trajectory predicting accuracy than one state-of-the-art model [maneuver-LSTM (M-LSTM)].","['Trajectory', 'Predictive models', 'Hidden Markov models', 'Time series analysis', 'Training', 'Brakes', 'Roads']","['Trajectory prediction', 'vehicle interactions', 'shortcut connection', 'long short-term memory (LSTM)']"
"In this paper, we strive to construct an efficient multi-hop network based on the sub-GHz low-power wide-area technology. Specifically, we investigate the combination of LoRa, a physical-layer standard that can provide several-kilometer outdoor coverage, and concurrent transmission (CT), a recently proposed multi-hop protocol that can significantly improve the network efficiency. The main contributions of this paper are threefold. 1) Since the CT enhances the network efficiency by allowing synchronized packet collisions, the performance of the physical-layer receiver under such packet collisions needs to be carefully examined to ensure the network reliability. We first extensively evaluate the LoRa receiver performance under CT to verify that LoRa is compatible to CT. Specifically, we find that, due to the time-domain and frequency-domain energy spreading effect, LoRa is robust to the packet collisions resulting from CT. 2) We further find the receiver performance under CT can be further improved by introducing timing offsets between the relaying packets. In view of this, we propose a timing delay insertion method, the offset-CT method, that adds random timing delay before the packets while preventing the timing offset from diverging over the multi-hop network. 3) We conduct proof-of-concept experiments to demonstrate the feasibility of CT-based LoRa multi-hop network and the performance improvement brought by the proposed offset-CT method.","['Receivers', 'Spread spectrum communication', 'Standards', 'Frequency shift keying', 'Timing', 'Protocols', 'Chirp']","['LoRa', 'concurrent transmission', 'multi-hop networks', 'mesh networks', 'communication networks', 'relay networks', 'ad hoc networks']"
"Non-orthogonal multiple access (NOMA) has been shown in the literature to have a better performance than OMA in terms of sum channel capacity; however, the capacity superiority of NOMA over OMA has been only proved for single antenna systems, and the proof for the capacity superiority of multiple-input multiple-output NOMA (MIMO-NOMA) over conventional MIMO-OMA has not been available yet. In this paper, we will provide our proof to demonstrate that the MIMO-NOMA is strictly better than MIMO-OMA in terms of sum channel capacity (except for the case where only one user is being communicated to), i.e., for any rate pair achieved by MIMO-OMA, there is a power split for which MIMO-NOMA can achieve rate pairs that are strictly larger. Based on this result, we prove that the MIMO-NOMA can also achieve a larger sum ergodic capacity than MIMO-OMA. Our analytical results are verified by simulations.","['NOMA', 'Performance evaluation', 'MIMO', 'Channel capacity', 'Analytical models']","['NOMA', 'OMA', 'MIMO', 'multiple access', 'capacity']"
"There is an immense need of a proof of delivery (PoD) of today's digital media and content, especially those that are subject to payment. Current PoD systems are mostly centralized and heavily dependent on a trusted third party (TTP) especially for payment. Such existing PoD systems often lack security, transparency, and visibility, and are not highly credible, as the TTP can be subject to failure, manipulation, corruption, compromise, and hacking. In this paper, we propose a decentralized PoD solution for PoD of digital assets. Our solution leverages key features of blockchain and Ethereum smart contracts to provide immutable and tamper-proof logs, accountability, and traceability. Ethereum smart contracts are used to orchestrate and govern all interactions and transactions including automatic payments in Ether cryptocurrency between customers, digital-content provider, and the file server hosting the digital content. All entities are incentivized to act honestly, and our solution has a mechanism to handle dispute if arisen among participants. The solution has an off-chain secure download phase involving the file server and customers. Moreover, our solution leverages the benefits of interplanetary file system to store the agreed upon terms and conditions between the smart contract actors. A security analysis of our proposed system has been provided. The full code of the smart contract has been publicly made available on Github.","['Servers', 'Media', 'Videos']","['Blockchain', 'Ethereum', 'smart contracts', 'proof of delivery', 'digital content']"
"In the past few years, the implementation of blockchain technology for various applications has been widely discussed in the research community and the industry. There are sufficient number of articles that discuss the possibility of applying blockchain technology in various areas, such as, healthcare, IoT, and business. However, in this article, we present a comparative analysis of core blockchain architecture, its fundamental concepts, and its applications in three major areas: the Internet-of-Things (IoT), healthcare, business and vehicular industry. For each area, we discuss in detail, challenges and solutions that have been proposed from the research community and industry. This research studies also presented the complete ecosystem of blockchain of all the papers we reviewed and summarized. Moreover, analysis is performed of various blockchain platforms, their consensus models, and applications. Finally, we discuss key aspects that are required for the widespread future adoption of blockchain technology in these major areas.","['Medical services', 'Industries', 'Ecosystems', 'Organizations']","['Blockchain', 'IoT blockchain', 'healthcare blockchain', 'permissioned blockchain', 'business blockchain']"
"Multilevel inverters are a new family of converters for dc-ac conversion for the medium and high voltage and power applications. In this paper, two new topologies for the staircase output voltage generations have been proposed with a lesser number of switch requirement. The first topology requires three dc voltage sources and ten switches to synthesize 15 levels across the load. The extension of the first topology has been proposed as the second topology, which consists of four dc voltage sources and 12 switches to achieve 25 levels at the output. Both topologies, apart from having lesser switch count, exhibit the merits in terms of reduced voltage stresses across the switches. In addition, a detailed comparative study of both topologies has been presented in this paper to demonstrate the features of the proposed topologies. Several experimental results have been included in this paper to validate the performances of the proposed topologies with different loading condition and dynamic changes in load and modulation indexes.","['Topology', 'Inverters', 'Switching converters', 'DC-AC power converters', 'Power electronics']","['Asymmetric', 'hybrid inverter', 'inverter topology', 'multilevel inverter', 'MLI', 'nearest level control', 'power electronics', 'single-phase inverter', 'reduce switch count']"
"Non-orthogonal multiple access (NOMA) is envisioned as a key technology to enhance the spectrum efficiency for 5G cellular networks. Meanwhile, ambient backscatter communication is a promising solution to the Internet of Things (IoT), due to its high spectrum efficiency and power efficiency. In this paper, we are interested in a symbiotic system of cellular and IoT networks and propose a backscatter-NOMA system, which incorporates a downlink NOMA system with a backscatter device (BD). In the proposed system, the base station (BS) transmits information to two cellular users according to the NOMA protocol, while a BD transmits its information over the BS signals to one cellular user using the passive radio technology. In particular, if the BS only serves the cellular user that decodes BD information, the backscatter-NOMA system turns into a symbiotic radio (SR) system. We derive the expressions of the outage probabilities and the ergodic rates and analyze the corresponding diversity orders and slopes for both backscatter-NOMA and SR systems. Finally, we provide the numerical results to verify the theoretical analysis and demonstrate the interrelationship between the cellular networks and the IoT networks.","['NOMA', 'Probability', 'Power system reliability', 'Backscatter', 'Signal to noise ratio', 'Interference', 'Symbiosis']","['Non-orthogonal multiple access (NOMA)', 'Internet-of-Things (IoT)', 'ambient backscatter communication (AmBC)', 'symbiotic radio (SR)', 'outage probability', 'ergodic rate']"
"Nowadays, coronavirus (COVID-19) is getting international attention due it considered as a life-threatened epidemic disease that hard to control the spread of infection around the world. Machine learning (ML) is one of intelligent technique that able to automatically predict the event with reasonable accuracy based on the experience and learning process. In the meantime, a rapid number of ML models have been proposed for predicate the cases of COVID-19. Thus, there is need for an evaluation and benchmarking of COVID-19 ML models which considered the main challenge of this study. Furthermore, there is no single study have addressed the problem of evaluation and benchmarking of COVID diagnosis models. However, this study proposed an intelligent methodology is to help the health organisations in the selection COVID-19 diagnosis system. The benchmarking and evaluation of diagnostic models for COVID-19 is not a trivial process. There are multiple criteria requires to evaluate and some of the criteria are conflicting with each other. Our study is formulated as a decision matrix (DM) that embedded mix of ten evaluation criteria and twelve diagnostic models for COVID-19. The multi-criteria decision-making (MCDM) method is employed to evaluate and benchmarking the different diagnostic models for COVID19 with respect to the evaluation criteria. An integrated MCDM method are proposed where TOPSIS applied for the benchmarking and ranking purpose while Entropy used to calculate the weights of criteria. The study results revealed that the benchmarking and selection problems associated with COVID19 diagnosis models can be effectively solved using the integration of Entropy and TOPSIS. The SVM (linear) classifier is selected as the best diagnosis model for COVID19 with the closeness coefficient value of 0.9899 for our case study data. Furthermore, the proposed methodology has solved the significant variance for each criterion in terms of ideal best and worst best value, beside issue when specific diagnosis models have same ideal best value.","['Benchmark testing', 'Medical diagnostic imaging', 'Reliability', 'Diagnostic radiography', 'Tools', 'COVID-19']","['COVID19 diagnostic', 'machine learning', 'benchmarking methodology', 'chest X-rays images', 'entropy', 'TOPSIS', 'multi-criteria decision-making']"
"The utilization of renewable energy sources (RESs) has become significant throughout the world, especially over the last two decades. Although high-level RESs penetration reduces negative environmental impact compared to conventional fossil fuel-based energy generation, control issues become more complex as the system inertia is significantly decreased due to the absence of conventional synchronous generators. Some other technical issues, high uncertainties, low fault ride through capability, high fault current, low generation reserve, and low power quality, arise due to RESs integration. Renewable energy like solar and wind are highly uncertain due to the intermittent nature of wind and sunlight. Cutting edge technologies including different control strategies, optimization techniques, energy storage devices, and fault current limiters are employed to handle those issues. This paper summarizes several challenges in the integration process of high-level RESs to the existing grid. The respective solutions to each challenge are presented and discussed. A comprehensive list of challenges and solutions, for both wind and solar energy integration cases, are well documented. Finally, the future recommendations are provided to solve the several problems of renewable energy integration which could be key research areas for the industry personnel and researchers.","['Renewable energy sources', 'Wind power generation', 'Frequency control', 'Synchronous generators', 'Uncertainty', 'Wind turbines']","['Renewable energy resources', 'solar and wind energy conversion', 'virtual inertia', 'fault ride through capability', 'fault current limiter', 'control of converter']"
"With the increasing demands on quality healthcare and the raising cost of care, pervasive healthcare is considered as a technological solutions to address the global health issues. In particular, the recent advances in Internet of Things have led to the development of Internet of Medical Things (IoMT). Although such low cost and pervasive sensing devices could potentially transform the current reactive care to preventative care, the security and privacy issues of such sensing system are often overlooked. As the medical devices capture and process very sensitive personal health data, the devices and their associated communications have to be very secured to protect the user's privacy. However, the miniaturized IoMT devices have very limited computation power and fairly limited security schemes can be implemented in such devices. In addition, with the widespread use of IoMT devices, managing and ensuring the security of IoMT systems are very challenging and which are the major issues hindering the adoption of IoMT for clinical applications. In this paper, the security and privacy challenges, requirements, threats, and future research directions in the domain of IoMT are reviewed providing a general overview of the state-of-the-art approaches.","['Medical services', 'Security', 'Privacy', 'Servers', 'Sensors', 'Medical devices', 'Routing protocols']","['Security', 'privacy', 'Internet of Medical Things', 'IoMT', 'mIoT', 'healthcare systems', 'survey']"
"Estimation of absolute temperature distributions is crucial for many thermal processes in the nonlinear distributed parameter systems, such as predicting the curing temperature distribution of the chip, the temperature distribution of the catalytic rod, and so on. In this work, a spatiotemporal model based on the Karhunen-Loève (KL) decomposition, the multilayer perceptron (MLP), and the long short-term memory (LSTM) network, named KL-MLP-LSTM, is developed for estimating temperature distributions with a three-step procedure. Firstly, the infinite-dimensional model is transformed into a finite-dimensional model, where the KL decomposition method is used for dimension reduction and spatial basis functions extraction. Secondly, a novel MLP-LSTM hybrid time series model is constructed to deal with the two inherently coupled nonlinearities. Finally, the spatiotemporal temperature distribution model can be reconstructed through spatiotemporal synthesis. The effectiveness of the proposed model is validated by the data from a snap curing oven thermal process. Satisfactory agreement between the results of the current model and the other well-established model shows that the KL-MLP-LSTM model is reliable for estimating the temperature distributions during the thermal process.","['Spatiotemporal phenomena', 'Mathematical model', 'Temperature distribution', 'Curing', 'Analytical models', 'Time series analysis', 'Data models']","['Spatiotemporal modeling', 'nonlinear distributed thermal processes', 'Karhunen-Loève decomposition', 'multilayer perceptron', 'long short-term memory']"
"This paper presents an optimised bidirectional Vehicle-to-Grid (V2G) operation, based on a fleet of Electric Vehicles (EVs) connected to a distributed power system, through a network of charging stations. The system is able to perform day-ahead scheduling of EV charging/discharging to reduce EV ownership charging cost through participating in frequency and voltage regulation services. The proposed system is able to respond to real-time EV usage data and identify the required changes that must be made to the day-ahead energy prediction, further optimising the use of EVs to support both voltage and frequency regulation. An optimisation strategy is established for V2G scheduling, addressing the initial battery State Of Charge (SOC), EV plug-in time, regulation prices, desired EV departure time, battery degradation cost and vehicle charging requirements. The effectiveness of the proposed system is demonstrated using a standardized IEEE 33-node distribution network integrating five EV charging stations. Two case studies have been undertaken to verify the contribution of this advanced energy supervision approach. Comprehensive simulation results clearly show an opportunity to provide frequency and voltage support while concurrently reducing EV charging costs, through the integration of V2G technology, especially during on-peak periods when the need for active and reactive power is high.","['Vehicle-to-grid', 'Regulation', 'Batteries', 'State of charge', 'Voltage control', 'Reactive power']","['Electric vehicle', 'vehicle-to-grid', 'battery degradation performance', 'frequency regulation service', 'voltage regulation service', 'charging cost', 'day-ahead scheduling', 'smart-grid']"
"Recently, the Internet of Things (IoT) concept has attracted a lot of attention due to its capability to translate our physical world into a digital cyber world with meaningful information. The IoT devices are smaller in size, sheer in number, contain less memory, use less energy, and have more computational capabilities. These scarce resources for IoT devices are powered by small operating systems (OSs) that are specially designed to support the IoT devices' diverse applications and operational requirements. These IoT OSs are responsible for managing the constrained resources of IoT devices efficiently and in a timely manner. In this paper, discussions on IoT devices and OS resource management are provided. In detail, the resource management mechanisms of the state-of-the-art IoT OSs, such as Contiki, TinyOS, and FreeRTOS, are investigated. The different dimensions of their resource management approaches (including process management, memory management, energy management, communication management, and file management) are studied, and their advantages and limitations are highlighted.","['Resource management', 'Protocols', 'Memory management', 'Sensors', 'Security', 'Energy efficiency', 'Internet of Things']","['Internet of Things', 'operating systems', 'resource management', 'Contiki', 'TinyOS', 'FreeRTOS']"
,"['Smart grids', 'Electric vehicle charging', 'Mathematical model', 'Batteries', 'Schedules']","['Electric vehicles', 'smart grids', 'blockchain technology', 'adaptive charging scheme']"
"The selection of variational mode decomposition (VMD) parameters usually adopts the empirical method, trial-and-error method, or single-objective optimization method. The above-mentioned method cannot achieve the global optimal effect. Therefore, a multi-objective particle swarm optimization (MOPSO) algorithm is proposed to optimize the parameters of VMD, and it is applied to the composite fault diagnosis of the gearbox. The specific steps are: first, symbol dynamic entropy (SDE) can effectively remove background noise, and use state mode probability and state transition to preserve fault information. Power spectral entropy (PSE) reflects the complexity of signal frequency composition. Therefore, the SDE and PSE are selected as fitness functions and then the Pareto frontier optimal solution set is obtained by the MOPSO algorithm. Finally, the optimal combination of VMD parameters (k, a) is obtained by normalization. The improved VMD is used to analyze the simulation signal and gearbox fault signal. The effectiveness of the proposed method is verified by comparing with the ensemble empirical mode decomposition (EEMD).","['Optimization', 'Fault diagnosis', 'Particle swarm optimization', 'Bandwidth', 'Entropy', 'Indexes', 'Convergence']","['Variational mode decomposition', 'multi-objective particle swarm', 'symbol dynamic entropy', 'power spectral entropy', 'fault diagnosis of the gearbox']"
"Predicting crop yield based on the environmental, soil, water and crop parameters has been a potential research topic. Deep-learning-based models are broadly used to extract significant crop features for prediction. Though these methods could resolve the yield prediction problem there exist the following inadequacies: Unable to create a direct non-linear or linear mapping between the raw data and crop yield values; and the performance of those models highly relies on the quality of the extracted features. Deep reinforcement learning provides direction and motivation for the aforementioned shortcomings. Combining the intelligence of reinforcement learning and deep learning, deep reinforcement learning builds a complete crop yield prediction framework that can map the raw data to the crop prediction values. The proposed work constructs a Deep Recurrent Q-Network model which is a Recurrent Neural Network deep learning algorithm over the Q-Learning reinforcement learning algorithm to forecast the crop yield. The sequentially stacked layers of Recurrent Neural network is fed by the data parameters. The Q- learning network constructs a crop yield prediction environment based on the input parameters. A linear layer maps the Recurrent Neural Network output values to the Q-values. The reinforcement learning agent incorporates a combination of parametric features with the threshold that assist in predicting crop yield. Finally, the agent receives an aggregate score for the actions performed by minimizing the error and maximizing the forecast accuracy. The proposed model efficiently predicts the crop yield outperforming existing models by preserving the original data distribution with an accuracy of 93.7%.","['Agriculture', 'Reinforcement learning', 'Predictive models', 'Deep learning', 'Machine learning algorithms', 'Feature extraction', 'Data models']","['Crop yield prediction', 'deep recurrent Q-network', 'deep reinforcement learning', 'intelligent agrarian application']"
"Vehicular networks are facing the challenges to support ubiquitous connections and high quality of service for numerous vehicles. To address these issues, mobile edge computing (MEC) is explored as a promising technology in vehicular networks by employing computing resources at the edge of vehicular wireless access networks. In this paper, we study the efficient task offloading schemes in vehicular edge computing networks. The vehicles perform the offloading time selection, communication, and computing resource allocations optimally, the mobility of vehicles and the maximum latency of tasks are considered. To minimize the system costs, including the costs of the required communication and computing resources, we first analyze the offloading schemes in the independent MEC servers scenario. The offloading tasks are processed by the MEC servers deployed at the access point (AP) independently. A mobility-aware task offloading scheme is proposed. Then, in the cooperative MEC servers scenario, the MEC servers can further offload the collected overloading tasks to the adjacent servers at the next AP on the vehicles’ moving direction. A location-based offloading scheme is proposed. In both scenarios, the tradeoffs between the task completed latency and the required communication and computation resources are mainly considered. Numerical results show that our proposed schemes can reduce the system costs efficiently, while the latency constraints are satisfied.","['Task analysis', 'Servers', 'Edge computing', 'Resource management', 'Computational modeling', '5G mobile communication', 'Roads']","['Vehicular network', 'edge computing', 'resource allocation', 'offloading', 'mobility']"
"A decision map contains complete and clear information about the image to be fused, and detecting the decision map is crucial to various image fusion issues, especially multi-focus image fusion. Nevertheless, in an attempt to obtain an approving image fusion effect, it is necessary and always difficult to obtain a decision map. In this paper, we address this problem with a novel image segmentation-based multi-focus image fusion algorithm, in which the task of detecting the decision map is treated as image segmentation between the focused and defocused regions in the source images. The proposed method achieves segmentation through a multi-scale convolutional neural network, which performs a multi-scale analysis on each input image to derive the respective feature maps on the region boundaries between the focused and defocused regions. The feature maps are then inter-fused to produce a fused feature map. Afterward, the fused map is post-processed using initial segmentation, morphological operation, and watershed to obtain the segmentation map/decision map. We illustrate that the decision map gained from the multi-scale convolutional neural network is trustworthy and that it can lead to high-quality fusion results. Experimental results evidently validate that the proposed algorithm can achieve an optimum fusion performance in light of both qualitative and quantitative evaluations.","['Image fusion', 'Image segmentation', 'Neural networks', 'Convolution', 'Transforms', 'Algorithm design and analysis', 'Morphological operations']","['Convolutional neural network', 'multi-focus image', 'decision map', 'image fusion']"
"Automatic assessing the location and extent of liver and liver tumor is critical for radiologists, diagnosis and the clinical process. In recent years, a large number of variants of U-Net based on Multi-scale feature fusion are proposed to improve the segmentation performance for medical image segmentation. Unlike the previous works which extract the context information of medical image via applying the multi-scale feature fusion, we propose a novel network named Multi-scale Attention Net (MA-Net) by introducing self-attention mechanism into our method to adaptively integrate local features with their global dependencies. The MA-Net can capture rich contextual dependencies based on the attention mechanism. We design two blocks: Position-wise Attention Block (PAB) and Multi-scale Fusion Attention Block (MFAB). The PAB is used to model the feature interdependencies in spatial dimensions, which capture the spatial dependencies between pixels in a global view. In addition, the MFAB is to capture the channel dependencies between any feature map by multi-scale semantic feature fusion. We evaluate our method on the dataset of MICCAI 2017 LiTS Challenge. The proposed method achieves better performance than other state-of-the-art methods. The Dice values of liver and tumors segmentation are 0.960 ± 0.03 and 0.749 ± 0.08 respectively.","['Image segmentation', 'Liver', 'Tumors', 'Semantics', 'Biomedical imaging', 'Feature extraction', 'Two dimensional displays']","['CT', 'liver tumor segmentation', 'deep learning', 'attention mechanism', 'context information']"
"As the number of inverters increases in the power grid, the stability of grid-tied inverters becomes an important concern for the power industry. In particular, a weak grid can lead to voltage fluctuations at the inverter terminals and consequently cause inverter instability. In this paper, impacts of circuit and control parameters on the stability of voltage source inverters are studied using a small-signal state-space model in the synchronously rotating dq-frame of reference. The full-order state-space model developed in this paper is directly extracted from the pulsewidth modulation switching pattern and enables the stability analysis of concurrent variations in the three-phase circuit and control parameters. This paper demonstrates that the full-order model of a grid-tied active (P) and reactive (Q) power (PQ)-controlled voltage source inverter (VSI) can be significantly reduced to a second-order model, preserving the overall system stability in the case of grid impedance variations. This paper also shows that a decrease in the grid inductance does not necessarily improve the stability of grid-tied VSIs. The system stability is a function of both the grid R/X ratio and grid inductance. Despite the grid-side inductor of the LCL filter is in series with the grid impedance, they have different impacts on the stability of a grid-tied PQ-controlled VSI, i.e., an increase in the filter inductance may improve the system stability in a weak grid. These findings are verified through simulated and experimentally obtained data.","['Power system stability', 'Circuit stability', 'Inverters', 'Stability criteria', 'State-space methods', 'Inductance']","['Grid-tied voltage-source inverter', 'weak grids', 'microgrids', 'active (P) and reactive (Q) power (PQ)-controlled inverters', 'stability analysis', 'reduced-order model']"
"This paper presents the design and the realization of broadband circularly polarized (CP) Fabry-Perot resonant antenna using a single superstrate for the fifth-generation (5G) wireless multiple-input-multiple-output (MIMO) applications. The antenna consists of a corner cut patch with a diagonal slot and a superstrate. The individual resonances of the corner cut patch and patch with diagonal slot are overlapped to improve the intrinsic narrow impedance and axial ratio (AR) bandwidths of the single-fed patch antennas. A half-wavelength spaced superstrate having a half-wavelength thickness is employed as a partially reflecting surface (PRS) for high gain and wide AR as well as impedance bandwidths. The design procedure and mechanisms of the PRS are discussed in detail through the equivalent circuit and ray tracing analysis. Simulated and measured results show that the proposed antennas have a wide operational bandwidth of 25-33 GHz (27.6%) for |S 11 | <; -10 dB with a stable gain achieving a maximum value of 14.1 dBiC and a wide 3-dB AR bandwidth ranging from 26-31.3 GHz (17%). This operational bandwidth of the antenna covers the proposed entire global 5G millimeter wave (mmWave) spectrum (26-29.5 GHz). Moreover, a 2 × 2 MIMO antenna is designed using the proposed antenna in such a way that the polarization diversity of the adjacent radiator is exploited, resulting in high isolation between antenna elements and low-envelope correlation coefficient, which makes it a suitable candidate for future 5G MIMO applications.","['5G mobile communication', 'MIMO communication', 'Broadband antennas', 'Bandwidth', 'Cavity resonators', 'Resonant frequency']","['Fifth-generation (5G)', 'millimeter wave', 'MIMO antenna', 'Fabry-Perot resonant antenna']"
"Any implant or prosthesis replacing a function or functions of an organ or group of organs should be biologically and sensorily integrated with the human body in order to increase their acceptance with their user. If this replacement is for a human hand, which is an important interface between humans and their environment, the acceptance issue and developing sensory-motor embodiment will be more challenging. Despite progress in prosthesis technologies, 50-60% of hand amputees wear a prosthetic device. One primary reason for the rejection of the prosthetic hands is that there is no or negligibly small feedback or tactile sensation from the hand to the user, making the hands less functional. In fact, the loss of a hand means interrupting the closed-loop sensory feedback between the brain (motor control) and the hand (sensory feedback through the nerves). The lack of feedback requires significant cognitive efforts from the user in order to do basic gestures and daily activities. To this aim, recently, there has been significant development in the provision of sensory feedback from transradial prosthetic hands, to enable the user take part in the control loop and improve user embodiment. Sensory feedback to the hand users can be provided via invasive and non-invasive methods. The latter includes the use of temperature, vibration, mechanical pressure and skin stretching, electrotactile stimulation, phantom limb stimulation, audio feedback, and augmented reality. This paper provides a comprehensive review of the non-invasive methods, performs their critical evaluation, and presents challenges and opportunities associated with the non-invasive sensory feedback methods.","['Prosthetic hand', 'Vibrations', 'Skin', 'Sensors', 'Surgery', 'Implants']","['Sensory feedback', 'prosthetics', 'non-invasive', 'electrotactile stimulation', 'mechanotactile stimulation', 'vibrotactile stimulation']"
"Underwater images play a key role in ocean exploration but often suffer from severe quality degradation due to light absorption and scattering in water medium. Although major breakthroughs have been made recently in the general area of image enhancement and restoration, the applicability of new methods for improving the quality of underwater images has not specifically been captured. In this paper, we review the image enhancement and restoration methods that tackle typical underwater image impairments, including some extreme degradations and distortions. First, we introduce the key causes of quality reduction in underwater images, in terms of the underwater image formation model (IFM). Then, we review underwater restoration methods, considering both the IFM-free and the IFM-based approaches. Next, we present an experimental-based comparative evaluation of the state-of-the-art IFM-free and IFM-based methods, considering also the prior-based parameter estimation algorithms of the IFM-based methods, using both subjective and objective analyses (the used code is freely available at https://github.com/wangyanckxx/Single-Underwater-Image-Enhancement-and-Color-Restoration). Starting from this paper, we pinpoint the key shortcomings of existing methods, drawing recommendations for future research in this area. Our review of underwater image enhancement and restoration provides researchers with the necessary background to appreciate challenges and opportunities in this important field.","['Image enhancement', 'Image color analysis', 'Image restoration', 'Scattering', 'Cameras', 'Histograms']","['Underwater image formation model', 'single underwater image enhancement', 'single underwater image restoration', 'background light estimation', 'transmission map estimation']"
"The intensive research in the fifth generation (5G) technology is a clear indication of technological revolution to meet the ever-increasing demand and needs for high speed communication as well as Internet of Thing (IoT) based applications. The timely upgradation in 5G technology standards is released by third generation partnership project (3GPP) which enables the researchers to refine the research objectives and contribute towards the development. The 5G technology will be supported by not only smartphones but also different IoT devices to provide different services like smart building, smart city, and many more which will require a 5G antenna with low latency, low path loss, and stable radiation pattern. This paper provides a comprehensive study of different antenna designs considering various 5G antenna design aspects like compactness, efficiency, isolation, etc. This review paper elaborates the state-of-the-art research on the different types of antennas with their performance enhancement techniques for 5G technology in recent years. Also, this paper precisely covers 5G specifications and categorization of antennas followed by a comparative analysis of different antenna designs. Till now, many 5G antenna designs have been proposed by the different researchers, but an exhaustive review of different types of 5G antenna with their performance enhancement method is not yet done. So, in this paper, we have attempted to explore the different types of 5G antenna designs, their performance enhancement techniques, comparison, and future breakthroughs in a holistic way.","['5G mobile communication', 'Broadband antennas', 'MIMO communication', 'Quality of service', 'Internet of Things', 'Dipole antennas']","['SISO', 'MIMO', 'wideband', 'multiband', '5G communication', 'metamaterial', 'corrugations', 'dielectric lens', 'defected ground structure (DGS)', 'antipodal Vivaldi antenna (AVA)', 'multi-element antenna', 'monopole', 'dipole', 'magneto-electric(ME) dipole', 'loop', 'fractal', 'inverted F antenna (IFA)', 'planar inverted F antenna (PIFA)']"
"Recently, the popularity of the Internet of Things (IoT) has led to a rapid development and significant advancement of ubiquitous applications seamlessly integrated within our daily life. Owing to the accompanying growth of the importance of privacy, a great deal of attention has focused on the issues of secure management and robust access control of IoT devices. In this paper, we propose the design of a blockchain connected gateway which adaptively and securely maintains user privacy preferences for IoT devices in the blockchain network. Individual privacy leakage can be prevented because the gateway effectively protects users' sensitive data from being accessed without their consent. A robust digital signature mechanism is proposed for the purposes of authentication and secure management of privacy preferences. Furthermore, we adopt the blockchain network as the underlying architecture of data processing and maintenance to resolve privacy disputes.","['Logic gates', 'Privacy', 'Contracts', 'Data privacy', 'Security', 'Biomedical monitoring', 'Object recognition']","['Blockchain', 'bluetooth low energy', 'Internet of Things (IoT)', 'security', 'privacy']"
"Heart failure is considered one of the leading cause of death around the world. The diagnosis of heart failure is a challenging task especially in under-developed and developing countries where there is a paucity of human experts and equipments. Hence, different researchers have developed different intelligent systems for automated detection of heart failure. However, most of these methods are facing the problem of overfitting i.e. the recently proposed methods improved heart failure detection accuracy on testing data while compromising heart failure detection accuracy on training data. Consequently, the constructed models overfit to the testing data. In order, to come up with an intelligent system that would show good performance on both training and testing data, in this paper we develop a novel diagnostic system. The proposed diagnostic system uses random search algorithm (RSA) for features selection and random forest model for heart failure prediction. The proposed diagnostic system is optimized using grid search algorithm. Two types of experiments are performed to evaluate the precision of the proposed method. In the first experiment, only random forest model is developed while in the second experiment the proposed RSA based random forest model is developed. Experiments are performed using an online heart failure database namely Cleveland dataset. The proposed method is efficient and less complex than conventional random forest model as it produces 3.3% higher accuracy than conventional random forest model while using only 7 features. Moreover, the proposed method shows better performance than five other state of the art machine learning models. In addition, the proposed method achieved classification accuracy of 93.33% while improving the training accuracy as well. Finally, the proposed method shows better performance than eleven recently proposed methods for heart failure detection.","['Heart', 'Diseases', 'Solid modeling', 'Feature extraction', 'Prediction algorithms']","['Heart failure', 'hyperparameters optimization', 'feature selection', 'random search algorithm', 'grid search algorithm']"
"Complicated weather conditions lead to intermittent, random and volatility in photovoltaic (PV) systems, which makes PV predictions difficult. A recurrent neural network (RNN) is considered to be an effective tool for time-series data prediction. However, when the weather changes intensely, the long-term sequence of multivariate may cause gradient vanishing (exploding) during the training of RNN, leading the prediction results to local optimum. Long short-term memory (LSTM) network is the deep structure of RNN. Due to its special hidden layer unit structure, it can preserve the trend information contained in the long-term sequence, which is allowed to solve the problems of RNN and improve performance. An LSTM-based approach is applied for short-term predictions in this study based on a timescale that encompasses global horizontal irradiance (GHI) one hour in advance and one day in advance. Inaccurate forecasts usually occur on cloudy days, and the results of ANN and SVR in the literature prove this. To improve prediction accuracy on cloudy days, the clearness-index was introduced as an input data for the LSTM model and to classify the type of weather by k-means during the data processing, where cloudy days are classified as the cloudy and the mixed(partially cloudy). NN models are established to compare the accuracy of different approaches and the cross-regional study is to prove whether the method can be generalizable. From the results of hourly forecast, the R 2 coefficient of LSTM on cloudy days and mixed days is exceeding 0.9, while the R 2 of RNN is only 0.70 and 0.79 in Atlanta and Hawaii. From the results of daily forecast, All R 2 on cloudy days is about 0.85. However, the LSTM is still very effective in improving of RNN and more accurate than other models.","['Clouds', 'Forecasting', 'Weather forecasting', 'Predictive models', 'Power generation', 'Artificial neural networks']","['LSTM', 'forecasting short-term solar irradiance', 'complicated weather', 'comparative research']"
"Automatic image detection of colonic polyps is still an unsolved problem due to the large variation of polyps in terms of shape, texture, size, and color, and the existence of various polyp-like mimics during colonoscopy. In this paper, we apply a recent region-based convolutional neural network (CNN) approach for the automatic detection of polyps in the images and videos obtained from colonoscopy examinations. We use a deep-CNN model (Inception Resnet) as a transfer learning scheme in the detection system. To overcome the polyp detection obstacles and the small number of polyp images, we examine image augmentation strategies for training deep networks. We further propose two efficient post-learning methods, such as automatic false positive learning and offline learning, both of which can be incorporated with the region-based detection system for reliable polyp detection. Using the large size of colonoscopy databases, experimental results demonstrate that the suggested detection systems show better performance than other systems in the literature. Furthermore, we show improved detection performance using the proposed post-learning schemes for colonoscopy videos.","['Training', 'Colonoscopy', 'Videos', 'Detectors', 'Image color analysis', 'Proposals', 'Feature extraction']","['Colonoscopy', 'convolutional neural network', 'image augmentation', 'polyp detection', 'region proposal network', 'transfer learning']"
"In this paper, we study fingerprinting-based indoor localization in commodity 5-GHz WiFi networks. We first theoretically and experimentally validate three hypotheses on the channel state information (CSI) data of 5-GHz OFDM channels. We then propose a system termed BiLoc, which uses bi-modality deep learning for localization in the indoor environment using off-the-shelf WiFi devices. We develop a deep learning-based algorithm to exploit bi-modal data, i.e., estimated angle of arrivings and average amplitudes (which are calibrated CSI data using several proposed techniques), for both the off-line and online stages of indoor fingerprinting. The proposed BiLoc system is implemented using commodity WiFi devices. Its superior performance is validated with extensive experiments under three typical indoor environments and through comparison with three benchmark schemes.","['Antennas', 'Wireless fidelity', 'OFDM', 'Feature extraction', 'Data mining', 'Machine learning', 'Antenna measurements']","['Indoor localization', 'fingerprinting', 'deep learning', '5GHz commodity WiFi', 'channel state information', 'bi-modality fingerprinting']"
"Search and rescue (SAR) operations can take significant advantage from supporting autonomous or teleoperated robots and multi-robot systems. These can aid in mapping and situational assessment, monitoring and surveillance, establishing communication networks, or searching for victims. This paper provides a review of multi-robot systems supporting SAR operations, with system-level considerations and focusing on the algorithmic perspectives for multi-robot coordination and perception. This is, to the best of our knowledge, the first survey paper to cover (i) heterogeneous SAR robots in different environments, (ii) active perception in multi-robot systems, while (iii) giving two complementary points of view from the multi-agent perception and control perspectives. We also discuss the most significant open research questions: shared autonomy, sim-to-real transferability of existing methods, awareness of victims' conditions, coordination and interoperability in heterogeneous multi-robot systems, and active perception. The different topics in the survey are put in the context of the different challenges and constraints that various types of robots (ground, aerial, surface, or underwater) encounter in different SAR environments (maritime, urban, wilderness, or other post-disaster scenarios). The objective of this survey is to serve as an entry point to the various aspects of multi-robot SAR systems to researchers in both the machine learning and control fields by giving a global overview of the main approaches being taken in the SAR robotics area.","['Robot kinematics', 'Multi-robot systems', 'Collaboration', 'Robot sensing systems', 'Planning']","['Robotics', 'search and rescue (SAR)', 'multi-robot systems (MRS)', 'machine learning (ML)', 'deep learning (DL)', 'active perception', 'active vision', 'multi-agent perception', 'autonomous robots']"
"Intelligent compound fault diagnosis of rotating machinery plays a crucial role for the security, high-efficiency, and reliability of modern manufacture machines, but identifying and decoupling the compound fault are still a great challenge. The traditional compound fault diagnosis methods focus on either bearing or gear fault diagnosis, where the compound fault is always regarded as an independent fault pattern in the process of fault diagnosis, and the relationship between the single fault and compound fault is not considered completely. To solve such a problem, a novel method called deep decoupling convolutional neural network is proposed for intelligent compound fault diagnosis. First, one-dimensional deep convolutional neural network is employed as the feature learning model, which can effectively learn the discriminative features from raw vibration signals. Second, multi-stack capsules are designed as the decoupling classifier to accurately identify and decouple the compound fault. Finally, the routing by agreement algorithm and the margin loss cost function are utilized to train and optimize the proposed model. The proposed method is validated by gearbox fault tests, and the experimental results demonstrate that the proposed method can effectively identify and decouple the compound fault.","['Fault diagnosis', 'Compounds', 'Convolutional neural networks', 'Gears', 'Vibrations', 'Feature extraction']","['Compound fault decoupling', 'deep decoupling convolutional neural network (DDCNN)', 'intelligent fault diagnosis', 'rotating machinery', 'decoupling classifier']"
"The growing development of IoT (Internet of Things) devices creates a large attack surface for cybercriminals to conduct potentially more destructive cyberattacks; as a result, the security industry has seen an exponential increase in cyber-attacks. Many of these attacks have effectively accomplished their malicious goals because intruders conduct cyber-attacks using novel and innovative techniques. An anomaly-based IDS (Intrusion Detection System) uses machine learning techniques to detect and classify attacks in IoT networks. In the presence of unpredictable network technologies and various intrusion methods, traditional machine learning techniques appear inefficient. In many research areas, deep learning methods have shown their ability to identify anomalies accurately. Convolutional neural networks are an excellent alternative for anomaly detection and classification due to their ability to automatically categorize main characteristics in input data and their effectiveness in performing faster computations. In this paper, we design and develop a novel anomaly-based intrusion detection model for IoT networks. First, a convolutional neural network model is used to create a multiclass classification model. The proposed model is then implemented using convolutional neural networks in 1D, 2D, and 3D. The proposed convolutional neural network model is validated using the BoT-IoT, IoT Network Intrusion, MQTT-IoT-IDS2020, and IoT-23 intrusion detection datasets. Transfer learning is used to implement binary and multiclass classification using a convolutional neural network multiclass pre-trained model. Our proposed binary and multiclass classification models have achieved high accuracy, precision, recall, and F1 score compared to existing deep learning implementations.","['Internet of Things', 'Deep learning', 'Security', 'Intrusion detection', 'Convolutional neural networks', 'Computational modeling', 'Neural networks']","['Internet of Things', 'anomaly detection', 'IoT intrusion detection', 'machine learning', 'deep learning', 'transfer learning', 'network security', 'convolutional neural network']"
"In recent years, there has been a paradigm shift in Internet of Things (IoT) from centralized cloud computing to edge computing (or fog computing). Developments in ICT have resulted in the significant increment of communication and computation capabilities of embedded devices and this will continue to increase in coming years. However, existing paradigms do not utilize low-level devices for any decision-making process. In fact, gateway devices are also utilized mostly for communication interoperability and some low-level processing. In this paper, we have proposed a new computing paradigm, named Edge Mesh, which distributes the decision-making tasks among edge devices within the network instead of sending all the data to a centralized server. All the computation tasks and data are shared using a mesh network of edge devices and routers. Edge Mesh provides many benefits, including distributed processing, low latency, fault tolerance, better scalability, better security, and privacy. These benefits are useful for critical applications, which require higher reliability, real-time processing, mobility support, and context awareness. We first give an overview of existing computing paradigms to establish the motivation behind Edge Mesh. Then, we describe in detail about the Edge Mesh computing paradigm, including the proposed software framework, research challenges, and benefits of Edge Mesh. We have also described the task management framework and done a preliminary study on task allocation problem in Edge Mesh. Different application scenarios, including smart home, intelligent transportation system, and healthcare, are presented to illustrate the significance of Edge Mesh computing paradigm.","['Cloud computing', 'Edge computing', 'Servers', 'Resource management', 'Security', 'Decision making', 'Sensors']","['Edge devices', 'Internet of Things', 'distributed intelligence', 'distributed computing', 'mesh network']"
"Uncertainty quantification plays a critical role in the process of decision making and optimization in many fields of science and engineering. The field has gained an overwhelming attention among researchers in recent years resulting in an arsenal of different methods. Probabilistic forecasting and in particular prediction intervals (PIs) are one of the techniques most widely used in the literature for uncertainty quantification. Researchers have reported studies of uncertainty quantification in critical applications such as medical diagnostics, bioinformatics, renewable energies, and power grids. The purpose of this survey paper is to comprehensively study neural network-based methods for construction of prediction intervals. It will cover how PIs are constructed, optimized, and applied for decision-making in presence of uncertainties. Also, different criteria for unbiased PI evaluation are investigated. The paper also provides some guidelines for further research in the field of neural network-based uncertainty quantification.","['Uncertainty', 'Probability density function', 'Artificial neural networks', 'Probabilistic logic', 'Forecasting', 'Upper bound']","['Prediction interval', 'uncertainty quantification', 'heteroscedastic uncertainty', 'neural network', 'forecast', 'time series data', 'regression', 'probability']"
"In recent years, the haze has caused serious troubles to people's lives, with the continuous increase of PM2.5 emissions. The accurate prediction of PM2.5 is very crucial for policy makers to make predictive measures. Due to the nonlinearity of the PM2.5 time series, it is difficult to predict accurately. Despite some studies about PM2.5 being proposed, the problem of the LSTM (long short-term memory) gradient disappearance and random selection of wavelet orders and layers isn't still solved. In this study, a novel model based on WT (wavelet transform)-SAE (stacked autoencoder)-LSTM is proposed. Firstly, six study sites from China are taken as examples and WT is used to decompose PM2.5 time series into several low-and high- frequency components based on different samples. Secondly, the decomposed components are predicted based on SAE-LSTM. Finally, the predicted results are reconstructed in view of all low-and high-frequency components and the predicted results are obtained. The results imply that: (1) the forecasting performance of SAE-LSTM is better than that of other models (e.g., BP (back propagation)) used for comparison; (2) for six different PM 2.5 samples, four orders five layers, five orders six layers, five orders seven layers, three orders six layers, five orders seven layers, and five orders six layers are the most appropriate. The conclusion that such a novel model may help to enhance the accuracy of PM 2.5 prediction can be drawn.","['Predictive models', 'Forecasting', 'Wavelet transforms', 'Logic gates', 'Neural networks', 'Atmospheric modeling']","['PM 2.5 time series', 'wavelet transform', 'stacked autoencoder', 'long short-term memory', 'prediction']"
"Blockchain technology is found to have its applicability in almost every domain because of its advantages such as crypto-security, transparency, immutability, decentralized data network. In present times, a smart healthcare system with a blockchain data network and healthcare 4.0 processes provides transparency, easy and faster accessibility, security, efficiency, etc. Healthcare 4.0 trends include industry 4.0 processes such as the internet of things (IoT), industrial IoT (IIoT), cognitive computing, artificial intelligence, cloud computing, fog computing, edge computing, etc. The goal of this work is to design a smart healthcare system and it is found to be possible through integration and interoperability of Blockchain 3.0 and Healthcare 4.0 in consideration with healthcare ground-realities. Here, healthcare 4.0 processes used for data accessibility are targeted to be validated through statistical simulation-optimization methods and algorithms. The blockchain is implemented in the Ethereum network, and with associated programming languages, tools, and techniques such as solidity, web3.js, Athena, etc. Further, this work prepares a comparative and comprehensive survey of state-of-the-art blockchain-based smart healthcare systems. The comprehensive survey includes methodology, applications, requirements, outcomes, future directions, etc. A list of groups, organizations, and enterprises are prepared that are working in electronic health records (EHR), electronic medical records (EMR) or electronic personal records (EPR) mainly, and a comparative analysis is drawn concerning adopting the blockchain technology in their processes. This work has explored optimization algorithms applicable to Healthcare 4.0 trends and improves the performance of blockchain-based decentralized applications for the smart healthcare system. Further, smart contracts and their designs are prepared for the proposed system to expedite the trust-building and payment systems. This work has considered simulation and implementation to validate the proposed approach. Simulation results show that the Gas value required (indicating block size and expenditure) lies within current Etherum network Gas limits. The proposed system is active because block utilization lies above 80%. Automated smart contract execution is below 20 seconds. A good number (average 3 per simulation time) is generated in the network that indicates a health competition. Although there is error observed in simulation and implementation that lies between 0.55% and 4.24%, these errors are not affecting overall system performance because simulated and actual (taken in state-of-the-art) data variations are negligible.","['Medical services', 'Blockchain', 'Smart contracts', 'Market research', 'Industries', 'Cloud computing']","['Block', 'blockchain 3.0', 'healthcare 4.0', 'Industrial IoT (IIoT)', 'Industry 4.0', 'Internet of Things (IoT)', 'mining optimization', 'smart solution', 'transaction']"
"Recently, software defined networks (SDNs) and cloud computing have been widely adopted by researchers and industry. However, widespread acceptance of these novel networking paradigms has been hampered by the security threats. Advances in the processing technologies have helped attackers in increasing the attacks too, for instance, the development of Denial of Service (DoS) attacks to distributed DoS (DDoS) attacks which are seldom identified by conventional firewalls. In this paper, we present the state of art of the DDoS attacks in SDN and cloud computing scenarios. Especially, we focus on the analysis of SDN and cloud computing architecture. Besides, we also overview the research works and open problems in identifying and tackling the DDoS attacks.","['Cloud computing', 'Computer crime', 'Software defined networking', 'IP networks', 'Organizations', 'Floods']","['Software defined network', 'cloud computing', 'distributed denial of service attacks (DDoS)', 'DDoS attack and detection', 'experimental setup', 'survey']"
"Cyber-Physical System (CPS) is a new kind of digital technology that increases its attention across academia, government, and industry sectors and covers a wide range of applications like agriculture, energy, medical, transportation, etc. The traditional power systems with physical equipment as a core element are more integrated with information and communication technology, which evolves into the Cyber-Physical Power System (CPPS). The CPPS consists of a physical system tightly integrated with cyber systems (control, computing, and communication functions) and allows the two-way flows of electricity and information for enabling smart grid technologies. Even though the digital technologies monitoring and controlling the electric power grid more efficiently and reliably, the power grid is vulnerable to cybersecurity risk and involves the complex interdependency between cyber and physical systems. Analyzing and resolving the problems in CPPS needs the modelling methods and systematic investigation of a complex interaction between cyber and physical systems. The conventional way of modelling, simulation, and analysis involves the separation of physical domain and cyber domain, which is not suitable for the modern CPPS. Therefore, an integrated framework needed to analyze the practical scenario of the unification of physical and cyber systems. A comprehensive review of different modelling, simulation, and analysis methods and different types of cyber-attacks, cybersecurity measures for modern CPPS is explored in this paper. A review of different types of cyber-attack detection and mitigation control schemes for the practical power system is presented in this paper. The status of the research in CPPS around the world and a new path for recommendations and research directions for the researchers working in the CPPS are finally presented.","['Power system stability', 'Analytical models', 'Monitoring', 'Computer security', 'Power grids', 'Control systems']","['Cyber-physical power system (CPPS)', 'CPPS modelling', 'CPPS simulation', 'cyber-physical social system (CPSS)', 'cyber attack', 'cyber security', 'smart grid']"
"Indoor localization has received wide attention recently due to the potential use of wide range of intelligent services. This paper presents a deep learning-based approach for indoor localization by utilizing transmission channel quality metrics, including received signal strength (RSS) and channel state information (CSI). We partition a rectangular room plane into two-dimensional blocks. Each block is regarded as a class, and we formulate the localization as a classification problem. Using RSS and CSI, we develop four deep neural networks implemented with multi-layer perceptron (MLP) and one-dimensional convolutional neural network (1D-CNN) to estimate the location of a subject in a room. The experimental results indicate that the 1D-CNN using CSI information achieves excellent localization performance with much lower network complexity.","['Fingerprint recognition', 'Receivers', 'Neural networks', 'Radio transmitters', 'Feature extraction', 'Wireless fidelity', 'Complexity theory']","['Indoor localization', 'deep learning', 'convolutional neural network (CNN)', 'received signal strength (RSS)', 'channel state information (CSI)']"
"Recently, researchers found that the intended generalizability of (deep) face recognition systems increases their vulnerability against attacks. In particular, the attacks based on morphed face images pose a severe security risk to face recognition systems. In the last few years, the topic of (face) image morphing and automated morphing attack detection has sparked the interest of several research laboratories working in the field of biometrics and many different approaches have been published. In this paper, a conceptual categorization and metrics for an evaluation of such methods are presented, followed by a comprehensive survey of relevant publications. In addition, technical considerations and tradeoffs of the surveyed methods are discussed along with open issues and challenges in the field.","['Face', 'Face recognition', 'Active shape model', 'Security', 'Measurement', 'Neural networks']","['Biometrics', 'face morphing attack', 'face recognition', 'image morphing', 'morphing attack detection']"
"Heart disease, one of the major causes of mortality worldwide, can be mitigated by early heart disease diagnosis. A clinical decision support system (CDSS) can be used to diagnose the subjects' heart disease status earlier. This study proposes an effective heart disease prediction model (HDPM) for a CDSS which consists of Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to detect and eliminate the outliers, a hybrid Synthetic Minority Over-sampling Technique-Edited Nearest Neighbor (SMOTE-ENN) to balance the training data distribution and XGBoost to predict heart disease. Two publicly available datasets (Statlog and Cleveland) were used to build the model and compare the results with those of other models (naive bayes (NB), logistic regression (LR), multilayer perceptron (MLP), support vector machine (SVM), decision tree (DT), and random forest (RF)) and of previous study results. The results revealed that the proposed model outperformed other models and previous study results by achieving accuracies of 95.90% and 98.40% for Statlog and Cleveland datasets, respectively. In addition, we designed and developed the prototype of the Heart Disease CDSS (HDCDSS) to help doctors/clinicians diagnose the patients'/subjects' heart disease status based on their current condition. Therefore, early treatment could be conducted to prevent the deaths caused by late heart disease diagnosis.","['Heart', 'Diseases', 'Predictive models', 'Support vector machines', 'Data models', 'Radio frequency', 'Machine learning']","['Heart disease', 'disease prediction model', 'clinical decision support system', 'outlier data', 'imbalanced data', 'machine learning']"
"Computer-aided detection, localization, and segmentation methods can help improve colonoscopy procedures. Even though many methods have been built to tackle automatic detection and segmentation of polyps, benchmarking of state-of-the-art methods still remains an open problem. This is due to the increasing number of researched computer vision methods that can be applied to polyp datasets. Benchmarking of novel methods can provide a direction to the development of automated polyp detection and segmentation tasks. Furthermore, it ensures that the produced results in the community are reproducible and provide a fair comparison of developed methods. In this paper, we benchmark several recent state-of-the-art methods using Kvasir-SEG, an open-access dataset of colonoscopy images for polyp detection, localization, and segmentation evaluating both method accuracy and speed. Whilst, most methods in literature have competitive performance over accuracy, we show that the proposed ColonSegNet achieved a better trade-off between an average precision of 0.8000 and mean IoU of 0.8100, and the fastest speed of 180 frames per second for the detection and localization task. Likewise, the proposed ColonSegNet achieved a competitive dice coefficient of 0.8206 and the best average speed of 182.38 frames per second for the segmentation task. Our comprehensive comparison with various state-of-the-art methods reveals the importance of benchmarking the deep learning methods for automated real-time polyp identification and delineations that can potentially transform current clinical practices and minimise miss-detection rates.","['Colonoscopy', 'Image segmentation', 'Benchmark testing', 'Real-time systems', 'Cancer', 'Videos', 'Biomedical imaging']","['Medical image segmentation', 'ColonSegNet', 'colonoscopy', 'polyps', 'deep learning', 'detection', 'localization', 'benchmarking', 'Kvasir-SEG']"
"The explosive growth of massive data generation from Internet of Things in industrial, agricultural and scientific communities has led to a rapid increase for data analytics in cloud data centers. The ubiquitous and pervasive demand for near-data processing urges the edge computing paradigm in recent years. Edge computing is promising for less network backbone bandwidth usage and thus less data center side processing pressure, as well as enhanced service responsiveness and data privacy protection. Computation offloading plays a crucial role in edge computing in terms of network packets transmission and system responsiveness through dynamic task partitioning between cloud data centers and edge servers and edge devices. In this paper a thorough literature review is conducted to reveal the state-of-the-art of computation offloading in edge computing. Various aspects of computation offloading, including energy consumption minimization, Quality of Services guarantee, and Quality of Experiences enhancement are surveyed. Moreover, resource scheduling approaches, gaming and tradeoffing among system performance and overheads for computation offloading decision making are also reviewed.","['Cloud computing', 'Edge computing', 'Computational modeling', 'Data centers', 'Internet of Things', 'Task analysis', 'Computer architecture']","['Edge computing', 'computation offloading', 'task partitioning', 'game theory', 'edge-cloud collaboration']"
"The essence of blockchain smart contracts lies in the execution of business logic code in a decentralized architecture in which the execution outcomes are trusted and agreed upon by all the executing nodes. Despite the decentralized and trustless architectures of the blockchain systems, smart contracts on their own cannot access data from the external world. Instead, smart contracts interact with off-chain external data sources, called oracles, whose primary job is to collect and provide data feeds and input to smart contracts. However, there is always risk of oracles providing corrupt, malicious, or inaccurate data. In this paper, we analyze and present the notion of trust in the oracles used in blockchain ecosystems. We analyze and compare trust-enabling features of the leading blockchain oracle approaches, techniques, and platforms. Moreover, we discuss open research challenges that should be addressed to ensure secure and trustworthy blockchain oracles.","['Blockchain', 'Smart contracts', 'Peer-to-peer computing', 'Feeds', 'Data models', 'Reliability', 'Ecosystems']","['Blockchain', 'data attestation', 'decentralization', 'oracles', 'smart contract', 'trust']"
"Fog computing (FC) is an emerging distributed computing platform aimed at bringing computation close to its data sources, which can reduce the latency and cost of delivering data to a remote cloud. This feature and related advantages are desirable for many Internet-of-Things applications, especially latency sensitive and mission intensive services. With comparisons to other computing technologies, the definition and architecture of FC are presented in this paper. The framework of resource allocation for latency reduction combined with reliability, fault tolerance, privacy, and underlying optimization problems are also discussed. We then investigate an application scenario and conduct resource optimization by formulating the optimization problem and solving it via a genetic algorithm. The resulting analysis generates some important insights on the scalability of the FC systems.","['Cloud computing', 'Computer architecture', 'Edge computing', 'Optimization', 'Logic gates', 'Big Data']","['Fog computing', 'genetic algorithms', 'Internet of Things', 'optimization']"
"Blockchain, a form of distributed ledger technology has attracted the interests of stakeholders across several sectors including healthcare. Its' potential in the multi-stakeholder operated sector like health has been responsible for several investments, studies, and implementations. Electronic Health Records (EHR) systems traditionally used for the exchange of health information amongst healthcare stakeholders have been criticised for centralising power, failures and attack-points with exchange data custodians. EHRs have struggled in the face of multi-stakeholder and system requirements while adhering to security, privacy, ethical and other regulatory constraints. Blockchain is promising amongst others to address the many EHR challenges, primarily trustless and secure exchange of health information amongst stakeholders. Many blockchain-in-healthcare frameworks have been proposed; some prototyped and/or implemented. This study leveraged the PRISMA framework to systematically search and evaluate the different models proposed; prototyped and/or implemented. The bibliometric and functional distribution of all 143 articles from this study were presented. This study evaluated 61 articles that discussed either prototypes or pilot or implementations. The technical and architectural analysis of these 61 articles for privacy, security, cost, and performance were detailed. Blockchain was found to solve the trust, security and privacy constraints of traditional EHRs often at significant performance, storage and cost trade-offs.","['Blockchain', 'Medical services', 'Systematics', 'Market research', 'Bitcoin']","['Bioinformatics', 'blockchain', 'DLT', 'distributed ledger technology', 'distributed computing', 'distributed databases', 'health information management', 'health information exchange', 'hospitals', 'pharmaceutical technology', 'telemedicine', 'digital health', 'eHealth', 'mHealth']"
"Demand side management (DSM) will play a significant role in the future smart grid by managing loads in a smart way. DSM programs, realized via home energy management systems for smart cities, provide many benefits; consumers enjoy electricity price savings and utility operates at reduced peak demand. In this paper, evolutionary algorithms-based (binary particle swarm optimization, genetic algorithm, and cuckoo search) DSM model for scheduling the appliances of residential users is presented. The model is simulated in time of use pricing environment for three cases: 1) traditional homes; 2) smart homes; and 3) smart homes with renewable energy sources. Simulation results show that the proposed model optimally schedules the appliances resulting in electricity bill and peaks reductions.","['Home appliances', 'Pricing', 'Scheduling', 'Renewable energy sources', 'Smart grids', 'Schedules']","['Appliance scheduling', 'binary particle swarm optimization', 'genetic algorithm', 'cuckoo search algorithm', 'energy management system', 'electricity pricing', 'smart grid']"
"This paper presents a metasurface-based single-layer low-profile circularly polarized (CP) antenna with the wideband operation and its multiple-input multiple-output (MIMO) configuration for fifth-generation (5G) communication systems. The antenna consists of a truncated corner patch and a metasurface (MS) of a 2 × 2 periodic square metallic plates. The distinguishing feature of this design is that all the radiating elements (radiator and MS) are printed on the single-layer of the dielectric substrate, which ensures the low-profile and low-cost features of the antenna while maintaining high gain and wideband characteristics. The wideband CP radiations are realized by exploiting surface-waves along the MS and its radiation mechanism is explained in detail. The single-layer antenna geometry has an overall compact size of 1.0λ 0 × 1.0λ 0 × 0.04λ 0 . Simulated and measured results show that the single-layer metasurface antenna has a wide 10 dB impedance bandwidth of 23.4 % (24.5 - 31 GHz) (23.4 %) and overlapping 3-dB axial ratio bandwidth of 16.8 % (25 - 29.6 GHz). The antenna also offers stable radiation patterns with a high radiation efficiency (>95%) and a flat gain of 11 dBic. Moreover, a 4-port (2 × 2) MIMO antenna is designed using the proposed design by placing each element perpendicular to each other. Without a dedicated decoupling structure, the MIMO antenna shows an excellent diversity performance in terms of isolation between antenna elements, envelope correlation coefficient, and channel capacity loss. Most importantly, the operational bandwidth of the antenna covers the millimeter-wave (mm-wave) band (25 - 29.5 GHz) assigned for 5G communication. These features of the proposed antenna system make it a suitable candidate for 5G smart devices and sensors.","['5G mobile communication', 'MIMO communication', 'Wideband', 'Broadband antennas', 'Millimeter wave communication']","['28 GHz', 'metasurface antenna', 'fifth-generation (5G)', 'millimeter-wave systems', 'MIMO', 'circular polarization']"
"The micro air vehicle link (MAVLink in short) is a communication protocol for unmanned systems (e.g., drones and robots). It specifies a comprehensive set of messages exchanged between unmanned systems and ground stations. This protocol is used in major autopilot systems, mainly ArduPilot and PX4, and provides powerful features not only for monitoring and controlling unmanned systems missions but also for their integration into the Internet. However, there is no technical survey and/or tutorial in the literature that presents these features or explains how to make use of them. Most of the references are online tutorials and basic technical reports, and none of them presents comprehensive and systematic coverage of the protocol. In this paper, we address this gap, and we propose an overview of the MAVLink protocol, the difference between its versions, and it is potential in enabling Internet connectivity to unmanned systems. We also discuss the security aspects of the MAVLink. To the best of our knowledge, this is the first technical survey and tutorial on the MAVLink protocol, which represents an important reference for unmanned systems users and developers.","['Protocols', 'Tutorials', 'Drones', 'Reliability', 'Payloads', 'Robots']","['MAVLink', 'ArduPilot', 'PX4', 'Unmanned Aerial Vehicles (UAVs)', 'Ground Control Stations (GCSs)']"
"Recent advancements in human-computer interaction research have led to the possibility of emotional communication via brain-computer interface systems for patients with neuropsychiatric disorders or disabilities. In this paper, we efficiently recognize emotional states by analyzing the features of electroencephalography (EEG) signals, which are generated from EEG sensors that noninvasively measure the electrical activity of neurons inside the human brain, and select the optimal combination of these features for recognition. In this paper, the scalp EEG data of 21 healthy subjects (12-14 years old) were recorded using a 14-channel EEG machine while the subjects watched images with four types of emotional stimuli (happy, calm, sad, or scared). After preprocessing, the Hjorth parameters (activity, mobility, and complexity) were used to measure the signal activity of the time series data. We selected the optimal EEG features using a balanced one-way ANOVA after calculating the Hjorth parameters for different frequency ranges. Features selected by this statistical method outperformed univariate and multivariate features. The optimal features were further processed for emotion classification using support vector machine, k-nearest neighbor, linear discriminant analysis, Naive Bayes, random forest, deep learning, and four ensembles methods (bagging, boosting, stacking, and voting). The results show that the proposed method substantially improves the emotion recognition rate with respect to the commonly used spectral power band method.","['Electroencephalography', 'Feature extraction', 'Emotion recognition', 'Electrodes', 'Sensors', 'Support vector machines', 'Medical services']","['EEG pattern recognition', 'Hjorth parameter', 'EEG feature extraction', 'EEG emotion recognition']"
"Due to a battery constraint in wireless sensor networks (WSNs), prolonging their lifetime is important. Energy-efficient routing techniques for WSNs play a great role in doing so. In this paper, we articulate this problem and classify current routing protocols for WSNs into two categories according to their orientation toward either homogeneous or heterogeneous WSNs. They are further classified into static and mobile ones. We give an overview of these protocols in each category by summarizing their characteristics, limitations, and applications. Finally, some open issues in energy-efficient routing protocol design for WSNs are indicated.","['Wireless sensor networks', 'Energy efficiency', 'Routing protocols', 'Mobile communication', 'Batteries', 'Design methodology']","['Wireless sensor networks (WSNs)', 'energy-efficient routing protocol', 'internet of things']"
"A vision of the future Internet is introduced in such a fashion that various computing devices are connected together to form a network called Internet of Things (IoT). This network will generate massive data that may be leveraged for entertainment, security, and most importantly user trust. Yet, trust is an imperative obstruction that may hinder the IoT growth and even delay the substantial squeeze of a number of applications. In this survey, an extensive analysis of trust management techniques along with their pros and cons is presented in a different context. In comparison with other surveys, the goal is to provide a systematic description of the most relevant trust management techniques to help researchers understand that how various systems fit together to bring preferred functionalities without examining different standards. Besides, the lessons learned are presented, and the views are argued regarding the primary goal trust which is likely to play in the future Internet.","['Servers', 'Internet of Things', 'Protocols', 'Authentication']","['Internet of Things', 'trust management techniques', 'trust contributions', 'trust limitations']"
"The ever increasing trend of renewable energy sources (RES) into the power system has increased the uncertainty in the operation and control of power system. The vulnerability of RES towards the unforeseeable variation of meteorological conditions demands additional resources to support. In such instance, energy storage systems (ESS) are inevitable as they are one among the various resources to support RES penetration. However, ESS has limited ability to fulfil all the requirements of a certain application. So, hybridization of multiple ESS to form a composite ESS is a potential solution. While integrating these different ESS, their power sharing control plays a crucial role to exploit the complementary characteristics of each other. Therefore, this article attempts to bring the numerous control strategies proposed in the literature at one place. Various control techniques implemented for HESS are critically reviewed and the notable observations are tabulated for better insights. Furthermore, the control techniques are classified into broad categories and they are briefly discussed with their limitations. From the carried-out analysis, the challenges faced towards the implementation of HESS for standalone and grid connected microgrid systems are presented. Finally, the future directions are laid out for the researchers to carry out the research and implementation of HESS technologies. Overall, this article would serve as a thorough guide on various control techniques implemented for HESS including their features, limitations and real-time applications.","['Batteries', 'Topology', 'Hybrid power systems', 'Renewable energy sources', 'Microgrids']","['Hybrid energy storage system', 'microgrid', 'intelligent control', 'renewable energy', 'energy management', 'power electronics']"
"The IoT has applications in many areas such as manufacturing, healthcare, and agriculture, to name a few. Recently, wearable devices have become popular with wide applications in the health monitoring system which has stimulated the growth of the Internet of Medical Things (IoMT). The IoMT has an important role to play in reducing the mortality rate by the early detection of disease. The prediction of heart disease is a key issue in the analysis of clinical dataset. The aim of the proposed investigation is to identify the key characteristics of heart disease prediction using machine learning techniques. Many studies have focused on heart disease diagnosis, but the accuracy of the findings is low. Therefore, to improve prediction accuracy, an IoMT framework for the diagnosis of heart disease using modified salp swarm optimization (MSSO) and an adaptive neuro-fuzzy inference system (ANFIS) is proposed. The proposed MSSO-ANFIS improves the search capability using the Levy flight algorithm. The regular learning process in ANFIS is dependent on gradient-based learning and has a tendency to become trapped in local minima. The learning parameters are optimized utilizing MSSO to provide better results for ANFIS. The following information is taken from medical records to predict the risk of heart disease: blood pressure (BP), age, sex, chest pain, cholesterol, blood sugar, etc. The heart condition is identified by classifying the received sensor data using MSSO-ANFIS. A simulation and analysis is conducted to show that MSSA-ANFIS works well in relation to disease prediction. The results of the simulation demonstrate that the MSSO-ANFIS prediction model achieves better accuracy than the other approaches. The proposed MSSO-ANFIS prediction model obtains an accuracy of 99.45 with a precision of 96.54, which is higher than the other approaches.","['Heart', 'Diseases', 'Biomedical monitoring', 'Predictive models', 'Monitoring', 'Medical diagnostic imaging']","['Internet of Things', 'heart disease', 'LCSA', 'ANFIS', 'MSSO', 'Internet of Medical Things']"
"In the digital healthcare era, it is of the utmost importance to harness medical information scattered across healthcare institutions to support in-depth data analysis and achieve personalized healthcare. However, the cyberinfrastructure boundaries of healthcare organizations and privacy leakage threats place obstacles on the sharing of medical records. Blockchain, as a public ledger characterized by its transparency, tamper-evidence, trustlessness, and decentralization, can help build a secure medical data exchange network. This paper surveys the state-of-the-art schemes on secure and privacy-preserving medical data sharing of the past decade with a focus on blockchain-based approaches. We classify them into permissionless blockchain-based approaches and permissioned blockchain-based approaches and analyze their advantages and disadvantages. We also discuss potential research topics on blockchain-based medical data sharing.","['Medical services', 'Cloud computing', 'Blockchain', 'Biomedical imaging', 'Data privacy', 'Cryptography']","['Access control', 'blockchain', 'encryption', 'medical data', 'privacy', 'security']"
"Due to the sheer global energy crisis, concerns about fuel exhaustion, electricity shortages, and global warming are becoming increasingly severe. Solar and wind energy, which are clean and renewable, provide solutions to these problems through distributed generators. Microgrids, as an essential interface to connect the power produced by renewable energy resources-based distributed generators to the power system, have become a research hotspot. Modern research in the field of microgrids has focused on the integration of microgrid technology at the load level. Due to the complexity of protection and control of multiple interconnected distributed generators, the traditional power grids are now outmoded. Microgrids are feasible alternatives to the conventional grid since they provide an integrating platform for micro-resources-based distributed generators, storage equipment, loads, and voltage source converters at the user end, all within a compact footprint. A microgrid can be architected to function either in grid-connected or standalone mode, depending upon the generation, integration potential to the main grid, and consumers’ requirements. The amalgamation of distributed energy resources-based microgrids to the conventional power system is giving rise to a new power framework. Nevertheless, the grids’ control, protection, operational stability, and reliability are major concerns. There has yet to be an effective real-time implementation and commercialization of micro-grids. This review article summarizes various concerns associated with microgrids’ technical and economic aspects and challenges, power flow controllers, microgrids’ role in smart grid development, main flaws, and future perspectives.","['Microgrids', 'Power system stability', 'Power harmonic filters', 'Harmonic analysis', 'Energy storage', 'Renewable energy sources', 'Power quality', 'Global warming']","['Distributed energy resources (DERs)', 'distributed generation (DG)', 'electrical energy storage devices (EESDs)', 'frequency control', 'micro-resources', 'microgrids (MGs)', 'microgrid control', 'power quality', 'power system stability', 'PQ~droop', 'renewable energy resources (RERs)', 'smart grid (SG)']"
"A smart contract is an agreement between two or more parties, which is executed by the computer code. The code does the execution without giving either party the ability to back out, so it ensures the trustless execution. The smart contract is one of the most important features in blockchain applications, which implements trusted transactions without third parties. However, with the rapid development, blockchain smart contracts have also exposed many security problems, and some attacks caused by contract vulnerabilities have led to terrible losses. In order to better deal with such dilemma, making a comprehensive survey about the security verification of blockchain smart contracts from major scientific databases is quite indispensable. Even though the significance of studying security verification of blockchain smart contracts is evident, it is really fresh yet. The major contributions of our survey work come from three aspects. First, after retrieving all-sided research studies, we select 53 most related papers to show the state-of-the art of this topic, where 20 papers focus on dealing with security assurance of blockchain smart contracts, and 33 papers focus on the correctness verification of blockchain smart contracts. Second, we propose a taxonomy toward the topic of security verification of blockchain smart contracts and discuss the pros and cons of each category of related studies. Third, through in-depth analysis of these studies, we come to know that the correctness verification of smart contracts based on the formal method has already become the more significant and more effective method to validate whether a smart contract is credible and accurate. So, we further present representative studies of formal verification of smart contracts in detail to demonstrate that using a formal method to validate blockchain smart contracts must have a promising and meritorious future.","['Smart contracts', 'Blockchain', 'Security', 'Law', 'Reliability', 'Taxonomy']","['Blockchain', 'formal method', 'security verification', 'smart contract', 'survey']"
"This paper presents a comparison of the expected lifetime for Internet of Things (IoT) devices operating in several wireless networks: the IEEE 802.15.4/e, Bluetooth low energy (BLE), the IEEE 802.11 power saving mode, the IEEE 802.11ah, and in new emerging long-range technologies, such as LoRa and SIGFOX. To compare all technologies on an equal basis, we have developed an analyzer that computes the energy consumption for a given protocol based on the power required in a given state (Sleep, Idle, Tx, and Rx) and the duration of each state. We consider the case of an energy constrained node that uploads data to a sink, analyzing the physical (PHY) layer under medium access control (MAC) constraints, and assuming IPv6 traffic whenever possible. This paper considers the energy spent in retransmissions due to corrupted frames and collisions as well as the impact of imperfect clocks. The comparison shows that the BLE offers the best lifetime for all traffic intensities in its capacity range. LoRa achieves long lifetimes behind 802.15.4 and BLE for ultra low traffic intensity; SIGFOX only matches LoRa for very small data sizes. Moreover, considering the energy consumption due to retransmissions of lost data packets only decreases the lifetimes without changing their relative ranking. We believe that these comparisons will give all users of IoT technologies indications about the technology that best fits their needs from the energy consumption point of view. Our analyzer will also help IoT network designers to select the right MAC parameters to optimize the energy consumption for a given application.","['Energy consumption', 'IEEE 802.15 Standard', 'Wireless networks', 'Hardware', 'Internet of Things', 'Media Access Protocol']","['Internet of Things (IoT)', 'wireless sensor networks', '6LoWPAN', '802.15.4e', 'TSCH', '802.11ah', 'Bluetooth low energy', 'LoRa', 'SIGFOX', 'energy consumption model', 'clock drift']"
"While customers rivet their eyes on Wi-Fi 6, in the bowels of the IEEE 802.11 Working Group that creates Wi-Fi standards, the next generation Wi-Fi is being developed. At the very first sight, the new IEEE 802.11be amendment to the Wi-Fi standard is nothing but scaled 11ax with doubled bandwidth and the increased number of spatial streams, which together provide data rates as high as 40 Gbps. A bit deeper dive into the 802.11 activities reveals that 11be will support real-time applications. In reality, 11be introduces many more revolutionary changes to Wi-Fi, which will form a basement for further Wi-Fi evolution. Although by now (May 2020), the development process is at the very early phase without any draft specification, the analysis of the discussion in the 802.11 Working Group gives insights into the main innovations of 11be. In addition to the ones above, they include native multi-link operation, channel sounding optimization that opens the door for massive MIMO, advanced PHY and MAC techniques, the cooperation of various access points. The paper analyzes hundreds of features proposed for the new technology, focusing on the open problems that can be solved by the researchers who want to contribute to the development of 802.11be.","['Wireless fidelity', 'IEEE 802.11 Standard', 'MIMO communication', 'Throughput', 'Media Access Protocol', 'OFDM']","['802.11be', 'extremely high throughput', '4096 QAM', '320 MHz', 'MU-MIMO', 'time-sensitive networking', 'multi-link operation', 'implicit sounding', 'distributed MU-MIMO']"
"Electricity is of great significance for national economic, social, and technological activities, such as material production, healthcare, and education. The nationwide electricity demand has grown rapidly over the past few decades. Therefore, efficient electricity demand estimation and management are required for better strategies planning, energy utilization, waste management, improving revenue, and maintenance of power systems. In this paper, we propose an empirical mode decomposition (EMD)-based deep learning approach which combines the EMD method with the long short-term memory network model to estimate electricity demand for the given season, day, and time interval of a day. For this purpose, the EMD algorithm decomposes a load time series signal into several intrinsic mode functions (IMFs) and residual. Then, a LSTM model is trained separately for each of the extracted IMFs and residual. Finally, the prediction results of all IMFs are combined by summation to determine an aggregated output for electricity demand. To demonstrate the applicability of the proposed approach, it is applied to electricity consumption data of city Chandigarh. Furthermore, the performance of the proposed approach is evaluated by comparing the prediction results with recurrent neural network (RNN), LSTM, and EMD-based RNN (EMD+RNN) models.","['Predictive models', 'Demand forecasting', 'Time series analysis', 'Machine learning', 'Recurrent neural networks', 'Load modeling', 'Support vector machines']","['Deep learning', 'electricity demand prediction', 'empirical mode decomposition', 'energy analytic', 'long short term memory network']"
"Condition monitoring and incipient fault diagnosis of rolling bearing is of great importance to detect failures and ensure reliable operations in rotating machinery. In this paper, a new multi-speed fault diagnostic approach is presented by using self-adaptive wavelet transform components generated from bearing vibration signals. The proposed approach is capable of discriminating signatures from four conditions of rolling bearing, i.e., normal bearing and three different types of defected bearings on outer race, inner race, and roller separately. Particle swarm optimization and Broyden-Fletche-Goldfarb-Shanno-based quasi-Newton minimization algorithms are applied to seek optimal parameters of Impulse Modeling-based continuous wavelet transform model. Then, a 3-D feature space of the statistical parameters and a nearest neighbor classifier are, respectively, applied for fault signature extraction and fault classification. Effectiveness of this approach is then evaluated, and the results have achieved an overall accuracy of 100%. Moreover, the generated discriminatory fault signatures are suitable for multi-speed fault data sets. This technique will be further implemented and tested in a real industrial environment.","['Rolling bearings', 'Vibrations', 'Continuous wavelet transforms', 'Optimization', 'Wavelet analysis']","['Fault diagnosis', 'vibration measurement', 'continuous wavelet transforms', 'roller bearing', 'particle swarm optimization', 'quasi-newton minimization', 'fault signatures']"
"This paper reviews the modeling of high-temperature superconductors (HTS) using the finiteelement method (FEM) based on the H-formulation of Maxwell's equations. This formulation has become the most popular numerical modeling method for simulating the electromagnetic behavior of HTS, especially thanks to the easiness of implementation in the commercial finite-element program COMSOL Multiphysics. Numerous studies prove that the H-formulation is able to simulate a wide scope of HTS topologies, from simple geometries such as HTS tapes and coils, to more complex HTS devices, up to large superconducting magnets. In this paper, we review the basics of the H-formulation, its evolution from 2D to 3D, its application for calculating critical currents and AC losses as well as magnetization of HTS bulks and tape stacks. We also review the use of the H-formulation for large-scale HTS applications, its use to solve multi-physics problems involving electromagnetic-thermal and electromagnetic-mechanical couplings, and its application to study the dynamic resistance of superconductors and flux pumps.","['High-temperature superconductors', 'Finite element analysis', 'Mathematical model', 'Electromagnetics', 'Superconducting magnets', 'Three-dimensional displays']","['Review', 'H -formulation', 'high temperature superconductor (HTS)', 'finite-element method (FEM)']"
"Road pavement cracks detection has been a hot research topic for quite a long time due to the practical importance of crack detection for road maintenance and traffic safety. Many methods have been proposed to solve this problem. This paper reviews the three major types of methods used in road cracks detection: image processing, machine learning and 3D imaging based methods. Image processing algorithms mainly include threshold segmentation, edge detection and region growing methods, which are used to process images and identify crack features. Crack detection based traditional machine learning methods such as neural network and support vector machine still relies on hand-crafted features using image processing techniques. Deep learning methods have fundamentally changed the way of crack detection and greatly improved the detection performance. In this work, we review and compare the deep learning neural networks proposed in crack detection in three ways, classification based, object detection based and segmentation based. We also cover the performance evaluation metrics and the performance of these methods on commonly-used benchmark datasets. With the maturity of 3D technology, crack detection using 3D data is a new line of research and application. We compare the three types of 3D data representations and study the corresponding performance of the deep neural networks for 3D object detection. Traditional and deep learning based crack detection methods using 3D data are also reviewed in detail.","['Roads', 'Image segmentation', 'Image edge detection', 'Three-dimensional displays', 'Deep learning']","['Crack detection', 'image processing', 'deep learning', '3D imaging']"
"Nowadays billions of smart devices or things are present in Internet of Things (IoT) environments, such as homes, hospitals, factories, and vehicles, all around the world. As a result, the number of interconnected devices is continuously and rapidly growing. These devices communicate with each other and with other services using various communication protocols for the transportation of sensor or event data. These protocols enable applications to collect, store, process, describe, and analyze data to solve a variety of problems. IoT also aims to provide secure, bi-directional communication between interconnected devices, such as sensors, actuators, microcontrollers or smart appliances, and corresponding cloud services. In this paper we analyze the growth of M2M protocol research (MQTT, AMQP, and CoAP) over the past 20 years, and show how the growth in MQTT research stands out from the rest. We also gather relevant application areas of MQTT, as the most widespread M2M/IoT protocol, by performing a detailed literature search in major digital research archives. Our quantitative evaluation presents some of the important MQTT-related studies published in the past five years, which we compare to discuss the main features, advantages, and limitations of the MQTT protocol. We also propose a taxonomy to compare the properties and features of various MQTT implementations, i.e. brokers and libraries currently available in the public domain to help researchers and end-users to efficiently choose a broker or client library based on their requirements. Finally, we discuss the relevant findings of our comparison and highlight open issues that need further research and attention.","['Protocols', 'Internet of Things', 'Machine-to-machine communications', 'Reliability', 'Quality of service', 'Cloud computing', 'ISO Standards']","['IoT', 'IoT protocols', 'MQTT', 'MQTT brokers', 'survey']"
"Deep neural networks (DNNs) have shown prominent performance in the field of object detection. However, DNNs usually run on powerful devices with high computational ability and sufficient memory, which have greatly limited their deployment for constrained environments such as embedded devices. YOLO is one of the state-of-the-art DNN-based object detection approaches with good performance both on speed and accuracy and Tiny-YOLO-V3 is its latest variant with a small model that can run on embedded devices. In this paper, Tinier-YOLO, which is originated from Tiny-YOLO-V3, is proposed to further shrink the model size while achieving improved detection accuracy and real-time performance. In Tinier-YOLO, the fire module in SqueezeNet is appointed by investigating the number of fire modules as well as their positions in the model in order to reduce the number of model parameters and then reduce the model size. For further improving the proposed Tinier-YOLO in terms of detection accuracy and real-time performance, the connectivity style between fire modules in Tinier-YOLO differs from SqueezeNet in that dense connection is introduced and fine designed to strengthen the feature propagation and ensure the maximum information flow in the network. The object detection performance is enhanced in Tinier-YOLO by using the passthrough layer that merges feature maps from the front layers to get fine-grained features, which can counter the negative effect of reducing the model size. The resulting Tinier-YOLO yields a model size of 8.9MB (almost 4× smaller than Tiny-YOLO-V3) while achieving 25 FPS real-time performance on Jetson TX1 and an mAP of 65.7% on PASCAL VOC and 34.0% on COCO. Tinier-YOLO alse posses comparable results in mAP and faster runtime speed with smaller model size and BFLOP/s value compared with other lightweight models like SqueezeNet SSD and MobileNet SSD.","['Object detection', 'Real-time systems', 'Feature extraction', 'Detectors', 'Convolution', 'Performance evaluation', 'Computational modeling']","['Constrained environments', 'dense connection', 'fire modules', 'passthrough layer', 'YOLO']"
"Many countries are challenged by the medical resources required for COVID-19 detection which necessitates the development of a low-cost, rapid tool to detect and diagnose the virus effectively for a large numbers of tests. Although a chest X-Ray scan is a useful candidate tool the images generated by the scans must be analyzed accurately and quickly if large numbers of tests are to be processed. COVID-19 causes bilateral pulmonary parenchymal ground-glass and consolidative pulmonary opacities, sometimes with a rounded morphology and a peripheral lung distribution. In this work, we aim to extract rapidly from chest X-Ray images the similar small regions that may contain the identifying features of COVID-19. This paper therefore proposes a hybrid COVID-19 detection model based on an improved marine predators algorithm (IMPA) for X-Ray image segmentation. The ranking-based diversity reduction (RDR) strategy is used to enhance the performance of the IMPA to reach better solutions in fewer iterations. RDR works on finding the particles that couldn't find better solutions within a consecutive number of iterations, and then moving those particles towards the best solutions so far. The performance of IMPA has been validated on nine chest X-Ray images with threshold levels between 10 and 100 and compared with five state-of-art algorithms: equilibrium optimizer (EO), whale optimization algorithm (WOA), sine cosine algorithm (SCA), Harris-hawks algorithm (HHA), and salp swarm algorithms (SSA). The experimental results demonstrate that the proposed hybrid model outperforms all other algorithms for a range of metrics. In addition, the performance of our proposed model was convergent on all numbers of thresholds level in the Structured Similarity Index Metric (SSIM) and Universal Quality Index (UQI) metrics.","['Image segmentation', 'COVID-19', 'Entropy', 'Clustering algorithms', 'X-ray imaging', 'Feature extraction', 'Measurement']","['COVID-19 detection', 'marine predators algorithm', 'ranking-based reduction diversity', 'Kapur’s entropy', 'image segmentation']"
"This paper proposes a new chaotic image encryption scheme, which employs Josephus traversing and mixed chaotic map. The scheme consists of three processes: key stream generation process; three-round scrambling process; and one-round diffusion process. The proposed mathematical model is applied for the key stream generator in the first process. The initial values and parameters are sensitive to both the secret keys in the new scheme and plain images. The second process employs the Josephus traversing in scrambling; then the rows and columns of pixels are exchanged. The third process can modify the pixel gray-level values and crack the strong correlations between adjacent pixels simultaneously. The initial conditions for chaotic systems are derived using external secret keys by applying some algebraic transformations to the key. Security analysis indicates that the new scheme is effective, which can resist common attacks.","['Encryption', 'Chaos', 'Logistics', 'Mathematical model', 'Diffusion processes']","['Gray-scale', 'image analysis', 'cryptography', 'encryption', 'diffusion processes', 'chaos']"
"Currently, Internet of Things (IoT) and blockchain technologies are experiencing exponential growth in academia and industry. Generally, IoT is a centralized system whose security and performance mainly rely on centralized servers. Therefore, users have to trust the centralized servers; in addition, it is difficult to coordinate external computing resources to improve the performance of IoT. Fortunately, the blockchain may provide this decentralization, high credibility and high security. Consequently, blockchain-based IoT may become a reasonable choice for the design of a decentralized IoT system. In this paper, we propose a novel blockchain-based threshold IoT service system: BeeKeeper. In the BeeKeeper system, servers can process a user's data by performing homomorphic computations on the data without learning anything from them. Furthermore, any node can become a leader's server if the node and the leader desire so. In this way, BeeKeeper's performance can continually increase by attracting external computing resources to join in it. Moreover, malicious nodes can be scrutinized. In addition, BeeKeeper is fault tolerant since a user's BeeKeeper protocol may work smoothly as long as a threshold number of its servers are active and honest. Finally, we deploy BeeKeeper on the Ethereum blockchain and give the corresponding performance evaluation. In our experiments, servers can generate their response with about 107 ms. Moreover, the performance of BeeKeeper mainly depends on the blockchain platform. For instance, the response time is about 22.5 s since the block interval of Ethereum blockchain is about 15 s. In fact, if we use some other blockchain with short block interval, the response time may be obviously short.","['Servers', 'Cryptography', 'Performance evaluation', 'Contracts', 'Protocols', 'Internet of Things']","['IoT', 'blockchain', 'secret sharing', 'secure multi-party computing']"
"Automated modulation classification plays a very important part in cognitive radio networks. Deep learning is also a powerful tool that we could not overlook its potential in addressing signal modulation recognition problem. In our last work, we propose a new data conversion algorithm in order to gain a better classification accuracy of communication signal modulation, but we still believe that the convolution neural network (CNN) can work better. However, its application to signal modulation recognition is often hampered by insufficient data and overfitting. Here, we propose a smart approach to programmatic data augmentation method by using the auxiliary classifier generative adversarial networks (ACGANs). The famous CNN model, AlexNet, has been utilized to be the classifier and ACGAN to be the generator, which will enlarge our data set. In order to alleviate the common issues in the traditional generative adversarial nets training, such as discriminator overfitting, generator disconverge, and mode collapse, we apply several training tricks in our training. With the result on original data set as our baseline, we will evaluate our result on enlarged data set to validate the ACGAN’s performance. The result shows that we can gain 0.1~6% increase in the classification accuracy in the ACGAN-based data set.","['Constellation diagram', 'Training', 'Gallium nitride', 'Phase shift keying', 'Image color analysis', 'Signal to noise ratio']","['Cognitive radio', 'modulation recognition', 'pattern recognition', 'classification algorithms', 'deep learning', 'convolutional networks', 'generative adversarial net']"
"Virtual inertia control is considered as an important part of microgrids with high renewable penetration. Virtual inertia emulation based on the derivative of frequency is one of the effective methods for improving system inertia and maintaining frequency stability. However, in this method, the ability to provide virtual damping is usually neglected in its design, and hence, its performance might be insufficient in the system with low damping. Confronted with this issue, this paper proposes a novel design and analysis of virtual inertia control to imitate damping and inertia properties simultaneously to the microgrid, enhancing frequency performance and stability. The proposed virtual inertia control uses the derivative technique to calculate the derivative of frequency for virtual inertia emulation. Trajectory sensitivities have been performed to analyze the dynamic impacts of the virtual inertia and virtual damping variables over the system performance. Time-domain simulations are also presented to evaluate the efficiency of the virtual damping and virtual inertia in enhancing system frequency stability. Finally, the efficiency and robustness of the proposed control technique are compared with the conventional inertia control under a wide range of system operation, including the decrease in system damping and inertia and high integrations of load variation and renewable energy.","['Damping', 'Microgrids', 'Frequency control', 'Power system stability', 'Stability analysis', 'Synchronous generators', 'Power system dynamics']","['Frequency stability', 'isolated microgrid', 'virtual inertia regulation', 'virtual synchronous machine']"
"In recent years, the IoT concept is more and more powerful, having set the goal of integrating billions of devices to the Internet. Thus, from this perspective, the interest allocated to low-power wireless networks of sensors is higher than ever. In this paper, the SigFox scalability is analyzed from the IoT concept point of view. In the scientific research, there are a series of papers which tackle the SigFox issues, oftentimes at a comparative study level, without evaluating the performance level of the communication protocol. This paper comes to fill this gap by creating a realistic SigFox communication model. Moreover, a developed and tested generator of SigFox traffic has been implemented, using SDRs. This allows the possibility of evaluating the performance level of WSN networks, of a large-scale high-density-type. Both of the suggested instruments represent the novelty of this paper. The obtained results show that the maximum number of sensors that can transmit data at the same time, using the proposed scenarios, is of approximately 100, in order to obtain a high level of performance when the number of available channels is 360. If we are to increase the number of sensors, an avalanche effect ensues which triggers the sharp decrease of the performance of the SigFox network. At the end of this work, a series of solutions are being suggested with the main purpose of increasing the performance level of large-scale, high-density SigFox networks.","['Logic gates', 'Sensors', 'Scalability', 'Europe', 'Wireless sensor networks', 'Internet of Things', 'Modulation']","['Internet of Things', 'scalability', 'wireless sensor networks']"
"Deep learning methods, such as convolution neural networks (CNNs), have achieved remarkable success in computer vision tasks. Hence, an increasing trend in using deep learning for electroencephalograph (EEG) analysis is evident. Extracting relevant information from CNN features is one of the key reasons behind the success of the CNN-based deep learning models. Some CNN models use convolutional features from different CNN layers with good effect. However, extraction and fusion of multilevel convolutional features remain unexplored for EEG applications. Moreover, cognitive computing and artificial intelligence experience increasing applications in all fields. Cognitive process is based on understanding human brain cognition through signals, such as EEG. Hence, deep learning can aid in developing cognitive systems and related applications by improving EEG decoding. The classification and recognition of EEG have consistently been challenging due to its characteristics of dynamic time series data and low signal-to-noise ratio. However, the information hidden in different convolution layers can aid in improving feature discrimination capability. In this paper, we use the EEG motor imagery data to uncover the benefits of extracting and fusing multilevel convolutional features from different CNN layers, which are abstract representations of the input at various levels. Our proposed CNN model can learn robust spectral and temporal features from the raw EEG data. We demonstrate that such multilevel feature fusion outperforms the models that use features only from the last layer. Our results are better than the state of the art for EEG decoding and classification.","['Electroencephalography', 'Feature extraction', 'Brain modeling', 'Deep learning', 'Convolution', 'Task analysis', 'Data mining']","['EEG motor imagery classification', 'deep learning', 'convolution neural network', 'multilevel feature fusion']"
"Internet of Medical Things (IoMT) is the collection of medical devices and related applications which link the healthcare IT systems through online computer networks. In the field of diagnosis, medical image classification plays an important role in prediction and early diagnosis of critical diseases. Medical images form an indispensable part of a patient's health record which can be applied to control, handle and treat the diseases. But, classification of images is a challenging task in computer-based diagnostics. In this research article, we have introduced a improved classifier i.e., Optimal Deep Learning (DL) for classification of lung cancer, brain image, and Alzheimer's disease. The researchers proposed the Optimal Feature Selection based Medical Image Classification using DL model by incorporating preprocessing, feature selection and classification. The main goal of the paper is to derive an optimal feature selection model for effective medical image classification. To enhance the performance of the DL classifier, Opposition-based Crow Search (OCS) algorithm is proposed. The OCS algorithm picks the optimal features from pre-processed images, here Multi-texture, grey level features were selected for the analysis. Finally, the optimal features improved the classification result and increased the accuracy, specificity and sensitivity in the diagnosis of medical images. The proposed results were implemented in MATLAB and compared with existing feature selection models and other classification approaches. The proposed model achieved the maximum performance in terms of accuracy, sensitivity and specificity being 95.22%, 86.45 % and 100% for the applied set of images.","['Medical diagnostic imaging', 'Feature extraction', 'Deep learning', 'Solid modeling', 'Cancer']","['IoMT', 'classification', 'deep learning', 'medical image', 'features', 'Crow search algorithm', 'optimization']"
"Video and images acquired by a visual system are seriously degraded under hazy and foggy weather, which will affect the detection, tracking, and recognition of targets. Thus, restoring the true scene from such a foggy video or image is of significance. The main goal of this paper was to summarize current video and image defogging algorithms. We first presented a review of the detection and classification method of a foggy image. Then, we summarized existing image defogging algorithms, including image restoration algorithms, image contrast enhancement algorithms, and fusion-based defogging algorithms. We also presented current video defogging algorithms. We summarized objective image quality assessment methods that have been widely used for the comparison of different defogging algorithms, followed by an experimental comparison of various classical image defogging algorithms. Finally, we presented the problems of video and image defogging which need to be further studied. The code of all algorithms will be available at <;uri xlink:href=""http://www.yongxu.org/lunwen.html"" xlink:type=""simple"">http://www.yongxu.org/lunwen.html<;/uri>.","['Classification algorithms', 'Visual systems', 'Image restoration', 'Meteorology', 'Feature extraction', 'Image classification', 'Defogging']","['Foggy image classification', 'image defogging', 'video defogging', 'image quality assessment']"
"Melanoma is a type of skin cancer with a high mortality rate. The different types of skin lesions result in an inaccurate diagnosis due to their high similarity. Accurate classification of the skin lesions in their early stages enables dermatologists to treat the patients and save their lives. This paper proposes a model for a highly accurate classification of skin lesions. The proposed model utilized the transfer learning and pre-trained model with GoogleNet. The model parameters are used as initial values, and then these parameters will be modified through training. The latest well-known public challenge dataset, ISIC 2019, is used to test the ability of the proposed model to classify different kinds of skin lesions. The proposed model successfully classified the eight different classes of skin lesions, namely, melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, vascular lesion, and Squamous cell carcinoma. The achieved classification accuracy, sensitivity, specificity, and precision percentages are 94.92%, 79.8%, 97%, and 80.36%, respectively. The proposed model can detect images that do not belong to any one of the eight classes where these images are classified as unknown images.","['Lesions', 'Skin', 'Melanoma', 'Computer architecture', 'Support vector machines', 'Feature extraction']","['Melanoma classification', 'skin lesions', 'convolution neural network', 'GoogleNet', 'ISIC 2019', 'bootstrap multiclass SVM', 'transfer learning']"
"The Internet of Things (IoT) is an emerging classical model, envisioned as a system of billions of small interconnected devices for posing the state-of-the-art findings to real-world glitches. Over the last decade, there has been an increasing research concentration in the IoT as an essential design of the constant convergence between human behaviors and their images on Information Technology. With the development of technologies, the IoT drives the deployment of across-the-board and self-organizing wireless networks. The IoT model is progressing toward the notion of a cyber-physical world, where things can be originated, driven, intermixed, and modernized to facilitate the emergence of any feasible association. This paper provides a summary of the existing IoT research that underlines enabling technologies, such as fog computing, wireless sensor networks, data mining, context awareness, real-time analytics, virtual reality, and cellular communications. Also, we present the lessons learned after acquiring a thorough representation of the subject. Thus, by identifying numerous open research challenges, it is presumed to drag more consideration into this novel paradigm.","['Smart cities', 'Wireless sensor networks', 'Internet of Things', 'Sensors', 'Edge computing', 'Data mining']","['Internet of Things', 'fog computing', 'wireless sensor networks', 'smart cities', 'cellular IoT', 'real-time analytics']"
"Emerging technologies rapidly change the essential qualities of modern societies in terms of smart environments. To utilize the surrounding environment data, tiny sensing devices and smart gateways are highly involved. It has been used to collect and analyze the real-time data remotely in all Industrial Internet of Things (IIoT). Since the IIoT environment gathers and transmits the data over insecure public networks, a promising solution known as authentication and key agreement (AKA) is preferred to prevent illegal access. In the medical industry, the Internet of Medical Things (IoM) has become an expert application system. It is used to gather and analyze the physiological parameters of patients. To practically examine the medical sensor-nodes, which are imbedded in the patient's body. It would in turn sense the patient medical information using smart portable devices. Since the patient information is so sensitive to reveal other than a medical professional, the security protection and privacy of medical data are becoming a challenging issue of the IoM. Thus, an anonymity-based user authentication protocol is preferred to resolve the privacy preservation issues in the IoM. In this paper, a Secure and Anonymous Biometric Based User Authentication Scheme (SAB-UAS) is proposed to ensure secure communication in healthcare applications. This paper also proves that an adversary cannot impersonate as a legitimate user to illegally access or revoke the smart handheld card. A formal analysis based on the random-oracle model and resource analysis is provided to show security and resource efficiencies in medical application systems. In addition, the proposed scheme takes a part of the performance analysis to show that it has high-security features to build smart healthcare application systems in the IoM. To this end, experimental analysis has been conducted for the analysis of network parameters using NS3 simulator. The collected results have shown superiority in terms of the packet delivery ratio, end-to-end delay, throughput rates, and routing overhead for the proposed SAB-UAS in comparison to other existing protocols.","['Authentication', 'Sensors', 'Wireless sensor networks', 'Password', 'Analytical models', 'Protocols']","['Authentication and key agreement', 'internet of medical things', 'security protection and privacy user authentication', 'random-oracle model and resource analysis', 'e-healthcare application', 'biometrics']"
"The Internet of Things (IoT) has been widely used because of its high efficiency and real-time collaboration. A wireless sensor network is the core technology to support the operation of the IoT, and the security problem is becoming more and more serious. Aiming at the problem that the existing malicious node detection methods in wireless sensor networks cannot be guaranteed by fairness and traceability of detection process, we present a blockchain trust model (BTM) for malicious node detection in wireless sensor networks. First, it gives the whole framework of the trust model. Then, it constructs the blockchain data structure which is used to detect malicious nodes. Finally, it realizes the detection of malicious nodes in 3D space by using the blockchain smart contract and the WSNs' quadrilateral measurement localization method, and the voting consensus results are recorded in the blockchain distributed. The simulation results show that the model can effectively detect malicious nodes in WSNs, and it can also ensure the traceability of the detection process.","['Wireless sensor networks', 'Blockchain', 'Smart contracts', 'Peer-to-peer computing', 'Intelligent sensors', 'Monitoring']","['Wireless sensor networks', 'blockchain', 'smart contract', 'malicious nodes', 'vote']"
"Wireless sensor networks (WSNs) have gained wide attention from researchers in the last few years because it has a vital role in countless applications. The main function of WSN is to process extracted data and to transmit it to remote locations. A large number of sensor nodes are deployed in the monitoring area. Therefore, deploying the minimum number of nodes that maintain full coverage and connectivity is of immense importance for research. Hence, coverage and connectivity issues, besides maximizing the network lifetime, represented the main concern to be considered in this paper. The key point of this paper is to classify different coverage techniques in WSNs into three main parts: coverage based on classical deployment techniques, coverage based on meta-heuristic techniques, and coverage based on self-scheduling techniques. Moreover, multiple comparisons among these techniques are provided considering their advantages and disadvantages. Additionally, performance metrics that must be considered in WSNs and comparison among different WSNs simulators are provided. Finally, open research issues, as well as recommendations for researchers, are discussed.","['Wireless sensor networks', 'Sensors', 'Monitoring', 'Mathematical model', 'Routing', 'Data mining', 'Measurement']","['Coverage', 'connectivity', 'deployment techniques', 'power consumption', 'wireless sensor network (WSN)']"
"Blockchain technology becomes increasingly popular. It also attracts scams, for example, a Ponzi scheme, a classic fraud, has been found making a notable amount of money on Blockchain, which has a very negative impact. To help to deal with this issue and to provide reusable research data sets for future research, this paper collects real-world samples and proposes an approach to detect Ponzi schemes implemented as smart contracts (i.e., smart Ponzi schemes) on the blockchain. First, 200 smart Ponzi schemes are obtained by manually checking more than 3,000 open source smart contracts on the Ethereum platform. Then, two kinds of features are extracted from the transaction history and operation codes of the smart contracts. Finally, a classification model is presented to detect smart Ponzi schemes. The extensive experiments show that the proposed model performs better than many traditional classification models and can achieve high accuracy for practical use. By using the proposed approach, we estimate that there are more than 500 smart Ponzi schemes running on Ethereum. Based on these results, we propose to build a uniform platform to evaluate and monitor every created smart contract for early warning of scams.","['Smart contracts', 'Blockchain', 'Feature extraction', 'Internet', 'High level languages', 'History']","['Blockchain', 'smart contract', 'Ponzi Schemes', 'ethereum', 'data mining']"
"Now-a-days image processing placed an important role for recognizing various diseases such as breast, lung, and brain tumors in earlier stage for giving the appropriate treatment. Presently, most cancer diagnosis worked according to the visual examination process with effectively. Human visual reviewing of infinitesimal biopsy pictures is exceptionally tedious, subjective, and conflicting due to between and intra-onlooker varieties. In this manner, the malignancy and it’s compose will be distinguished in a beginning time for finish treatment and fix. This brain tumor classification system using machine learning-based back propagation neural networks (MLBPNN) causes pathologists to enhance the exactness and proficiency in location of threat and to limit the entomb onlooker variety. Moreover, the technique may assist doctors with analyzing the picture cell by utilizing order and bunching calculations by recoloring qualities of the phones. The different picture preparing steps required for disease location from biopsy pictures incorporate procurement, upgrade, and division; include extraction, picture portrayal, characterization, and basic leadership. In this paper, MLBPNN is analyzed with the help of infra-red sensor imaging technology. Then, the computational multifaceted nature of neural distinguishing proof incredibly diminished when the entire framework is deteriorated into a few subsystems. The features are extracted using fractal dimension algorithm and then the most significant features are selected using multi fractal detection technique to reduce the complexity. This imaging sensor is integrated via wireless infrared imaging sensor which is produced to transmit the tumor warm data to a specialist clinician to screen the wellbeing condition and for helpful control of ultrasound measurements level, especially if there should arise an occurrence of elderly patients living in remote zones.","['Tumors', 'Magnetic resonance imaging', 'Feature extraction', 'Fractals', 'Biological neural networks', 'Image segmentation']","['Wireless infrared imaging sensor', 'infra-red sensor', 'principal component analysis gray level covariance matrix', 'machine learning based neural networks']"
"Falls are abnormal activity events that occur infrequently; however, they are serious health problems among elderly individuals. With the advancements of technologies, falls have been widely studied by scientific researchers to minimize serious consequences and negative impacts. Fall detection and fall prevention are two strategies to tackle fall issues with a variety of sensing techniques and classifier models. Currently, many reviews on fall-related technologies have been presented and analyzed; however, most of them give surveys on the subfield of fall-related systems, while others are not extensive and comprehensive reviews. In fact, the latest researches have a new trend of fusion-based methods to improve the performance of the fall-related systems based on a combination of different sensors or classifier models. Adaptive threshold and radio frequency-based systems are also researched and proposed recently, which are seldom mentioned in other reviews. Therefore, a global taxonomy for current fall-related studies from four aspects, including current literature reviews, fall detection, and prevention systems based on different sensor apparatus and analytic algorithm, low power techniques, and sensor placements for fall-related systems are conducted in this paper. Several research challenges and issues in the fall-related field are also discussed and analyzed. The objective of this review paper is to conclude and provide a good position of current fall-related studies to inspire researchers in this field.","['Sensor systems', 'Taxonomy', 'Senior citizens', 'Bibliographies', 'Systematics', 'Aging']","['Adaptive algorithm', 'classification algorithms', 'fall detection', 'fall prevention', 'low power techniques', 'sensing techniques']"
"With a wide scope to explore and harness the oceanic sources of interest, the field of underwater wireless sensor networks (UWSNs) is attracting a growing interest of researchers. Owing to the real-time remote data monitoring requirements, underwater acoustic sensor networks (UASNs) emerged as a preferred network to a great extent. In UASN, the limited availability and non-rechargeability of energy resources along with the relative inaccessibility of deployed sensor nodes for energy replenishments necessitated the evolution of several energy optimization techniques. Clustering is one such technique that increases system scalability and reduces energy consumption. Besides clustering, coverage and connectivity are two significant properties that decide the proper detection and communication of events of interest in UWSN due to unstable underwater environment. Underwater communication is also possible with non-acoustic communication techniques like radio frequency, magnetic induction, and underwater free-space optics. In this paper, we surveyed clustering, coverage, and connectivity issues of UASN and qualitatively compared their performance. Particularly, the impact of these non-conventional communication techniques on clustering, coverage, and connectivity aspects is demonstrated. Additionally, we highlighted some key open issues related to the UWSN. This paper provides a broad view of existing algorithms of clustering, coverage, and connectivity based on acoustic communication. It also provides a useful guidance to the researchers in UWSN from various other communication techniques' perspective.","['Wireless sensor networks', 'Radio frequency', 'Clustering algorithms', 'Wireless communication', 'Underwater acoustics', 'Monitoring', 'Magnetoacoustic effects']","['Clustering', 'connectivity', 'coverage', 'RF', 'MI', 'UWFSO', 'acoustic', 'underwater wireless sensor networks']"
"Healthcare supply chains are complex structures spanning across multiple organizational and geographical boundaries, providing critical backbone to services vital for everyday life. The inherent complexity of such systems can introduce impurities including inaccurate information, lack of transparency and limited data provenance. Counterfeit drugs is one consequence of such limitations within existing supply chains which not only has serious adverse impact on human health but also causes severe economic loss to the healthcare industry. Consequently, existing studies have emphasized the need for a robust, end-to-end track and trace system for pharmaceutical supply chains. Therein, an end-to-end product tracking system across the pharmaceutical supply chain is paramount to ensuring product safety and eliminating counterfeits. Most existing track and trace systems are centralized leading to data privacy, transparency and authenticity issues in healthcare supply chains. In this article, we present an Ethereum blockchain-based approach leveraging smart contracts and decentralized off-chain storage for efficient product traceability in the healthcare supply chain. The smart contract guarantees data provenance, eliminates the need for intermediaries and provides a secure, immutable history of transactions to all stakeholders. We present the system architecture and detailed algorithms that govern the working principles of our proposed solution. We perform testing and validation, and present cost and security analysis of the system to evaluate its effectiveness to enhance traceability within pharmaceutical supply chains.","['Drugs', 'Supply chains', 'Medical services', 'Blockchain', 'Stakeholders', 'Smart contracts', 'Industries']","['Blockchain', 'drug counterfeiting', 'traceability', 'healthcare', 'supply chain', 'trust', 'security']"
"Images captured under poor illumination conditions often exhibit characteristics such as low brightness, low contrast, a narrow gray range, and color distortion, as well as considerable noise, which seriously affect the subjective visual effect on human eyes and greatly limit the performance of various machine vision systems. The role of low-light image enhancement is to improve the visual effect of such images for the benefit of subsequent processing. This paper reviews the main techniques of low-light image enhancement developed over the past decades. First, we present a new classification of these algorithms, dividing them into seven categories: gray transformation methods, histogram equalization methods, Retinex methods, frequency-domain methods, image fusion methods, defogging model methods and machine learning methods. Then, all the categories of methods, including subcategories, are introduced in accordance with their principles and characteristics. In addition, various quality evaluation methods for enhanced images are detailed, and comparisons of different algorithms are discussed. Finally, the current research progress is summarized, and future research directions are suggested.","['Image enhancement', 'Image color analysis', 'Brightness', 'Color', 'Lighting', 'Visual effects', 'Machine learning algorithms']","['Review', 'survey', 'low-light image enhancement', 'Retinex method', 'image enhancement', 'quality evaluation']"
"An intrusion detection system (IDS) is an important protection instrument for detecting complex network attacks. Various machine learning (ML) or deep learning (DL) algorithms have been proposed for implementing anomaly-based IDS (AIDS). Our review of the AIDS literature identifies some issues in related work, including the randomness of the selected algorithms, parameters, and testing criteria, the application of old datasets, or shallow analyses and validation of the results. This paper comprehensively reviews previous studies on AIDS by using a set of criteria with different datasets and types of attacks to set benchmarking outcomes that can reveal the suitable AIDS algorithms, parameters, and testing criteria. Specifically, this paper applies 10 popular supervised and unsupervised ML algorithms for identifying effective and efficient ML-AIDS of networks and computers. These supervised ML algorithms include the artificial neural network (ANN), decision tree (DT), k-nearest neighbor (k-NN), naive Bayes (NB), random forest (RF), support vector machine (SVM), and convolutional neural network (CNN) algorithms, whereas the unsupervised ML algorithms include the expectation-maximization (EM), k-means, and self-organizing maps (SOM) algorithms. Several models of these algorithms are introduced, and the turning and training parameters of each algorithm are examined to achieve an optimal classifier evaluation. Unlike previous studies, this study evaluates the performance of AIDS by measuring the true positive and negative rates, accuracy, precision, recall, and F-Score of 31 ML-AIDS models. The training and testing time for ML-AIDS models are also considered in measuring their performance efficiency given that time complexity is an important factor in AIDSs. The ML-AIDS models are tested by using a recent and highly unbalanced multiclass CICIDS2017 dataset that involves real-world network attacks. In general, the k-NN-AIDS, DT-AIDS, and NB-AIDS models obtain the best results and show a greater capability in detecting web attacks compared with other models that demonstrate irregular and inferior results.","['Classification algorithms', 'Feature extraction', 'Training', 'Benchmark testing', 'Support vector machines', 'Self-organizing feature maps', 'Radio frequency']","['Cyberattacks', 'intrusion detection system', 'machine learning', 'supervised and unsupervised learning']"
"Feature selection (FS) is one of the important tasks of data preprocessing in data analytics. The data with a large number of features will affect the computational complexity, increase a huge amount of resource usage and time consumption for data analytics. The objective of this study is to analyze relevant and significant features of huge network traffic to be used to improve the accuracy of traffic anomaly detection and to decrease its execution time. Information Gain is the most feature selection technique used in Intrusion Detection System (IDS) research. This study uses Information Gain, ranking and grouping the features according to the minimum weight values to select relevant and significant features, and then implements Random Forest (RF), Bayes Net (BN), Random Tree (RT), Naive Bayes (NB) and J48 classifier algorithms in experiments on CICIDS-2017 dataset. The experiment results show that the number of relevant and significant features yielded by Information Gain affects significantly the improvement of detection accuracy and execution time. Specifically, the Random Forest algorithm has the highest accuracy of 99.86% using the relevant selected features of 22, whereas the J48 classifier algorithm provides an accuracy of 99.87% using 52 relevant selected features with longer execution time.","['Feature extraction', 'Classification algorithms', 'Anomaly detection', 'Information filters', 'Support vector machines', 'Filtering algorithms']","['Feature selection', 'anomaly detection', 'information gain', 'CICIDS-2017 dataset', 'classifier algorithm']"
"The IoT (Internet of Things) connect systems, applications, data storage, and services that may be a new gateway for cyber-attacks as they continuously offer services in the organization. Currently, software piracy and malware attacks are high risks to compromise the security of IoT. These threats may steal important information that causes economic and reputational damages. In this paper, we have proposed a combined deep learning approach to detect the pirated software and malware-infected files across the IoT network. The TensorFlow deep neural network is proposed to identify pirated software using source code plagiarism. The tokenization and weighting feature methods are used to filter the noisy data and further, to zoom the importance of each token in terms of source code plagiarism. Then, the deep learning approach is used to detect source code plagiarism. The dataset is collected from Google Code Jam (GCJ) to investigate software piracy. Apart from this, the deep convolutional neural network is used to detect malicious infections in IoT network through color image visualization. The malware samples are obtained from Maling dataset for experimentation. The experimental results indicate that the classification performance of the proposed solution to measure the cybersecurity threats in IoT are better than the state of the art methods.","['Malware', 'Feature extraction', 'Plagiarism', 'Computer crime', 'Internet of Things', 'Computer languages']","['Internet of Things', 'data mining', 'cyber security', 'software piracy', 'malware detection']"
"One of the major research topics in unmanned aerial vehicle (UAV) collaborative control systems is the problem of multi-UAV target assignment and path planning (MUTAPP). It is a complicated optimization problem in which target assignment and path planning are solved separately. However, recalculation of the optimal results is too slow for real-time operations in dynamic environments because of the large number of calculations required. In this paper, we propose an artificial intelligence method named simultaneous target assignment and path planning (STAPP) based on a multi-agent deep deterministic policy gradient (MADDPG) algorithm, which is a type of multi-agent reinforcement learning algorithm. In STAPP, the MUTAPP problem is first constructed as a multi-agent system. Then, the MADDPG framework is used to train the system to solve target assignment and path planning simultaneously according to a corresponding reward structure. The proposed system can deal with dynamic environments effectively as its execution only requires the locations of the UAVs, targets, and threat areas. Real-time performance can be guaranteed as the neural network used in the system is simple. In addition, we develop a technique to improve the training effect and use experiments to demonstrate the effectiveness of our method.","['Path planning', 'Heuristic algorithms', 'Optimization', 'Training', 'Reinforcement learning', 'Task analysis', 'Unmanned aerial vehicles']","['Multi-UAV', 'target assignment and path planning', 'multi-agent reinforcement learning', 'MADDPG', 'dynamic environments']"
"Hurricanes regularly cause widespread and prolonged power outages along the U.S. coastline. These power outages have significant impacts on other infrastructure dependent on electric power and on the population living in the impacted area. Efficient and effective emergency response planning within power utilities, other utilities dependent on electric power, private companies, and local, state, and federal government agencies benefit from accurate estimates of the extent and spatial distribution of power outages in advance of an approaching hurricane. A number of models have been developed for predicting power outages in advance of a hurricane, but these have been specific to a given utility service area, limiting their use to support wider emergency response planning. In this paper, we describe the development of a hurricane power outage prediction model applicable along the full U.S. coastline using only publicly available data, we demonstrate the use of the model for Hurricane Sandy, and we use the model to estimate what the impacts of a number of historic storms, including Typhoon Haiyan, would be on current U.S. energy infrastructure.","['Storms', 'Hurricanes', 'Predictive models', 'Contingency planning', 'Emergency services', 'Power outages', 'Power system restoration']","['Hurricane', 'storm response planning', 'outage prediction', 'outage model']"
"One of the most common types of human malignancies is skin cancer, which is chiefly diagnosed visually, initiating with a clinical screening followed by dermoscopic analysis, histopathological assessment, and a biopsy. Due to the fine-grained differences in the appearance of skin lesions, automated classification is quite challenging through images. To attain highly segregated and potentially general tasks against the finely grained object categorized, deep convolutional neural networks (CNNs) are used. In this paper, we propose a new prediction model that classifies skin lesions into benign or malignant lesions based on a novel regularizer technique. Hence, this is a binary classifier that discriminates between benign or malignant lesions. The proposed model achieved an average accuracy of 97.49%, which in turns showed its superiority over other state-of-the-art methods. The performance of CNN in terms of AUC-ROC with an embedded novel regularizer is tested on multiple use cases. The area under the curve (AUC) achieved for nevus against melanoma lesion, seborrheic keratosis versus basal cell carcinoma lesion, seborrheic keratosis versus melanoma lesion, solar lentigo versus melanoma lesion is 0.77, 0.93, 0.85, and 0.86, respectively. Our results showed that the proposed learning model outperformed the existing algorithm and can be used to assist medical practitioners in classifying various skin lesions.","['Melanoma', 'Skin', 'Lesions', 'Feature extraction', 'Convolution']","['Convolutional neural network', 'skin lesion', 'novel regularizer', 'AUC-ROC']"
"Battery State-of-Health (SOH) estimation is of utmost importance for the performance and cost-effectiveness of electric vehicles. Incremental capacity analysis (ICA) has been ubiquitously used for battery SOH estimation. However, challenges remain with regard to the characteristic parameter selection, estimation viability and feasibility for practical implementation. In this paper, a novel ICA-based method for battery SOH estimation is proposed, with the goals to identify the most effective characteristic parameters of IC curves, optimize the SOH model parameters for better prediction accuracy and enhance its applicability in realistic battery management systems. To this end, the IC curve is first derived and filtered using the wavelet filtering, with the peak value and position extracted as health factors (HFs). Then, the correlations between SOH and HFs are explored through the grey correlation analysis. The SOH model is further established based on the Gaussian process regression (GPR), in which the optimal hyper parameters are calculated through the conjugate gradient method and the multi-island genetic algorithm (MIGA). The effects of different HFs and kernel functions are also analyzed. The effectiveness of the proposed MIGA-GPR SOH model is validated by experimentation.","['Estimation', 'Degradation', 'Wavelet transforms', 'Integrated circuits', 'Gaussian processes']","['Batteries', 'incremental capacity analysis', 'state of health', 'Gaussian process regression', 'multi-island genetic algorithm']"
"Energy storage systems are playing an increasingly important role in a variety of applications, such as electric vehicles or grid-connected systems. In this context, supercapacitors (SCs) are gaining ground due to their high power density, good performance, and long maintenance-free lifetime. For this reason, SCs are a hot research topic, and several papers are being published on material engineering, performance characterization, modeling. and post-mortem analysis. A compilation of the most important millstones on this topic is essential to keep researchers on related fields updated about new potentials of this technology. This review paper covers recent research aspects and applications of SCs, highlighting the relationship between material properties and electrical characteristics. It begins with an explanation of the energy storage mechanisms and materials used by SCs. Based on these materials, the SCs are classified, their key features are summarized, and their electrochemical characteristics are related to electrical performance. Given the high interest in system modeling and a large number of papers published on this topic, modeling techniques are classified, explained, and compared, addressing their strengths and weaknesses, and the experimental techniques used to measure the modeled properties are described. Finally, SCs are successfully used in the market sectors, as well as their growth expectations are analyzed. The analysis presented herein gives the account of the expansion that the SC market is currently undergoing and identifies the most promising research trends on this field.","['Electrodes', 'Supercapacitors', 'Market research', 'Electrolytes', 'Batteries']","['Electrical performance', 'electrochemistry', 'energy storage', 'experimental characterization', 'modeling', 'supercapacitor']"
"In the telecom sector, a huge volume of data is being generated on a daily basis due to a vast client base. Decision makers and business analysts emphasized that attaining new customers is costlier than retaining the existing ones. Business analysts and customer relationship management (CRM) analyzers need to know the reasons for churn customers, as well as, behavior patterns from the existing churn customers’ data. This paper proposes a churn prediction model that uses classification, as well as, clustering techniques to identify the churn customers and provides the factors behind the churning of customers in the telecom sector. Feature selection is performed by using information gain and correlation attribute ranking filter. The proposed model first classifies churn customers data using classification algorithms, in which the Random Forest (RF) algorithm performed well with 88.63% correctly classified instances. Creating effective retention policies is an essential task of the CRM to prevent churners. After classification, the proposed model segments the churning customer’s data by categorizing the churn customers in groups using cosine similarity to provide group-based retention offers. This paper also identified churn factors that are essential in determining the root causes of churn. By knowing the significant churn factors from customers’ data, CRM can improve productivity, recommend relevant promotions to the group of likely churn customers based on similar behavior patterns, and excessively improve marketing campaigns of the company. The proposed churn prediction model is evaluated using metrics, such as accuracy, precision, recall, f-measure, and receiving operating characteristics (ROC) area. The results reveal that our proposed churn prediction model produced better churn classification using the RF algorithm and customer profiling using k-means clustering. Furthermore, it also provides factors behind the churning of churn customers through the rules generated by using the attribute-selected classifier algorithm.","['Telecommunications', 'Companies', 'Predictive models', 'Customer relationship management', 'Data mining', 'Decision trees', 'Classification algorithms']","['Churn prediction', 'retention', 'telecom', 'CRM', 'machine learning']"
"Breast cancer (BC) is one of the primary causes of cancer death among women. Early detection of BC allows patients to receive appropriate treatment, thus increasing the possibility of survival. In this work, a new deep-learning (DL) model based on the transfer-learning (TL) technique is developed to efficiently assist in the automatic detection and diagnosis of the BC suspected area based on two techniques namely 80-20 and cross-validation. DL architectures are modeled to be problem-specific. TL uses the knowledge gained during solving one problem in another relevant problem. In the proposed model, the features are extracted from the mammographic image analysis- society (MIAS) dataset using a pre-trained convolutional neural network (CNN) architecture such as Inception V3, ResNet50, Visual Geometry Group networks (VGG)-19, VGG-16, and Inception-V2 ResNet. Six evaluation metrics for evaluating the performance of the proposed model in terms of accuracy, sensitivity, specificity, precision, F-score, and area under the ROC curve (AUC) has been chosen. Experimental results show that the TL of the VGG16 model is powerful for BC diagnosis by classifying the mammogram breast images with overall accuracy, sensitivity, specificity, precision, F-score, and AUC of 98.96%, 97.83%, 99.13%, 97.35%, 97.66%, and 0.995, respectively for 80-20 method and 98.87%, 97.27%, 98.2%, 98.84%, 98.04%, and 0.993 for 10-fold cross-validation method.","['Feature extraction', 'Breast', 'Sensitivity', 'Image segmentation', 'Histograms', 'Training', 'Residual neural networks']","['Breast cancer', 'machine learning', 'deep-learning', 'transfer learning', 'image classification', 'convolutional neural networks']"
"This paper presents a literature review on pattern recognition of electromyography (EMG) signals and its applications. The EMG technology is introduced and the most relevant aspects for the design of an EMG-based system are highlighted, including signal acquisition and filtering. EMG-based systems have been used with relative success to control upper- and lower-limb prostheses, electronic devices and machines, and for monitoring human behavior. Nevertheless, the existing systems are still inadequate and are often abandoned by their users, prompting for further research. Besides controlling prostheses, EMG technology is also beneficial for the development of machine learning-based devices that can capture the intention of able-bodied users by detecting their gestures, opening the way for new human-machine interaction (HMI) modalities. This paper also reviews the current feature extraction techniques, including signal processing and data dimensionality reduction. Novel classification methods and approaches for detecting non-trained gestures are discussed. Finally, current applications are reviewed, through the comparison of different EMG systems and discussion of their advantages and drawbacks.","['Electrodes', 'Electromyography', 'Muscles', 'Sensors', 'Pattern recognition', 'Electric potential', 'Band-pass filters']","['EMG', 'human-machine interaction', 'pattern classification', 'regression']"
"In open and dynamic multiagent systems (MASs), agents often need to rely on resources or services provided by other agents to accomplish their goals. During this process, agents are exposed to the risk of being exploited by others. These risks, if not mitigated, can cause serious breakdowns in the operation of MASs and threaten their long-term wellbeing. To protect agents from the uncertainty in the behavior of their interaction partners, the age-old mechanism of trust between human beings is re-contexted into MASs. The basic idea is to let agents self-police the MAS by rating each other on the basis of their observed behavior and basing future interaction decisions on such information. Over the past decade, a large number of trust management models were proposed. However, there is a lack of research effort in several key areas, which are critical to the success of trust management in MASs where human beings and agents coexist. The purpose of this paper is to give an overview of existing research in trust management in MASs. We analyze existing trust models from a game theoretic perspective to highlight the special implications of including human beings in an MAS, and propose a possible research agenda to advance the state of the art in this field.","['Game theory', 'Decision making', 'Uncertainty', 'Analytical models', 'Context awareness', 'Multi-agent systems', 'Computational modeling', 'Trust management']","['Game theory', 'multi-agent systems', 'reputation', 'trust']"
"New advances in electronic commerce systems and communication technologies have made the credit card the potentially most popular method of payment for both regular and online purchases; thus, there is significantly increased fraud associated with such transactions. Fraudulent credit card transactions cost firms and consumers large financial losses every year, and fraudsters continuously attempt to find new technologies and methods for committing fraudulent transactions. The detection of fraudulent transactions has become a significant factor affecting the greater utilization of electronic payment. Thus, there is a need for efficient and effective approaches for detecting fraud in credit card transactions. This paper proposes an intelligent approach for detecting fraud in credit card transactions using an optimized light gradient boosting machine (OLightGBM). In the proposed approach, a Bayesian-based hyperparameter optimization algorithm is intelligently integrated to tune the parameters of a light gradient boosting machine (LightGBM). To demonstrate the effectiveness of our proposed OLightGBM for detecting fraud in credit card transactions, experiments were performed using two real-world public credit card transaction data sets consisting of fraudulent transactions and legitimate ones. Based on a comparison with other approaches using the two data sets, the proposed approach outperformed the other approaches and achieved the highest performance in terms of accuracy (98.40%), Area under receiver operating characteristic curve (AUC) (92.88%), Precision (97.34%) and F1-score (56.95%).","['Credit cards', 'Machine learning algorithms', 'Feature extraction', 'Boosting', 'Bayes methods']","['Credit card fraud', 'electronic commerce', 'machine learning', 'optimization methods']"
"In the early months of the COVID-19 pandemic with no designated cure or vaccine, the only way to break the infection chain is self-isolation and maintaining the physical distancing. In this article, we present a potential application of the Internet of Things (IoT) in healthcare and physical distance monitoring for pandemic situations. The proposed framework consists of three parts: a lightweight and low-cost IoT node, a smartphone application (app), and fog-based Machine Learning (ML) tools for data analysis and diagnosis. The IoT node tracks health parameters, including body temperature, cough rate, respiratory rate, and blood oxygen saturation, then updates the smartphone app to display the user health conditions. The app notifies the user to maintain a physical distance of 2 m (or 6 ft), which is a key factor in controlling virus spread. In addition, a Fuzzy Mamdani system (running at the fog server) considers the environmental risk and user health conditions to predict the risk of spreading infection in real time. The environmental risk conveys from the virtual zone concept and provides updated information for different places. Two scenarios are considered for the communication between the IoT node and fog server, 4G/5G/WiFi, or LoRa, which can be selected based on environmental constraints. The required energy usage and bandwidth (BW) are compared for various event scenarios. The COVID-SAFE framework can assist in minimizing the coronavirus exposure risk.","['COVID-19', 'Servers', 'Internet of Things', 'Bluetooth', 'Temperature sensors']","['IoT', 'health monitoring', 'smart healthcare', 'pandemic', 'COVID-19']"
"The wind power generation is a rapidly growing grid integrated renewable energy (RE) technology with an installed capacity of 539.291 GW. The capability of the wind energy conversion system (WECS) to remain integrated into the utility network in the case of low voltage events is called low-voltage ride-through (LVRT) capability. This paper offers a comprehensive overview of improvement techniques of the LVRT capability in WECS to increase the wind energy penetration level in the utility grid. Exhibited portrait manifests a broad spectrum of 1) wind turbines, 2) electrical generators used for wind power applications, 3) international grid codes applicable for grid integration of WECS, 4) LVRT fundamentals in WECS, 5) wind turbines LVRT methods by doubly fed induction generator (DFIG), 6) wind turbines LVRT methods by permanent magnet synchronous generators (PMSG), and 7) LVRT methods of wind turbines using squirrel cage induction generator (SCIG). This ready-reckoner paper critically reviews and classifies more than 190 research papers on LVRT issues, practices, and available technologies for grid integration in wind energy systems, and it aims to be a quick reference for the researchers, designers, manufacturers, and engineers working in the same field.","['Wind power generation', 'Wind turbines', 'Wind energy', 'Doubly fed induction generators', 'Mathematical model', 'Low voltage']","['DFIG', 'grid codes', 'LVRT', 'PMSG', 'SCIG', 'wind energy conversion system']"
"This paper presents an automatic content-based image retrieval (CBIR) system for brain tumors on T1-weighted contrast-enhanced magnetic resonance images (CE-MRI). The key challenge in CBIR systems for MR images is the semantic gap between the low-level visual information captured by the MRI machine and the high-level information perceived by the human evaluator. The traditional feature extraction methods focus only on low-level or high-level features and use some handcrafted features to reduce this gap. It is necessary to design a feature extraction framework to reduce this gap without using handcrafted features by encoding/combining low-level and high-level features. Deep learning is very powerful for feature representation that can depict low-level and high-level information completely and embed the phase of feature extraction in self-learning. Therefore, we propose a deep convolutional neural network VGG19-based novel feature extraction framework and apply closed-form metric learning to measure the similarity between the query image and database images. Furthermore, we adopt transfer learning and propose a block-wise fine-tuning strategy to enhance the retrieval performance. The extensive experiments are performed on a publicly available CE-MRI dataset that consists of three types of brain tumors (i.e., glioma, meningioma, and pituitary tumor) collected from 233 patients with a total of 3064 images across the axial, coronal, and sagittal views. Our method is more generic, as we do not use any handcrafted features; it requires minimal preprocessing, tested as robust on fivefold cross-validation, can achieve a fivefold mean average precision of 96.13%, and outperforms the state-of-the-art CBIR systems on the CE-MRI dataset.","['Feature extraction', 'Tumors', 'Deep learning', 'Biomedical imaging', 'Task analysis', 'Shape', 'Measurement']","['Brain tumor retrieval', 'block-wise fine-tuning', 'closed-form metric learning', 'convolutional neural networks', 'feature extraction', 'transfer learning']"
"To improve the recognition model accuracy of crop disease leaves and locating diseased leaves, this paper proposes an improved Faster RCNN to detect healthy tomato leaves and four diseases: powdery mildew, blight, leaf mold fungus and ToMV. First, we use a depth residual network to replace VGG16 for image feature extraction so we can obtain deeper disease features. Second, the k-means clustering algorithm is used to cluster the bounding boxes. We improve the anchoring according to the clustering results. The improved anchor frame tends toward the real bounding box of the dataset. Finally, we carry out a k-means experiment with three kinds of different feature extraction networks. The experimental results show that the improved method for crop leaf disease detection had 2.71% higher recognition accuracy and a faster detection speed than the original Faster RCNN.","['Diseases', 'Feature extraction', 'Agriculture', 'Object detection', 'Clustering algorithms']","['Faster RCNN', 'disease recognition', 'deep residual network', 'K-means clustering', 'disease diagnosis']"
"An adaptive fuzzy logic (FL)-based new maximum power point (MPP) tracking (MPPT) methodology for controlling photovoltaic (PV) systems is proposed, designed, and implemented in this paper. The existing methods for implementing FL-based MPPTs lack for adaptivity with the operating point, which varies in wide range in practical PV systems with operating irradiance and ambient temperature. The new proposed adaptive FL-based MPPT (AFL-MPPT) algorithm is simple, accurate, and provides faster convergence to optimal operating point. The effectiveness and feasibility verifications of the proposed AFL-MPPT methodology are validated with considering various operating conditions at slow and fast change of solar radiation. In addition, the simplified implementation of the proposed algorithm is carried out using C-block in PSIM software environment, wherein the proposed algorithm and system are simulated. Additionally, experimental results are performed using a floating-point digital signal processing (DSP) controller (TMS320F28335) for verifying the feasibility of the proposed AFL-MPPT methodology. The results of simulations and experimental prototypes show great consistency and prove the capability of the new AFL-MPPT methodology to extract MPPT rapidly and precisely. The new proposed AFL-MPPT method achieves accurate output power of the PV system with smooth and low ripple. In addition, the new proposed AFL-MPPT method benefits fast dynamics and it reaches steady state within 0.01 s.","['Fuzzy logic', 'Maximum power point trackers', 'Mathematical model', 'Photovoltaic systems', 'Control systems', 'Design methodology']","['DSP controller', 'energy efficiency', 'fuzzy logic (FL)', 'MPPT', 'photovoltaic systems']"
"Real-time scene parsing through object detection running on an embedded device is very challenging, due to limited memory and computing power of embedded devices. To deal with these challenges, we redesign a lightweight network without notably reducing detection accuracy. Based on the Darknet-53, we use depth separable convolutions and pointwise group convolutions to reduce the parameter size of the network. A feature extraction backbone network with a parameter size of only 16 percent of darknet-53 is constructed. Meanwhile, in order to compensate for the degradation of accuracy, we have added a Multi-Scale Feature Pyramid Network based on a simple U-shaped structure to improve the performance of multi-scale object detection, which called it Mini-YOLOv3. It has smaller model size and fewer trainable parameters and floating point operations (FLOPs) in comparison of YOLOv3. We evaluate Mini-YOLOv3 on MS-COCO benchmark dataset; The parameter size of Mini-YOLOv3 is only 23% of YOLOv3 and achieves comparable detection accuracy as YOLOv3 but only requires 1/2 detect time, Specifically, Mini-YOLOv3 achieves mAP-50 of 52.1 at speed of 67 fps.","['Feature extraction', 'Object detection', 'Real-time systems', 'Detectors', 'Performance evaluation', 'Standards', 'Computational modeling']","['Real-time object detector', 'embedded applications', 'convolutional neural network (CNN)', 'YOLOv3']"
"Software-defined networking (SDN) is a novel network paradigm that enables flexible management for networks. As the network size increases, the single centralized controller cannot meet the increasing demand for flow processing. Thus, the promising solution for SDN with large-scale networks is the multi-controller. In this paper, we present a compressive survey for multi-controller research in SDN. First, we introduce the overview of multi-controller, including the origin of multi-controller and its challenges. Then, we classify multi-controller research into four aspects (scalability, consistency, reliability, and load balancing) depending on the process of implementing the multi-controller. Finally, we propose some relevant research issues to deal with in the future and conclude the multi-controller research.","['Switches', 'Scalability', 'Reliability', 'Process control', 'Routing', 'Load management']","['Software-defined networking', 'multi-controller', 'scalability', 'consistency', 'reliability', 'load balancing']"
"The use of unmanned aerial vehicles (UAVs) has been considered to be an efficient platform for monitoring critical infrastructures spanning over geographical areas. UAVs have also demonstrated exceptional feasibility when collecting data due to the wide wireless sensor networks in which they operate. Based on environmental information such as prohibited airspace, geo-locational conditions, flight risk, and sensor deployment statistics, we developed an optimal flight path planning mechanism by using multiobjective bio-inspired algorithms. In this paper, we first acquire data sensing points from the entire sensor field, in which UAV communicates with sensors to obtain sensor data, then we determine the best flight path between neighboring acquisition points. Using the proposed joint genetic algorithm and ant colony optimization from possible UAV flight paths, an optimal one is selected in accordance with sensing, energy, time, and risk utilities. The simulation results show that our method can obtain dynamic environmental adaptivity and high utility in various practical situations.","['Sensors', 'Path planning', 'Unmanned aerial vehicles', 'Three-dimensional displays', 'Genetic algorithms', 'Topology', 'Energy consumption']","['Bio-inspired algorithms', 'multi-objectives', 'optimal path', 'sensor networks', 'unmanned aerial vehicle']"
"Recent advancements in the Internet of Health Things (IoHT) have ushered in the wide adoption of IoT devices in our daily health management. For IoHT data to be acceptable by stakeholders, applications that incorporate the IoHT must have a provision for data provenance, in addition to the accuracy, security, integrity, and quality of data. To protect the privacy and security of IoHT data, federated learning (FL) and differential privacy (DP) have been proposed, where private IoHT data can be trained at the owner’s premises. Recent advancements in hardware GPUs even allow the FL process within smartphone or edge devices having the IoHT attached to their edge nodes. Although some of the privacy concerns of IoHT data are addressed by FL, fully decentralized FL is still a challenge due to the lack of training capability at all federated nodes, the scarcity of high-quality training datasets, the provenance of training data, and the authentication required for each FL node. In this paper, we present a lightweight hybrid FL framework in which blockchain smart contracts manage the edge training plan, trust management, and authentication of participating federated nodes, the distribution of global or locally trained models, the reputation of edge nodes and their uploaded datasets or models. The framework also supports the full encryption of a dataset, the model training, and the inferencing process. Each federated edge node performs additive encryption, while the blockchain uses multiplicative encryption to aggregate the updated model parameters. To support the full privacy and anonymization of the IoHT data, the framework supports lightweight DP. This framework was tested with several deep learning applications designed for clinical trials with COVID-19 patients. We present here the detailed design, implementation, and test results, which demonstrate strong potential for wider adoption of IoHT-based health management in a secure way.","['Data models', 'Training', 'Data privacy', 'Security', 'Blockchain', 'Deep learning', 'Computational modeling']","['Blockchain', 'Internet of Health Things', 'homomorphic encryption', 'federated learning', 'provenance']"
"The smart grid (SG) paradigm is the next technological leap of the conventional electrical grid, contributing to the protection of the physical environment and providing multiple advantages such as increased reliability, better service quality, and the efficient utilization of the existing infrastructure and the renewable energy resources. However, despite the fact that it brings beneficial environmental, economic, and social changes, the existence of such a system possesses important security and privacy challenges, since it includes a combination of heterogeneous, co-existing smart, and legacy technologies. Based on the rapid evolution of the cyber-physical systems (CPS), both academia and industry have developed appropriate measures for enhancing the security surface of the SG paradigm using, for example, integrating efficient, lightweight encryption and authorization mechanisms. Nevertheless, these mechanisms may not prevent various security threats, such as denial of service (DoS) attacks that target on the availability of the underlying systems. An efficient countermeasure against several cyberattacks is the intrusion detection and prevention system (IDPS). In this paper, we examine the contribution of the IDPSs in the SG paradigm, providing an analysis of 37 cases. More detailed, these systems can be considered as a secondary defense mechanism, which enhances the cryptographic processes, by timely detecting or/and preventing potential security violations. For instance, if a cyberattack bypasses the essential encryption and authorization mechanisms, then the IDPS systems can act as a secondary protection service, informing the system operator for the presence of the specific attack or enabling appropriate preventive countermeasures. The cases we study focused on the advanced metering infrastructure (AMI), supervisory control and data acquisition (SCADA) systems, substations, and synchrophasors. Based on our comparative analysis, the limitations and the shortcomings of the current IDPS systems are identified, whereas appropriate recommendations are provided for future research efforts.","['Computer crime', 'Substations', 'Smart grids', 'Intrusion detection', 'Encryption', 'Smart meters']","['Advanced metering infrastructure', 'cyberattacks', 'intrusion detection system', 'intrusion prevention system', 'SCADA', 'security', 'smart grid', 'substation', 'synchrophasor']"
"In every aspect of human life, sound plays an important role. From personal security to critical surveillance, sound is a key element to develop the automated systems for these fields. Few systems are already in the market, but their efficiency is a point of concern for their implementation in real-life scenarios. The learning capabilities of the deep learning architectures can be used to develop the sound classification systems to overcome efficiency issues of the traditional systems. Our aim, in this paper, is to use the deep learning networks for classifying the environmental sounds based on the generated spectrograms of these sounds. We used the spectrogram images of environmental sounds to train the convolutional neural network (CNN) and the tensor deep stacking network (TDSN). We used two datasets for our experiment: ESC-10 and ESC-50. Both systems were trained on these datasets, and the achieved accuracy was 77% and 49% in CNN and 56% in TDSN trained on the ESC-10. From this experiment, it is concluded that the proposed approach for sound classification using the spectrogram images of sounds can be efficiently used to develop the sound classification and recognition systems.","['Spectrogram', 'Stacking', 'Convolutional neural networks', 'Deep learning', 'Feature extraction', 'Computer architecture']","['Deep learning', 'convolutional neural network', 'tensor deep stacking networks', 'spectrograms']"
"Direction of arrival (DOA) estimation from the perspective of sparse signal representation has attracted tremendous attention in past years, where the underlying spatial sparsity reconstruction problem is linked to the compressive sensing (CS) framework. Although this is an area with ongoing intensive research and new methods and results are reported regularly, it is time to have a review about the basic approaches and methods for CS-based DOA estimation, in particular for the underdetermined case. We start from the basic time-domain CS-based formulation for narrowband arrays and then move to the case for recently developed methods for sparse arrays based on the co-array concept. After introducing two specifically designed structures (the two-level nested array and the co-prime array) for optimizing the virtual sensors corresponding to the difference co-array, this CS-based DOA estimation approach is extended to the wideband case by employing the group sparsity concept, where a much larger physical aperture can be achieved by allowing a larger unit inter-element spacing and therefore leading to further improved performance. Finally, a specifically designed uniform linear array structure with associated CS-based underdetermined DOA estimation is presented to exploit the difference co-array concept in the spatio-spectral domain, leading to a significant increase in degrees of freedom. Representative simulation results for typical narrowband and wideband scenarios are provided to demonstrate their performance.","['Direction-of-arrival estimation', 'Estimation', 'Sensor arrays', 'Wideband', 'Narrowband', 'Signal reconstruction']","['Compressive sensing', 'direction of arrival estimation', 'underdetermined', 'difference co-array', 'sparse array structures']"
"Due to the significant properties of unpredictability, ergodicity, and initial state sensitivity, chaotic system is widely used as a useful tool in image encryption. In this paper, we propose a 2-dimensional logistic-modulated-sine-coupling-logistic chaotic map (LSMCL), where we use the logistic map to modulate Sine map and couple the result of modulation and Sine map together. In terms of chaotic trajectory, Lyapunov exponent, and Kolmogorov entropy, comparing with other existing chaotic maps, we can observe that LSMCL has better chaotic performance. Furthermore, we propose an LSMCL-based image encryption algorithm with two rounds of permutation and diffusion operation. First, we provide a secret key generation procedure to generate the initial values and do permutation operation with the chaotic matrix by LSMCL. Furthermore, in diffusion procedure, we use two different chaotic matrices generated by LSMCL to change the pixel values in row and column. Finally, we provide some theoretical analyses and simulations to confirm the security and the validity of the proposed algorithm.","['Chaotic communication', 'Logistics', 'Encryption', 'Two dimensional displays', 'Trajectory', 'Modulation']","['Coupling', 'logistic map', 'modulation', 'Sine map', 'hyper-chaotic system']"
"Brain-computer interface provides a new communication bridge between the human mind and devices, depending largely on the accurate classification and identification of non-invasive EEG signals. Recently, the deep learning approaches have been widely used in many fields to extract features and classify various types of data successfully. However, the deep learning approach requires massive data to train its neural networks, and the amount of data impacts greatly on the quality of the classifiers. This paper proposes a novel approach that combines deep learning and data augmentation for EEG classification. We applied the empirical mode decomposition on the EEG frames and mixed their intrinsic mode functions to create new artificial EEG frames, followed by transforming all EEG data into tensors as inputs of the neural network by complex Morlet wavelets. We proposed two neural networks-convolutional neural network and wavelet neural network-to train the weights and classify two classes of motor imagery signals. The wavelet neural network is a new type of neural network using wavelets to replace the convolutional layers. The experimental results show that the artificial EEG frames substantially improve the training of neural networks, and both two networks yield relatively higher classification accuracies compared to prevailing approaches. Meanwhile, we also verified the performance of our new proposed wavelet neural network model in the classification of steady-state visual evoked potentials.","['Electroencephalography', 'Brain modeling', 'Deep learning', 'Training', 'Feature extraction', 'Biological neural networks']","['Motor imagery classification', 'deep learning', 'convolutional neural network', 'wavelet neural network', 'empirical mode decomposition', 'artificial EEG frames']"
"Tomato leaf disease seriously affects the yield of tomato. It is extremely vital for agricultural economy to identify agricultural diseases. The traditional data augmentation methods, such as rotation, flip and translation, are severely limited, which cannot achieve good generalization results. To improve the recognition accuracy of tomato leaf diseases, a new method of data augmentation by generative adversarial networks (GANs) is proposed for leaf disease recognition in this work. Generated images augmented by deep convolutional generative adversarial networks (DCGAN) and original images as the input of GoogLeNet, this model can achieve a top-1 average identification accuracy of 94.33%. By adjusting the hyper-parameters, modifying the architecture of the convolutional neural networks, and selecting different generative adversarial networks, an improved model for training and testing 5 classes of tomato leaf images was obtained. Meanwhile, images generated by DCGAN not only enlarge the size of the data set, but also have the characteristics of diversity, which makes the model have a good generalization effect. We have also visually confirmed that the images generated by DCGAN have much better quality and are more convincing through the t-Distributed Stochastic Neighbor Embedding (t-SNE) and Visual Turing Test. Experiments with tomato leaf disease identification show that DCGAN can generate data that approximate to real images, which can be used to (1) provide a larger data set for the training of large neural networks, and improve the performance of the recognition model through highly discriminating image generation technology; (2) reduce the cost of data collection; (3) enhance the diversity of data and the generalization ability of the recognition models.","['Diseases', 'Training', 'Generative adversarial networks', 'Gallium nitride', 'Data models', 'Image recognition', 'Neural networks']","['Tomato leaf disease', 'data augmentation', 'generative adversarial networks', 'generalization', 'recognition accuracy']"
"Epilepsy is a health problem that seriously affects the quality of humans for many years. Therefore, it is important to accurately analyze and recognize epilepsy based on EEG signals, and for a long time, researchers have attempted to extract new features from the signals for epilepsy recognition. However, it is very difficult to select useful features from a large number of them in this diagnostic application. As the development of artificial intelligence progresses, unsupervised feature learning based on the deep learning model can obtain features that can better describe identified objects from unlabeled data. In this paper, the deep convolution network and autoencoders-based model, named as AE-CDNN, is constructed in order to perform unsupervised feature learning from EEG in epilepsy. We extract features by AE-CDNN model and classify the features based on two public EEG data sets. Experimental results showed that the classification results of features obtained by AE-CDNN are more optimal than features obtained by principal component analysis and sparse random projection. Using several common classifiers to classify features obtained by AE-CDNN model results in high accuracy and not inferior to the research results from most recent studies. The results also showed that the features of AE-CDNN model are clear, effective, and easy to learn. These features can speed up the convergence and reduce the training times of classifiers. Therefore, the AE-CDNN model can be effectively applied to feature extraction of EEG in epilepsy.","['Electroencephalography', 'Feature extraction', 'Brain modeling', 'Convolution', 'Epilepsy', 'Training', 'Deconvolution']","['EEG', 'unsupervised learning', 'feature extraction', 'CNN', 'epileptic seizure']"
"Alzheimer's disease (AD) is an irreversible progressive neurodegenerative disorder. Mild cognitive impairment (MCI) is the prodromal state of AD, which is further classified into a progressive state (i.e., pMCI) and a stable state (i.e., sMCI). With the development of deep learning, the convolutional neural networks (CNNs) have made great progress in image recognition using magnetic resonance imaging (MRI) and positron emission tomography (PET) for AD diagnosis. However, due to the limited availability of these imaging data, it is still challenging to effectively use CNNs for AD diagnosis. Toward this end, we design a novel deep learning framework. Specifically, the virtues of 3D-CNN and fully stacked bidirectional long short-term memory (FSBi-LSTM) are exploited in our framework. First, we design a 3D-CNN architecture to derive deep feature representation from both MRI and PET. Then, we apply FSBi-LSTM on the hidden spatial information from deep feature maps to further improve its performance. Finally, we validate our method on the AD neuroimaging initiative (ADNI) dataset. Our method achieves average accuracies of 94.82%, 86.36%, and 65.35% for differentiating AD from normal control (NC), pMCI from NC, and sMCI from NC, respectively, and outperforms the related algorithms in the literature.","['Magnetic resonance imaging', 'Three-dimensional displays', 'Feature extraction', 'Kernel', 'Deep learning', 'Diseases', 'Two dimensional displays']","['Alzheimer’s disease', '3D-CNN', 'FSBi-LSTM', 'multi-modal fusion']"
"Physical unclonable functions (PUFs) are increasingly used for authentication and identification applications as well as the cryptographic key generation. An important feature of a PUF is the reliance on minute random variations in the fabricated hardware to derive a trusted random key. Currently, most PUF designs focus on exploiting process variations intrinsic to the CMOS technology. In recent years, progress in emerging nanoelectronic devices has demonstrated an increase in variation as a consequence of scaling down to the nanoregion. To date, emerging PUFs with nanotechnology have not been fully established, but they are expected to emerge. Initial research in this area aims to provide security primitives for emerging integrated circuits with nanotechnology. In this paper, we review emerging nanotechnology-based PUFs.","['Nanotechnology', 'Cloning', 'Physical unclonable functions', 'Nanoscale devices', 'Object recognition', 'CMOS integrated circuits', 'Computer security', 'Private key cryptography', 'Cryptography']","['Physical unclonable functions', 'hardware security', 'nanoelectronic devices', 'nanotechnology', 'reconfigurable PUF', 'strong PUF']"
"In this paper, we present a deep learning based method for blind hyperspectral unmixing in the form of a neural network autoencoder. We show that the linear mixture model implicitly puts certain architectural constraints on the network, and it effectively performs blind hyperspectral unmixing. Several different architectural configurations of both shallow and deep encoders are evaluated. Also, deep encoders are tested using different activation functions. Furthermore, we investigate the performance of the method using three different objective functions. The proposed method is compared to other benchmark methods using real data and previously established ground truths of several common data sets. Experiments show that the proposed method compares favorably to other commonly used hyperspectral unmixing methods and exhibits robustness to noise. This is especially true when using spectral angle distance as the network's objective function. Finally, results indicate that a deeper and a more sophisticated encoder does not necessarily give better results.","['Linear programming', 'Decoding', 'Hyperspectral imaging', 'Machine learning', 'Neural networks', 'Spatial resolution']","['Hyperspectral unmixing', 'autoencoder', 'deep learning', 'neural network', 'spectral angle distance', 'endmember extraction']"
"Considering a future scenario in which a driverless Electric Vehicle (EV) needs an automatic charging system without human intervention. In this regard, there is a requirement for a fully automatable, fast, safe, cost-effective, and reliable charging infrastructure that provides a profitable business model and fast adoption in the electrified transportation systems. These qualities can be comprehended through wireless charging systems. Wireless Power Transfer (WPT) is a futuristic technology with the advantage of flexibility, convenience, safety, and the capability of becoming fully automated. In WPT methods resonant inductive wireless charging has to gain more attention compared to other wireless power transfer methods due to high efficiency and easy maintenance. This literature presents a review of the status of Resonant Inductive Wireless Power Transfer Charging technology also highlighting the present status and its future of the wireless EV market. First, the paper delivers a brief history throw lights on wireless charging methods, highlighting the pros and cons. Then, the paper aids a comparative review of different type’s inductive pads, rails, and compensations technologies done so far. The static and dynamic charging techniques and their characteristics are also illustrated. The role and importance of power electronics and converter types used in various applications are discussed. The batteries and their management systems as well as various problems involved in WPT are also addressed. Different trades like cyber security economic effects, health and safety, foreign object detection, and the effect and impact on the distribution grid are explored. Prospects and challenges involved in wireless charging systems are also highlighting in this work. We believe that this work could help further the research and development of WPT systems.","['Inductive charging', 'Batteries', 'Wireless communication', 'Vehicle dynamics', 'History', 'Wireless power transfer', 'Receivers']","['Electric vehicle charging', 'wireless power transfer', 'inductive wireless charging', 'magnetic resonance charging', 'compensator networks']"
"We consider the problem of spectrum sharing in a cognitive radio system consisting of a primary user and a secondary user. The primary user and the secondary user work in a non-cooperative manner. Specifically, the primary user is assumed to update its transmitted power based on a pre-defined power control policy. The secondary user does not have any knowledge about the primary user's transmit power, or its power control strategy. The objective of this paper is to develop a learning-based power control method for the secondary user in order to share the common spectrum with the primary user. To assist the secondary user, a set of sensor nodes are spatially deployed to collect the received signal strength information at different locations in the wireless environment. We develop a deep reinforcement learning-based method, which the secondary user can use to intelligently adjust its transmit power such that after a few rounds of interaction with the primary user, both users can transmit their own data successfully with required qualities of service. Our experimental results show that the secondary user can interact with the primary user efficiently to reach a goal state (defined as a state in which both users can successfully transmit their data) from any initial states within a few number of steps.","['Power control', 'Interference', 'Receivers', 'Cognitive radio', 'Signal to noise ratio', 'Machine learning', 'Quality of service']","['Spectrum sharing', 'power control', 'cognitive radio', 'deep reinforcement learning']"
"Prognostics and health management is a promising methodology to cope with the risks of failure in advance and has been implemented in many well-known applications including battery systems. Since the estimation of battery capacity is critical for safe operation and decision making, battery capacity should be estimated precisely. In this regard, we leverage measurable data such as voltage, current, and temperature profiles from the battery management system whose patterns vary in cycles as aging. Based on these data, the relationship between capacity and charging profiles is learned by neural networks. Specifically, to estimate the state of health accurately we apply feedforward neural network, convolutional neural network, and long short-term memory. Our results show that the proposed multi-channel technique based on voltage, current, and temperature profiles outperforms the conventional method that uses only voltage profile by up to 25%-58% in terms of mean absolute percentage error.","['Estimation', 'Aging', 'Lithium-ion batteries', 'Battery charge measurement', 'Temperature measurement', 'Temperature distribution']","['Lithium-ion battery', 'neural network', 'remaining useful life', 'capacity estimation', 'state of health']"
"The future wireless networks promise to provide ubiquitous connectivity to a multitude of devices with diversified traffic patterns wherever and whenever needed. For the sake of boosting resilience against faults, natural disasters, and unexpected traffic, the unmanned aerial vehicle (UAV)-assisted wireless communication systems can provide a unique opportunity to cater for such demands in a timely fashion without relying on the overly engineered cellular network. However, for UAV-assisted communication, issues of capacity, coverage, and energy efficiency are considered of paramount importance. The case of non-orthogonal multiple access (NOMA) is investigated for aerial base station (BS). NOMA's viability is established by formulating the sum-rate problem constituting a function of power allocation and UAV altitude. The optimization problem is constrained to meet individual user-rates arisen by orthogonal multiple access (OMA) bringing it at par with NOMA. The relationship between energy efficiency and altitude of a UAV inspires the solution to the aforementioned problem considering two cases, namely, altitude fixed NOMA and altitude optimized NOMA. The latter allows exploiting the extra degrees of freedom of UAV-BS mobility to enhance the spectral efficiency and the energy efficiency. Hence, it saves joules in the operational cost of the UAV. Finally, a constrained coverage expansion methodology, facilitated by NOMA user rate gain is also proposed. Results are presented for various environment settings to conclude NOMA manifesting better performance in terms of sum-rate, coverage, and energy efficiency.","['NOMA', 'Unmanned aerial vehicles', 'Energy consumption', 'Propagation losses', 'Channel models', 'Wireless networks']","['Non-orthogonal multiple access (NOMA)', 'orthogonal multiple access (OMA)', 'unmanned aerial vehicle (UAV)', 'sum-rate maximization', 'coverage maximization', 'aerial cells', 'energy efficiency']"
"This paper investigates the secrecy performance of a two-user downlink non-orthogonal multiple access systems. Both single-input and single-output and multiple-input and single-output systems with different transmit antenna selection (TAS) strategies are considered. Depending on whether the base station has the global channel state information of both the main and wiretap channels, the exact closed-form expressions for the secrecy outage probability (SOP) with suboptimal antenna selection and optimal antenna selection schemes are obtained and compared with the traditional space-time transmission scheme. To obtain further insights, the asymptotic analysis of the SOP in high average channel power gains regime is presented and it is found that the secrecy diversity order for all the TAS schemes with fixed power allocation is zero. Furthermore, an effective power allocation scheme is proposed to obtain the non-zero diversity order with all the TAS schemes. Monte Carlo simulations are performed to verify the proposed analytical results.","['NOMA', 'Resource management', 'Transmitting antennas', 'Base stations', 'MISO', 'Signal to noise ratio']","['Non-orthogonal multiple access', 'physical layer security', 'transmit antenna selection', 'secrecy outage probability']"
"Owing to the explosive expansion of wireless communication and networking technologies, cost-effective unmanned aerial vehicles (UAVs) have recently emerged and soon they will occupy the major part of our sky. UAVs can be exploited to efficiently accomplish complex missions when cooperatively organized as an ad hoc network, thus creating the well-known flying ad hoc networks (FANETs). The establishment of such networks is not feasible without deploying an efficient networking model allowing a reliable exchange of information between UAVs. FANET inherits common features and characteristics from mobile ad hoc networks (MANETs) and their sub-classes, such as vehicular ad hoc networks (VANETs) and wireless sensor networks (WSNs). Unfortunately, UAVs are often deployed in the sky adopting a mobility model dictated by the nature of missions that they are expected to handle, and therefore, differentiate themselves from any traditional networks. Moreover, several flying constraints and the highly dynamic topology of FANETs make the design of routing protocols a complicated task. In this paper, a comprehensive survey is presented covering the architecture, the constraints, the mobility models, the routing techniques, and the simulation tools dedicated to FANETs. A classification, descriptions, and comparative studies of an important number of existing routing protocols dedicated to FANETs are detailed. Furthermore, the paper depicts future challenge perspectives, helping scientific researchers to discover some themes that have been addressed only ostensibly in the literature and need more investigation. The novelty of this survey is its uniqueness to provide a complete analysis of the major FANET routing protocols and to critically compare them according to different constraints based on crucial parameters, thus better presenting the state of the art of this specific area of research.","['Routing protocols', 'Ad hoc networks', 'Routing', 'Wireless sensor networks', 'Wireless communication', 'Unmanned aerial vehicles', 'Organizations']","['UAV', 'FANET', 'mobility', 'simulation', 'routing protocols']"
"The research area of ambient assisted living has led to the development of activity recognition systems (ARS) based on human activity recognition (HAR). These systems improve the quality of life and the health care of the elderly and dependent people. However, before making them available to end users, it is necessary to evaluate their performance in recognizing activities of daily living, using data set benchmarks in experimental scenarios. For that reason, the scientific community has developed and provided a huge amount of data sets for HAR. Therefore, identifying which ones to use in the evaluation process and which techniques are the most appropriate for prediction of HAR in a specific context is not a trivial task and is key to further progress in this area of research. This work presents a systematic review of the literature of the sensor-based data sets used to evaluate ARS. On the one hand, an analysis of different variables taken from indexed publications related to this field was performed. The sources of information are journals, proceedings, and books located in specialized databases. The analyzed variables characterize publications by year, database, type, quartile, country of origin, and destination, using scientometrics, which allowed identification of the data set most used by researchers. On the other hand, the descriptive and functional variables were analyzed for each of the identified data sets: occupation, annotation, approach, segmentation, representation, feature selection, balancing and addition of instances, and classifier used for recognition. This paper provides an analysis of the sensor-based data sets used in HAR to date, identifying the most appropriate dataset to evaluate ARS and the classification techniques that generate better results.","['Intelligent sensors', 'Feature extraction', 'Benchmark testing', 'Activity recognition', 'Monitoring', 'Object recognition']","['Ambient assisted living–AAL', 'human activity recognition–HAR', 'activities of daily living–ADL', 'activity recognition systems–ARS', 'dataset']"
"Smart contracts are programs that reside within decentralized blockchains and are executed pursuant to triggered instructions. A smart contract acts in a similar way to a traditional agreement but negates the necessity for the involvement of a third party. Smart contracts are capable of initiating their commands automatically, thus eliminating the involvement of a regulatory body. As a consequence of blockchain's immutable feature, smart contracts are developed in a manner that is distinct from traditional software. Once deployed to the blockchain, a smart contract cannot be modified or updated for security patches, thus encouraging developers to implement strong security strategies before deployment in order to avoid potential exploitation at a later time. However, the most recent dreadful attacks and the multifarious existing vulnerabilities which result as a consequence of the absence of security patches have challenged the sustainability of this technology. Attacks such as the Decentralized Autonomous Organization (DAO) attack and the Parity Wallet hack have cost millions of dollars simply as a consequence of naïve bugs in the smart contract code. In this paper, we classify blockchain exploitation techniques into 4 categories based on the attack rationale; attacking consensus protocols, bugs in the smart contract, malware running in the operating system, and fraudulent users. We then focus on smart contract vulnerabilities, analyzing the 7 most important attack techniques to determine the real impact on smart contract technology. We reveal that even adopting the 10 most widely used tools to detect smart contract vulnerabilities, these still contain known vulnerabilities, providing a dangerously false sense of security. We conclude the paper with a discussion about recommendations and future research lines to progress towards a secure smart contract solution.","['Smart contracts', 'Blockchain', 'Bitcoin', 'Computer bugs']","['Smart contracts', 'attack techniques', 'DApp', 'Ethereum', 'vulnerability']"
"Nowadays, motor imagery (MI) electroencephalogram (EEG) signal classification has become a hotspot in the research field of brain computer interface (BCI). More recently, deep learning has emerged as a promising technique to automatically extract features of raw MI EEG signals and then classify them. However, deep learning-based methods still face two challenging problems in practical MI EEG signal classification applications: (1) Generally, training a deep learning model successfully needs a large amount of labeled data. However, most of the EEG signal data is unlabeled and it is quite difficult or even impossible for human experts to label all the signal samples manually. (2) It is extremely time-consuming and computationally expensive to train a deep learning model from scratch. To cope with these two challenges, a deep transfer convolutional neural network (CNN) framework based on VGG-16 is proposed for EEG signal classification. The proposed framework consists of a VGG-16 CNN model pre-trained on the ImageNet and a target CNN model which shares the same structure with VGG-16 except for the softmax output layer. The parameters of the pre-trained VGG-16 CNN model are directly transferred to the target CNN model used for MI EEG signal classification. Then, front-layers parameters in the target model are frozen, while later-layers parameters are fine-tuned by the target MI dataset. The target dataset is composed of time-frequency spectrum images of EEG signals. The performance of the proposed framework is verified on the public benchmark dataset 2b from the BCI competition IV. The experimental results show that the proposed framework improves the accuracy and efficiency performance of EEG signal classification compared with traditional methods, including support vector machine (SVM), artificial neural network (ANN), and standard CNN.","['Electroencephalography', 'Feature extraction', 'Brain modeling', 'Task analysis', 'Computational modeling', 'Time-frequency analysis', 'Deep learning']","['Motor imagery (MI)', 'electroencephalogram (EEG)', 'signal classification', 'short time Fourier transform (STFT)', 'VGG-16', 'transfer learning']"
"Mobile cellular networks have become both the generators and carriers of massive data. Big data analytics can improve the performance of mobile cellular networks and maximize the revenue of operators. In this paper, we introduce a unified data model based on the random matrix theory and machine learning. Then, we present an architectural framework for applying the big data analytics in the mobile cellular networks. Moreover, we describe several illustrative examples, including big signaling data, big traffic data, big location data, big radio waveforms data, and big heterogeneous data, in mobile cellular networks. Finally, we discuss a number of open research challenges of the big data analytics in the mobile cellular networks.","['Cellular networks', 'Mobile communication', 'Big data', 'Data analytics', 'Random matrix theory', 'Machine learning']","['Big data analytics', 'mobile cellular networks']"
"Electronic medical records can help people prevent diseases, improve cure rates, provide a significant basis for medical institutions and pharmaceutical companies, and provide legal evidence for medical negligence and medical disputes. However, the integrity and security problems of electronic medical data still intractable. In this paper, based on the ciphertext policy attribute-based encryption system and IPFS storage environment, combined with blockchain technology, we constructed an attribute-based encryption scheme for secure storage and efficient sharing of electronic medical records in IPFS storage environment. Our scheme is based on ciphertext policy attribute encryption, which effectively controls the access of electronic medical data without affecting efficient retrieval. Meanwhile, we store the encrypted electronic medical data in the decentralized InterPlanetary File System (IPFS), which not only ensures the security of the storage platform but also solves the problem of the single point of failure. Besides, we leverage the non-tamperable and traceable nature of blockchain technology to achieve secure storage and search for medical data. The security proof shows that our scheme achieves selective security for the choose keyword attacks. Performance analysis and real data set simulation experiments shows that our scheme is efficient and feasible.","['Servers', 'Medical diagnostic imaging', 'Cloud computing', 'Encryption', 'Blockchain', 'Electronic medical records']","['Access control', 'attribute-based encryption', 'blockchain', 'electronic medical records', 'InterPlanetary File System (IPFS)']"
"In the age of industry 4.0, deep learning has attracted increasing interest for various research applications. In recent years, deep learning models have been extensively implemented in machinery fault detection and diagnosis (FDD) systems. The deep architecture's automated feature learning process offers great potential to solve problems with traditional fault detection and diagnosis (TFDD) systems. TFDD relies on manual feature selection, which requires prior knowledge of the data and is time intensive. However, the high performance of deep learning comes with challenges and costs. This paper presents a review of deep learning challenges related to machinery fault detection and diagnosis systems. The potential for future work on deep learning implementation in FDD systems is briefly discussed.","['Deep learning', 'Fault detection', 'Machinery', 'Computational modeling', 'Feature extraction', 'Training', 'Vibrations']","['Deep learning', 'fault detection and diagnosis', 'current challenges', 'future developments']"
"A smart home network will support various smart devices and applications, e.g., home automation devices, E-health devices, regular computing devices, and so on. Most devices in a smart home access the Internet through a home gateway (HGW). In this paper, we propose a software-definednetwork (SDN)-HGW framework to better manage distributed smart home networks and support the SDN controller of the core network. The SDN controller enables efficient network quality-of-service management based on real-time traffic monitoring and resource allocation of the core network. However, it cannot provide network management in distributed smart homes. Our proposed SDN-HGW extends the control to the access network, i.e., a smart home network, for better end-to-end network management. Specifically, the proposed SDN-HGW can achieve distributed application awareness by classifying data traffic in a smart home network. Most existing traffic classification solutions, e.g., deep packet inspection, cannot provide real-time application awareness for encrypted data traffic. To tackle those issues, we develop encrypted data classifiers (denoted as DataNets) based on three deep learning schemes, i.e., multilayer perceptron, stacked autoencoder, and convolutional neural networks, using an open data set that has over 200 000 encrypted data samples from 15 applications. A data preprocessing scheme is proposed to process raw data packets and the tested data set so that DataNet can be created. The experimental results show that the developed DataNets can be applied to enable distributed application-aware SDN-HGW in future smart home networks.","['Smart homes', 'Cryptography', 'Machine learning', 'Quality of service', 'Logic gates', 'Smart devices']","['Encrypted traffic classification', 'home gateway', 'distributed network management', 'deep learning', 'SDN']"
"Financial news has been proven to be a crucial factor which causes fluctuations in stock prices. However, previous studies heavily relied on analyzing shallow features and ignored the structural relation among words in a sentence. Several sentiment analysis studies have tried to point out the relationship between investors' reaction and news events. However, the sentiment dataset was usually constructed from the lingual dataset which is unrelated to the financial sector and led to poor performance. This paper proposes a novel framework to predict the directions of stock prices by using both financial news and sentiment dictionary. The original contributions of this paper include the proposal of a novel two-stream gated recurrent unit network and Stock2Vec-a sentiment word embedding trained on financial news dataset and Harvard IV-4. Two main experiments are conducted: the first experiment predicts S&P 500 index stock price directions using the historical S&P 500 prices and the articles crawled from Reuters and Bloomberg, and the second experiment forecasts the price trends of VN-index using VietStock news and stock prices from cophieu68. Results show that: 1) two-stream GRU outperforms state-of-the-art models; 2) Stock2Vec is more efficient in dealing with financial datasets; and 3) applying the model, a simulation scenario proves that our model is effective for the stock sector.","['Dictionaries', 'Sentiment analysis', 'Market research', 'Feature extraction', 'Machine learning', 'Internet']","['Deep learning', 'natural language processing', 'stock trends', 'sentiment analysis']"
"In this paper, a new dense dielectric (DD) patch array antenna prototype operating at 28 GHz for future fifth generation (5G) cellular networks is presented. This array antenna is proposed and designed with a standard printed circuit board process to be suitable for integration with radio frequency/microwave circuitry. The proposed structure employs four circular-shaped DD patch radiator antenna elements fed by a 1-to-4 Wilkinson power divider. To improve the array radiation characteristics, a ground structure based on a compact uniplanar electromagnetic bandgap unit cell has been used. The DD patch shows better radiation and total efficiencies compared with the metallic patch radiator. For further gain improvement, a dielectric layer of a superstrate is applied above the array antenna. The measured impedance bandwidth of the proposed array antenna ranges from 27 to beyond 32 GHz for a reflection coefficient (S11) of less than -10 dB. The proposed design exhibits stable radiation patterns over the whole frequency band of interest, with a total realized gain more than 16 dBi. Due to the remarkable performance of the proposed array, it can be considered as a good candidate for 5G communication applications.","['Dielectrics', 'Microwave antenna arrays', 'Antenna arrays', 'Antenna measurements', 'Prototypes', 'Radiators', 'Mobile communication', 'Electromagnetic band gap', 'Cellular networks', 'Patch antennas']","['Dense dielectric (DD) patch', 'superstrate', 'Wilkinson power divider', 'fifth generation (5G) wireless communications', 'printed circuit board (PCB)', 'electromagnetic bandgap (EBG)']"
"With the emergence of industry 4.0, the oil and gas (O&G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&G project life cycles. The deployment of emerging technologies allows O&G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&G industry. When considering the geographical distribution for the DT related research in the O&G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&G industry is still at an early stage.","['Industries', 'Digital twin', 'Data models', 'Bibliographies', 'Oils', 'Market research', 'Companies']","['Digitalization', 'digital twin (DT)', 'industry 4.0', 'extended reality', 'industrial Internet of Things (IIoT)', 'oil and gas']"
"Recently, smart cities, smart homes, and smart medical systems have challenged the functionality and connectivity of the large-scale Internet of Things (IoT) devices. Thus, with the idea of offloading intensive computing tasks from them to edge nodes (ENs), edge computing emerged to supplement these limited devices. Benefit from this advantage, IoT devices can save more energy and still maintain the quality of the services they should provide. However, computational offload decisions involve federation and complex resource management and should be determined in the real-time face to dynamic workloads and radio environments. Therefore, in this work, we use multiple deep reinforcement learning (DRL) agents deployed on multiple edge nodes to indicate the decisions of the IoT devices. On the other hand, with the aim of making DRL-based decisions feasible and further reducing the transmission costs between the IoT devices and edge nodes, federated learning (FL) is used to train DRL agents in a distributed fashion. The experimental results demonstrate the effectiveness of the decision scheme and federated learning in the dynamic IoT system.","['Task analysis', 'Computational modeling', 'Edge computing', 'Delays', 'Optimization', 'Internet of Things', 'Resource management']","['Federated learning', 'computation offloading', 'IoT', 'edge computing']"
"Today, conventional power systems are evolving to smart grids, which encompass clusters of AC/DC microgrids, interfaced through power electronics converters. In such systems, increasing penetration of the power electronics-based distributed generations, energy storages, and modern loads provide a great opportunity for power quality control. In this paper, an overview of the power quality control of smart hybrid AC/DC microgrids is presented. Different types of power quality issues are studied first, with consideration of real-world hybrid microgrid examples, including data centers, electric railway systems, and electric vehicles charging stations. It shows that compared to traditional centralized power quality compensations, smart interfacing power converters from distributed generations, energy storages, and loads, and the AC and DC subgrids interfacing converters are promising candidates for power quality control. To realize the smart interfacing converters’ power quality control, both primary converters control and secondary system coordination are required. In this paper, a thorough review of the primary control of interfacing converters to integrate the power quality compensation are presented, with a focus on the hybrid AC/DC microgrid harmonics compensation and unbalance compensation. For multiple interfacing converters, the secondary control with system-level coordination and optimization for harmonics and unbalance compensation (considering both unbalance and harmonics in single-phase and three-phase systems) are also presented. Challenges like low switching frequency of interfacing converters, parallel interfacing converters operation, and interfacing converters communications are discussed, and typical solutions for primary and secondary controls to deal with them are presented. The paper also includes rich case study results.","['Power quality', 'Microgrids', 'Power harmonic filters', 'Harmonic analysis', 'Hybrid power systems', 'Data centers']","['Harmonic compensation', 'hybrid AC/DC microgrid', 'interfacing power electronics converters', 'power quality', 'primary and secondary control', 'smart converters', 'smart grids', 'unbalance compensation']"
"This paper presents a chaotic encryption-based blind digital image watermarking technique applicable to both grayscale and color images. Discrete cosine transform (DCT) is used before embedding the watermark in the host image. The host image is divided into 8 × 8 nonoverlapping blocks prior to DCT application, and the watermark bit is embedded by modifying difference between DCT coefficients of adjacent blocks. Arnold transform is used in addition to chaotic encryption to add double-layer security to the watermark. Three different variants of the proposed algorithm have been tested and analyzed. The simulation results show that the proposed scheme is robust to most of the image processing operations like joint picture expert group compression, sharpening, cropping, and median filtering. To validate the efficiency of the proposed technique, the simulation results are compared with certain state-of-art techniques. The comparison results illustrate that the proposed scheme performs better in terms of robustness, security, and imperceptivity. Given the merits of the proposed scheme, it can be used in applications like e-healthcare and telemedicine to robustly hide electronic health records in medical images.","['Watermarking', 'Robustness', 'Discrete cosine transforms', 'Encryption', 'Payloads']","['Electronic healthcare', 'Arnold transform', 'blind watermarking', 'chaos', 'DCT', 'encryption', 'robustness']"
"The commercial fifth-generation (5G) wireless communications networks have already been deployed with the aim of providing high data rates. However, the rapid growth in the number of smart devices and the emergence of the Internet of Everything (IoE) applications, which require an ultra-reliable and low-latency communication, will result in a substantial burden on the 5G wireless networks. As such, the data rate that could be supplied by 5G networks will unlikely sustain the enormous ongoing data traffic explosion. This has motivated research into continuing to advance the existing wireless networks toward the future generation of cellular systems, known as sixth generation (6G). Therefore, it is essential to provide a prospective vision of the 6G and the key enabling technologies for realizing future networks. To this end, this paper presents a comprehensive review/survey of the future evolution of 6G networks. Specifically, the objective of the paper is to provide a comprehensive review/survey about the key enabling technologies for 6G networks, which include a discussion about the main operation principles of each technology, envisioned potential applications, current state-of-the-art research, and the related technical challenges. Overall, this paper provides useful information for industries and academic researchers and discusses the potentials for opening up new research directions.","['6G mobile communication', '5G mobile communication', 'Wireless communication', 'Long Term Evolution', 'Broadband communication', 'Multiaccess communication', 'Reliability']","['6G', 'intelligent reflecting surfaces', 'orthogonal multiple access', 'NOMA', 'rate-splitting multiple access', 'spatial modulation', 'cell-free massive MIMO', 'mmWave', 'terahertz (THz)', 'holographic radio', 'full duplex', 'energy harvesting', 'backscatter', 'edge computing', 'optical wireless communications', 'blockchain', 'artificial intelligence', 'machine learning']"
"Model predictive control (MPC) has become one of the well-established modern control methods for three-phase inverters with an output LC filter, where a high-quality voltage with low total harmonic distortion (THD) is needed. Although it is an intuitive controller, easy to understand and implement, it has the significant disadvantage of requiring a large number of online calculations for solving the optimization problem. On the other hand, the application of model-free approaches such as those based on artificial neural networks approaches is currently growing rapidly in the area of power electronics and drives. This paper presents a new control scheme for a two-level converter based on combining MPC and feed-forward ANN, with the aim of getting lower THD and improving the steady and dynamic performance of the system for different types of loads. First, MPC is used, as an expert, in the training phase to generate data required for training the proposed neural network. Then, once the neural network is fine-tuned, it can be successfully used online for voltage tracking purpose, without the need of using MPC. The proposed ANN-based control strategy is validated through simulation, using MATLAB/Simulink tools, taking into account different loads conditions. Moreover, the performance of the ANN-based controller is evaluated, on several samples of linear and non-linear loads under various operating conditions, and compared to that of MPC, demonstrating the excellent steady-state and dynamic performance of the proposed ANN-based control strategy.","['Inverters', 'Switches', 'Mathematical model', 'Neural networks', 'Voltage control', 'Training']","['Three-phase inverter', 'model predictive control', 'artificial neural network', 'UPS systems']"
"Bolted joint are among the key components that enable the robust assembly of a wide variety of structures. However, due to wear and tear over time, bolted joint may loosen, and if not detected in its early stages, can lead to devastating results. A monitoring method that can detect bolted joint looseness prior to bolt failure will be essential for the continued operation of the host structure and depending on the situation, the safety of the occupants. Prior research has proven the electromechanical impedance method (EMI) to be an effective technique for detecting the loosening of bolted joints, however, EMI-based methods until now are focused on qualitative health monitoring, which can only provide limited information about the damage. Thus, this paper attempts to quantify EMI based methods through the integration of fractal contact theory, the result of which is a novel electromechanical impedance model for quantitative monitoring of bolted looseness. The method determines the effective impedance of the bolted joint and is applied to develop the relationship between the electrical impedance of a piezoceramic patch installed on the joint and the mechanical impedance of the bolted joint. The mechanical impedance of the bolted joint under various preloads is computed by using the fractal contact theory. Then, the bolted looseness can be monitored quantitatively. At last, a set of verification tests under different applied preload of bolted joint are conducted to verify the validity of the proposed model in this paper.","['Impedance', 'Monitoring', 'Fractals', 'Electromagnetic interference', 'Strain', 'Fasteners', 'Electric fields']","['Structural health monitoring', 'bolted looseness monitoring', 'electromechanical impedance modeling', 'fractal contact theory', 'piezoceramic transducers']"
"As the population in cities continues to increase rapidly, air pollution becomes a serious issue from public health to social economy. Among all pollutants, fine particulate matters (PM2.5) directly related to various serious health concerns, e.g., lung cancer, premature death, asthma, and cardiovascular and respiratory diseases. To enhance the quality of urban living, sensors are deployed to create smart cities. In this paper, we present a participatory urban sensing framework for PM2.5 monitoring with more than 2500 devices deployed in Taiwan and 29 other countries. It is one of the largest deployment project for PM2.5 monitor in the world as we know until May 2017. The key feature of the framework is its open system architecture, which is based on the principles of open hardware, open source software, and open data. To facilitate the deployment of the framework, we investigate the accuracy issue of low-cost particle sensors with a comprehensive set of comparison evaluations to identify the most reliable sensor. By working closely with government authorities, industry partners, and maker communities, we can construct an effective eco-system for participatory urban sensing of PM2.5 particles. Based on our deployment achievements to date, we provide a number of data services to improve environmental awareness, trigger on-demand responses, and assist future government policymaking. The proposed framework is highly scalable and sustainable with the potential to facilitate the Internet of Things, smart cities, and citizen science in the future.","['Sensors', 'Monitoring', 'Lungs', 'Air pollution', 'Hardware', 'Atmospheric modeling']","['Air pollution', 'crowdsourcing', 'environmental monitoring', 'Internet of Things']"
"About half of the people who develop heart failure (HF) die within five years of diagnosis. Over the years, researchers have developed several machine learning-based models for the early prediction of HF and to help cardiologists to improve the diagnosis process. In this paper, we introduce an expert system that stacks two support vector machine (SVM) models for the effective prediction of HF. The first SVM model is linear and L_{1} regularized. It has the capability to eliminate irrelevant features by shrinking their coefficients to zero. The second SVM model is L_{2} regularized. It is used as a predictive model. To optimize the two models, we propose a hybrid grid search algorithm (HGSA) that is capable of optimizing the two models simultaneously. The effectiveness of the proposed method is evaluated using six different evaluation metrics: accuracy, sensitivity, specificity, the Matthews correlation coefficient (MCC), ROC charts, and area under the curve (AUC). The experimental results confirm that the proposed method improves the performance of a conventional SVM model by 3.3%. Moreover, the proposed method shows better performance compared to the ten previously proposed methods that achieved accuracies in the range of 57.85%–91.83%. In addition, the proposed method also shows better performance than the other state-of-the-art machine learning ensemble models.","['Support vector machines', 'Predictive models', 'Heart', 'Expert systems', 'Kernel', 'Optimization', 'Diseases']","['Clinical expert system', 'feature selection', 'heart failure prediction', 'hybrid grid search algorithm', 'support vector machine']"
"In this paper, we propose a solution to the problem of scheduling of a smart home appliance operation in a given time range. In addition to power-consuming appliances, we adopt a photovoltaic (PV) panel as a power-producing appliance that acts as a micro-grid. An appliance operation is modeled in terms of uninterruptible sequence phases, given in a load demand profile with a goal of minimizing electricity cost fulfilling duration, energy requirement, and user preference constraints. An optimization algorithm, which can provide a schedule for smart home appliance usage, is proposed based on the mixed-integer programming technique. Simulation results demonstrate the utility of our proposed solution for appliance scheduling. We further show that adding a PV system in the home results in the reduction of electricity bills and the export of energy to the national grid in times when solar energy production is more than the demand of the home.","['Smart phones', 'Household applicances', 'Scheduling', 'Home automation', 'Smart grids']","['Appliance scheduling', 'optimization', 'branch and- bound', 'smart home network', 'smart grid']"
"Mobile learning applications have been growing in demand and popularity and have become a common phenomenon in modern educational systems, especially with the implementation of mobile learning projects. This study applies the Unified Theory of Acceptance and Use Technology (UTAUT) model to examine the effects of different factors that were identified from the literature on students' acceptance of mobile learning applications in higher education. The data was collected from a 697 university students responded to an online questionnaire. SEM method was used for data analysis. The results showed that perceived information quality, perceived compatibility, perceived trust, perceived awareness, and availability of resources, self-efficacy, and perceived security are the main motivators of students' acceptance of mobile learning system, and consequently success the implementation of mobile learning projects. Results from this study provide the necessary information as to how higher education institutions can enhance students' acceptance of mobile learning system in order to support the usage of mobile technologies in learning and teaching process. These results offer important implications for mobile learning acceptance and usage.","['Learning systems', 'Education', 'Security', 'Information systems', 'Predictive models', 'Mobile applications', 'Analytical models']","['Mobile learning acceptance', 'adoption', 'information system acceptance', 'success factors', 'UTAUT model']"
"To address the problem of detecting malicious codes in malware and extracting the corresponding evidences in mobile devices, we construct a consortium blockchain framework, which is composed of a detecting consortium chain shared by test members and a public chain shared by users. Specifically, in view of different malware families in Android-based system, we perform feature modeling by utilizing statistical analysis method, so as to extract malware family features, including software package feature, permission and application feature, and function call feature. Moreover, for reducing false-positive rate and improving the detecting ability of malware variants, we design a multi-feature detection method of Android-based system for detecting and classifying malware. In addition, we establish a fact-base of distributed Android malicious codes by blockchain technology. The experimental results show that, compared with the previously published algorithms, the new proposed method can achieve higher detection accuracy in limited time with lower false-positive and false-negative rates.","['Malware', 'Feature extraction', 'Androids', 'Humanoid robots', 'Encryption', 'Mobile communication']","['Consortium Blockchain', 'malware detection', 'multi-feature']"
"Predicting the presence of Microaneurysms in the fundus images and the identification of diabetic retinopathy in early-stage has always been a major challenge for decades. Diabetic Retinopathy (DR) is affected by prolonged high blood glucose level which leads to microvascular complications and irreversible vision loss. Microaneurysms formation and macular edema in the retinal is the initial sign of DR and diagnosis at the right time can reduce the risk of non proliferated diabetic retinopathy. The rapid improvement of deep learning makes it gradually become an efficient technique to provide an interesting solution for medical image analysis problems. The proposed system analysis the presence of microaneurysm in fundus image using convolutional neural network algorithms that embeds deep learning as a core component accelerated with GPU(Graphics Processing Unit) which will perform medical image detection and segmentation with high-performance and low-latency inference. The semantic segmentation algorithm is utilized to classify the fundus picture as normal or infected. Semantic segmentation divides the image pixels based on their common semantic to identify the feature of microaneurysm. This provides an automated system that will assist ophthalmologists to grade the fundus images as early NPDR, moderate NPDR, and severe NPDR. The Prognosis of Microaneurysm and early diagnosis system for non - proliferative diabetic retinopathy system has been proposed that is capable to train effectively a deep convolution neural network for semantic segmentation of fundus images which can increase the efficiency and accuracy of NPDR (non proliferated diabetic retinopathy) prediction.","['Diabetes', 'Retinopathy', 'Retina', 'Lesions', 'Image segmentation', 'Semantics', 'Biomedical imaging']","['Microaneurysm', 'diabetic retinopathy', 'deep convolution neural network', 'semantic segmentation', 'non proliferated diabetic retinopathy']"
"This paper proposes a novel tuning design of proportional integral derivative (PID) controller via an improved kidney-inspired algorithm (IKA) with a new objective function. The main objective of the proposed approach is to optimize the transient response of the AVR system by minimizing the maximum overshoot, settling time, rise time and peak time values of the terminal voltage, and eliminating the steady state error. After obtaining the optimal values of the three gains of the PID controller (K P , K I , and K D ) with the proposed approach, the transient response analysis was performed and compared with some of the current heuristic algorithms-based approaches in literature to show the superiority of the optimized PID controller. In order to evaluate the stability of the automatic voltage regulator (AVR) system tuned by IKA method, the pole/zero map analysis and Bode analysis are performed. Finally, the robustness analysis of the proposed approach has been carried out with variations in the parameters of the AVR system. The numerical simulation results demonstrated that the proposed IKA tuned PID controller has better control performances compared to the other existing approaches. The essence of the presented study points out that the proposed approach may successfully be applied for the AVR system.","['Optimization', 'Heuristic algorithms', 'Tuning', 'Linear programming', 'Transient response', 'Voltage control', 'Generators']","['Automatic voltage regulator', 'improved kidney-inspired algorithm', 'PID tuning', 'robustness analysis', 'transient response']"
"A significant growth in solar photovoltaic (PV) installation has observed during the last decade in standalone and grid-connected power generation systems. The solar PV system has a non-linear output characteristic because of weather intermittency, which tends to have a substantial effect on overall PV system output. Hence, to optimize the output of a PV system, different maximum power point tracking (MPPT) techniques have been used. But, the confusion lies while selecting an appropriate MPPT, as every method has its own merits and demerits. Therefore, a proper review of these techniques is essential. A “Google Scholar” survey of the last five years (2015-2020) was conducted. It has found that overall seventy-one review articles are published on different MPPT techniques; out of those seventy-one, only four are on uniform solar irradiance, seven on non-uniform and none on hybrid optimization MPPT techniques. Most of them have discussed the limited number of MPPT techniques, and none of them has discussed the online and offline under uniform and hybrid MPPT techniques under non-uniform solar irradiance conditions all together in one. Unfortunately, very few attempts have made in this regard. Therefore, a comprehensive review paper on this topic is need of time, in which almost all the well-known MPPT techniques should be encapsulated in one paper. This article focuses on classifications of online, offline, and hybrid optimization MPPT algorithms, under the uniform and non-uniform irradiance conditions. It summarizes various MPPT methods along with their mathematical expression, operating principle, and block diagram/flow charts. This research will provide a valuable pathway to researchers, energy engineers, and strategists for future research and implementation in the field of maximum power point tracking optimization.","['Maximum power point trackers', 'Radiation effects', 'Optimization', 'Meteorology', 'Hybrid power systems']","['Maximum power point tracking', 'photovoltaic array', 'uniform solar irradiance', 'non-uniform solar irradiation', 'online and offline MPPT', 'hybrid MPPT methods']"
"A multimodal biometric system integrates information from more than one biometric modality to improve the performance of each individual biometric system and make the system robust to spoof attacks. In this paper, we propose a secure multimodal biometric system that uses convolution neural network (CNN) and Q-Gaussian multi support vector machine (QG-MSVM) based on a different level fusion. We developed two authentication systems with two different level fusion algorithms: a feature level fusion and a decision level fusion. The feature extraction for individual modalities is performed using CNN. In this step, we selected two layers from CNN that achieved the highest accuracy, in which each layer is regarded as separated feature descriptors. After that, we combined them using the proposed internal fusion to generate the biometric templates. In the next step, we applied one of the cancelable biometric techniques to protect these templates and increase the security of the proposed system. In the authentication stage, we applied QG-MSVM as a classifier for authentication to improve the performance. Our systems were tested on several publicly available databases for ECG and fingerprint. The experimental results show that the proposed multimodal systems are efficient, robust, and reliable than existing multimodal authentication systems.","['Electrocardiography', 'Fingerprint recognition', 'Authentication', 'Feature extraction', 'Databases', 'Convolution', 'Filtering']","['Authentication', 'CNN', 'decision fusion', 'ECG', 'feature fusion', 'fingerprint', 'multimodal']"
"An approach is proposed to reduce mutual coupling between two closely spaced radiating elements. This is achieved by inserting a fractal isolator between the radiating elements. The fractal isolator is an electromagnetic bandgap structure based on metamaterial. With this technique, the gap between radiators is reduced to ~0.65λ for the reduction in the mutual coupling of up to 37, 21, 20, and 31 dB in the X-, Ku-, K-, and Ka-bands, respectively. With the proposed technique, the two-element antenna is shown to operate over a wide frequency range, i.e., 8.7-11.7, 11.9-14.6, 15.6-17.1, 22-26, and 29-34.2 GHz. Maximum gain improvement is 71% with no deterioration in the radiation patterns. The antenna's characteristics were validated through measurement. The proposed technique can be applied retrospectively and is applicable in closely placed patch antennas in arrays found in multiple-input multiple-output and radar systems.","['Fractals', 'Isolators', 'Mutual coupling', 'Patch antennas', 'Antenna radiation patterns', 'Couplings', 'Photonic band gap']","['Fractal', 'EM bandgap', 'two-element patch antenna', 'mutual coupling reduction', 'metamaterials', 'multiple-input multiple-output (MIMO)', 'radar']"
"Diagnosis is a critical preventive step in Coronavirus research which has similar manifestations with other types of pneumonia. CT scans and X-rays play an important role in that direction. However, processing chest CT images and using them to accurately diagnose COVID-19 is a computationally expensive task. Machine Learning techniques have the potential to overcome this challenge. This article proposes two optimization algorithms for feature selection and classification of COVID-19. The proposed framework has three cascaded phases. Firstly, the features are extracted from the CT scans using a Convolutional Neural Network (CNN) named AlexNet. Secondly, a proposed features selection algorithm, Guided Whale Optimization Algorithm (Guided WOA) based on Stochastic Fractal Search (SFS), is then applied followed by balancing the selected features. Finally, a proposed voting classifier, Guided WOA based on Particle Swarm Optimization (PSO), aggregates different classifiers’ predictions to choose the most voted class. This increases the chance that individual classifiers, e.g. Support Vector Machine (SVM), Neural Networks (NN), k-Nearest Neighbor (KNN), and Decision Trees (DT), to show significant discrepancies. Two datasets are used to test the proposed model: CT images containing clinical findings of positive COVID-19 and CT images negative COVID-19. The proposed feature selection algorithm (SFS-Guided WOA) is compared with other optimization algorithms widely used in recent literature to validate its efficiency. The proposed voting classifier (PSO-Guided-WOA) achieved AUC (area under the curve) of 0.995 that is superior to other voting classifiers in terms of performance metrics. Wilcoxon rank-sum, ANOVA, and T-test statistical tests are applied to statistically assess the quality of the proposed algorithms as well.","['Computed tomography', 'Feature extraction', 'Optimization', 'Support vector machines', 'Sensitivity', 'Lung', 'Machine learning']","['COVID-19', 'CT scans', 'convolutional neural network', 'guided whale optimization algorithm', 'features selection', 'voting ensemble']"
"For heterogeneous network, which has been viewed as one pioneering technology for making cellular networks be evolved into 5G systems, reducing energy consumption by dynamically switching off base stations (BSs) has attracted increasing attention recently. With aiming at optimization on energy saving only or another energy-related performance tradeoffs, several BS switch-off strategies have been proposed from different design perspectives, such as random, distance-aware, load-aware, and auction-based strategies. Furthermore, work has been done to consider joint design for BS switch-off strategy and another strategies, such as user association, resource allocation, and physical-layer interference cancellation strategies. Finally, there have been research results about this topic in emerging cloud radio access networks. In this paper, we take an overview on these technologies and present the state of the art on each aspect. Some challenges that need to be solved in this research filed for future work are also described.","['Switches', 'Heterogeneous networks', '5G mobile communication', 'Energy consumption', 'Base stations', 'Resource management', 'Cloud computing', 'Green products', 'Radio access networks']","['Energy consumption', 'BS switch-off (or called sleeping) strategy', 'heterogeneous networks', 'cloud radio access networks', 'greener 5G systems']"
"Digital twinning is one of the top ten technology trends in the last couple of years, due to its high applicability in the industrial sector. The integration of big data analytics and artificial intelligence/machine learning (AI-ML) techniques with digital twinning, further enriches its significance and research potential with new opportunities and unique challenges. To date, a number of scientific models have been designed and implemented related to this evolving topic. However, there is no systematic review of digital twinning, particularly focusing on the role of AI-ML and big data, to guide the academia and industry towards future developments. Therefore, this article emphasizes the role of big data and AI-ML in the creation of digital twins (DTs) or DT-based systems for various industrial applications, by highlighting the current state-of-the-art deployments. We performed a systematic review on top of multidisciplinary electronic bibliographic databases, in addition to existing patents in the field. Also, we identified development-tools that can facilitate various levels of the digital twinning. Further, we designed a big data driven and AI-enriched reference architecture that leads developers to a complete DT-enabled system. Finally, we highlighted the research potential of AI-ML for digital twinning by unveiling challenges and current opportunities.","['Big Data', 'Digital twin', 'Patents', 'Industries', 'Systematics', 'Tools', 'Libraries']","['Digital twin', 'artificial intelligence', 'machine learning', 'big data', 'industry 40']"
"An edge detection is important for its reliability and security which delivers a better understanding of object recognition in the applications of computer vision, such as pedestrian detection, face detection, and video surveillance. This paper introduced two fundamental limitations encountered in edge detection: edge connectivity and edge thickness, those have been used by various developments in the state-of-the-art. An optimal selection of the threshold for effectual edge detection has constantly been a key challenge in computer vision. Therefore, a robust edge detection algorithm using multiple threshold approaches (B-Edge) is proposed to cover both the limitations. The majorly used canny edge operator focuses on two thresholds selections and still witnesses a few gaps for optimal results. To handle the loopholes of the canny edge operator, our method selects the simulated triple thresholds that target to the prime issues of the edge detection: image contrast, effective edge pixels selection, errors handling, and similarity to the ground truth. The qualitative and quantitative experimental evaluations demonstrate that our edge detection method outperforms competing algorithms for mentioned issues. The proposed approach endeavors an improvement for both grayscale and colored images.","['Image edge detection', 'Frequency-domain analysis', 'Computer vision', 'Sensitivity', 'Classification algorithms', 'Clustering algorithms', 'Synthetic aperture radar']","['Edge', 'edge connectivity', 'edge detection', 'edge width uniformity', 'threshold']"
"Classification features are crucial for an intrusion detection system (IDS), and the detection performance of IDS will change dramatically when providing different input features. Moreover, the large number of network traffic and their high-dimensional features will result in a very lengthy classification process. Recently, there is an increasing interest in the application of deep learning approaches for classification and learn feature representations. So, in this paper, we propose using the stacked sparse autoencoder (SSAE), an instance of a deep learning strategy, to extract high-level feature representations of intrusive behavior information. The original classification features are introduced into SSAE to learn the deep sparse features automatically for the first time. Then, the low-dimensional sparse features are used to build different basic classifiers. We compare SSAE with other feature extraction methods proposed by previous researchers. The experimental results both in binary classification and multiclass classification indicate the following: 1) the high-dimensional sparse features learned by SSAE are more discriminative for intrusion behaviors compared to previous methods and 2) the classification process of basic classifiers is significantly accelerated by using high-dimensional sparse features. In summary, it is shown that the SSAE is a feasible and efficient feature extraction method and provides a new research method for intrusion detection.","['Feature extraction', 'Intrusion detection', 'Machine learning', 'Machine learning algorithms', 'Anomaly detection']","['Intrusion detection', 'deep learning', 'machine learning', 'SSAE', 'feature extraction']"
"Hybrid analog/digital precoding architectures are a low-complexity alternative for fully digital precoding in millimeter-wave (mmWave) MIMO wireless systems. This is motivated by the reduction in the number of radio frequency and mixed signal hardware components. Hybrid precoding involves a combination of analog and digital processing that enables both beamforming and spatial multiplexing gains in mmWave systems. This paper develops hybrid analog/digital precoding and combining designs for mmWave multiuser systems, based on the mean-squared error (MSE) criteria. In the first design with the analog combiners being determined at the users, the proposed hybrid minimum MSE (MMSE) precoder is realized by minimizing the sum-MSE of the data streams intended for the users. In the second design, both the hybrid precoder and combiners are jointly designed in an iterative manner to minimize a weighted sum-MSE cost function. By leveraging the sparse structure of mmWave channels, the MMSE precoding/combining design problems are then formulated as sparse reconstruction problems. An orthogonal matching pursuit-based algorithm is then developed to determine the MMSE precoder and combiners. Simulation results show the performance advantages of the proposed precoding/combining designs in various system settings.","['Radio frequency', 'Precoding', 'Algorithm design and analysis', 'Antenna arrays', 'MIMO', 'Matching pursuit algorithms', 'Baseband']","['Millimeter wave', 'multiple-input multiple-output (MIMO)', 'antenna arrays', 'beamforming', 'precoding', 'sparse reconstruction', 'minimum mean squared-error (MMSE)']"
"Urban intelligence is an emerging concept which guides a series of infrastructure developments in modern smart cities. Human-computer interaction (HCI) is the interface between residents and the smart cities, it plays a key role in bridging the gap in applicating information technologies in modern cities. Hand gestures have been widely acknowledged as a promising HCI method, recognition human hand gestures using surface electromyogram (sEMG) is an important research topic in the application of sEMG. However, state-of-the-art signal processing technologies are not robust in feature extraction and pattern recognition with sEMG signals, several technical problems are still yet to be solved. For example, how to maintain the availability of myoelectric control in intermittent use, since pattern recognition qualities are greatly affected by time variability, but it is unavoidable during daily use. How to ensure the reliability and effectiveness of myoelectric control system also important in developing a good human-machine interface. In this paper, linear discriminant analysis (LDA) and extreme learning machine (ELM) are implemented in hand gesture recognition system, which is able to reduce the redundant information in sEMG signals and improve recognition efficiency and accuracy. The characteristic map slope (CMS) is extracted by using the feature re-extraction method because CMS can strengthen the relationship of features cross time domain and enhance the feasibility of cross-time identification. This study is focusing on optimizing the time differences in sEMG pattern recognition, the experimental results are beneficial to reducing the time differences in gesture recognition based on sEMG. The recognition framework proposed in this paper can enhance the generalization ability of HCI in the long term use and it also simplifies the data collection stage before training the device ready for daily use, which is of great significance to improve the time generalization performance of an HCI system.","['Feature extraction', 'Gesture recognition', 'Electromyography', 'Human computer interaction', 'Thumb', 'Time-domain analysis']","['Urban intelligence', 'human-computer interaction', 'sEMG', 'gesture recognition']"
"We present a large-scale study exploring the capability of temporal deep neural networks to interpret natural human kinematics and introduce the first method for active biometric authentication with mobile inertial sensors. At Google, we have created a first-of-its-kind data set of human movements, passively collected by 1500 volunteers using their smartphones daily over several months. We compare several neural architectures for efficient learning of temporal multi-modal data representations, propose an optimized shift-invariant dense convolutional mechanism, and incorporate the discriminatively trained dynamic features in a probabilistic generative framework taking into account temporal characteristics. Our results demonstrate that human kinematics convey important information about user identity and can serve as a valuable component of multi-modal authentication systems. Finally, we demonstrate that the proposed model can also be successfully applied in a visual context.","['Authentication', 'Biometrics', 'Access control', 'Learning systems', 'Mobile computing', 'Neural networks', 'Recurrent neural networks', 'Context modeling', 'Kinematics', 'Biosensors']","['Authentication', 'Biometrics (access control)', 'Learning', 'Mobile computing', 'Recurrent neural networks']"
"In computer vision, convolutional networks (CNNs) often adopt pooling to enlarge receptive field which has the advantage of low computational complexity. However, pooling can cause information loss and thus is detrimental to further operations such as features extraction and analysis. Recently, dilated filter has been proposed to tradeoff between receptive field size and efficiency. But the accompanying gridding effect can cause a sparse sampling of input images with checkerboard patterns. To address this problem, in this paper, we propose a novel multi-level wavelet CNN (MWCNN) model to achieve a better tradeoff between receptive field size and computational efficiency. The core idea is to embed wavelet transform into CNN architecture to reduce the resolution of feature maps while at the same time, increasing receptive field. Specifically, MWCNN for image restoration is based on U-Net architecture, and inverse wavelet transform (IWT) is deployed to reconstruct the high resolution (HR) feature maps. The proposed MWCNN can also be viewed as an improvement of dilated filter and a generalization of average pooling and can be applied to not only image restoration tasks, but also any CNNs requiring a pooling operation. The experimental results demonstrate the effectiveness of the proposed MWCNN for tasks, such as image denoising, single image super-resolution, JPEG image artifacts removal and object classification.","['Image restoration', 'Task analysis', 'Discrete wavelet transforms', 'Computer architecture', 'Feature extraction']","['Convolutional networks', 'receptive field size', 'efficiency', 'multi-level wavelet']"
"This paper studies the application of cooperative techniques for non-orthogonal multiple access (NOMA). More particularly, the fixed gain amplify-and-forward (AF) relaying with NOMA is investigated over Nakagami-m fading channels. Two scenarios are considered insightfully: 1) the first scenario is that the base station (BS) intends to communicate with multiple users through the assistance of AF relaying, where the direct links are existent between the BS and users and 2) the second scenario is that the AF relaying is inexistent between the BS and users. To characterize the performance of the considered scenarios, new closed-form expressions for both exact and asymptomatic outage probabilities are derived. Based on the analytical results, the diversity orders achieved by the users are obtained. For the first and second scenarios, the diversity order for the nth user are μ(n + 1) and μn, respectively. Simulation results unveil that NOMA is capable of outperforming orthogonal multiple access (OMA) in terms of outage probability and system throughput. It is also worth noting that NOMA can provide better fairness compared with conventional OMA. By comparing the two scenarios, cooperative NOMA scenario can provide better outage performance relative to the second scenario.","['NOMA', 'Fading channels', 'Interference', 'Signal to noise ratio', 'Silicon carbide', 'Throughput', 'Electronic mail']","['Non-orthogonal multiple access', 'amplify-and-forward relaying', 'Nakagami-m fading channels', 'diversity order']"
"Multi-class pest detection is one of the crucial components in pest management involving localization in addition to classification which is much more difficult than generic object detection because of the apparent differences among pest species. This paper proposes a region-based end-to-end approach named PestNet for large-scale multi-class pest detection and classification based on deep learning. PestNet consists of three major parts. First, a novel module channel-spatial attention (CSA) is proposed to be fused into the convolutional neural network (CNN) backbone for feature extraction and enhancement. The second one is called region proposal network (RPN) that is adopted for providing region proposals as potential pest positions based on extracted feature maps from images. Position-sensitive score map (PSSM), the third component, is used to replace fully connected (FC) layers for pest classification and bounding box regression. Furthermore, we apply contextual regions of interest (RoIs) as contextual information of pest features to improve detection accuracy. We evaluate PestNet on our newly collected large-scale pests' image dataset, Multi-class Pests Dataset 2018 (MPD2018) captured by our designed task-specific image acquisition equipment, covering more than 80k images with over 580k pests labeled by agricultural experts and categorized in 16 classes. The experimental results show that the proposed PestNet performs well on multi-class pest detection with 75.46% mean average precision (mAP), which outperforms the state-of-the-art methods.","['Feature extraction', 'Proposals', 'Insects', 'Training', 'Deep learning', 'Task analysis', 'Computer architecture']","['Channel-spatial attention', 'convolutional neural network', 'multi-class pest detection', 'position-sensitive score map', 'region proposal network']"
"In order to overcome the problem of power generation in distributed energy, microgrid(MG) emerges as an alternative scheme. Compared with the ac microgrids, the dc microgrids have the advantages of high system efficiency, good power quality, low cost, and simple control. However, due to the complexity of the distributed generation system, the conventional droop control shows the drawbacks of low current sharing accuracy. Therefore, the improved primary control methods to enhance current sharing accuracy are systematically reviewed, such as particle swarm optimization programming, probabilistic algorithm and voltage correction factor scheme. However, it is difficult to achieve stable and coordinated operation of the dc microgrids by relying on the primary control. Hence, the various secondary control approaches, such as dynamic current sharing scheme, muti-agent system (MAS) control and virtual voltage control methods have been summarized for voltage regulation. Furthermore, the energy management system (EMS), modular-based energy router (MBER) and other coordinated control methods are reviewed to achieve power management. Besides, various control methods to compensate the effect of communication delay are summarized. Moreover, linear matrix inequality (LMI), Lyapunov-Krasovskii functional stability and Takagi-Sugeno model prediction scheme can be adopted to eliminate the influence of communication delay. In addition, due to the constant power loads (CPL) exhibit negative impedance characteristics, which may result in the output oscillation of filter. Thus, various control approaches have been reviewed to match the impedance, such as the nonlinear disturbance observer (NDO) feedforward compensation method, linear programming algorithm, hybrid potential theory and linear system analysis of polyhedral uncertainty. The merits and drawbacks of those control strategies are compared in this paper. Finally, the future research trends of hierarchical control and stability in dc microgrids and dc microgrid clusters are also presented.","['Voltage control', 'Stability criteria', 'Microgrids', 'Impedance', 'Power system stability', 'Delays']","['DC microgrid', 'nonlinear droop control', 'multi-agent system', 'consensus control', 'communication delay', 'constant power load', 'hierarchical control']"
"As the next generation network architecture, software-defined networking (SDN) has exciting application prospects. Its core idea is to separate the forwarding layer and control layer of network system, where network operators can program packet forwarding behavior to significantly improve the innovation capability of network applications. Traffic engineering (TE) is an important network application, which studies measurement and management of network traffic, and designs reasonable routing mechanisms to guide network traffic to improve utilization of network resources, and better meet requirements of the network quality of service (QoS). Compared with the traditional networks, the SDN has many advantages to support TE due to its distinguish characteristics, such as isolation of control and forwarding, global centralized control, and programmability of network behavior. This paper focuses on the traffic engineering technology based on the SDN. First, we propose a reference framework for TE in the SDN, which consists of two parts, traffic measurement and traffic management. Traffic measurement is responsible for monitoring and analyzing real-time network traffic, as a prerequisite for traffic management. In the proposed framework, technologies related to traffic measurement include network parameters measurement, a general measurement framework, and traffic analysis and prediction; technologies related to traffic management include traffic load balancing, QoS-guarantee scheduling, energy-saving scheduling, and traffic management for the hybrid IP/SDN. Current existing technologies are discussed in detail, and our insights into future development of TE in the SDN are offered.","['Software defined networks', 'Telecommunication traffic', 'Telecommunication network management', 'Network monitoring', 'Quality of service', 'Current measurement', 'Energy measurement', 'Next generatoin networking', 'Resource management']","['Software-defined networkin', 'SDN', 'traffic engineering', 'network monitoring', 'network measurement', 'network management']"
"Data offloading plays an important role for the mobile data explosion problem that occurs in cellular networks. This paper proposed an idea and control scheme for offloading vehicular communication traffic in the cellular network to vehicle to vehicle (V2V) paths that can exist in vehicular ad hoc networks (VANETs). A software-defined network (SDN) inside the mobile edge computing (MEC) architecture, which is abbreviated as the SDNi-MEC server, is devised in this paper to tackle the complicated issues of VANET V2V offloading. Using the proposed SDNi-MEC architecture, each vehicle reports its contextual information to the context database of the SDNi-MEC server, and the SDN controller of the SDNi-MEC server calculates whether there is a V2V path between the two vehicles that are currently communicating with each other through the cellular network. This proposed method: 1) uses each vehicle's context; 2) adopts a centralized management strategy for calculation and notification; and 3) tries to establish a VANET routing path for paired vehicles that are currently communicating with each other using a cellular network. The performance analysis for the proposed offloading control scheme based on the SDNi-MEC server architecture shows that it has better throughput in both the cellular networking link and the V2V paths when the vehicle's density is in the middle.","['Vehicular ad hoc networks', 'Cellular networks', 'Servers', 'Computer architecture', 'Cloud computing', 'Software defined networking', 'Routing']","['Cellular networks', 'mobile edge computing (MEC)', 'software defined network (SDN)', 'VANET offloading', 'V2V communication']"
"Load forecasting is a pivotal part of the power utility companies. To provide load-shedding free and uninterrupted power to the consumer, decision-makers in the utility sector must forecast the future demand for electricity with a minimum error percentage. Load prediction with less percentage of error can save millions of dollars to the utility companies. There are numerous Machine Learning (ML) techniques to amicably forecast electricity demand, among which the hybrid models show the best result. Two or more than two predictive models are amalgamated to design a hybrid model, each of which provides improved performances by the merit of individual algorithms. This paper reviews the current state-of-the-art of electric load forecasting technologies and presents recent works pertaining to the combination of different ML algorithms into two or more methods for the construction of hybrid models. A comprehensive study of each single and multiple load forecasting model is performed with an in-depth analysis of their advantages, disadvantages, and functions. A comparison between their performance in terms of Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) values are developed with pertinent literature of several models to aid the researchers with the selection of suitable models for load prediction.","['Load forecasting', 'Predictive models', 'Load modeling', 'Support vector machines', 'Prediction algorithms', 'Forecasting', 'Clustering algorithms']","['Load forecasting', 'predictive models', 'machine learning', 'support vector machines', 'artificial neural networks', 'computational intelligence', 'power industry', 'smart grid']"
"The grid denotes the electric grid which consists of communication lines, control stations, transformers, and distributors that aids in supplying power from the electrical plant to the consumers. Presently, the electric grid constitutes humongous power production units which generates millions of megawatts of power distributed across several demographic regions. There is a dire need to efficiently manage this power supplied to the various consumer domains such as industries, smart cities, household and organizations. In this regard, a smart grid with intelligent systems is being deployed to cater the dynamic power requirements. A smart grid system follows the Cyber-Physical Systems (CPS) model, in which Information Technology (IT) infrastructure is integrated with physical systems. In the scenario of the smart grid embedded with CPS, the Machine Learning (ML) module is the IT aspect and the power dissipation units are the physical entities. In this research, a novel Multidirectional Long Short-Term Memory (MLSTM) technique is being proposed to predict the stability of the smart grid network. The results obtained are evaluated against other popular Deep Learning approaches such as Gated Recurrent Units (GRU), traditional LSTM and Recurrent Neural Networks (RNN). The experimental results prove that the MLSTM approach outperforms the other ML approaches.","['Smart grids', 'Power system stability', 'Stability analysis', 'Machine learning algorithms', 'Predictive models', 'Machine learning', 'Prediction algorithms']","['Multidirectional long short-term memory (MLSTM)', 'machine learning (ML)', 'smart grid (SG)', 'cyber physical systems (CPS)']"
"The sharing of electronic health records (EHRs) has great positive significance for research of disease and doctors' diagnosis. In recent years, cloud-based electronic medical record sharing scheme has brought a lot of conveniences, but the centralization of cloud exposes threats inevitably to data security and privacy preservation. Blockchain technology can be seen as a promising solution to address these problems on account of its unique propertis of decentration, anonymity, unforgeability and verifiability. In this paper, we propose a blockchain based secure and privacy-preserving EHR sharing protocol. Data requester can search desired keyword from data provider to find relevant EHRs on the EHR consortium blockchain and get the re-encryption ciphertext from cloud server after getting the data owner's authorization. The scheme mainly uses searchable encryption and conditional proxy re-encryption to realize data security, privacy preservation, and access control. Furthermore, proof of authorization is designed as the consensus mechanism for consortium blockchain to guarantee system's availability. Security analysis demonstrates that the proposed protocol can achieve security goals. Besides, we emulate the cryptographic primitives and implement the proposed scheme on Ethereum platform. Performance evaluation shows that the proposed scheme has high computational efficiency.","['Blockchain', 'Cloud computing', 'Data privacy', 'Data security', 'Medical diagnostic imaging', 'Privacy']","['Electronic health records', 'data sharing', 'blockchain', 'data security', 'privacy preservation']"
"In recent years, chaos-based image encryption algorithms have aroused extensive research interest. However, some image encryption algorithms still have several security defects, and the research on cryptanalysis is relatively inadequate. This paper performs the cryptanalysis of a newly proposed color image encryption scheme using RT-enhanced chaotic tent map. By using chosen-plaintext attacks, the equivalent keys of the cryptosystem are successfully broken, so that the target ciphertext image can be decoded. Based on the cryptanalysis, we then proposed an improved encryption algorithm. A new logistic-tent map is proposed and applied to the improved encryption algorithm, and a parameter related to the SHA-3 hash value of the plaintext image is introduced as a secret key parameter so that the improved algorithm can resist chosen-plaintext attacks. The security analysis and experimental tests for the improved algorithm are given in detail, which show that the improved algorithm can significantly increase the security of encryption images while still possessing all the merits of the original algorithm.","['Encryption', 'Color', 'Chaotic communication', 'Transforms', 'Machinery']","['Chaotic cryptography', 'cryptanalysis', 'image encryption', 'logistic-tent map (LTM)']"
"Society and individuals are negatively influenced both politically and socially by the widespread increase of fake news either way generated by humans or machines. In the era of social networks, the quick rotation of news makes it challenging to evaluate its reliability promptly. Therefore, automated fake news detection tools have become a crucial requirement. To address the aforementioned issue, a hybrid Neural Network architecture, that combines the capabilities of CNN and LSTM, is used with two different dimensionality reduction approaches, Principle Component Analysis (PCA) and Chi-Square. This work proposed to employ the dimensionality reduction techniques to reduce the dimensionality of the feature vectors before passing them to the classifier. To develop the reasoning, this work acquired a dataset from the Fake News Challenges (FNC) website which has four types of stances: agree, disagree, discuss, and unrelated. The nonlinear features are fed to PCA and chi-square which provides more contextual features for fake news detection. The motivation of this research is to determine the relative stance of a news article towards its headline. The proposed model improves results by ~4% and ~20% in terms of Accuracy and F1-score . The experimental results show that PCA outperforms than Chi-square and state-of-the-art methods with 97.8% accuracy.","['Feature extraction', 'Principal component analysis', 'Task analysis', 'Machine learning', 'Pregnancy', 'Computer science', 'Computational modeling']","['Fake news detection', 'text mining', 'deep learning', 'PCA', 'Chi-square', 'CNN-LSTM', 'word embedding']"
"Network function virtualization (NFV) has already been a new paradigm for network architectures. By migrating NFs from dedicated hardware to virtualization platform, NFV can effectively improve the flexibility to deploy and manage service function chains (SFCs). However, resource allocation for requested SFC in NFV-based infrastructures is not trivial as it mainly consists of three phases: virtual network functions (VNFs) chain composition, VNFs forwarding graph embedding, and VNFs scheduling. The decision of these three phases can be mutually dependent, which also makes it a tough task. Therefore, a coordinated approach is studied in this paper to jointly optimize NFV resource allocation in these three phases. We apply a general cost model to consider both network costs and service performance. The coordinate NFV-RA is formulated as a mixed-integer linear programming, and a heuristic-based algorithm (JoraNFV) is proposed to get the near optimal solution. To make the coordinated NFV-RA more tractable, JoraNFV is divided into two sub-algorithms, one-hop optimal traffic scheduling and a multi-path greedy algorithm for VNF chain composition and VNF forwarding graph embedding. Last, extensive simulations are performed to evaluate the performance of JoraNFV, and results have shown that JoraNFV can get a solution within 1.25 times of the optimal solution with reasonable execution time, which indicates that JoraNFV can be used for online NFV planning.","['Resource management', 'Noise measurement', 'Telecommunication traffic', 'Scheduling', 'Mix integer linear programming', 'Virtual function placement']","['NFV', 'resource allocation', 'service function chain', 'traffic scheduling', 'virtual function placement']"
"Women who have recovered from breast cancer (BC) always fear its recurrence. The fact that they have endured the painstaking treatment makes recurrence their greatest fear. However, with current advancements in technology, early recurrence prediction can help patients receive treatment earlier. The availability of extensive data and advanced methods make accurate and fast prediction possible. This research aims to compare the accuracy of a few existing data mining algorithms in predicting BC recurrence. It embeds a particle swarm optimization as feature selection into three renowned classifiers, namely, naive Bayes, K-nearest neighbor, and fast decision tree learner, with the objective of increasing the accuracy level of the prediction model.","['Breast cancer', 'Feature extraction', 'Data mining', 'Predictive models', 'Data models', 'Classification algorithms']","['Breast cancer', 'recurrence', 'feature selection', 'REPTree', 'naïve Bayes', 'K-nearest neighbor', 'particle swarm optimization']"
"Ship detection is of great importance and full of challenges in the field of remote sensing. The complexity of application scenarios, the redundancy of detection region, and the difficulty of dense ship detection are all the main obstacles that limit the successful operation of traditional methods in ship detection. In this paper, we propose a brand new detection model based on multitask rotational region convolutional neural network to solve the problems above. This model is mainly consisting of five consecutive parts: dense feature pyramid network, adaptive region of interest (ROI) align, rotational bounding box regression, prow direction prediction and rotational nonmaximum suppression (R-NMS). First of all, the low-level location information and high-level semantic information are fully utilized through multiscale feature networks. Then, we design adaptive ROI align to obtain high quality proposals which remain complete spatial and semantic information. Unlike most previous approaches, the prediction obtained by our method is the minimum bounding rectangle of the object with less redundant regions. Therefore, the rotational region detection framework is more suitable to detect the dense object than traditional detection model. Additionally, we can find the berthing and sailing direction of ship through prediction. A detailed evaluation based on SRSS for rotation detection shows that our detection method has a competitive performance.","['Marine vehicles', 'Feature extraction', 'Proposals', 'Object detection', 'Remote sensing', 'Semantics', 'Detection algorithms']","['Convolutional neural network', 'remote sensing', 'ship detection']"
"In this paper, a method for detecting rapid rice disease based on FCM-KM and Faster R-CNN fusion is proposed to address various problems with the rice disease images, such as noise, blurred image edge, large background interference and low detection accuracy. Firstly, the method uses a two-dimensional filtering mask combined with a weighted multilevel median filter (2DFM-AMMF) for noise reduction, and uses a faster two-dimensional Otsu threshold segmentation algorithm (Faster 2D-Otsu) to reduce the interference of complex background with the detection of target blade in the image. Then the dynamic population firefly algorithm based on the chaos theory as well as the maximum and minimum distance algorithm is applied for optimization of the K-Means clustering algorithm (FCM-KM) to determine the optimal clustering class k value while addressing the tendency of the algorithm to fall into the local optimum problem. Combined with the R-CNN algorithm for the identification of rice diseases, FCM-KM analysis is conducted to determine the different sizes of the Faster R-CNN target frame. As revealed by the application results of 3010 images, the accuracy and time required for detection of rice blast, bacterial blight and blight were 96.71%/0.65s, 97.53%/0.82s and 98.26%/0.53s, respectively, indicating clearly that the method is more capable of detecting rice diseases and improving the identification accuracy of Faster R-CNN algorithm, while reducing the time required for identification.","['Diseases', 'Clustering algorithms', 'Feature extraction', 'Heuristic algorithms', 'Filtering', 'Image segmentation', 'Microorganisms']","['Chaos theory', 'faster R-CNN', 'firefly algorithm', 'Otsu threshold segmentation', 'K-means clustering algorithm', 'rice disease detection', 'weighted multistage median filter']"
"Maintaining frequency stability of low inertia microgrids with high penetration of renewable energy sources (RESs) is a critical challenge. Solving this challenge, the inertia of microgrids would be enhanced by virtual inertia control-based energy storage systems. However, in such systems, the virtual inertia constant is fixed and selection of its value will significantly affect frequency stability of microgrids under different penetration levels of RESs. Higher frequency oscillations may occur due to the fixed virtual inertia constant or unsuitable selection of its value. To overcome such a problem and provide adaptive inertia control, this paper proposes a self-adaptive virtual inertia control system using fuzzy logic for ensuring stable frequency stabilization, which is required for successful microgrid operation in the presence of high RESs penetration. In this concept, the virtual inertia constant is automatically adjusted based on input signals of real power injection of RESs and system frequency deviations, avoiding unsuitable selection and delivering rapid inertia response. To verify the efficiency of the proposed control method, the contrastive simulation results are compared with the conventional method for serious load disturbances and various rates of RESs penetration. The proposed control method shows remarkable performance in transient response improvement and fast damping of oscillations, preserving robustness of operation.","['Microgrids', 'Frequency control', 'Power system stability', 'Power generation', 'Stability analysis', 'Control systems', 'Fuzzy logic']","['Frequency control', 'fuzzy logic', 'intelligent control', 'islanded microgrid', 'virtual inertia control', 'virtual synchronous generator']"
"Since the noise statistics of large-scale battery energy storage systems (BESSs) are often unknown or inaccurate in actual applications, the estimation precision of state of charge (SOC) of BESSs using extended Kalman filter (EKF) or unscented Kalman filter (UKF) is usually inaccurate or even divergent. To resolve this problem, a method based on adaptive UKF (AUKF) with a noise statistics estimator is proposed to estimate accurately SOC of BESSs. The noise statistics estimator based on the modified Sage-Husa maximum posterior is aimed to estimate adaptively the mean and error covariance of measurement and system process noises online for the AUKF when the prior noise statistics are unknown or inaccurate. The accuracy and adaptation of the proposed method is validated by the comparison with the UKF and EKF under different real-time conditions. The comparison shows that the proposed method can achieve better SOC estimation accuracy when the noise statistics of BESSs are unknown or inaccurate.","['Batteries', 'State of charge', 'Estimation', 'Kalman filters', 'Integrated circuit modeling', 'Battery charge measurement', 'Voltage measurement']","['Adaptive unscented Kalman filter', 'battery energy storage systems', 'noise statistics estimator', 'state of charge']"
"The artificial intelligence (AI) techniques have been widely used in the transient stability analysis of a power system. They are recognized as the most promising approaches for predicting the post-fault transient stability status with the use of phasor measurement units data. However, the popular AI methods used for power systems are often “black boxes,” which result in the poor interpretation of the model. In this paper, a transient stability prediction method based on extreme gradient boosting is proposed. In this model, a decision graph and feature importance scores are provided to discover the relationship between the features of the power system and transient stability. Meanwhile, the key features are selected according to the feature importance scores to remove redundant variables. The simulation results on the New England 39-bus system have demonstrated the superiority of the proposed model over the prior methods in the computation speed and prediction accuracy. Finally, an algorithm is proposed to interpret the prediction results for a specific fault of the power system, which further improves the interpretability of the model and makes it attractive for real-time transient stability prediction.","['Power system stability', 'Transient analysis', 'Generators', 'Stability criteria', 'Predictive models']","['Feature importance scores', 'model interpretation', 'XGBoost model', 'transient stability prediction']"
