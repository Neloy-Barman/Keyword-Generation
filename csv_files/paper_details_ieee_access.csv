abstracts,ieee_keywords,author_keywords
"The global bandwidth shortage facing wireless carriers has motivated the exploration of the underutilized millimeter wave (mm-wave) frequency spectrum for future broadband cellular communication networks. There is, however, little knowledge about cellular mm-wave propagation in densely populated indoor and outdoor environments. Obtaining this information is vital for the design and operation of future fifth generation cellular networks that use the mm-wave spectrum. In this paper, we present the motivation for new mm-wave cellular systems, methodology, and hardware for measurements and offer a variety of measurement results that show 28 and 38 GHz frequencies can be used when employing steerable directional antennas at base stations and mobile devices.",[],[]
"Motivated by the recent explosion of interest around blockchains, we examine whether they make a good fit for the Internet of Things (IoT) sector. Blockchains allow us to have a distributed peer-to-peer network where non-trusting members can interact with each other without a trusted intermediary, in a verifiable manner. We review how this mechanism works and also look into smart contracts-scripts that reside on the blockchain that allow for the automation of multi-step processes. We then move into the IoT domain, and describe how a blockchain-IoT combination: 1) facilitates the sharing of services and resources leading to the creation of a marketplace of services between devices and 2) allows us to automate in a cryptographically verifiable manner several existing, time-consuming workflows. We also point out certain issues that should be considered before the deployment of a blockchain network in an IoT setting: from transactional privacy to the expected value of the digitized assets traded on the network. Wherever applicable, we identify solutions and workarounds. Our conclusion is that the blockchain-IoT combination is powerful and can cause significant transformations across several industries, paving the way for new business models and novel, distributed applications.","['Distributed processing', 'Internet of things', 'Cryptography', 'Privacy', 'Blockchains', 'Automation', 'Peer-to-peer computing']","['blockchain', 'distributed systems', 'Internet of Things']"
"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","['Conferences', 'Machine learning', 'Market research', 'Prediction algorithms', 'Machine learning algorithms', 'Biological system modeling']","['Explainable artificial intelligence', 'interpretable machine learning', 'black-box models']"
"The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges.","['Internet of things', 'Medical services', 'Network security', 'Network architecture', 'Biological system modeling', 'Market research']","['Internet of Things', 'Health Care', 'Services', 'Applications', 'Networks', 'Architectures', 'Platforms', 'Security', 'Technologies', 'Industries', 'Policies', 'Challenges']"
"In the near future, i.e., beyond 4G, some of the prime objectives or demands that need to be addressed are increased capacity, improved data rate, decreased latency, and better quality of service. To meet these demands, drastic improvements need to be made in cellular network architecture. This paper presents the results of a detailed survey on the fifth generation (5G) cellular network architecture and some of the key emerging technologies that are helpful in improving the architecture and meeting the demands of users. In this detailed survey, the prime focus is on the 5G cellular network architecture, massive multiple input multiple output technology, and device-to-device communication (D2D). Along with this, some of the emerging technologies that are addressed in this paper include interference management, spectrum sharing with cognitive radio, ultra-dense networks, multi-radio access technology association, full duplex radios, millimeter wave solutions for 5G cellular networks, and cloud technologies for 5G radio access networks and software defined networks. In this paper, a general probable 5G cellular network architecture is proposed, which shows that D2D, small cell access points, network cloud, and the Internet of Things can be a part of 5G cellular network architecture. A detailed survey is included regarding current research projects being conducted in different countries by research groups and institutions that are working on 5G technologies.","['5G mobile communication', 'Cloud computing', 'MIMO', 'Radio access networks', 'Cellular networks']","['5G', 'Cloud', 'D2D', 'Massive MIMO', 'mm-wave', 'Relay', 'Small-cel']"
"The future of mobile communications looks exciting with the potential new use cases and challenging requirements of future 6th generation (6G) and beyond wireless networks. Since the beginning of the modern era of wireless communications, the propagation medium has been perceived as a randomly behaving entity between the transmitter and the receiver, which degrades the quality of the received signal due to the uncontrollable interactions of the transmitted radio waves with the surrounding objects. The recent advent of reconfigurable intelligent surfaces in wireless communications enables, on the other hand, network operators to control the scattering, reflection, and refraction characteristics of the radio waves, by overcoming the negative effects of natural wireless propagation. Recent results have revealed that reconfigurable intelligent surfaces can effectively control the wavefront, e.g., the phase, amplitude, frequency, and even polarization, of the impinging signals without the need of complex decoding, encoding, and radio frequency processing operations. Motivated by the potential of this emerging technology, the present article is aimed to provide the readers with a detailed overview and historical perspective on state-of-the-art solutions, and to elaborate on the fundamental differences with other technologies, the most important open research issues to tackle, and the reasons why the use of reconfigurable intelligent surfaces necessitates to rethink the communication-theoretic models currently employed in wireless networks. This article also explores theoretical performance limits of reconfigurable intelligent surface-assisted communication systems using mathematical techniques and elaborates on the potential use cases of intelligent surfaces in 6G and beyond wireless networks.","['Wireless networks', '5G mobile communication', 'Surface waves', 'STEM', '6G mobile communication']","['6G', 'large intelligent surfaces', 'meta-surfaces', 'reconfigurable intelligent surfaces', 'smart reflect-arrays', 'software-defined surfaces', 'wireless communications', 'wireless networks']"
"Frequencies from 100 GHz to 3 THz are promising bands for the next generation of wireless communication systems because of the wide swaths of unused and unexplored spectrum. These frequencies also offer the potential for revolutionary applications that will be made possible by new thinking, and advances in devices, circuits, software, signal processing, and systems. This paper describes many of the technical challenges and opportunities for wireless communication and sensing applications above 100 GHz, and presents a number of promising discoveries, novel approaches, and recent results that will aid in the development and implementation of the sixth generation (6G) of wireless networks, and beyond. This paper shows recent regulatory and standard body rulings that are anticipating wireless products and services above 100 GHz and illustrates the viability of wireless cognition, hyper-accurate position location, sensing, and imaging. This paper also presents approaches and results that show how long distance mobile communications will be supported to above 800 GHz since the antenna gains are able to overcome air-induced attenuation, and present methods that reduce the computational complexity and simplify the signal processing used in adaptive antenna arrays, by exploiting the Special Theory of Relativity to create a cone of silence in over-sampled antenna arrays that improve performance for digital phased array antennas. Also, new results that give insights into power efficient beam steering algorithms, and new propagation and partition loss models above 100 GHz are given, and promising imaging, array processing, and position location results are presented. The implementation of spatial consistency at THz frequencies, an important component of channel modeling that considers minute changes and correlations over space, is also discussed. This paper offers the first in-depth look at the vast applications of THz wireless products and applications and provides approaches for how to reduce power and increase performance across several problem domains, giving early evidence that THz techniques are compelling and available for future wireless communications.","['Wireless communication', 'Wireless sensor networks', 'Antenna arrays', 'Bandwidth', 'Communication system security', 'Cognition', 'Imaging']","['mmWave', 'millimeter wave', '5G', 'D-band', '6G', 'channel sounder', 'propagation measurements', 'Terahertz (THz)', 'array processing', 'imaging', 'scattering theory', 'cone of silence', 'digital phased arrays', 'digital beamformer', 'signal processing for THz', 'position location', 'channel modeling', 'THz applications', 'wireless cognition', 'network offloading']"
"The use of unmanned aerial vehicles (UAVs) is growing rapidly across many civil application domains, including real-time monitoring, providing wireless coverage, remote sensing, search and rescue, delivery of goods, security and surveillance, precision agriculture, and civil infrastructure inspection. Smart UAVs are the next big revolution in the UAV technology promising to provide new opportunities in different applications, especially in civil infrastructure in terms of reduced risks and lower cost. Civil infrastructure is expected to dominate more than $45 Billion market value of UAV usage. In this paper, we present UAV civil applications and their challenges. We also discuss the current research trends and provide future insights for potential UAV uses. Furthermore, we present the key challenges for UAV civil applications, including charging challenges, collision avoidance and swarming challenges, and networking and security-related challenges. Based on our review of the recent literature, we discuss open research challenges and draw high-level insights on how these challenges might be approached.","['Unmanned aerial vehicles', 'Market research', 'Wireless sensor networks', 'Wireless communication', 'Communication system security', 'Security', 'Surveillance']","['Civil infrastructure inspection', 'delivery of goods', 'precision agriculture', 'real-time monitoring', 'remote sensing', 'search and rescue', 'security and surveillance', 'UAVs', 'wireless coverage']"
"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.","['Machine learning', 'Perturbation methods', 'Computer vision', 'Computational modeling', 'Neural networks', 'Task analysis', 'Predictive models']","['Deep learning', 'adversarial perturbation', 'black-box attack', 'white-box attack', 'adversarial learning', 'perturbation detection']"
"Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.","['Intrusion detection', 'Machine learning', 'Recurrent neural networks', 'Training', 'Computational modeling', 'Testing', 'Support vector machines']","['Recurrent neural networks', 'RNN-IDS', 'intrusion detection', 'deep learning', 'machine learning']"
"Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to the healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively. The high accuracy of this computer-aided diagnostic tool can significantly improve the speed and accuracy of COVID-19 diagnosis. This would be extremely useful in this pandemic where disease burden and need for preventive measures are at odds with available resources.","['Diseases', 'Lung', 'Databases', 'X-ray imaging', 'Machine learning', 'Tools', 'COVID-19']","['Artificial intelligence', 'COVID-19 pneumonia', 'machine learning', 'transfer learning', 'viral pneumonia', 'computer-aided diagnostic tool']"
"The Internet of Things (IoT) now permeates our daily lives, providing important measurement and collection tools to inform our every decision. Millions of sensors and devices are continuously producing data and exchanging important messages via complex networks supporting machine-to-machine communications and monitoring and controlling critical smart-world infrastructures. As a strategy to mitigate the escalation in resource congestion, edge computing has emerged as a new paradigm to solve IoT and localized computing needs. Compared with the well-known cloud computing, edge computing will migrate data computation or storage to the network “edge”, near the end users. Thus, a number of computation nodes distributed across the network can offload the computational stress away from the centralized data center, and can significantly reduce the latency in message exchange. In addition, the distributed structure can balance network traffic and avoid the traffic peaks in IoT networks, reducing the transmission latency between edge/cloudlet servers and end users, as well as reducing response times for real-time IoT applications in comparison with traditional cloud services. Furthermore, by transferring computation and communication overhead from nodes with limited battery supply to nodes with significant power resources, the system can extend the lifetime of the individual nodes. In this paper, we conduct a comprehensive survey, analyzing how edge computing improves the performance of IoT networks. We categorize edge computing into different groups based on architecture, and study their performance by comparing network latency, bandwidth occupation, energy consumption, and overhead. In addition, we consider security issues in edge computing, evaluating the availability, integrity, and the confidentiality of security strategies of each group, and propose a framework for security evaluation of IoT networks with edge computing. Finally, we compare the performance of various IoT applications (smart city, smart grid, smart transportation, and so on) in edge computing and traditional cloud computing architectures.","['Edge computing', 'Cloud computing', 'Logic gates', 'Servers', 'Security', 'Intelligent sensors']","['Edge computing', 'Internet of Things', 'survey']"
"Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.","['Deep learning', 'Training', 'Computer architecture', 'Feature extraction', 'Recurrent neural networks', 'Feedforward neural networks']","['Machine learning algorithm', 'optimization', 'artificial intelligence', 'deep neural network architectures', 'convolution neural network', 'backpropagation', 'supervised and unsupervised learning']"
"The Internet of Things (IoT) is a promising technology which tends to revolutionize and connect the global world via heterogeneous smart devices through seamless connectivity. The current demand for machine-type communications (MTC) has resulted in a variety of communication technologies with diverse service requirements to achieve the modern IoT vision. More recent cellular standards like long-term evolution (LTE) have been introduced for mobile devices but are not well suited for low-power and low data rate devices such as the IoT devices. To address this, there is a number of emerging IoT standards. Fifth generation (5G) mobile network, in particular, aims to address the limitations of previous cellular standards and be a potential key enabler for future IoT. In this paper, the state-of-the-art of the IoT application requirements along with their associated communication technologies are surveyed. In addition, the third generation partnership project cellular-based low-power wide area solutions to support and enable the new service requirements for Massive to Critical IoT use cases are discussed in detail, including extended coverage global system for mobile communications for the Internet of Things, enhanced machine-type communications, and narrowband-Internet of Things. Furthermore, 5G new radio enhancements for new service requirements and enabling technologies for the IoT are introduced. This paper presents a comprehensive review related to emerging and enabling technologies with main focus on 5G mobile networks that is envisaged to support the exponential traffic growth for enabling the IoT. The challenges and open research directions pertinent to the deployment of massive to critical IoT applications are also presented in coming up with an efficient context-aware congestion control mechanism.","['5G mobile communication', 'Machine-to-machine communications', 'Mobile computing', 'Internet of Things']","['Internet of Things', 'long-term evolution', 'machine-type communications', '5G new radio']"
"With the advances in new-generation information technologies, especially big data and digital twin, smart manufacturing is becoming the focus of global manufacturing transformation and upgrading. Intelligence comes from data. Integrated analysis for the manufacturing big data is beneficial to all aspects of manufacturing. Besides, the digital twin paves a way for the cyber-physical integration of manufacturing, which is an important bottleneck to achieve smart manufacturing. In this paper, the big data and digital twin in manufacturing are reviewed, including their concept as well as their applications in product design, production planning, manufacturing, and predictive maintenance. On this basis, the similarities and differences between big data and digital twin are compared from the general and data perspectives. Since the big data and digital twin can be complementary, how they can be integrated to promote smart manufacturing are discussed.","['Big Data', 'Data mining', 'Internet', 'Product design', 'Real-time systems']","['Big data', 'digital twin', 'smart manufacturing', 'comprehensive comparison', 'convergence']"
"Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.","['Machine learning', 'Pattern recognition', 'Big data', 'Natural language processing', 'Data processing', 'Information analysis']","['Classifier design and evaluation', 'feature representation', 'machine learning', 'neural nets models', 'parallel processing']"
"The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.","['Image analysis', 'Machine learning algorithms', 'Medical diagnostic imaging', 'Convolution']","['Convolutional neural networks', 'medical image analysis', 'machine learning', 'deep learning']"
"Underwater wireless information transfer is of great interest to the military, industry, and the scientific community, as it plays an important role in tactical surveillance, pollution monitoring, oil control and maintenance, offshore explorations, climate change monitoring, and oceanography research. In order to facilitate all these activities, there is an increase in the number of unmanned vehicles or devices deployed underwater, which require high bandwidth and high capacity for information transfer underwater. Although tremendous progress has been made in the field of acoustic communication underwater, however, it is limited by bandwidth. All this has led to the proliferation of underwater optical wireless communication (UOWC), as it provides higher data rates than the traditional acoustic communication systems with significantly lower power consumption and simpler computational complexities for short-range wireless links. UOWC has many potential applications ranging from deep oceans to coastal waters. However, the biggest challenge for underwater wireless communication originates from the fundamental characteristics of ocean or sea water; addressing these challenges requires a thorough understanding of complex physio-chemical biological systems. In this paper, the main focus is to understand the feasibility and the reliability of high data rate underwater optical links due to various propagation phenomena that impact the performance of the system. This paper provides an exhaustive overview of recent advances in UOWC. Channel characterization, modulation schemes, coding techniques, and various sources of noise which are specific to UOWC are discussed. This paper not only provides exhaustive research in underwater optical communication but also aims to provide the development of new ideas that would help in the growth of future underwater communication. A hybrid approach to an acousto-optic communication system is presented that complements the existing acoustic system, resulting in high data rates, low latency, and an energy-efficient system.","['Wireless communication', 'Optical fiber communication', 'Bandwidth allocation', 'Acoustic communication', 'Oceans', 'Underwater communication', 'Radio frequency', 'Modulation', 'Climate change']","['Underwater optical wireless', 'optical beam propagation', 'visible light', 'radio frequency', 'acoustic communication', 'hybrid optical-acoustic system', 'modulation and coding']"
"Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision, and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this paper is to provide a comprehensive study and an updated review on sparse representation and to supply guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: 1) sparse representation with l 0 -norm minimization; 2) sparse representation with lp-norm (0 <; p <; 1) minimization; 3) sparse representation with l 1 -norm minimization; 4) sparse representation with l 2 ,1-norm minimization; and 5) sparse representation with l2-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: 1) greedy strategy approximation; 2) constrained optimization; 3) proximity algorithm-based optimization; and 4) homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. In particular, an experimentally comparative study of these sparse representation algorithms was presented.","['Sparse matrices', 'Algorithm design and analysis', 'Signal processing algorithms', 'Approximation algorithms', 'Approximation methods', 'Signal processing']","['Sparse representation', 'compressive sensing', 'greedy algorithm', 'constrained optimization', 'proximal algorithm', 'homotopy algorithm', 'dictionary learning']"
"Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01-0.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data by passing them into many hidden layers. Through a rigorous experimental testing, it is confirmed that DNNs perform well in comparison with the classical machine learning classifiers. Finally, we propose a highly scalable and hybrid DNNs framework called scale-hybrid-IDS-AlertNet which can be used in real-time to effectively monitor the network traffic and host-level events to proactively alert possible cyberattacks.","['Intrusion detection', 'Computer security', 'Deep learning', 'Malware', 'Benchmark testing']","['Cyber security', 'intrusion detection', 'malware', 'big data', 'machine learning', 'deep learning', 'deep neural networks', 'cyberattacks', 'cybercrime']"
"The significant benefits associated with microgrids have led to vast efforts to expand their penetration in electric power systems. Although their deployment is rapidly growing, there are still many challenges to efficiently design, control, and operate microgrids when connected to the grid, and also when in islanded mode, where extensive research activities are underway to tackle these issues. It is necessary to have an across-the-board view of the microgrid integration in power systems. This paper presents a review of issues concerning microgrids and provides an account of research in areas related to microgrids, including distributed generation, microgrid value propositions, applications of power electronics, economic issues, microgrid operation and control, microgrid clusters, and protection and communications issues.",[],[]
"Digital Twin technology is an emerging concept that has become the centre of attention for industry and, in more recent years, academia. The advancements in industry 4.0 concepts have facilitated its growth, particularly in the manufacturing industry. The Digital Twin is defined extensively but is best described as the effortless integration of data between a physical and virtual machine in either direction. The challenges, applications, and enabling technologies for Artificial Intelligence, Internet of Things (IoT) and Digital Twins are presented. A review of publications relating to Digital Twins is performed, producing a categorical review of recent papers. The review has categorised them by research areas: manufacturing, healthcare and smart cities, discussing a range of papers that reflect these areas and the current state of research. The paper provides an assessment of the enabling technologies, challenges and open research for Digital Twins.","['Smart cities', 'Data analysis', 'Manufacturing', 'Data models', 'Internet of Things', 'Computational modeling']","['Digital twins', 'applications', 'enabling technologies', 'industrial Internet of Things (IIoT)', 'Internet of Things (IoT)', 'machine learning', 'deep learning', 'literature review']"
"The Internet of Things (IoT) is the next era of communication. Using the IoT, physical objects can be empowered to create, receive, and exchange data in a seamless manner. Various IoT applications focus on automating different tasks and are trying to empower the inanimate physical objects to act without any human intervention. The existing and upcoming IoT applications are highly promising to increase the level of comfort, efficiency, and automation for the users. To be able to implement such a world in an ever-growing fashion requires high security, privacy, authentication, and recovery from attacks. In this regard, it is imperative to make the required changes in the architecture of the IoT applications for achieving end-to-end secure IoT environments. In this paper, a detailed review of the security-related challenges and sources of threat in the IoT applications is presented. After discussing the security issues, various emerging and existing technologies focused on achieving a high degree of trust in the IoT applications are discussed. Four different technologies, blockchain, fog computing, edge computing, and machine learning, to increase the level of security in IoT are discussed.","['Internet of Things', 'Security', 'Edge computing', 'Computer architecture', 'Privacy', 'Blockchain']","['Internet of Things (IoT)', 'IoT security', 'blockchain', 'fog computing', 'edge computing', 'machine learning', 'IoT applications', 'distributed systems']"
"The dissemination of patients' medical records results in diverse risks to patients' privacy as malicious activities on these records cause severe damage to the reputation, finances, and so on of all parties related directly or indirectly to the data. Current methods to effectively manage and protect medical records have been proved to be insufficient. In this paper, we propose MeDShare, a system that addresses the issue of medical data sharing among medical big data custodians in a trust-less environment. The system is blockchain-based and provides data provenance, auditing, and control for shared medical data in cloud repositories among big data entities. MeDShare monitors entities that access data for malicious use from a data custodian system. In MeDShare, data transitions and sharing from one entity to the other, along with all actions performed on the MeDShare system, are recorded in a tamper-proof manner. The design employs smart contracts and an access control mechanism to effectively track the behavior of the data and revoke access to offending entities on detection of violation of permissions on data. The performance of MeDShare is comparable to current cutting edge solutions to data sharing among cloud service providers. By implementing MeDShare, cloud service providers and other data guardians will be able to achieve data provenance and auditing while sharing medical data with entities such as research and medical institutions with minimal risk to data privacy.","['Cloud computing', 'Contracts', 'Access control', 'Electronic mail', 'Monitoring', 'Public key']","['Access control', 'blockchain', 'cloud computing', 'data sharing', 'electronic medical records', 'privacy']"
"Recent technological advancements have led to a deluge of data from distinctive domains (e.g., health care and scientific sensors, user-generated data, Internet and financial companies, and supply chain systems) over the past two decades. The term big data was coined to capture the meaning of this emerging trend. In addition to its sheer volume, big data also exhibits other unique characteristics as compared with traditional data. For instance, big data is commonly unstructured and require more real-time analysis. This development calls for new system architectures for data acquisition, transmission, storage, and large-scale data processing mechanisms. In this paper, we present a literature survey and system tutorial for big data analytics platforms, aiming to provide an overall picture for nonexpert readers and instill a do-it-yourself spirit for advanced audiences to customize their own big-data solutions. First, we present the definition of big data and discuss big data challenges. Next, we present a systematic framework to decompose big data systems into four sequential modules, namely data generation, data acquisition, data storage, and data analytics. These four modules form a big data value chain. Following that, we present a detailed survey of numerous approaches and mechanisms from research and industry communities. In addition, we present the prevalent Hadoop framework for addressing big data challenges. Finally, we outline several evaluation benchmarks and potential research directions for big data systems.","['Big data', 'Information analysis', 'Scalability', 'Data acquisition', 'Medical services', 'Supply chain management', 'Sensor phenomena and characterization', 'Sensor systems', 'Real-time systems', 'Tutorials']","['Big data analytics', 'cloud computing', 'data acquisition', 'data storage', 'data analytics', 'Hadoop']"
"With the developments and applications of the new information technologies, such as cloud computing, Internet of Things, big data, and artificial intelligence, a smart manufacturing era is coming. At the same time, various national manufacturing development strategies have been put forward, such as Industry 4.0, Industrial Internet, manufacturing based on Cyber-Physical System, and Made in China 2025. However, one of specific challenges to achieve smart manufacturing with these strategies is how to converge the manufacturing physical world and the virtual world, so as to realize a series of smart operations in the manufacturing process, including smart interconnection, smart interaction, smart control and management, etc. In this context, as a basic unit of manufacturing, shop-floor is required to reach the interaction and convergence between physical and virtual spaces, which is not only the imperative demand of smart manufacturing, but also the evolving trend of itself. Accordingly, a novel concept of digital twin shopfloor (DTS) based on digital twin is explored and its four key components are discussed, including physical shop-floor, virtual shop-floor, shop-floor service system, and shop-floor digital twin data. What is more, the operation mechanisms and implementing methods for DTS are studied and key technologies as well as challenges ahead are investigated, respectively.","['Production', 'Manufacturing', 'Convergence', 'Data models', 'Aerospace electronics', 'Optimization', 'Cloud computing']","['Smart manufacturing', 'digital twin shop-floor (DTS)', 'digital twin', 'virtual shop-floor (VS)', 'shop-floor service system (SSS)', 'shop-floor digital twin data (SDTD)', 'convergence', 'cyber-physical system (CPS)']"
"Fully convolutional neural networks (FCNs) have been shown to achieve the state-of-the-art performance on the task of classifying time series sequences. We propose the augmentation of fully convolutional networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the data set. The proposed long short term memory fully convolutional network (LSTM-FCN) achieves the state-of-the-art performance compared with others. We also explore the usage of attention mechanism to improve time series classification with the attention long short term memory fully convolutional network (ALSTM-FCN). The attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose refinement as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared with other techniques.","['Time series analysis', 'Recurrent neural networks', 'Feature extraction', 'Convolution', 'Computer architecture', 'Machine learning', 'Machine learning algorithms']","['Convolutional neural network', 'long short term memory recurrent neural network', 'time series classification']"
"The unprecedented outbreak of the 2019 novel coronavirus, termed as COVID-19 by the World Health Organization (WHO), has placed numerous governments around the world in a precarious position. The impact of the COVID-19 outbreak, earlier witnessed by the citizens of China alone, has now become a matter of grave concern for virtually every country in the world. The scarcity of resources to endure the COVID-19 outbreak combined with the fear of overburdened healthcare systems has forced a majority of these countries into a state of partial or complete lockdown. The number of laboratory-confirmed coronavirus cases has been increasing at an alarming rate throughout the world, with reportedly more than 3 million confirmed cases as of 30 April 2020. Adding to these woes, numerous false reports, misinformation, and unsolicited fears in regards to coronavirus, are being circulated regularly since the outbreak of the COVID-19. In response to such acts, we draw on various reliable sources to present a detailed review of all the major aspects associated with the COVID-19 pandemic. In addition to the direct health implications associated with the outbreak of COVID-19, this study highlights its impact on the global economy. In drawing things to a close, we explore the use of technologies such as the Internet of Things (IoT), Unmanned Aerial Vehicles (UAVs), blockchain, Artificial Intelligence (AI), and 5G, among others, to help mitigate the impact of COVID-19 outbreak.","['COVID-19', 'Viruses (medical)', 'Pandemics', 'Artificial intelligence', 'Blockchain', '5G mobile communication']","['Coronavirus', 'COVID-19', 'pandemic', 'transmission stages', 'global economic impact', 'UAVs for disaster management', 'Blockchain', 'IoMT applications', 'IoT', 'AI', '5G']"
"Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).","['Diseases', 'Heart', 'Data mining', 'Support vector machines', 'Feature extraction', 'Machine learning', 'Predictive models']","['Machine learning', 'heart disease prediction', 'feature selection', 'prediction model', 'classification algorithms', 'cardiovascular disease (CVD)']"
"Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in people’s life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning algorithms for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline thoroughly and deeply, in this survey, we analyze the methods of existing typical detection models and describe the benchmark datasets at first. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.","['Deep learning', 'Training data', 'Image classification', 'Transportation', 'Object detection', 'Detectors', 'Computer architecture', 'Computer vision', 'Pipelines']","['Classification', 'deep learning', 'localization', 'object detection', 'typical pipelines']"
"With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.","['Diseases', 'Hospitals', 'Prediction algorithms', 'Machine learning algorithms', 'Big Data', 'Data models']","['Big data analytics', 'machine learning', 'healthcare']"
"As the explosive growth of smart devices and the advent of many new applications, traffic volume has been growing exponentially. The traditional centralized network architecture cannot accommodate such user demands due to heavy burden on the backhaul links and long latency. Therefore, new architectures, which bring network functions and contents to the network edge, are proposed, i.e., mobile edge computing and caching. Mobile edge networks provide cloud computing and caching capabilities at the edge of cellular networks. In this survey, we make an exhaustive review on the state-of-the-art research efforts on mobile edge networks. We first give an overview of mobile edge networks, including definition, architecture, and advantages. Next, a comprehensive survey of issues on computing, caching, and communication techniques at the network edge is presented. The applications and use cases of mobile edge networks are discussed. Subsequently, the key enablers of mobile edge networks, such as cloud technology, SDN/NFV, and smart devices are discussed. Finally, open research challenges and future directions are presented as well.","['Mobile communication', 'Cloud computing', 'Mobile computing', 'Edge computing', 'Computer architecture', 'Network architecture', 'Mobile handsets']","['Mobile edge computing', 'mobile edge caching', 'D2D', 'SDN', 'NFV', 'content delivery', 'computational offloading']"
"Internet of Things (IoT) technology has attracted much attention in recent years for its potential to alleviate the strain on healthcare systems caused by an aging population and a rise in chronic illness. Standardization is a key issue limiting progress in this area, and thus this paper proposes a standard model for application in future IoT healthcare systems. This survey paper then presents the state-of-the-art research relating to each area of the model, evaluating their strengths, weaknesses, and overall suitability for a wearable IoT healthcare system. Challenges that healthcare IoT faces including security, privacy, wearability, and low-power operation are presented, and recommendations are made for future research directions.","['Medical services', 'Monitoring', 'Internet of Things', 'Biomedical monitoring', 'Cloud computing', 'Strain', 'Standards']","['Biomedical engineering', 'body sensor networks', 'intelligent systems', 'Internet of Things (IoT)', 'communications standards', 'security', 'wearable sensors']"
"Due to the current structure of digital factory, it is necessary to build the smart factory to upgrade the manufacturing industry. Smart factory adopts the combination of physical technology and cyber technology and deeply integrates previously independent discrete systems making the involved technologies more complex and precise than they are now. In this paper, a hierarchical architecture of the smart factory was proposed first, and then the key technologies were analyzed from the aspects of the physical resource layer, the network layer, and the data application layer. In addition, we discussed the major issues and potential solutions to key emerging technologies, such as Internet of Things (IoT), big data, and cloud computing, which are embedded in the manufacturing process. Finally, a candy packing line was used to verify the key technologies of smart factory, which showed that the overall equipment effectiveness of the equipment is significantly improved.","['Manufacturing', 'Production facilities', 'Computer architecture', 'Cloud computing', 'Robot kinematics']","['Smart factory', 'big data', 'cloud computing', 'cyber-physical systems', 'industrial Internet of Things']"
"The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks. Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance. Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin. In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented. After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications. Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application. Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications.","['Peer-to-peer computing', 'Bitcoin', 'Internet of Things', 'Cloud computing', 'Computer architecture']","['IoT', 'blockchain', 'traceability', 'consensus', 'distributed systems', 'BIoT', 'fog computing', 'edge computing']"
"Digital twin can be defined as a virtual representation of a physical asset enabled through data and simulators for real-time prediction, optimization, monitoring, controlling, and improved decision making. Recent advances in computational pipelines, multiphysics solvers, artificial intelligence, big data cybernetics, data processing and management tools bring the promise of digital twins and their impact on society closer to reality. Digital twinning is now an important and emerging trend in many applications. Also referred to as a computational megamodel, device shadow, mirrored system, avatar or a synchronized virtual prototype, there can be no doubt that a digital twin plays a transformative role not only in how we design and operate cyber-physical intelligent systems, but also in how we advance the modularity of multi-disciplinary systems to tackle fundamental barriers not addressed by the current, evolutionary modeling practices. In this work, we review the recent status of methodologies and techniques related to the construction of digital twins mostly from a modeling perspective. Our aim is to provide a detailed coverage of the current challenges and enabling technologies along with recommendations and reflections for various stakeholders.","['Digital twin', 'Real-time systems', 'Monitoring', 'Solid modeling', 'Biological system modeling', 'Big Data', 'Buildings']","['Digital twin', 'artificial intelligence', 'machine learning', 'big data cybernetics', 'hybrid analysis and modeling']"
"Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.","['Automation', 'Task analysis', 'Systems architecture', 'Accidents', 'Planning', 'Vehicle dynamics', 'Robot sensing systems']","['Autonomous vehicles', 'control', 'robotics', 'automation', 'intelligent vehicles', 'intelligent transportation systems']"
"This paper promotes the concept of smart and connected communities SCC, which is evolving from the concept of smart cities. SCC are envisioned to address synergistically the needs of remembering the past (preservation and revitalization), the needs of living in the present (livability), and the needs of planning for the future (attainability). Therefore, the vision of SCC is to improve livability, preservation, revitalization, and attainability of a community. The goal of building SCC for a community is to live in the present, plan for the future, and remember the past. We argue that Internet of Things (IoT) has the potential to provide a ubiquitous network of connected devices and smart sensors for SCC, and big data analytics has the potential to enable the move from IoT to real-time control desired for SCC. We highlight mobile crowdsensing and cyber-physical cloud computing as two most important IoT technologies in promoting SCC. As a case study, we present TreSight, which integrates IoT and big data analytics for smart tourism and sustainable cultural heritage in the city of Trento, Italy.","['Internet of things', 'Smart cities', 'Big data', 'Sensors', 'Cultural differences', 'Economics', 'Urban areas', 'Data analytics', 'Sustainable development']","['Internet of Things', 'Big Data Analytics', 'Smart and Connected Communities', 'Smart Cities', 'Smart Tourism', 'Sustainable Cultural Heritage']"
"With the development of the Internet, cyber-attacks are changing rapidly and the cyber security situation is not optimistic. This survey report describes key literature surveys on machine learning (ML) and deep learning (DL) methods for network analysis of intrusion detection and provides a brief tutorial description of each ML/DL method. Papers representing each method were indexed, read, and summarized based on their temporal or thermal correlations. Because data are so important in ML/DL methods, we describe some of the commonly used network datasets used in ML/DL, discuss the challenges of using ML/DL for cybersecurity and provide suggestions for research directions.","['Machine learning', 'Intrusion detection', 'Feature extraction', 'Machine learning algorithms', 'Computer security']","['Cybersecurity', 'intrusion detection', 'deep learning', 'machine learning']"
"Mobile edge computing (MEC) is a promising paradigm to provide cloud-computing capabilities in close proximity to mobile devices in fifth-generation (5G) networks. In this paper, we study energy-efficient computation offloading (EECO) mechanisms for MEC in 5G heterogeneous networks. We formulate an optimization problem to minimize the energy consumption of the offloading system, where the energy cost of both task computing and file transmission are taken into consideration. Incorporating the multi-access characteristics of the 5G heterogeneous network, we then design an EECO scheme, which jointly optimizes offloading and radio resource allocation to obtain the minimal energy consumption under the latency constraints. Numerical results demonstrate energy efficiency improvement of our proposed EECO scheme.","['Cloud computing', 'Mobile handsets', '5G mobile communication', 'Energy consumption', 'Energy efficiency', 'Heterogeneous networks']","['Energy-efficiency', 'offloading', 'mobile edge computing', '5G']"
"Data mining and analytics have played an important role in knowledge discovery and decision making/supports in the process industry over the past several decades. As a computational engine to data mining and analytics, machine learning serves as basic tools for information extraction, data pattern recognition and predictions. From the perspective of machine learning, this paper provides a review on existing data mining and analytics applications in the process industry over the past several decades. The state-of-the-art of data mining and analytics are reviewed through eight unsupervised learning and ten supervised learning algorithms, as well as the application status of semi-supervised learning algorithms. Several perspectives are highlighted and discussed for future researches on data mining and analytics in the process industry.","['Data mining', 'Industries', 'Data models', 'Machine learning algorithms', 'Analytical models', 'Manufacturing', 'Predictive models']","['Data mining', 'data analytics', 'machine learning', 'process industry']"
"Python has become the programming language of choice for research and industry projects related to data science, machine learning, and deep learning. Since optimization is an inherent part of these research fields, more optimization related frameworks have arisen in the past few years. Only a few of them support optimization of multiple conflicting objectives at a time, but do not provide comprehensive tools for a complete multi-objective optimization task. To address this issue, we have developed pymoo, a multi-objective optimization framework in Python. We provide a guide to getting started with our framework by demonstrating the implementation of an exemplary constrained multi-objective optimization scenario. Moreover, we give a high-level overview of the architecture of pymoo to show its capabilities followed by an explanation of each module and its corresponding sub-modules. The implementations in our framework are customizable and algorithms can be modified/extended by supplying custom operators. Moreover, a variety of single, multi- and many-objective test problems are provided and gradients can be retrieved by automatic differentiation out of the box. Also, pymoo addresses practical needs, such as the parallelization of function evaluations, methods to visualize low and high-dimensional spaces, and tools for multi-criteria decision making. For more information about pymoo, readers are encouraged to visit: https://pymoo.org.","['Optimization', 'Python', 'Tools', 'Task analysis', 'Data visualization', 'Evolutionary computation']","['Customization', 'genetic algorithm', 'multi-objective optimization', 'python']"
"The Internet of Things (IoT)-centric concepts like augmented reality, high-resolution video streaming, self-driven cars, smart environment, e-health care, etc. have a ubiquitous presence now. These applications require higher data-rates, large bandwidth, increased capacity, low latency and high throughput. In light of these emerging concepts, IoT has revolutionized the world by providing seamless connectivity between heterogeneous networks (HetNets). The eventual aim of IoT is to introduce the plug and play technology providing the end-user, ease of operation, remotely access control and configurability. This paper presents the IoT technology from a bird's eye view covering its statistical/architectural trends, use cases, challenges and future prospects. The paper also presents a detailed and extensive overview of the emerging 5G-IoT scenario. Fifth Generation (5G) cellular networks provide key enabling technologies for ubiquitous deployment of the IoT technology. These include carrier aggregation, multiple-input multiple-output (MIMO), massive-MIMO (M-MIMO), coordinated multipoint processing (CoMP), device-to-device (D2D) communications, centralized radio access network (CRAN), software-defined wireless sensor networking (SD-WSN), network function virtualization (NFV) and cognitive radios (CRs). This paper presents an exhaustive review for these key enabling technologies and also discusses the new emerging use cases of 5G-IoT driven by the advances in artificial intelligence, machine and deep learning, ongoing 5G initiatives, quality of service (QoS) requirements in 5G and its standardization issues. Finally, the paper discusses challenges in the implementation of 5G-IoT due to high data-rates requiring both cloud-based platforms and IoT devices based edge computing.","['5G mobile communication', 'Market research', 'Protocols', 'Internet of Things', 'Quality of service', 'Security', 'Next generation networking']","['Internet of Things (IoT)', '5G', 'carrier aggregation', 'CoMP', 'CRAN', 'CRs', 'HetNets', 'MIMO', 'M-MIMO', 'NFV', 'SD-WSN', 'QoS']"
"Hybrid analog/digital multiple-input multiple-output architectures were recently proposed as an alternative for fully digital-precoding in millimeter wave wireless communication systems. This is motivated by the possible reduction in the number of RF chains and analog-to-digital converters. In these architectures, the analog processing network is usually based on variable phase shifters. In this paper, we propose hybrid architectures based on switching networks to reduce the complexity and the power consumption of the structures based on phase shifters. We define a power consumption model and use it to evaluate the energy efficiency of both structures. To estimate the complete MIMO channel, we propose an open-loop compressive channel estimation technique that is independent of the hardware used in the analog processing stage. We analyze the performance of the new estimation algorithm for hybrid architectures based on phase shifters and switches. Using the estimate, we develop two algorithms for the design of the hybrid combiner based on switches and analyze the achieved spectral efficiency. Finally, we study the tradeoffs between power consumption, hardware complexity, and spectral efficiency for hybrid architectures based on phase shifting networks and switching networks. Numerical results show that architectures based on switches obtain equal or better channel estimation performance to that obtained using phase shifters, while reducing hardware complexity and power consumption. For equal power consumption, all the hybrid architectures provide similar spectral efficiencies.","['Channel estimation', 'Radio frequency', 'Power demand', 'Phase shifters', 'MIMO', 'Switches', 'Antenna arrays', 'Millimeter wave communication', 'Hybrid systems']","['Millimeter wave', 'hybrid architecture', 'switches', 'channel estimation', 'precoding']"
"What is index modulation (IM)? This is an interesting question that we have started to hear more and more frequently over the past few years. The aim of this paper is to answer this question in a comprehensive manner by covering not only the basic principles and emerging variants of IM, but also reviewing the most recent as well as promising advances in this field toward the application scenarios foreseen in next-generation wireless networks. More specifically, we investigate three forms of IM: spatial modulation, channel modulation and orthogonal frequency division multiplexing (OFDM) with IM, which consider the transmit antennas of a multiple-input multiple-output system, the radio frequency mirrors (parasitic elements) mounted at a transmit antenna and the subcarriers of an OFDM system for IM techniques, respectively. We present the up-to-date advances in these three promising frontiers and discuss possible future research directions for IM-based schemes toward low-complexity, spectrum- and energy-efficient next-generation wireless networks.","['OFDM', 'Indexes', 'Phase modulation', 'MIMO', 'Optical transmitters', 'Optical modulation']","['5G wireless networks', 'channel modulation', 'cognitive radio networks', 'cooperative networks', 'full-duplex networks', 'index modulation', 'MIMO systems', 'multi-carrier systems', 'multi-user systems', 'OFDM', 'OFDM with index modulation', 'practical implementations', 'reconfigurable antennas', 'spatial modulation', 'vehicular communications', 'visible light communications']"
"Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics.","['Hidden Markov models', 'Speech recognition', 'Neural networks', 'Deep learning', 'Feature extraction', 'Computer architecture', 'Acoustics']","['Speech recognition', 'deep neural network', 'systematic review']"
"The k-means algorithm is generally the most known and used clustering method. There are various extensions of k-means to be proposed in the literature. Although it is an unsupervised learning to clustering in pattern recognition and machine learning, the k-means algorithm and its extensions are always influenced by initializations with a necessary number of clusters a priori. That is, the k-means algorithm is not exactly an unsupervised clustering method. In this paper, we construct an unsupervised learning schema for the k-means algorithm so that it is free of initializations without parameter selection and can also simultaneously find an optimal number of clusters. That is, we propose a novel unsupervised k-means (U-k-means) clustering algorithm with automatically finding an optimal number of clusters without giving any initialization and parameter selection. The computational complexity of the proposed U-k-means clustering algorithm is also analyzed. Comparisons between the proposed U-k-means and other existing methods are made. Experimental results and comparisons actually demonstrate these good aspects of the proposed U-k-means clustering algorithm.","['Clustering algorithms', 'Indexes', 'Linear programming', 'Entropy', 'Clustering methods', 'Unsupervised learning', 'Machine learning algorithms']","['Clustering', 'K-means', 'number of clusters', 'initializations', 'unsupervised learning schema', 'Unsupervised k-means (U-k-means)']"
"The past decade has witnessed the rapid evolution in blockchain technologies, which has attracted tremendous interests from both the research communities and industries. The blockchain network was originated from the Internet financial sector as a decentralized, immutable ledger system for transactional data ordering. Nowadays, it is envisioned as a powerful backbone/framework for decentralized data processing and data-driven self-organization in flat, open-access networks. In particular, the plausible characteristics of decentralization, immutability, and self-organization are primarily owing to the unique decentralized consensus mechanisms introduced by blockchain networks. This survey is motivated by the lack of a comprehensive literature review on the development of decentralized consensus mechanisms in blockchain networks. In this paper, we provide a systematic vision of the organization of blockchain networks. By emphasizing the unique characteristics of decentralized consensus in blockchain networks, our in-depth review of the state-of-the-art consensus protocols is focused on both the perspective of distributed consensus system design and the perspective of incentive mechanism design. From a game-theoretic point of view, we also provide a thorough review of the strategy adopted for self-organization by the individual nodes in the blockchain backbone networks. Consequently, we provide a comprehensive survey of the emerging applications of blockchain networks in a broad area of telecommunication. We highlight our special interest in how the consensus mechanisms impact these applications. Finally, we discuss several open issues in the protocol design for blockchain consensus and the related potential research directions.","['Blockchain', 'Protocols', 'Organizations', 'Bitcoin', 'Scalability', 'Standards organizations']","['Blockchain', 'permissionless consensus', 'Byzantine fault tolerance', 'block mining', 'incentive mechanisms', 'game theory', 'P2P networks']"
"Voluminous amounts of data have been produced, since the past decade as the miniaturization of Internet of things (IoT) devices increases. However, such data are not useful without analytic power. Numerous big data, IoT, and analytics solutions have enabled people to obtain valuable insight into large data generated by IoT devices. However, these solutions are still in their infancy, and the domain lacks a comprehensive survey. This paper investigates the state-of-the-art research efforts directed toward big IoT data analytics. The relationship between big data analytics and IoT is explained. Moreover, this paper adds value by proposing a new architecture for big IoT data analytics. Furthermore, big IoT data analytic types, methods, and technologies for big data mining are discussed. Numerous notable use cases are also presented. Several opportunities brought by data analytics in IoT paradigm are then discussed. Finally, open research challenges, such as privacy, big data mining, visualization, and integration, are presented as future research directions.","['Big Data', 'Data analysis', 'Tools', 'Computer architecture', 'Business', 'Sensors', 'Data mining']","['Big data', 'Internet of Things', 'data analytics', 'distributed computing', 'smart city']"
"The grand objective of 5G wireless technology is to support three generic services with vastly heterogeneous requirements: enhanced mobile broadband (eMBB), massive machine-type communications (mMTCs), and ultra-reliable low-latency communications (URLLCs). Service heterogeneity can be accommodated by network slicing, through which each service is allocated resources to provide performance guarantees and isolation from the other services. Slicing of the radio access network (RAN) is typically done by means of orthogonal resource allocation among the services. This paper studies the potential advantages of allowing for non-orthogonal sharing of RAN resources in uplink communications from a set of eMBB, mMTC, and URLLC devices to a common base station. The approach is referred to as heterogeneous nonorthogonal multiple access (H-NOMA), in contrast to the conventional NOMA techniques that involve users with homogeneous requirements and hence can be investigated through a standard multiple access channel. The study devises a communication-theoretic model that accounts for the heterogeneous requirements and characteristics of the three services. The concept of reliability diversity is introduced as a design principle that leverages the different reliability requirements across the services in order to ensure performance guarantees with non-orthogonal RAN slicing. This paper reveals that H-NOMA can lead, in some regimes, to significant gains in terms of performance tradeoffs among the three generic services as compared to orthogonal slicing.","['NOMA', 'Reliability', 'Wireless communication', 'Resource management', 'Radio spectrum management', 'Time-frequency analysis', '5G mobile communication']","['5G mobile communication', 'machine-to-machine communications', 'multiaccess communication', 'NOMA', 'wireless communication']"
"6G and beyond will fulfill the requirements of a fully connected world and provide ubiquitous wireless connectivity for all. Transformative solutions are expected to drive the surge for accommodating a rapidly growing number of intelligent devices and services. Major technological breakthroughs to achieve connectivity goals within 6G include: (i) a network operating at the THz band with much wider spectrum resources, (ii) intelligent communication environments that enable a wireless propagation environment with active signal transmission and reception, (iii) pervasive artificial intelligence, (iv) large-scale network automation, (v) an all-spectrum reconfigurable front-end for dynamic spectrum access, (vi) ambient backscatter communications for energy savings, (vii) the Internet of Space Things enabled by CubeSats and UAVs, and (viii) cell-free massive MIMO communication networks. In this roadmap paper, use cases for these enabling techniques as well as recent advancements on related topics are highlighted, and open problems with possible solutions are discussed, followed by a development timeline outlining the worldwide efforts in the realization of 6G. Going beyond 6G, promising early-stage technologies such as the Internet of NanoThings, the Internet of BioNanoThings, and quantum communications, which are expected to have a far-reaching impact on wireless communications, have also been discussed at length in this paper.","['6G mobile communication', 'Wireless communication', '5G mobile communication', 'Automation', 'Internet', 'Measurement', 'Communication system security']","['6G', 'wireless communications', 'terahertz band', 'intelligent communication environments', 'pervasive artificial intelligence', 'network automation', 'all-spectrum reconfigurable transceivers', 'ambient backscatter communications', 'cell-free massive MIMO', 'Internet of NanoThings', 'Internet of BioNanoThings', 'quantum communications']"
"Recently, artificial intelligence (AI) and blockchain have become two of the most trending and disruptive technologies. Blockchain technology has the ability to automate payment in cryptocurrency and to provide access to a shared ledger of data, transactions, and logs in a decentralized, secure, and trusted manner. Also with smart contracts, blockchain has the ability to govern interactions among participants with no intermediary or a trusted third party. AI, on the other hand, offers intelligence and decision-making capabilities for machines similar to humans. In this paper, we present a detailed survey on blockchain applications for AI. We review the literature, tabulate, and summarize the emerging blockchain applications, platforms, and protocols specifically targeting AI area. We also identify and discuss open research challenges of utilizing blockchain technologies for AI.","['Blockchain', 'Smart contracts', 'Machine learning', 'Decision making', 'Machine learning algorithms', 'Data mining']","['Artificial intelligence', 'machine learning', 'blockchain', 'cybersecurity', 'smart contracts', 'consensus protocols']"
"A variety of rechargeable batteries are now available in world markets for powering electric vehicles (EVs). The lithium-ion (Li-ion) battery is considered the best among all battery types and cells because of its superior characteristics and performance. The positive environmental impacts and recycling potential of lithium batteries have influenced the development of new research for improving Li-ion battery technologies. However, the cost reduction, safe operation, and mitigation of negative ecological impacts are now a common concern for advancement. This paper provides a comprehensive study on the state of the art of Li-ion batteries including the fundamentals, structures, and overall performance evaluations of different types of lithium batteries. A study on a battery management system for Li-ion battery storage in EV applications is demonstrated, which includes a cell condition monitoring, charge, and discharge control, states estimation, protection and equalization, temperature control and heat management, battery fault diagnosis, and assessment aimed at enhancing the overall performance of the system. It is observed that the Li-ion batteries are becoming very popular in vehicle applications due to price reductions and lightweight with high power density. However, the management of the charging and discharging processes, CO2 and greenhouse gases emissions, health effects, and recycling and refurbishing processes have still not been resolved satisfactorily. Consequently, this review focuses on the many factors, challenges, and problems and provides recommendations for sustainable battery manufacturing for future EVs. This review will hopefully lead to increasing efforts toward the development of an advanced Li-ion battery in terms of economics, longevity, specific power, energy density, safety, and performance in vehicle applications.","['Lithium-ion batteries', 'Lithium', 'Cathodes', 'Ions', 'Anodes', 'Electrolytes']","['Lithium-ion battery', 'state-of-the-art of lithium-ion battery', 'energy management system', 'electric vehicle']"
"Unlike previous studies on the Metaverse based on Second Life, the current Metaverse is based on the social value of Generation Z that online and offline selves are not different. With the technological development of deep learning-based high-precision recognition models and natural generation models, Metaverse is being strengthened with various factors, from mobile-based always-on access to connectivity with reality using virtual currency. The integration of enhanced social activities and neural-net methods requires a new definition of Metaverse suitable for the present, different from the previous Metaverse. This paper divides the concepts and essential techniques necessary for realizing the Metaverse into three components (i.e., hardware, software, and contents) and three approaches (i.e., user interaction, implementation, and application) rather than marketing or hardware approach to conduct a comprehensive analysis. Furthermore, we describe essential methods based on three components and techniques to Metaverse’s representative Ready Player One, Roblox, and Facebook research in the domain of films, games, and studies. Finally, we summarize the limitations and directions for implementing the immersive Metaverse as social influences, constraints, and open challenges.","['Augmented reality', 'Avatars', 'Metaverse', 'Artificial intelligence', 'Solid modeling', 'Games', 'Virtual reality']","['Artificial intelligence', 'metaverse', 'cyber world', 'avatar', 'extended reality']"
"When, in 1956, Artificial Intelligence (AI) was officially declared a research field, no one would have ever predicted the huge influence and impact its description, prediction, and prescription capabilities were going to have on our daily lives. In parallel to continuous advances in AI, the past decade has seen the spread of broadband and ubiquitous connectivity, (embedded) sensors collecting descriptive high dimensional data, and improvements in big data processing techniques and cloud computing. The joint usage of such technologies has led to the creation of digital twins, artificial intelligent virtual replicas of physical systems. Digital Twin (DT) technology is nowadays being developed and commercialized to optimize several manufacturing and aviation processes, while in the healthcare and medicine fields this technology is still at its early development stage. This paper presents the results of a study focused on the analysis of the state-of-the-art definitions of DT, the investigation of the main characteristics that a DT should possess, and the exploration of the domains in which DT applications are currently being developed. The design implications derived from the study are then presented: they focus on socio-technical design aspects and DT lifecycle. Open issues and challenges that require to be addressed in the future are finally discussed.","['Artificial intelligence', 'Data models', 'Atmospheric modeling', 'Military aircraft', 'Big Data', 'Sensors']","['Artificial intelligence', 'digital twin', 'human-computer interaction', 'Internet of Things', 'machine learning', 'sensor systems']"
"Blockchain is the underlying technology of a number of digital cryptocurrencies. Blockchain is a chain of blocks that store information with digital signatures in a decentralized and distributed network. The features of blockchain, including decentralization, immutability, transparency and auditability, make transactions more secure and tamper proof. Apart from cryptocurrency, blockchain technology can be used in financial and social services, risk management, healthcare facilities, and so on. A number of research studies focus on the opportunity that blockchain provides in various application domains. This paper presents a comparative study of the tradeoffs of blockchain and also explains the taxonomy and architecture of blockchain, provides a comparison among different consensus mechanisms and discusses challenges, including scalability, privacy, interoperability, energy consumption and regulatory issues. In addition, this paper also notes the future scope of blockchain technology.","['Blockchain', 'Bitcoin', 'Peer-to-peer computing', 'Digital signatures', 'Computer architecture', 'Government']","['Blockchain', 'distributed ledger', 'consensus procedures', 'cryptocurrency', 'smart contract', 'selfish mining', 'energy consumption']"
"Battery technology is the bottleneck of the electric vehicles (EVs). It is important, both in theory and practical application, to do research on the modeling and state estimation of batteries, which is essential to optimizing energy management, extending the life cycle, reducing cost, and safeguarding the safe application of batteries in EVs. However, the batteries, with strong time-variables and nonlinear characteristics, are further influenced by such random factors such as driving loads, operational conditions, in the application of EVs. The real-time, accurate estimation of their state is challenging. The classification of the estimation methodologies for estimating state-of-charge (SoC) of battery focusing with the estimation method/algorithm, advantages, drawbacks, and estimation error are systematically and separately discussed. Especially for the battery packs existing of the inevitable inconsistency in cell capacity, resistance and voltage, the advanced characterizing monomer selection, and bias correction-based method has been described and discussed. The review also presents the key feedback factors that are indispensable for accurate estimation of battery SoC, it will be helpful for ensuring the SoC estimation accuracy. It will be very helpful for choosing an appropriate method to develop a reliable and safe battery management system and energy management strategy of the EVs. Finally, the paper also highlights a number of key factors and challenges, and presents the possible recommendations for the development of next generation of smart SoC estimation and battery management systems for electric vehicles and battery energy storage system.","['Batteries', 'Estimation', 'Integrated circuit modeling', 'Mathematical model', 'Battery charge measurement', 'State of charge', 'Electric vehicles']","['Batteries', 'data-driven estimation', 'electric vehicles', 'model based estimation', 'multi-scale', 'state of charge']"
"The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause–effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.","['Big Data', 'Machine learning algorithms', 'Data mining', 'Algorithm design and analysis', 'Data analysis', 'Support vector machines', 'Classification algorithms']","['Big Data', 'Big Data Vs', 'data analysis', 'data analytics', 'deep learning', 'distributed computing', 'machine learning', 'neural networks']"
"To meet the fast-growing energy demand and, at the same time, tackle environmental concerns resulting from conventional energy sources, renewable energy sources are getting integrated in power networks to ensure reliable and affordable energy for the public and industrial sectors. However, the integration of renewable energy in the ageing electrical grids can result in new risks/challenges, such as security of supply, base load energy capacity, seasonal effects, and so on. Recent research and development in microgrids have proved that microgrids, which are fueled by renewable energy sources and managed by smart grids (use of smart sensors and smart energy management system), can offer higher reliability and more efficient energy systems in a cost-effective manner. Further improvement in the reliability and efficiency of electrical grids can be achieved by utilizing dc distribution in microgrid systems. DC microgrid is an attractive technology in the modern electrical grid system because of its natural interface with renewable energy sources, electric loads, and energy storage systems. In the recent past, an increase in research work has been observed in the area of dc microgrid, which brings this technology closer to practical implementation. This paper presents the state-of-the-art dc microgrid technology that covers ac interfaces, architectures, possible grounding schemes, power quality issues, and communication systems. The advantages of dc grids can be harvested in many applications to improve their reliability and efficiency. This paper also discusses benefits and challenges of using dc grid systems in several applications. This paper highlights the urgent need of standardizations for dc microgrid technology and presents recent updates in this area.","['Microgrids', 'Power system reliability', 'Reliability', 'Grounding', 'Renewable energy sources', 'Batteries', 'Power quality']","['DC microgrid', 'architectures', 'power quality', 'grounding', 'communication network', 'smart grid and standardization']"
"The 5G System is being developed and enhanced to provide unparalleled connectivity to connect everyone and everything, everywhere. The first version of the 5G System, based on the Release 15 (“Rel-15”) version of the specifications developed by 3GPP, comprising the 5G Core (5GC) and 5G New Radio (NR) with 5G User Equipment (UE), is currently being deployed commercially throughout the world both at sub-6 GHz and at mmWave frequencies. Concurrently, the second phase of 5G is being standardized by 3GPP in the Release 16 (“Rel-16”) version of the specifications which will be completed by March 2020. While the main focus of Rel-15 was on enhanced mobile broadband services, the focus of Rel-16 is on new features for URLLC (Ultra-Reliable Low Latency Communication) and Industrial IoT, including Time Sensitive Communication (TSC), enhanced Location Services, and support for Non-Public Networks (NPNs). In addition, some crucial new features, such as NR on unlicensed bands (NR-U), Integrated Access & Backhaul (IAB) and NR Vehicle-to-X (V2X), are also being introduced as part of Rel-16, as well as enhancements for massive MIMO, wireless and wireline convergence, the Service Based Architecture (SBA) and Network Slicing. Finally, the number of use cases, types of connectivity and users, and applications running on top of 5G networks, are all expected to increase dramatically, thus motivating additional security features to counter security threats which are expected to increase in number, scale and variety. In this paper, we discuss the Rel-16 features and provide an outlook towards Rel-17 and beyond, covering both new features and enhancements of existing features. 5G Evolution will focus on three main areas: enhancements to features introduced in Rel-15 and Rel-16, features that are needed for operational enhancements, and new features to further expand the applicability of the 5G System to new markets and use cases.","['5G mobile communication', 'Computer architecture', '3GPP', 'Data analysis', 'Wireless communication', 'Convergence', 'Security']","['5G new radio', '5G core', '5G system', 'non-public network', 'industrial IoT', 'time sensitive communication', 'ultra-reliable low-latency communications', 'integrated access and backhaul', 'converged edge and core clouds', 'positioning', 'NR-unlicensed', 'non-terrestrial network']"
"This paper presents an in-depth analysis of the majority of the deep neural networks (DNNs) proposed in the state of the art for image recognition. For each DNN, multiple performance indices are observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and inference time. The behavior of such performance indices and some combinations of them are analyzed and discussed. To measure the indices, we experiment the use of DNNs on two different computer architectures, a workstation equipped with a NVIDIA Titan X Pascal, and an embedded system based on a NVIDIA Jetson TX1 board. This experimentation allows a direct comparison between DNNs running on machines with very different computational capacities. This paper is useful for researchers to have a complete view of what solutions have been explored so far and in which research directions are worth exploring in the future, and for practitioners to select the DNN architecture(s) that better fit the resource constraints of practical deployments and applications. To complete this work, all the DNNs, as well as the software used for the analysis, are available online.","['Computational modeling', 'Graphics processing units', 'Computational complexity', 'Memory management', 'Embedded systems']","['Deep neural networks', 'convolutional neural networks', 'image recognition']"
"Motion planning is a fundamental research area in robotics. Sampling-based methods offer an efficient solution for what is otherwise a rather challenging dilemma of path planning. Consequently, these methods have been extended further away from basic robot planning into further difficult scenarios and diverse applications. A comprehensive survey of the growing body of work in sampling-based planning is given here. Simulations are executed to evaluate some of the proposed planners and highlight some of the implementation details that are often left unspecified. An emphasis is placed on contemporary research directions in this field. We address planners that tackle current issues in robotics. For instance, real-life kinodynamic planning, optimal planning, replanning in dynamic environments, and planning under uncertainty are discussed. The aim of this paper is to survey the state of the art in motion planning and to assess selected planners, examine implementation details and above all shed a light on the current challenges in motion planning and the promising approaches that will potentially overcome those problems.","['Planning', 'Measurement', 'Heuristic algorithms', 'Robot sensing systems', 'Path planning', 'Vegetation']","['Planning', 'sampling', 'randomization', 'RRT', 'PRM', 'path', 'motion', 'autonomous robots']"
"Despite the perception people may have regarding the agricultural process, the reality is that today's agriculture industry is data-centered, precise, and smarter than ever. The rapid emergence of the Internet-of-Things (IoT) based technologies redesigned almost every industry including “smart agriculture” which moved the industry from statistical to quantitative approaches. Such revolutionary changes are shaking the existing agriculture methods and creating new opportunities along a range of challenges. This article highlights the potential of wireless sensors and IoT in agriculture, as well as the challenges expected to be faced when integrating this technology with the traditional farming practices. IoT devices and communication techniques associated with wireless sensors encountered in agriculture applications are analyzed in detail. What sensors are available for specific agriculture application, like soil preparation, crop status, irrigation, insect and pest detection are listed. How this technology helping the growers throughout the crop stages, from sowing until harvesting, packing and transportation is explained. Furthermore, the use of unmanned aerial vehicles for crop surveillance and other favorable applications such as optimizing crop yield is considered in this article. State-of-the-art IoT-based architectures and platforms used in agriculture are also highlighted wherever suitable. Finally, based on this thorough review, we identify current and future trends of IoT in agriculture and highlight potential research challenges.","['Agriculture', 'Soil', 'Production', 'Intelligent sensors', 'Sociology']","['Food quality and quantity', 'Internet-of-Things (IoTs)', 'smart agriculture', 'advanced agriculture practices', 'urban farming', 'agriculture robots', 'automation', 'future food expectation']"
"Non-orthogonal multiple access (NOMA) has recently been considered as a key enabling technique for 5G cellular systems. In NOMA, by exploiting the channel gain differences, multiple users are multiplexed into transmission power domain and then non-orthogonally scheduled for transmission on the same spectrum resources. Successive interference cancellation (SIC) is then applied at the receivers to decode the message signals. In this paper, first, we briefly describe the differences in the working principles of uplink and downlink NOMA transmissions in a cellular wireless system. Then, for both uplink and downlink NOMAs, we formulate a sum-throughput maximization problem in a cell such that the user clustering (i.e., grouping users into a single cluster or multiple clusters) and power allocations in NOMA clusters can be optimized under transmission power constraints, minimum rate requirements of the users, and SIC constraints. Due to the combinatorial nature of the formulated mixed integer non-linear programming problem, we solve the problem in two steps, i.e., by first grouping users into clusters and then optimizing their respective power allocations. In particular, we propose a low-complexity sub-optimal user grouping scheme. The proposed scheme exploits the channel gain differences among users in an NOMA cluster and groups them into a single cluster or multiple clusters in order to enhance the sum-throughput of the system. For a given set of NOMA clusters, we then derive the optimal power allocation policy that maximizes the sum-throughput per NOMA cluster and in turn maximizes the overall system throughput. Using Karush-Kuhn-Tucker optimality conditions, closed-form solutions for optimal power allocations are derived for any cluster size, considering both uplink and downlink NOMA systems. Numerical results compare the performances of NOMA and OMA and illustrate the significance of NOMA in various network scenarios.","['NOMA', 'Resource management', 'Uplink', 'Downlink', '5G mobile communication', 'Multiplexing', 'Silicon carbide']","['5G cellular', 'non-orthogonal multiple access (NOMA)', 'orthogonal multiple access (OMA)', 'power allocation', 'throughput maximization', 'user grouping']"
"The recent expansion of the Internet of Things (IoT) and the consequent explosion in the volume of data produced by smart devices have led to the outsourcing of data to designated data centers. However, to manage these huge data stores, centralized data centers, such as cloud storage cannot afford auspicious way. There are many challenges that must be addressed in the traditional network architecture due to the rapid growth in the diversity and number of devices connected to the internet, which is not designed to provide high availability, real-time data delivery, scalability, security, resilience, and low latency. To address these issues, this paper proposes a novel blockchain-based distributed cloud architecture with a software defined networking (SDN) enable controller fog nodes at the edge of the network to meet the required design principles. The proposed model is a distributed cloud architecture based on blockchain technology, which provides low-cost, secure, and on-demand access to the most competitive computing infrastructures in an IoT network. By creating a distributed cloud infrastructure, the proposed model enables cost-effective high-performance computing. Furthermore, to bring computing resources to the edge of the IoT network and allow low latency access to large amounts of data in a secure manner, we provide a secure distributed fog node architecture that uses SDN and blockchain techniques. Fog nodes are distributed fog computing entities that allow the deployment of fog services, and are formed by multiple computing resources at the edge of the IoT network. We evaluated the performance of our proposed architecture and compared it with the existing models using various performance measures. The results of our evaluation show that performance is improved by reducing the induced delay, reducing the response time, increasing throughput, and the ability to detect real-time attacks in the IoT network with low performance overheads.","['Cloud computing', 'Computer architecture', 'Edge computing', 'Performance evaluation', 'Peer-to-peer computing', 'Distributed databases', 'Security']","['Internet of things', 'software defined networking', 'security', 'blockchain', 'cloud computing', 'fog computing', 'edge computing']"
"Recurrent neural network (RNN) and long short-term memory (LSTM) have achieved great success in processing sequential multimedia data and yielded the state-of-the-art results in speech recognition, digital signal processing, video processing, and text data analysis. In this paper, we propose a novel action recognition method by processing the video data using convolutional neural network (CNN) and deep bidirectional LSTM (DB-LSTM) network. First, deep features are extracted from every sixth frame of the videos, which helps reduce the redundancy and complexity. Next, the sequential information among frame features is learnt using DB-LSTM network, where multiple layers are stacked together in both forward pass and backward pass of DB-LSTM to increase its depth. The proposed method is capable of learning long term sequences and can process lengthy videos by analyzing features for a certain time interval. Experimental results show significant improvements in action recognition using the proposed method on three benchmark data sets including UCF-101, YouTube 11 Actions, and HMDB51 compared with the state-of-the-art action recognition methods.","['Feature extraction', 'Computer architecture', 'Streaming media', 'Visualization', 'Microprocessors', 'Logic gates', 'Shape']","['Action recognition', 'deep learning', 'recurrent neural network', 'deep bidirectional long short-term memory', 'convolution neural network']"
"Internet of Things is smartly changing various existing research areas into new themes, including smart health, smart home, smart industry, and smart transport. Relying on the basis of “smart transport,” Internet of Vehicles (IoV) is evolving as a new theme of research and development from vehicular ad hoc networks (VANETs). This paper presents a comprehensive framework of IoV with emphasis on layered architecture, protocol stack, network model, challenges, and future aspects. Specifically, following the background on the evolution of VANETs and motivation on IoV an overview of IoV is presented as the heterogeneous vehicular networks. The IoV includes five types of vehicular communications, namely, vehicle-to-vehicle, vehicle-to-roadside, vehicle-to-infrastructure of cellular networks, vehicle-to-personal devices, and vehicle-to-sensors. A five layered architecture of IoV is proposed considering functionalities and representations of each layer. A protocol stack for the layered architecture is structured considering management, operational, and security planes. A network model of IoV is proposed based on the three network elements, including cloud, connection, and client. The benefits of the design and development of IoV are highlighted by performing a qualitative comparison between IoV and VANETs. Finally, the challenges ahead for realizing IoV are discussed and future aspects of IoV are envisioned.","['Intelligent vehicles', 'Internet of things', 'Vehicular ad hoc networks', 'Cloud computing', 'Computer architecture', 'Heterogeneous networks', 'Reliability']","['Vehicular adhoc networks', 'Internet of Vehicles', 'cloud computing', 'heterogeneous networks']"
"The use of renewable energy resources, such as solar, wind, and biomass will not diminish their availability. Sunlight being a constant source of energy is used to meet the ever-increasing energy need. This review discusses the world's energy needs, renewable energy technologies for domestic use, and highlights public opinions on renewable energy. A systematic review of the literature was conducted from 2009 to 2018. During this process, more than 300 articles were classified and 42 papers were filtered for critical review. The literature analysis showed that despite serious efforts at all levels to reduce reliance on fossil fuels by promoting renewable energy as its alternative, fossil fuels continue to contribute 73.5% to the worldwide electricity production in 2017. Conversely, renewable sources contributed only 26.5%. Furthermore, this study highlights that the lack of public awareness is a major barrier to the acceptance of renewable energy technologies. The results of this study show that worldwide energy crises can be managed by integrating renewable energy sources in the power generation. Moreover, in order to facilitate the development of renewable energy technologies, this systematic review has highlighted the importance of public opinion and performed a real-time analysis of public tweets. This example of tweet analysis is a relatively novel initiative in a review study that will seek to direct the attention of future researchers and policymakers toward public opinion and recommend the implications to both academia and industries.","['Fossil fuels', 'Systematics', 'Information technology', 'Computer science', 'Wind', 'Biomass']","['Energy policies', 'public opinion', 'renewable energy sources (RES)', 'renewable energy technology (RET)', 'solar energy', 'wind energy']"
"Coronavirus (COVID-19) is a viral disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The spread of COVID-19 seems to have a detrimental effect on the global economy and health. A positive chest X-ray of infected patients is a crucial step in the battle against COVID-19. Early results suggest that abnormalities exist in chest X-rays of patients suggestive of COVID-19. This has led to the introduction of a variety of deep learning systems and studies have shown that the accuracy of COVID-19 patient detection through the use of chest X-rays is strongly optimistic. Deep learning networks like convolutional neural networks (CNNs) need a substantial amount of training data. Because the outbreak is recent, it is difficult to gather a significant number of radiographic images in such a short time. Therefore, in this research, we present a method to generate synthetic chest X-ray (CXR) images by developing an Auxiliary Classifier Generative Adversarial Network (ACGAN) based model called CovidGAN. In addition, we demonstrate that the synthetic images produced from CovidGAN can be utilized to enhance the performance of CNN for COVID-19 detection. Classification using CNN alone yielded 85% accuracy. By adding synthetic images produced by CovidGAN,the accuracy increased to 95%. We hope this method will speed up COVID-19 detection and lead to more robust systems of radiology.","['Generative adversarial networks', 'Training', 'Biomedical imaging', 'X-ray imaging', 'Computer architecture', 'Machine learning', 'COVID-19']","['Deep learning', 'convolutional neural networks', 'generative adversarial networks', 'synthetic data augmentation', 'COVID-19 detection']"
"Driven by the emergence of new compute-intensive applications and the vision of the Internet of Things (IoT), it is foreseen that the emerging 5G network will face an unprecedented increase in traffic volume and computation demands. However, end users mostly have limited storage capacities and finite processing capabilities, thus how to run compute-intensive applications on resource-constrained users has recently become a natural concern. Mobile edge computing (MEC), a key technology in the emerging fifth generation (5G) network, can optimize mobile resources by hosting compute-intensive applications, process large data before sending to the cloud, provide the cloud-computing capabilities within the radio access network (RAN) in close proximity to mobile users, and offer context-aware services with the help of RAN information. Therefore, MEC enables a wide variety of applications, where the real-time response is strictly required, e.g., driverless vehicles, augmented reality, robotics, and immerse media. Indeed, the paradigm shift from 4G to 5G could become a reality with the advent of new technological concepts. The successful realization of MEC in the 5G network is still in its infancy and demands for constant efforts from both academic and industry communities. In this survey, we first provide a holistic overview of MEC technology and its potential use cases and applications. Then, we outline up-to-date researches on the integration of MEC with the new technologies that will be deployed in 5G and beyond. We also summarize testbeds and experimental evaluations, and open source activities, for edge computing. We further summarize lessons learned from state-of-the-art research works as well as discuss challenges and potential future directions for MEC research.","['Cloud computing', '5G mobile communication', 'Edge computing', 'Internet of Things', 'Radio access networks', 'NOMA', 'Wireless communication']","['5G and beyond network', 'heterogeneous networks', 'Internet of Things', 'machine learning', 'edge computing', 'non-orthogonal multiple access', 'testbeds', 'unmanned aerial vehicle', 'wireless power transfer and energy harvesting']"
"U-net is an image segmentation technique developed primarily for image segmentation tasks. These traits provide U-net with a high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in nearly all major image modalities, from CT scans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. Given that U-net's potential is still increasing, this narrative literature review examines the numerous developments and breakthroughs in the U-net architecture and provides observations on recent trends. We also discuss the many innovations that have advanced in deep learning and discuss how these tools facilitate U-net. In addition, we review the different image modalities and application areas that have been enhanced by U-net.","['Image segmentation', 'Convolution', 'Biomedical imaging', 'Three-dimensional displays', 'Logic gates', 'Deep learning', 'Computer architecture']","['Biomedical imaging', 'deep learning', 'neural network architecture', 'segmentation', 'U-net']"
"In new product development, time to market (TTM) is critical for the success and profitability of next generation products. When these products include sophisticated electronics encased in 3D packaging with complex geometries and intricate detail, TTM can be compromised - resulting in lost opportunity. The use of advanced 3D printing technology enhanced with component placement and electrical interconnect deposition can provide electronic prototypes that now can be rapidly fabricated in comparable time frames as traditional 2D bread-boarded prototypes; however, these 3D prototypes include the advantage of being embedded within more appropriate shapes in order to authentically prototype products earlier in the development cycle. The fabrication freedom offered by 3D printing techniques, such as stereolithography and fused deposition modeling have recently been explored in the context of 3D electronics integration - referred to as 3D structural electronics or 3D printed electronics. Enhanced 3D printing may eventually be employed to manufacture end-use parts and thus offer unit-level customization with local manufacturing; however, until the materials and dimensional accuracies improve (an eventuality), 3D printing technologies can be employed to reduce development times by providing advanced geometrically appropriate electronic prototypes. This paper describes the development process used to design a novelty six-sided gaming die. The die includes a microprocessor and accelerometer, which together detect motion and upon halting, identify the top surface through gravity and illuminate light-emitting diodes for a striking effect. By applying 3D printing of structural electronics to expedite prototyping, the development cycle was reduced from weeks to hours.","['Product development', 'Three dimensional displays', 'Printing', 'Product life cycle management', 'Economics', 'Consumer electronics', 'Time to market', 'Prototypes', 'Packaging']","['3D printed electronics', 'additive manufacturing', 'direct-print', 'electronic gaming die', 'hybrid manufacturing', 'rapid prototyping', 'structural electronics', 'three-dimensional electronics']"
"A network traffic classifier (NTC) is an important part of current network monitoring systems, being its task to infer the network service that is currently used by a communication flow (e.g., HTTP and SIP). The detection is based on a number of features associated with the communication flow, for example, source and destination ports and bytes transmitted per packet. NTC is important, because much information about a current network flow can be learned and anticipated just by knowing its network service (required latency, traffic volume, and possible duration). This is of particular interest for the management and monitoring of Internet of Things (IoT) networks, where NTC will help to segregate traffic and behavior of heterogeneous devices and services. In this paper, we present a new technique for NTC based on a combination of deep learning models that can be used for IoT traffic. We show that a recurrent neural network (RNN) combined with a convolutional neural network (CNN) provides best detection results. The natural domain for a CNN, which is image processing, has been extended to NTC in an easy and natural way. We show that the proposed method provides better detection results than alternative algorithms without requiring any feature engineering, which is usual when applying other models. A complete study is presented on several architectures that integrate a CNN and an RNN, including the impact of the features chosen and the length of the network flows used for training.","['Ports (Computers)', 'Telecommunication traffic', 'Feature extraction', 'Recurrent neural networks', 'Machine learning', 'Payloads', 'Biological neural networks']","['Convolutional neural network', 'deep learning', 'network traffic classification', 'recurrent neural network']"
"Cyber-physical system (CPS) is a new trend in the Internet-of-Things related research works, where physical systems act as the sensors to collect real-world information and communicate them to the computation modules (i.e. cyber layer), which further analyze and notify the findings to the corresponding physical systems through a feedback loop. Contemporary researchers recommend integrating cloud technologies in the CPS cyber layer to ensure the scalability of storage, computation, and cross domain communication capabilities. Though there exist a few descriptive models of the cloud-based CPS architecture, it is important to analytically describe the key CPS properties: computation, control, and communication. In this paper, we present a digital twin architecture reference model for the cloud-based CPS, C2PS, where we analytically describe the key properties of the C2PS. The model helps in identifying various degrees of basic and hybrid computation-interaction modes in this paradigm. We have designed C2PS smart interaction controller using a Bayesian belief network, so that the system dynamically considers current contexts. The composition of fuzzy rule base with the Bayes network further enables the system with reconfiguration capability. We also describe analytically, how C2PS subsystem communications can generate even more complex system-of-systems. Later, we present a telematics-based prototype driving assistance application for the vehicular domain of C2PS, VCPS, to demonstrate the efficacy of the architecture reference model.","['Cloud computing', 'Computer architecture', 'Sensors', 'Social network services', 'Analytical models', 'Urban areas', 'Computational modeling']","['Digital twin', 'cyber-physical systems', 'Internet-of-Things', 'social internet of vehicles', 'sensing-as-a-service', 'analytical model']"
"Ultra-wideband millimeter-wave (mmWave) propagation measurements were conducted in the 28- and 73-GHz frequency bands in a typical indoor office environment in downtown Brooklyn, New York, on the campus of New York University. The measurements provide large-scale path loss and temporal statistics that will be useful for ultra-dense indoor wireless networks for future mmWave bands. This paper presents the details of measurements that employed a 400 Megachips-per-second broadband sliding correlator channel sounder, using rotatable highly directional horn antennas for both co-polarized and cross-polarized antenna configurations. The measurement environment was a closed-plan in-building scenario that included a line-of-sight and non-line-of-sight corridor, a hallway, a cubicle farm, and adjacent-room communication links. Well-known and new single-frequency and multi-frequency directional and omnidirectional large-scale path loss models are presented and evaluated based on more than 14 000 directional power delay profiles acquired from unique transmitter and receiver antenna pointing angle combinations. Omnidirectional path loss models, synthesized from the directional measurements, are provided for the case of arbitrary polarization coupling, as well as for the specific cases of co-polarized and cross-polarized antenna orientations. The results show that novel large-scale path loss models provided here are simpler and more physically based compared to previous 3GPP and ITU indoor propagation models that require more model parameters and offer very little additional accuracy and lack a physical basis. Multipath time dispersion statistics for mmWave systems using directional antennas are presented for co-polarization, crosspolarization, and combined-polarization scenarios, and show that the multipath root mean square delay spread can be reduced when using transmitter and receiver antenna pointing angles that result in the strongest received power. Raw omnidirectional path loss data and closed-form optimization formulas for all path loss models are given in the Appendices.","['Millimeter wave communication', 'Wideband', 'Propagation measurements', 'Ultra wideband communication', '5G mobile communication', 'Loss measurement', 'Path planning', 'Polarization', 'Multipath channels']","['Millimeter-wave', 'mmWave', 'path loss', '5G', 'indoor hotspot', 'RMS delay spread', 'small cell', 'channel sounder', 'propagation', '28 GHz', '73 GHz', 'multipath', 'polarization']"
"The Internet of Things (IoT) is a dynamic global information network consisting of Internet-connected objects, such as radio frequency identifications, sensors, and actuators, as well as other instruments and smart appliances that are becoming an integral component of the Internet. Over the last few years, we have seen a plethora of IoT solutions making their way into the industry marketplace. Context-aware communications and computing have played a critical role throughout the last few years of ubiquitous computing and are expected to play a significant role in the IoT paradigm as well. In this paper, we examine a variety of popular and innovative IoT solutions in terms of context-aware technology perspectives. More importantly, we evaluate these IoT solutions using a framework that we built around well-known context-aware computing theories. This survey is intended to serve as a guideline and a conceptual framework for context-aware product development and research in the IoT paradigm. It also provides a systematic exploration of existing IoT products in the marketplace and highlights a number of potentially significant research directions and trends.","['Context awareness', 'Internet of things', 'Market research', 'Information networks', 'Internet', 'Globalization', 'Interconnections', 'Product development', 'Futures research']","['Internet of things', 'industry solutions', 'contextawareness', 'product review', 'IoT marketplace']"
"Over the past three decades, significant developments have been made in hyperspectral imaging due to which it has emerged as an effective tool in numerous civil, environmental, and military applications. Modern sensor technologies are capable of covering large surfaces of earth with exceptional spatial, spectral, and temporal resolutions. Due to these features, hyperspectral imaging has been effectively used in numerous remote sensing applications requiring estimation of physical parameters of many complex surfaces and identification of visually similar materials having fine spectral signatures. In the recent years, ground based hyperspectral imaging has gained immense interest in the research on electronic imaging for food inspection, forensic science, medical surgery and diagnosis, and military applications. This review focuses on the fundamentals of hyperspectral image analysis and its modern applications such as food quality and safety assessment, medical diagnosis and image guided surgery, forensic document examination, defense and homeland security, remote sensing applications such as precision agriculture and water resource management and material identification and mapping of artworks. Moreover, recent research on the use of hyperspectral imaging for examination of forgery detection in questioned documents, aided by deep learning, is also presented. This review can be a useful baseline for future research in hyperspectral image analysis.","['Hyperspectral imaging', 'Spatial resolution', 'Imaging', 'Safety']","['Agriculture', 'document images', 'food quality and safety', 'hyperspectral imaging', 'medical imaging', 'remote sensing']"
"In this paper, an improved ant colony optimization (ICMPACO) algorithm based on the multi-population strategy, co-evolution mechanism, pheromone updating strategy, and pheromone diffusion mechanism is proposed to balance the convergence speed and solution diversity, and improve the optimization performance in solving the large-scale optimization problem. In the proposed ICMPACO algorithm, the optimization problem is divided into several sub-problems and the ants in the population are divided into elite ants and common ants in order to improve the convergence rate, and avoid to fall into the local optimum value. The pheromone updating strategy is used to improve optimization ability. The pheromone diffusion mechanism is used to make the pheromone released by ants at a certain point, which gradually affects a certain range of adjacent regions. The co-evolution mechanism is used to interchange information among different sub-populations in order to implement information sharing. In order to verify the optimization performance of the ICMPACO algorithm, the traveling salesmen problem (TSP) and the actual gate assignment problem are selected here. The experiment results show that the proposed ICMPACO algorithm can effectively obtain the best optimization value in solving TSP and effectively solve the gate assignment problem, obtain better assignment result, and it takes on better optimization ability and stability.","['Optimization', 'Convergence', 'Scheduling', 'Sociology', 'Statistics', 'Heuristic algorithms', 'Ant colony optimization']","['Co-evolution mechanism', 'ACO', 'pheromone updating strategy', 'pheromone diffusion mechanism', 'hybrid strategy', 'assignment problem']"
"5G is the next cellular generation and is expected to quench the growing thirst for taxing data rates and to enable the Internet of Things. Focused research and standardization work have been addressing the corresponding challenges from the radio perspective while employing advanced features, such as network densification, massive multiple-input-multiple-output antennae, coordinated multi-point processing, inter-cell interference mitigation techniques, carrier aggregation, and new spectrum exploration. Nevertheless, a new bottleneck has emerged: the backhaul. The ultra-dense and heavy traffic cells should be connected to the core network through the backhaul, often with extreme requirements in terms of capacity, latency, availability, energy, and cost efficiency. This pioneering survey explains the 5G backhaul paradigm, presents a critical analysis of legacy, cutting-edge solutions, and new trends in backhauling, and proposes a novel consolidated 5G backhaul framework. A new joint radio access and backhaul perspective is proposed for the evaluation of backhaul technologies which reinforces the belief that no single solution can solve the holistic 5G backhaul problem. This paper also reveals hidden advantages and shortcomings of backhaul solutions, which are not evident when backhaul technologies are inspected as an independent part of the 5G network. This survey is key in identifying essential catalysts that are believed to jointly pave the way to solving the beyond-2020 backhauling challenge. Lessons learned, unsolved challenges, and a new consolidated 5G backhaul vision are thus presented.","['5G mobile communication', 'Cellular networks', 'Backhaul communication', 'Microcells', 'Heterogeneous networks', 'Internet of things', 'MIMO', 'Market research', 'Software defined radio', 'Interference (signal)']","['5G', 'backhaul', 'fronthaul', 'small cells', 'heterogeneous network', 'C-RAN', 'SDN', 'SON', 'backhaul as a service']"
"The recent outbreak of COVID-19 has taken the world by surprise, forcing lockdowns and straining public health care systems. COVID-19 is known to be a highly infectious virus, and infected individuals do not initially exhibit symptoms, while some remain asymptomatic. Thus, a non-negligible fraction of the population can, at any given time, be a hidden source of transmissions. In response, many governments have shown great interest in smartphone contact tracing apps that help automate the difficult task of tracing all recent contacts of newly identified infected individuals. However, tracing apps have generated much discussion around their key attributes, including system architecture, data management, privacy, security, proximity estimation, and attack vulnerability. In this article, we provide the first comprehensive review of these much-discussed tracing app attributes. We also present an overview of many proposed tracing app examples, some of which have been deployed countrywide, and discuss the concerns users have reported regarding their usage. We close by outlining potential research directions for next-generation app design, which would facilitate improved tracing and security performance, as well as wide adoption by the population at large.","['Servers', 'Security', 'Privacy', 'Computer architecture', 'Viruses (medical)', 'Data privacy', 'Bluetooth', 'COVID-19']","['Contact tracing', 'privacy', 'security']"
"Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.","['Machine learning', 'Data models', 'Mathematical model', 'Kernel', 'Biological system modeling', 'Approximation algorithms', 'Data mining']","['Explainable machine learning', 'informed machine learning', 'interpretability', 'scientific consistency', 'transparency']"
"Deep learning has exploded in the public consciousness, primarily as predictive and analytical products suffuse our world, in the form of numerous human-centered smart-world systems, including targeted advertisements, natural language assistants and interpreters, and prototype self-driving vehicle systems. Yet to most, the underlying mechanisms that enable such human-centered smart products remain obscure. In contrast, researchers across disciplines have been incorporating deep learning into their research to solve problems that could not have been approached before. In this paper, we seek to provide a thorough investigation of deep learning in its applications and mechanisms. Specifically, as a categorical collection of state of the art in deep learning research, we hope to provide a broad reference for those seeking a primer on deep learning and its various implementations, platforms, algorithms, and uses in a variety of smart-world systems. Furthermore, we hope to outline recent key advancements in the technology, and provide insight into areas, in which deep learning can improve investigation, as well as highlight new areas of research that have yet to see the application of deep learning, but could nonetheless benefit immensely. We hope this survey provides a valuable reference for new deep learning practitioners, as well as those seeking to innovate in the application of deep learning.","['Machine learning', 'Neurons', 'Neural networks', 'Task analysis', 'Learning systems', 'Computational modeling', 'Training']","['Human-centered smart systems', 'deep learning', 'platform', 'neural networks', 'emergent applications', 'Internet of Things', 'cyber-physical systems', 'survey', 'networking', 'security']"
"In the past two decades, reversible data hiding (RDH), also referred to as lossless or invertible data hiding, has gradually become a very active research area in the field of data hiding. This has been verified by more and more papers on increasingly wide-spread subjects in the field of RDH research that have been published these days. In this paper, the various RDH algorithms and researches have been classified into the following six categories: 1) RDH into image spatial domain; 2) RDH into image compressed domain (e.g., JPEG); 3) RDH suitable for image semi-fragile authentication; 4) RDH with image contrast enhancement; 5) RDH into encrypted images, which is expected to have wide application in the cloud computation; and 6) RDH into video and into audio. For each of these six categories, the history of technical developments, the current state of the arts, and the possible future researches are presented and discussed. It is expected that the RDH technology and its applications in the real word will continue to move ahead.","['Data hiding', 'Video communication', 'Reversible data hiding', 'Classification algorithms', 'Image coding', 'Transform coding', 'Cryptography', 'Authentication']","['Reversible data hiding', 'lossless data hiding', 'invertible data hiding', 'histogram shifting', 'difference expansion', 'prediction-error', 'sorting', 'robust reversible data hiding', 'video reversible data hiding', 'audio reversible data hiding']"
"The focus of wireless research is increasingly shifting toward 6G as 5G deployments get underway. At this juncture, it is essential to establish a vision of future communications to provide guidance for that research. In this paper, we attempt to paint a broad picture of communication needs and technologies in the timeframe of 6G. The future of connectivity is in the creation of digital twin worlds that are a true representation of the physical and biological worlds at every spatial and time instant, unifying our experience across these physical, biological and digital worlds. New themes are likely to emerge that will shape 6G system requirements and technologies, such as: (i) new man-machine interfaces created by a collection of multiple local devices acting in unison; (ii) ubiquitous universal computing distributed among multiple local devices and the cloud; (iii) multi-sensory data fusion to create multi-verse maps and new mixed-reality experiences; and (iv) precision sensing and actuation to control the physical world. With rapid advances in artificial intelligence, it has the potential to become the foundation for the 6G air interface and network, making data, compute and energy the new resources to be exploited for achieving superior performance. In addition, in this paper we discuss the other major technology transformations that are likely to define 6G: (i) cognitive spectrum sharing methods and new spectrum bands; (ii) the integration of localization and sensing capabilities into the system definition, (iii) the achievement of extreme performance requirements on latency and reliability; (iv) new network architecture paradigms involving sub-networks and RAN-Core convergence; and (v) new security and privacy schemes.","['6G mobile communication', '5G mobile communication', 'Robot sensing systems', 'Biology', 'Digital twin', 'User interfaces']","['6G', 'AI/ML driven air interface', 'network localization and sensing', 'cognitive spectrum sharing', 'sub-terahertz', 'RAN-Core convergence', 'subnetworks', 'security', 'privacy', 'network as a platform']"
"Due to the significant advancement of the Internet of Things (IoT) in the healthcare sector, the security, and the integrity of the medical data became big challenges for healthcare services applications. This paper proposes a hybrid security model for securing the diagnostic text data in medical images. The proposed model is developed through integrating either 2-D discrete wavelet transform 1 level (2D-DWT-1L) or 2-D discrete wavelet transform 2 level (2D-DWT-2L) steganography technique with a proposed hybrid encryption scheme. The proposed hybrid encryption schema is built using a combination of Advanced Encryption Standard, and Rivest, Shamir, and Adleman algorithms. The proposed model starts by encrypting the secret data; then it hides the result in a cover image using 2D-DWT-1L or 2D-DWT-2L. Both color and gray-scale images are used as cover images to conceal different text sizes. The performance of the proposed system was evaluated based on six statistical parameters; the peak signal-to-noise ratio (PSNR), mean square error (MSE), bit error rate (BER), structural similarity (SSIM), structural content (SC), and correlation. The PSNR values were relatively varied from 50.59 to 57.44 in case of color images and from 50.52 to 56.09 with the gray scale images. The MSE values varied from 0.12 to 0.57 for the color images and from 0.14 to 0.57 for the gray scale images. The BER values were zero for both images, while SSIM, SC, and correlation values were ones for both images. Compared with the state-of-the-art methods, the proposed model proved its ability to hide the confidential patient's data into a transmitted cover image with high imperceptibility, capacity, and minimal deterioration in the received stego-image.","['Encryption', 'Medical diagnostic imaging', 'Medical services', 'Data models']","['Cryptography', 'DWT-1level', 'DWT-2level', 'encryption', 'healthcare services', 'Internet of Things', 'medical images', 'steganography']"
"Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.","['Dimensionality reduction', 'Principal component analysis', 'Machine learning algorithms', 'Support vector machines', 'Medical diagnostic imaging', 'Feature extraction']","['Cardiotocography dataset', 'dimensionality reduction', 'feature engineering', 'linear discriminant analysis', 'machine learning', 'principal component analysis']"
"One of the key enablers of future wireless communications is constituted by massive multiple-input multiple-output (MIMO) systems, which can improve the spectral efficiency by orders of magnitude. In existing massive MIMO systems, however, conventional phased arrays are used for beamforming. This method results in excessive power consumption and high hardware costs. Recently, reconfigurable intelligent surface (RIS) has been considered as one of the revolutionary technologies to enable energy-efficient and smart wireless communications, which is a two-dimensional structure with a large number of passive elements. In this paper, we develop a new type of high-gain yet low-cost RIS that bears 256 elements. The proposed RIS combines the functions of phase shift and radiation together on an electromagnetic surface, where positive intrinsic-negative (PIN) diodes are used to realize 2-bit phase shifting for beamforming. This radical design forms the basis for the world’s first wireless communication prototype using RIS having 256 two-bit elements. The prototype consists of modular hardware and flexible software that encompass the following: the hosts for parameter setting and data exchange, the universal software radio peripherals (USRPs) for baseband and radio frequency (RF) signal processing, as well as the RIS for signal transmission and reception. Our performance evaluation confirms the feasibility and efficiency of RISs in wireless communications. We show that, at 2.3 GHz, the proposed RIS can achieve a 21.7 dBi antenna gain. At the millimeter wave (mmWave) frequency, that is, 28.5 GHz, it attains a 19.1 dBi antenna gain. Furthermore, it has been shown that the RIS-based wireless communication prototype developed is capable of significantly reducing the power consumption.","['Wireless communication', 'Phased arrays', 'Prototypes', 'Radio frequency', 'MIMO communication']","['Massive MIMO', 'prototype', 'reconfigurable intelligent surface (RIS)', 'wireless communication']"
"A microgrid (MG) is a local entity that consists of distributed energy resources (DERs) to achieve local power reliability and sustainable energy utilization. The MG concept or renewable energy technologies integrated with energy storage systems (ESS) have gained increasing interest and popularity because it can store energy at off-peak hours and supply energy at peak hours. However, existing ESS technology faces challenges in storing energy due to various issues, such as charging/discharging, safety, reliability, size, cost, life cycle, and overall management. Thus, an advanced ESS is required with regard to capacity, protection, control interface, energy management, and characteristics to enhance the performance of ESS in MG applications. This paper comprehensively reviews the types of ESS technologies, ESS structures along with their configurations, classifications, features, energy conversion, and evaluation process. Moreover, details on the advantages and disadvantages of ESS in MG applications have been analyzed based on the process of energy formations, material selection, power transfer mechanism, capacity, efficiency, and cycle period. Existing reviews critically demonstrate the current technologies for ESS in MG applications. However, the optimum management of ESSs for efficient MG operation remains a challenge in modern power system networks. This review also highlights the key factors, issues, and challenges with possible recommendations for the further development of ESS in future MG applications. All the highlighted insights of this review significantly contribute to the increasing effort toward the development of a cost-effective and efficient ESS model with a prolonged life cycle for sustainable MG implementation.","['Batteries', 'Power system reliability', 'Reliability', 'Renewable energy sources', 'Lithium', 'Fuel cells']","['Energy storage system', 'microgrid', 'distributed energy resources', 'ESS technologies', 'energy management']"
"Multi-agent systems (MASs) have received tremendous attention from scholars in different disciplines, including computer science and civil engineering, as a means to solve complex problems by subdividing them into smaller tasks. The individual tasks are allocated to autonomous entities, known as agents. Each agent decides on a proper action to solve the task using multiple inputs, e.g., history of actions, interactions with its neighboring agents, and its goal. The MAS has found multiple applications, including modeling complex systems, smart grids, and computer networks. Despite their wide applicability, there are still a number of challenges faced by MAS, including coordination between agents, security, and task allocation. This survey provides a comprehensive discussion of all aspects of MAS, starting from definitions, features, applications, challenges, and communications to evaluation. A classification on MAS applications and challenges is provided along with references for further studies. We expect this paper to serve as an insightful and comprehensive resource on the MAS for researchers and practitioners in the area.","['Task analysis', 'Multi-agent systems', 'Computer science', 'Security', 'Australia', 'Computational modeling', 'Decision making']","['Multi-agent systems', 'survey', 'MAS applications', 'challenges']"
"Internet of things (IoT) is a promising technology which provides efficient and reliable solutions towards the modernization of several domains. IoT based solutions are being developed to automatically maintain and monitor agricultural farms with minimal human involvement. The article presents many aspects of technologies involved in the domain of IoT in agriculture. It explains the major components of IoT based smart farming. A rigorous discussion on network technologies used in IoT based agriculture has been presented, that involves network architecture and layers, network topologies used, and protocols. Furthermore, the connection of IoT based agriculture systems with relevant technologies including cloud computing, big data storage and analytics has also been presented. In addition, security issues in IoT agriculture have been highlighted. A list of smart phone based and sensor based applications developed for different aspects of farm management has also been presented. Lastly, the regulations and policies made by several countries to standardize IoT based agriculture have been presented along with few available success stories. In the end, some open research issues and challenges in IoT agriculture field have been presented.","['Agriculture', 'Monitoring', 'Internet of Things', 'Protocols', 'Temperature sensors', 'Temperature measurement', 'Data acquisition']","['IoT', 'smart farming', 'applications', 'protocols', 'network', 'architecture', 'platforms', 'industries', 'security', 'challenges', 'technologies', 'policies']"
"Fog computing paradigm extends the storage, networking, and computing facilities of the cloud computing toward the edge of the networks while offloading the cloud data centers and reducing service latency to the end users. However, the characteristics of fog computing arise new security and privacy challenges. The existing security and privacy measurements for cloud computing cannot be directly applied to the fog computing due to its features, such as mobility, heterogeneity, and large-scale geo-distribution. This paper provides an overview of existing security and privacy concerns, particularly for the fog computing. Afterward, this survey highlights ongoing research effort, open challenges, and research trends in privacy and security issues for fog computing.","['Edge computing', 'Cloud computing', 'Privacy', 'Computational modeling', 'Authentication', 'Electronic mail']","['Fog', 'fog computing', 'fog networking', 'security', 'privacy', 'IoT', 'privacy threats', 'security threats']"
"In traditional cloud storage systems, attribute-based encryption (ABE) is regarded as an important technology for solving the problem of data privacy and fine-grained access control. However, in all ABE schemes, the private key generator has the ability to decrypt all data stored in the cloud server, which may bring serious problems such as key abuse and privacy data leakage. Meanwhile, the traditional cloud storage model runs in a centralized storage manner, so single point of failure may leads to the collapse of system. With the development of blockchain technology, decentralized storage mode has entered the public view. The decentralized storage approach can solve the problem of single point of failure in traditional cloud storage systems and enjoy a number of advantages over centralized storage, such as low price and high throughput. In this paper, we study the data storage and sharing scheme for decentralized storage systems and propose a framework that combines the decentralized storage system interplanetary file system, the Ethereum blockchain, and ABE technology. In this framework, the data owner has the ability to distribute secret key for data users and encrypt shared data by specifying access policy, and the scheme achieves fine-grained access control over data. At the same time, based on smart contract on the Ethereum blockchain, the keyword search function on the cipher text of the decentralized storage systems is implemented, which solves the problem that the cloud server may not return all of the results searched or return wrong results in the traditional cloud storage systems. Finally, we simulated the scheme in the Linux system and the Ethereum official test network Rinkeby, and the experimental results show that our scheme is feasible.","['Encryption', 'Cloud computing', 'Access control', 'Contracts', 'Data privacy']","['ABE', 'Ethereum blockchain', 'smart contract', 'IPFS', 'access control', 'keyword searchable']"
"For more than a decade now, radio frequency identification (RFID) technology has been quite effective in providing anti-counterfeits measures in the supply chain. However, the genuineness of RFID tags cannot be guaranteed in the post supply chain, since these tags can be rather easily cloned in the public space. In this paper, we propose a novel product ownership management system (POMS) of RFID-attached products for anti-counterfeits that can be used in the post supply chain. For this purpose, we leverage the idea of Bitcoin's blockchain that anyone can check the proof of possession of balance. With the proposed POMS, a customer can reject the purchase of counterfeits even with genuine RFID tag information, if the seller does not possess their ownership. We have implemented a proof-of-concept experimental system employing a blockchain-based decentralized application platform, Ethereum, and evaluated its cost performance. Results have shown that, typically, the cost of managing the ownership of a product with up to six transfers is less than U.S. $1.","['Supply chains', 'Bitcoin', 'RFID tags', 'Protocols', 'Internet']","['Anti-counterfeits technology', 'POMS (products ownership management system)', 'blockchain', 'Ethereum', 'security']"
"With the rising interest in autonomous vehicles, developing radio access technologies (RATs) that enable reliable and low-latency vehicular communications has become of paramount importance. Dedicated short-range communications (DSRCs) and cellular V2X (C-V2X) are two present-day technologies that are capable of supporting day-1 vehicular applications. However, these RATs fall short of supporting communication requirements of many advanced vehicular applications, which are believed to be critical in enabling fully autonomous vehicles. Both the DSRC and C-V2X are undergoing extensive enhancements in order to support advanced vehicular applications that are characterized by high reliability, low latency, and high throughput requirements. These RAT evolutions—the IEEE 802.11bd for the DSRC and NR V2X for C-V2X—can supplement today’s vehicular sensors in enabling autonomous driving. In this paper, we survey the latest developments in the standardization of 802.11bd and NR V2X. We begin with a brief description of the two present-day vehicular RATs. In doing so, we highlight their inability to guarantee the quality of service requirements of many advanced vehicular applications. We then look at the two RAT evolutions, i.e., the IEEE 802.11bd and NR V2X, outline their objectives, describe their salient features, and provide an in-depth description of key mechanisms that enable these features. While both, the IEEE 802.11bd and NR V2X, are in their initial stages of development, we shed light on their preliminary performance projections and compare and contrast the two evolutionary RATs with their respective predecessors.","['Radio access technologies', 'Vehicle-to-everything', '3GPP', 'Communication channels', '5G mobile communication', 'Reliability']","['C-V2X', 'DSRC', 'IEEE 802.11bd', 'NR V2X']"
"Alternaria leaf spot, Brown spot, Mosaic, Grey spot, and Rust are five common types of apple leaf diseases that severely affect apple yield. However, the existing research lacks an accurate and fast detector of apple diseases for ensuring the healthy development of the apple industry. This paper proposes a deep learning approach that is based on improved convolutional neural networks (CNNs) for the real-time detection of apple leaf diseases. In this paper, the apple leaf disease dataset (ALDD), which is composed of laboratory images and complex images under real field conditions, is first constructed via data augmentation and image annotation technologies. Based on this, a new apple leaf disease detection model that uses deep-CNNs is proposed by introducing the GoogLeNet Inception structure and Rainbow concatenation. Finally, under the hold-out testing dataset, using a dataset of 26,377 images of diseased apple leaves, the proposed INAR-SSD (SSD with Inception module and Rainbow concatenation) model is trained to detect these five common apple leaf diseases. The experimental results show that the INAR-SSD model realizes a detection performance of 78.80% mAP on ALDD, with a high-detection speed of 23.13 FPS. The results demonstrate that the novel INAR-SSD model provides a high-performance solution for the early diagnosis of apple leaf diseases that can perform real-time detection of these diseases with higher accuracy and faster detection speed than previous methods.","['Diseases', 'Feature extraction', 'Real-time systems', 'Convolutional neural networks', 'Training', 'Deep learning', 'Agriculture']","['Apple leaf diseases', 'real-time detection', 'deep learning', 'convolutional neural networks', 'feature fusion']"
"The globalized production and the distribution of agriculture production bring a renewed focus on the safety, quality, and the validation of several important criteria in agriculture and food supply chains. The growing number of issues related to food safety and contamination risks has established an immense need for effective traceability solution that acts as an essential quality management tool ensuring adequate safety of products in the agricultural supply chain. Blockchain is a disruptive technology that can provide an innovative solution for product traceability in agriculture and food supply chains. Today's agricultural supply chains are complex ecosystem involving several stakeholders making it cumbersome to validate several important criteria such as country of origin, stages in crop development, conformance to quality standards, and monitor yields. In this paper, we propose an approach that leverages the Ethereum blockchain and smart contracts efficiently perform business transactions for soybean tracking and traceability across the agricultural supply chain. Our proposed solution eliminates the need for a trusted centralized authority, intermediaries and provides transactions records, enhancing efficiency and safety with high integrity, reliability, and security. The proposed solution focuses on the utilization of smart contracts to govern and control all interactions and transactions among all the participants involved within the supply chain ecosystem. All transactions are recorded and stored in the blockchain's immutable ledger with links to a decentralized file system (IPFS) and thus providing to all a high level of transparency and traceability into the supply chain ecosystem in a secure, trusted, reliable, and efficient manner.","['Supply chains', 'Blockchain', 'Smart contracts', 'Agriculture', 'Safety', 'Data mining', 'Contamination']","['Blockchain', 'Ethereum', 'smart contracts', 'traceability', 'Soybean', 'agricultural supply chain', 'food safety']"
"Fog computing-enhanced Internet of Things (IoT) has recently received considerable attention, as the fog devices deployed at the network edge can not only provide low latency, location awareness but also improve real-time and quality of services in IoT application scenarios. Privacy-preserving data aggregation is one of typical fog computing applications in IoT, and many privacy-preserving data aggregation schemes have been proposed in the past years. However, most of them only support data aggregation for homogeneous IoT devices, and cannot aggregate hybrid IoT devices' data into one in some real IoT applications. To address this challenge, in this paper, we present a lightweight privacy-preserving data aggregation scheme, called Lightweight Privacy-preserving Data Aggregation, for fog computing-enhanced IoT. The proposed LPDA is characterized by employing the homomorphic Paillier encryption, Chinese Remainder Theorem, and one-way hash chain techniques to not only aggregate hybrid IoT devices' data into one, but also early filter injected false data at the network edge. Detailed security analysis shows LPDA is really secure and privacy-enhanced with differential privacy techniques. In addition, extensive performance evaluations are conducted, and the results indicate LPDA is really lightweight in fog computing-enhanced IoT.","['Security', 'Data aggregation', 'Edge computing', 'Real-time systems', 'Internet of Things', 'Aggregates', 'Computational modeling']","['Internet of Things', 'fog computing', 'privacy-preserving aggregation', 'lightweight', 'differential privacy']"
"The development of an anomaly-based intrusion detection system (IDS) is a primary research direction in the field of intrusion detection. An IDS learns normal and anomalous behavior by analyzing network traffic and can detect unknown and new attacks. However, the performance of an IDS is highly dependent on feature design, and designing a feature set that can accurately characterize network traffic is still an ongoing research issue. Anomaly-based IDSs also have the problem of a high false alarm rate (FAR), which seriously restricts their practical applications. In this paper, we propose a novel IDS called the hierarchical spatial-temporal features-based intrusion detection system (HAST-IDS), which first learns the low-level spatial features of network traffic using deep convolutional neural networks (CNNs) and then learns high-level temporal features using long short-term memory networks. The entire process of feature learning is completed by the deep neural networks automatically; no feature engineering techniques are required. The automatically learned traffic features effectively reduce the FAR. The standard DARPA1998 and ISCX2012 data sets are used to evaluate the performance of the proposed system. The experimental results show that the HAST-IDS outperforms other published approaches in terms of accuracy, detection rate, and FAR, which successfully demonstrates its effectiveness in both feature learning and FAR reduction.","['Telecommunication traffic', 'Feature extraction', 'Intrusion detection', 'Recurrent neural networks', 'Natural language processing']","['Network intrusion detection', 'deep neural networks', 'representation learning']"
"Blockchain technology enables the creation of a decentralized environment, where transactions and data are not under the control of any third party organization. Any transaction ever completed is recorded in a public ledger in a verifiable and permanent way. Based on the blockchain technology, we propose a global higher education credit platform, named EduCTX. This platform is based on the concept of the European Credit Transfer and Accumulation System (ECTS). It constitutes a globally trusted, decentralized higher education credit, and grading system that can offer a globally unified viewpoint for students and higher education institutions (HEIs), as well as for other potential stakeholders, such as companies, institutions, and organizations. As a proof of concept, we present a prototype implementation of the environment, based on the open-source Ark Blockchain Platform. Based on a globally distributed peer-to-peer network, EduCTX will process, manage, and control ECTX tokens, which represent credits that students gain for completed courses, such as ECTS. HEIs are the peers of the blockchain network. The platform is a first step toward a more transparent and technologically advanced form of higher education systems. The EduCTX platform represents the basis of the EduCTX initiative, which anticipates that various HEIs would join forces in order to create a globally efficient, simplified, and ubiquitous environment in order to avoid language and administrative barriers. Therefore, we invite and encourage HEIs to join the EduCTX initiative and the EduCTX blockchain network.","['Education', 'Prototypes', 'Europe', 'Bitcoin', 'Open source software', 'Organizations']","['Blockchain', 'higher education', 'ECTS', 'tokens']"
"Due to the proliferation of ICT during the last few decades, there is an exponential increase in the usage of various smart applications such as smart farming, smart healthcare, supply-chain & logistics, business, tourism and hospitality, energy management etc. However, for all the aforementioned applications, security and privacy are major concerns keeping in view of the usage of the open channel, i.e., Internet for data transfer. Although many security solutions and standards have been proposed over the years to enhance the security levels of aforementioned smart applications, but the existing solutions are either based upon the centralized architecture (having single point of failure) or having high computation and communication costs. Moreover, most of the existing security solutions have focussed only on few aspects and fail to address scalability, robustness, data storage, network latency, auditability, immutability, and traceability. To handle the aforementioned issues, blockchain technology can be one of the solutions. Motivated from these facts, in this paper, we present a systematic review of various blockchain-based solutions and their applicability in various Industry 4.0-based applications. Our contributions in this paper are in four fold. Firstly, we explored the current state-of-the-art solutions in the blockchain technology for the smart applications. Then, we illustrated the reference architecture used for the blockchain applicability in various Industry 4.0 applications. Then, merits and demerits of the traditional security solutions are also discussed in comparison to their countermeasures. Finally, we provided a comparison of existing blockchain-based security solutions using various parameters to provide deep insights to the readers about its applicability in various applications.","['Industries', 'Medical services', 'Computer architecture', 'Data privacy', 'Internet']","['Blockchain', 'consensus algorithms', 'cyber-physical systems', 'IoT', 'smart grid', 'supply chain management', 'intelligent transportation']"
"In the field of agricultural information, the automatic identification and diagnosis of maize leaf diseases is highly desired. To improve the identification accuracy of maize leaf diseases and reduce the number of network parameters, the improved GoogLeNet and Cifar10 models based on deep learning are proposed for leaf disease recognition in this paper. Two improved models that are used to train and test nine kinds of maize leaf images are obtained by adjusting the parameters, changing the pooling combinations, adding dropout operations and rectified linear unit functions, and reducing the number of classifiers. In addition, the number of parameters of the improved models is significantly smaller than that of the VGG and AlexNet structures. During the recognition of eight kinds of maize leaf diseases, the GoogLeNet model achieves a top - 1 average identification accuracy of 98.9%, and the Cifar10 model achieves an average accuracy of 98.8%. The improved methods are possibly improved the accuracy of maize leaf disease, and reduced the convergence iterations, which can effectively improve the model training and recognition efficiency.","['Diseases', 'Training', 'Machine learning', 'Support vector machines', 'Testing', 'Convolutional neural networks']","['Deep learning', 'deep convolutional neural networks', 'identification', 'image processing', 'leaf diseases']"
"The vision of Industry 4.0, otherwise known as the fourth industrial revolution, is the integration of massively deployed smart computing and network technologies in industrial production and manufacturing settings for the purposes of automation, reliability, and control, implicating the development of an Industrial Internet of Things (I-IoT). Specifically, I-IoT is devoted to adopting the IoT to enable the interconnection of anything, anywhere, and at any time in the manufacturing system context to improve the productivity, efficiency, safety, and intelligence. As an emerging technology, I-IoT has distinct properties and requirements that distinguish it from consumer IoT, including the unique types of smart devices incorporated, network technologies and quality-of-service requirements, and strict needs of command and control. To more clearly understand the complexities of I-IoT and its distinct needs and to present a unified assessment of the technology from a systems’ perspective, in this paper, we comprehensively survey the body of existing research on I-IoT. Particularly, we first present the I-IoT architecture, I-IoT applications (i.e., factory automation and process automation), and their characteristics. We then consider existing research efforts from the three key system aspects of control, networking, and computing. Regarding control, we first categorize industrial control systems and then present recent and relevant research efforts. Next, considering networking, we propose a three-dimensional framework to explore the existing research space and investigate the adoption of some representative networking technologies, including 5G, machine-to-machine communication, and software-defined networking. Similarly, concerning computing, we again propose a second three-dimensional framework that explores the problem space of computing in I-IoT and investigate the cloud, edge, and hybrid cloud and edge computing platforms. Finally, we outline particular challenges and future research needs in control, networking, and computing systems, as well as for the adoption of machine learning in an I-IoT context.","['Manufacturing', 'Automation', 'Machine learning', 'Sensors', 'Production', 'Control systems', 'Cloud computing']","['Industrial Internet of Things', 'industrial cyber physical systems', 'application and service', 'control', 'networking', 'computing', 'machine learning', 'big data analytics', 'survey', 'future research directions']"
"Smart world is envisioned as an era in which objects (e.g., watches, mobile phones, computers, cars, buses, and trains) can automatically and intelligently serve people in a collaborative manner. Paving the way for smart world, Internet of Things (IoT) connects everything in the smart world. Motivated by achieving a sustainable smart world, this paper discusses various technologies and issues regarding green IoT, which further reduces the energy consumption of IoT. Particularly, an overview regarding IoT and green IoT is performed first. Then, the hot green information and communications technologies (ICTs) (e.g., green radio-frequency identification, green wireless sensor network, green cloud computing, green machine to machine, and green data center) enabling green IoT are studied, and general green ICT principles are summarized. Furthermore, the latest developments and future vision about sensor cloud, which is a novel paradigm in green IoT, are reviewed and introduced, respectively. Finally, future research directions and open problems about green IoT are presented. Our work targets to be an enlightening and latest guidance for research with respect to green IoT and smart world.","['Internet of things', 'Radio frequency identification', 'Wireless sensor networks', 'Machine-to-machine communications', 'Cloud computing', 'Data centers']","['Smart world', 'internet of things', 'green', 'radio frequency identification', 'wireless sensor network', 'cloud computing', 'machine to machine', 'data center', 'sensor-cloud']"
"New high-data-rate multimedia services and applications are evolving continuously and exponentially increasing the demand for wireless capacity of fifth-generation (5G) and beyond. The existing radio frequency (RF) communication spectrum is insufficient to meet the demands of future high-datarate 5G services. Optical wireless communication (OWC), which uses an ultra-wide range of unregulated spectrum, has emerged as a promising solution to overcome the RF spectrum crisis. It has attracted growing research interest worldwide in the last decade for indoor and outdoor applications. OWC offloads huge data traffic applications from RF networks. A 100 Gb/s data rate has already been demonstrated through OWC. It offers services indoors as well as outdoors, and communication distances range from several nm to more than 10000 km. This paper provides a technology overview and a review on optical wireless technologies, such as visible light communication, light fidelity, optical camera communication, free space optical communication, and light detection and ranging. We survey the key technologies for understanding OWC and present state-of-the-art criteria in aspects, such as classification, spectrum use, architecture, and applications. The key contribution of this paper is to clarify the differences among different promising optical wireless technologies and between these technologies and their corresponding similar existing RF technologies.","['Wireless communication', '5G mobile communication', 'Radio frequency', 'Optical transmitters', 'Optical fiber communication', 'Optical sensors']","['Free space optical communication', 'infrared', 'light detection and ranging', 'light fidelity', 'optical camera communication', 'optical wireless communication', 'radio frequency', 'ultraviolet', 'visible light', 'visible light communication']"
"COVID-19 outbreak has put the whole world in an unprecedented difficult situation bringing life around the world to a frightening halt and claiming thousands of lives. Due to COVID-19's spread in 212 countries and territories and increasing numbers of infected cases and death tolls mounting to 5,212,172 and 334,915 (as of May 22 2020), it remains a real threat to the public health system. This paper renders a response to combat the virus through Artificial Intelligence (AI). Some Deep Learning (DL) methods have been illustrated to reach this goal, including Generative Adversarial Networks (GANs), Extreme Learning Machine (ELM), and Long/Short Term Memory (LSTM). It delineates an integrated bioinformatics approach in which different aspects of information from a continuum of structured and unstructured data sources are put together to form the user-friendly platforms for physicians and researchers. The main advantage of these AI-based platforms is to accelerate the process of diagnosis and treatment of the COVID-19 disease. The most recent related publications and medical reports were investigated with the purpose of choosing inputs and targets of the network that could facilitate reaching a reliable Artificial Neural Network-based tool for challenges associated with COVID-19. Furthermore, there are some specific inputs for each platform, including various forms of the data, such as clinical data and medical imaging which can improve the performance of the introduced approaches toward the best responses in practical applications.","['Artificial intelligence', 'COVID-19', 'Medical diagnostic imaging', 'Databases']","['Artificial intelligence', 'big data', 'bioinformatics', 'biomedical informatics', 'COVID-19', 'deep learning', 'diagnosis', 'machine learning', 'treatment']"
"The growing popularity and development of data mining technologies bring serious threat to the security of individual,'s sensitive information. An emerging research topic in data mining, known as privacy-preserving data mining (PPDM), has been extensively studied in recent years. The basic idea of PPDM is to modify the data in such a way so as to perform data mining algorithms effectively without compromising the security of sensitive information contained in the data. Current studies of PPDM mainly focus on how to reduce the privacy risk brought by data mining operations, while in fact, unwanted disclosure of sensitive information may also happen in the process of data collecting, data publishing, and information (i.e., the data mining results) delivering. In this paper, we view the privacy issues related to data mining from a wider perspective and investigate various approaches that can help to protect sensitive information. In particular, we identify four different types of users involved in data mining applications, namely, data provider, data collector, data miner, and decision maker. For each type of user, we discuss his privacy concerns and the methods that can be adopted to protect sensitive information. We briefly introduce the basics of related research topics, review state-of-the-art approaches, and present some preliminary thoughts on future research directions. Besides exploring the privacy-preserving approaches for each type of user, we also review the game theoretical approaches, which are proposed for analyzing the interactions among different users in a data mining scenario, each of whom has his own valuation on the sensitive information. By differentiating the responsibilities of different users with respect to security of sensitive information, we would like to provide some useful insights into the study of PPDM.","['Privacy', 'Data mining', 'Game theory', 'Tracking', 'Computer security', 'Data privacy', 'Algorithm design and analysis']","['data mining', 'sensitive information', 'privacypreserving data mining', 'anonymization', 'provenance', 'game theory', 'privacy auction', 'anti-tracking']"
"Diverse proprietary network appliances increase both the capital and operational expense of service providers, meanwhile causing problems of network ossification. Network function virtualization (NFV) is proposed to address these issues by implementing network functions as pure software on commodity and general hardware. NFV allows flexible provisioning, deployment, and centralized management of virtual network functions. Integrated with SDN, the software-defined NFV architecture further offers agile traffic steering and joint optimization of network functions and resources. This architecture benefits a wide range of applications (e.g., service chaining) and is becoming the dominant form of NFV. In this survey, we present a thorough investigation of the development of NFV under the software-defined NFV architecture, with an emphasis on service chaining as its application. We first introduce the software-defined NFV architecture as the state of the art of NFV and present relationships between NFV and SDN. Then, we provide a historic view of the involvement from middlebox to NFV. Finally, we introduce significant challenges and relevant solutions of NFV, and discuss its future research directions by different application domains.","['Computer architecture', 'Virtualization', 'Hardware', 'Software', 'Home appliances', 'Control systems', 'Servers']","['Software-defined networks', 'network function virtualization', 'middlebox', 'service chain', 'network virtualization']"
"Emerging technologies such as the Internet of Things (IoT) require latency-aware computation for real-time application processing. In IoT environments, connected things generate a huge amount of data, which are generally referred to as big data. Data generated from IoT devices are generally processed in a cloud infrastructure because of the on-demand services and scalability features of the cloud computing paradigm. However, processing IoT application requests on the cloud exclusively is not an efficient solution for some IoT applications, especially time-sensitive ones. To address this issue, Fog computing, which resides in between cloud and IoT devices, was proposed. In general, in the Fog computing environment, IoT devices are connected to Fog devices. These Fog devices are located in close proximity to users and are responsible for intermediate computation and storage. One of the key challenges in running IoT applications in a Fog computing environment are resource allocation and task scheduling. Fog computing research is still in its infancy, and taxonomy-based investigation into the requirements of Fog infrastructure, platform, and applications mapped to current research is still required. This survey will help the industry and research community synthesize and identify the requirements for Fog computing. This paper starts with an overview of Fog computing in which the definition of Fog computing, research trends, and the technical differences between Fog and cloud are reviewed. Then, we investigate numerous proposed Fog computing architectures and describe the components of these architectures in detail. From this, the role of each component will be defined, which will help in the deployment of Fog computing. Next, a taxonomy of Fog computing is proposed by considering the requirements of the Fog computing paradigm. We also discuss existing research works and gaps in resource allocation and scheduling, fault tolerance, simulation tools, and Fog-based microservices. Finally, by addressing the limitations of current research works, we present some open issues, which will determine the future research direction for the Fog computing paradigm.","['Edge computing', 'Cloud computing', 'Computer architecture', 'Market research', 'Internet of Things', 'Resource management', 'Taxonomy']","['Fog computing', 'Internet of Things (IoT)', 'fog devices', 'fault tolerance', 'IoT application', 'microservices']"
"Intrusion detection is a fundamental part of security tools, such as adaptive security appliances, intrusion detection systems, intrusion prevention systems, and firewalls. Various intrusion detection techniques are used, but their performance is an issue. Intrusion detection performance depends on accuracy, which needs to improve to decrease false alarms and to increase the detection rate. To resolve concerns on performance, multilayer perceptron, support vector machine (SVM), and other techniques have been used in recent work. Such techniques indicate limitations and are not efficient for use in large data sets, such as system and network data. The intrusion detection system is used in analyzing huge traffic data; thus, an efficient classification technique is necessary to overcome the issue. This problem is considered in this paper. Well-known machine learning techniques, namely, SVM, random forest, and extreme learning machine (ELM) are applied. These techniques are well-known because of their capability in classification. The NSL–knowledge discovery and data mining data set is used, which is considered a benchmark in the evaluation of intrusion detection mechanisms. The results indicate that ELM outperforms other approaches.","['Intrusion detection', 'Support vector machines', 'Radio frequency', 'Training', 'Forestry', 'Kernel']","['Detection rate', 'extreme learning machine', 'false alarms', 'NSL–KDD', 'random forest', 'support vector machine']"
"In the past years, traditional pattern recognition methods have made great progress. However, these methods rely heavily on manual feature extraction, which may hinder the generalization model performance. With the increasing popularity and success of deep learning methods, using these techniques to recognize human actions in mobile and wearable computing scenarios has attracted widespread attention. In this paper, a deep neural network that combines convolutional layers with long short-term memory (LSTM) was proposed. This model could extract activity features automatically and classify them with a few model parameters. LSTM is a variant of the recurrent neural network (RNN), which is more suitable for processing temporal sequences. In the proposed architecture, the raw data collected by mobile sensors was fed into a two-layer LSTM followed by convolutional layers. In addition, a global average pooling layer (GAP) was applied to replace the fully connected layer after convolution for reducing model parameters. Moreover, a batch normalization layer (BN) was added after the GAP layer to speed up the convergence, and obvious results were achieved. The model performance was evaluated on three public datasets (UCI, WISDM, and OPPORTUNITY). Finally, the overall accuracy of the model in the UCI-HAR dataset is 95.78%, in the WISDM dataset is 95.85%, and in the OPPORTUNITY dataset is 92.63%. The results show that the proposed model has higher robustness and better activity detection capability than some of the reported results. It can not only adaptively extract activity features, but also has fewer parameters and higher accuracy.","['Feature extraction', 'Activity recognition', 'Acceleration', 'Deep learning', 'Sensor phenomena and characterization', 'Accelerometers']","['Human activity recognition', 'convolution', 'long short-term memory', 'mobile sensors']"
"Brain tumor classification is a crucial task to evaluate the tumors and make a treatment decision according to their classes. There are many imaging techniques used to detect brain tumors. However, MRI is commonly used due to its superior image quality and the fact of relying on no ionizing radiation. Deep learning (DL) is a subfield of machine learning and recently showed a remarkable performance, especially in classification and segmentation problems. In this paper, a DL model based on a convolutional neural network is proposed to classify different brain tumor types using two publicly available datasets. The former one classifies tumors into (meningioma, glioma, and pituitary tumor). The other one differentiates between the three glioma grades (Grade II, Grade III, and Grade IV). The datasets include 233 and 73 patients with a total of 3064 and 516 images on T1-weighted contrast-enhanced images for the first and second datasets, respectively. The proposed network structure achieves a significant performance with the best overall accuracy of 96.13% and 98.7%, respectively, for the two studies. The results indicate the ability of the model for brain tumor multi-classification purposes.","['Tumors', 'Feature extraction', 'Cancer', 'Task analysis', 'Magnetic resonance imaging', 'Convolutional neural networks', 'Training']","['Brain tumor', 'convolutional neural network', 'data augmentation', 'deep learning', 'MRI']"
"Wind energy has seen great development during the past decade. However, wind turbine availability and reliability, especially for offshore sites, still need to be improved, which strongly affect the cost of wind energy. Wind turbine operational cost is closely depending on component failure and repair rate, while fault detection and isolation will be very helpful to improve the availability and reliability factors. In this paper, an efficient machine learning method, random forests (RFs) in combination with extreme gradient boosting (XGBoost), is used to establish the data-driven wind turbine fault detection framework. In the proposed design, RF is used to rank the features by importance, which are either direct sensor signals or constructed variables from prior knowledge. Then, based on the top-ranking features, XGBoost trains the ensemble classifier for each specific fault. In order to verify the effectiveness of the proposed approach, numerical simulations using the state-of-the-art wind turbine simulator FAST are conducted for three different types of wind turbines in both the below and above rated conditions. It is shown that the proposed approach is robust to various wind turbine models including offshore ones in different working conditions. Besides, the proposed ensemble classifier is able to protect against overfitting, and it achieves better wind turbine fault detection results than the support vector machine method when dealing with multidimensional data.","['Wind turbines', 'Generators', 'Actuators', 'Blades', 'Radio frequency', 'Fault detection', 'Support vector machines']","['Wind turbines', 'fault detection', 'data-driven', 'random forests', 'extreme gradient boosting']"
"Generative adversarial network (GANs) is one of the most important research avenues in the field of artificial intelligence, and its outstanding data generation capacity has received wide attention. In this paper, we present the recent progress on GANs. First, the basic theory of GANs and the differences among different generative models in recent years were analyzed and summarized. Then, the derived models of GANs are classified and introduced one by one. Third, the training tricks and evaluation metrics were given. Fourth, the applications of GANs were introduced. Finally, the problem, we need to address, and future directions were discussed.","['Gallium nitride', 'Generators', 'Generative adversarial networks', 'Training', 'Feature extraction', 'Data models', 'Unsupervised learning']","['Deep learning', 'machine learning', 'unsupervised learning', 'generative adversarial networks']"
"Software defined networking (SDN) brings about innovation, simplicity in network management, and configuration in network computing. Traditional networks often lack the flexibility to bring into effect instant changes because of the rigidity of the network and also the over dependence on proprietary services. SDN decouples the control plane from the data plane, thus moving the control logic from the node to a central controller. A wireless sensor network (WSN) is a great platform for low-rate wireless personal area networks with little resources and short communication ranges. However, as the scale of WSN expands, it faces several challenges, such as network management and heterogeneous-node networks. The SDN approach to WSNs seeks to alleviate most of the challenges and ultimately foster efficiency and sustainability in WSNs. The fusion of these two models gives rise to a new paradigm: Software defined wireless sensor networks (SDWSN). The SDWSN model is also envisioned to play a critical role in the looming Internet of Things paradigm. This paper presents a comprehensive review of the SDWSN literature. Moreover, it delves into some of the challenges facing this paradigm, as well as the major SDWSN design requirements that need to be considered to address these challenges.","['Wireless sensor networks', 'Software', 'Protocols', 'Iron', 'Internet of Things', 'Switches']","['Software defined wireless sensor networks', 'software defined networking', 'wireless sensor networks']"
"The recent advances in embedded processing have enabled the vision based systems to detect fire during surveillance using convolutional neural networks (CNNs). However, such methods generally need more computational time and memory, restricting its implementation in surveillance networks. In this research paper, we propose a cost-effective fire detection CNN architecture for surveillance videos. The model is inspired from GoogleNet architecture, considering its reasonable computational complexity and suitability for the intended problem compared to other computationally expensive networks such as AlexNet. To balance the efficiency and accuracy, the model is fine-tuned considering the nature of the target problem and fire data. Experimental results on benchmark fire datasets reveal the effectiveness of the proposed framework and validate its suitability for fire detection in CCTV surveillance systems compared to state-of-the-art methods.","['Fires', 'Surveillance', 'Computer architecture', 'Videos', 'Color', 'Feature extraction', 'Convolution']","['Fire detection', 'image classification', 'real-world applications', 'deep learning', 'CCTV video analysis']"
"With the purpose of identifying cyber threats and possible incidents, intrusion detection systems (IDSs) are widely deployed in various computer networks. In order to enhance the detection capability of a single IDS, collaborative intrusion detection networks (or collaborative IDSs) have been developed, which allow IDS nodes to exchange data with each other. However, data and trust management still remain two challenges for current detection architectures, which may degrade the effectiveness of such detection systems. In recent years, blockchain technology has shown its adaptability in many fields, such as supply chain management, international payment, interbanking, and so on. As blockchain can protect the integrity of data storage and ensure process transparency, it has a potential to be applied to intrusion detection domain. Motivated by this, this paper provides a review regarding the intersection of IDSs and blockchains. In particular, we introduce the background of intrusion detection and blockchain, discuss the applicability of blockchain to intrusion detection, and identify open challenges in this direction.","['Intrusion detection', 'Collaboration', 'Peer-to-peer computing', 'Monitoring', 'Resistance']","['Blockchain technology', 'intrusion detection', 'collaborative network', 'trust management', 'data sharing and management']"
"Internet of Things (IoT) is one of the evolutionary directions of the Internet. This paper focuses on the low earth orbit (LEO) satellite constellation-based IoT services for their irreplaceable functions. In many cases, IoT devices are distributed in remote areas (e.g., desert, ocean, and forest) in some special applications, they are placed in some extreme topography, where are unable to have direct terrestrial network accesses and can only be covered by satellite. Comparing with the traditional geostationary earth orbit (GEO) systems, LEO satellite constellation has the advantages of low propagation delay, small propagation loss and global coverage. Furthermore, revision of existing IoT protocol are necessary to enhance the compatibility of the LEO satellite constellation-based IoT with terrestrial IoT systems. In this paper, we provide an overview of the architecture of the LEO satellite constellation-based IoT including the following topics: LEO satellite constellation structure, efficient spectrum allocation, heterogeneous networks compatibility, and access and routing protocols.","['Low earth orbit satellites', 'Monitoring', 'Satellite constellations', 'Protocols', 'Machine-to-machine communications', 'Sensors']","['Internet of things (IoT)', 'LEO satellite constellation', 'low-power wide-area network (LPWAN)', 'long range (LoRa)', 'machine-to-machine (M2M) communications', 'narrow band internet of things (NB-IoT)']"
"After many years of rigid conventional procedures of production, industrial manufacturing is going through a process of change toward flexible and intelligent manufacturing, the so-called Industry 4.0. In this paper, human-robot collaboration has an important role in smart factories since it contributes to the achievement of higher productivity and greater efficiency. However, this evolution means breaking with the established safety procedures as the separation of workspaces between robot and human is removed. These changes are reflected in safety standards related to industrial robotics since the last decade, and have led to the development of a wide field of research focusing on the prevention of human-robot impacts and/or the minimization of related risks or their consequences. This paper presents a review of the main safety systems that have been proposed and applied in industrial robotic environments that contribute to the achievement of safe collaborative human-robot work. Additionally, a review is provided of the current regulations along with new concepts that have been introduced in them. The discussion presented in this paper includes multidisciplinary approaches, such as techniques for estimation and the evaluation of injuries in human-robot collisions, mechanical and software devices designed to minimize the consequences of human-robot impact, impact detection systems, and strategies to prevent collisions or minimize their consequences when they occur.","['Service robots', 'Safety', 'Robot sensing systems', 'Collaboration', 'Standards', 'Collision avoidance']","['Safety', 'industrial robot', 'human-robot collaboration', 'industrial standards', 'Industry 4.0']"
"Blockchain technologies have recently come to the forefront of the research and industrial communities as they bring potential benefits for many industries. This is due to their practical capabilities in solving many issues currently inhibiting further advances in various industrial domains. Securely recording and sharing transactional data, establishing automated and efficient supply chain processes, and enhancing transparency across the whole value chain are some examples of these issues. Blockchain offers an effective way to tackle these issues using distributed, shared, secure, and permissioned transactional ledgers. The employment of blockchain technologies and the possibility of applying them in different situations enables many industrial applications through increased efficiency and security; enhanced traceability and transparency; and reduced costs. In this paper, different industrial application domains where the use of blockchain technologies has been proposed are reviewed. This paper explores the opportunities, benefits, and challenges of incorporating blockchain in different industrial applications. Furthermore, the paper attempts to identify the requirements that support the implementation of blockchain for different industrial applications. The review reveals that several opportunities are available for utilizing blockchain in various industrial sectors; however, there are still some challenges to be addressed to achieve better utilization of this technology.",[],[]
"In the era of smart cities, there are a plethora of applications where the localization of indoor environments is important, from monitoring and tracking in smart buildings to proximity marketing and advertising in shopping malls. The success of these applications is based on the development of a cost-efficient and robust real-time system capable of accurately localizing objects. In most outdoor localization systems, global positioning system (GPS) is used due to its ease of implementation and accuracy up to five meters. However, due to the limited space that comes with performing localization of indoor environments and the large number of obstacles found indoors, GPS is not a suitable option. Hence, accurately and efficiently locating objects is a major challenge in indoor environments. Recent advancements in the Internet of Things (IoT) along with novel wireless technologies can alleviate the problem. Small-size and cost-efficient IoT devices which use wireless protocols can provide an attractive solution. In this paper, we compare four wireless technologies for indoor localization: Wi-Fi (IEEE 802.11n-2009 at the 2.4 GHz band), Bluetooth low energy, Zigbee, and long-range wide-area network. These technologies are compared in terms of localization accuracy and power consumption when IoT devices are used. The received signal strength indicator (RSSI) values from each modality were used and trilateration was performed for localization. The RSSI data set is available online. The experimental results can be used as an indicator in the selection of a wireless technology for an indoor localization system following application requirements.","['Wireless fidelity', 'Wireless communication', 'Performance evaluation', 'Global Positioning System', 'Receivers', 'ZigBee', 'Hardware']","['Indoor localization accuracy', 'power consumption', 'Internet of Things', 'RSSI', 'WiFi', 'Bluetooth low energy', 'Zigbee', 'LoRaWAN']"
"The Blockchain technology can be defined as a distributed ledger database for recording transactions between parties verifiably and permanently. Blockchain emerged as a leading technology layer for financial applications. Nevertheless, in the past years, the attention of researchers and practitioners moved to the application of the Blockchain technologies to other domains. Recently, it represents the backbone of a new digital supply chain. Thanks to its capability of ensuring data immutability and public accessibility of data streams, Blockchain can increase the efficiency, reliability, and transparency of the overall supply chain, and optimize the inbound processes. The literature concerning Blockchain in non-financial applications mainly focused on the technological part and the Business Process Modeling, lacking in terms of standard methodology for designing a strategy to develop and validate the overall Blockchain solution and integrate it in the Business Strategy. Thus, this paper aims to overcome this lack. First, we integrate the current literature filling the lack concerning the digital strategy, creating a standard methodology to design Blockchain technology use cases, which are not related to finance applications. Second, we present the results of a use case in the fresh food delivery, showing the critical aspects of implementing a Blockchain solution. Moreover, the paper discusses how the Blockchain will help in reducing the logistics costs and in optimizing the operations and the research challenges.","['Supply chains', 'Standards']","['Blockchain', 'hyperledger', 'supply chain']"
"The Internet of Drones (IoD) is a layered network control architecture designed mainly for coordinating the access of unmanned aerial vehicles to controlled airspace, and providing navigation services between locations referred to as nodes. The IoD provides generic services for various drone applications, such as package delivery, traffic surveillance, search and rescue, and more. In this paper, we present a conceptual model of how such an architecture can be organized and we specify the features that an IoD system based on our architecture should implement. For doing so, we extract key concepts from three existing large scale networks, namely the air traffic control network, the cellular network, and the Internet, and explore their connections to our novel architecture for drone traffic management. A simulation platform for IoD is being implemented, which can be accessed from www.IoDnet.org in the future.","['Internet of things', 'Drones', 'Network architecture', 'Internet', 'Navigation', 'Atmospheric modeling', 'Surveillance', 'Traffic control']","['Layered architecture', 'Internet of Drones (IoD)', 'Internet', 'cellular network', 'air traffic control (ATC)', 'low altitude air traffic management', 'unmanned aerial vehicle (UAV)']"
"Wireless mediums, such as RF, optical, or acoustical, provide finite resources for the purposes of remote sensing (such as radar) and data communications. Often, these two functions are at odds with one another and compete for these resources. Applications for wireless technology are growing rapidly, and RF convergence is already presenting itself as a requirement for both users as consumer and military system requirements evolve. The broad solution space to this complex problem encompasses cooperation or codesigning of systems with both sensing and communications functions. By jointly considering the systems during the design phase, rather than perpetuating a notion of mutual interference, both system's performance can be improved. We provide a point of departure for future researchers that will be required to solve this problem by presenting the applications, topologies, levels of system integration, the current state of the art, and outlines of future information-centric systems.","['Radio frequency', 'Radar communication', 'Wireless sensor networks', 'Laser radar', 'Remote sensing', 'Radar remote sensing']","['RF convergence', 'radar communications co-existence', 'joint sensing-communications', 'wireless resources']"
"This paper reviews the basic concepts of rays, ray tracing algorithms, and radio propagation modeling using ray tracing methods. We focus on the fundamental concepts and the development of practical ray tracing algorithms. The most recent progress and a future perspective of ray tracing are also discussed. We envision propagation modeling in the near future as an intelligent, accurate, and real-time system in which ray tracing plays an important role. This review is especially useful for experts who are developing new ray tracing algorithms to enhance modeling accuracy and improve computational speed.","['Ray tracing', 'Radio propagation', 'Modeling', 'Acceleration', 'Algorithm design and analysis', 'Radio propagation', 'Propagation modeling']","['Radio propagation', 'propagation modeling', 'acceleration algorithm']"
"Lithium-ion battery is an appropriate choice for electric vehicle (EV) due to its promising features of high voltage, high energy density, low self-discharge and long lifecycles. The successful operation of EV is highly dependent on the operation of battery management system (BMS). State of charge (SOC) is one of the vital paraments of BMS which signifies the amount of charge left in a battery. A good estimation of SOC leads to long battery life and prevention of catastrophe from battery failure. Besides, an accurate and robust SOC estimation has great significance towards an efficient EV operation. However, SOC estimation is a complex process due to its dependency on various factors such as battery age, ambient temperature, and many unknown factors. This review presents the recent SOC estimation methods highlighting the model-based and data-driven approaches. Model-based methods attempt to model the battery behavior incorporating various factors into complex mathematical equations in order to accurately estimate the SOC while the data-driven methods adopt an approach of learning the battery's behavior by running complex algorithms with a large amount of measured battery data. The classifications of model-based and data-driven based SOC estimation are explained in terms of estimation model/algorithm, benefits, drawbacks, and estimation error. In addition, the review highlights many factors and challenges and delivers potential recommendations for the development of SOC estimation methods in EV applications. All the highlighted insights of this review will hopefully lead to increased efforts toward the enhancement of SOC estimation method of lithium-ion battery for the future high-tech EV applications.","['State of charge', 'Batteries', 'Estimation', 'Temperature measurement', 'Mathematical model', 'Integrated circuit modeling']","['State of charge', 'lithium-ion battery', 'electric vehicle', 'model-based approaches', 'data-driven approaches']"
"With the rapid development of the Internet of Everything (IoE), the number of smart devices connected to the Internet is increasing, resulting in large-scale data, which has caused problems such as bandwidth load, slow response speed, poor security, and poor privacy in traditional cloud computing models. Traditional cloud computing is no longer sufficient to support the diverse needs of today's intelligent society for data processing, so edge computing technologies have emerged. It is a new computing paradigm for performing calculations at the edge of the network. Unlike cloud computing, it emphasizes closer to the user and closer to the source of the data. At the edge of the network, it is lightweight for local, small-scale data storage and processing. This article mainly reviews the related research and results of edge computing. First, it summarizes the concept of edge computing and compares it with cloud computing. Then summarize the architecture of edge computing, keyword technology, security and privacy protection, and finally summarize the applications of edge computing.","['Cloud computing', 'Edge computing', 'Real-time systems', 'Internet of Things', 'Bandwidth', 'Security', 'Data privacy']","['Edge computing', 'cloud computing', 'Internet of Things']"
"Blockchain-based decentralized cryptocurrencies have drawn much attention and been widely-deployed in recent years. Bitcoin, the first application of blockchain, achieves great success and promotes more development in this field. However, Bitcoin encounters performance problems of low throughput and high transaction latency. Other cryptocurrencies based on proof-of-work also inherit the flaws, leading to more concerns about the scalability of blockchain. This paper attempts to cover the existing scaling solutions for blockchain and classify them by level. In addition, we make comparisons between different methods and list some potential directions for solving the scalability problem of blockchain.","['Blockchain', 'Scalability', 'Bitcoin', 'Throughput', 'Measurement']","['Blockchain', 'scalability']"
"In this paper, we review the background and state-of-the-art of the narrow-band Internet of Things (NB-IoT). We first introduce NB-IoT general background, development history, and standardization. Then, we present NB-IoT features through the review of current national and international studies on NB-IoT technology, where we focus on basic theories and key technologies, i.e., connection count analysis theory, delay analysis theory, coverage enhancement mechanism, ultra-low power consumption technology, and coupling relationship between signaling and data. Subsequently, we compare several performances of NB-IoT and other wireless and mobile communication technologies in aspects of latency, security, availability, data transmission rate, energy consumption, spectral efficiency, and coverage area. Moreover, we analyze five intelligent applications of NB-IoT, including smart cities, smart buildings, intelligent environment monitoring, intelligent user services, and smart metering. Finally, we summarize security requirements of NB-IoT, which need to be solved urgently. These discussions aim to provide a comprehensive overview of NB-IoT, which can help readers to understand clearly the scientific problems and future research directions of NB-IoT.","['Couplings', 'Security', '3GPP', 'Long Term Evolution', 'Uplink', 'Internet of Things', 'GSM']","['Intelligent application', 'Internet of Things', 'LPWAN', 'LTE', 'NB-IoT']"
"The purpose of this study was to assess the impact of Artificial Intelligence (AI) on education. Premised on a narrative and framework for assessing AI identified from a preliminary analysis, the scope of the study was limited to the application and effects of AI in administration, instruction, and learning. A qualitative research approach, leveraging the use of literature review as a research design and approach was used and effectively facilitated the realization of the study purpose. Artificial intelligence is a field of study and the resulting innovations and developments that have culminated in computers, machines, and other artifacts having human-like intelligence characterized by cognitive abilities, learning, adaptability, and decision-making capabilities. The study ascertained that AI has extensively been adopted and used in education, particularly by education institutions, in different forms. AI initially took the form of computer and computer related technologies, transitioning to web-based and online intelligent education systems, and ultimately with the use of embedded computer systems, together with other technologies, the use of humanoid robots and web-based chatbots to perform instructors' duties and functions independently or with instructors. Using these platforms, instructors have been able to perform different administrative functions, such as reviewing and grading students' assignments more effectively and efficiently, and achieve higher quality in their teaching activities. On the other hand, because the systems leverage machine learning and adaptability, curriculum and content has been customized and personalized in line with students' needs, which has fostered uptake and retention, thereby improving learners experience and overall quality of learning.","['Education', 'Technological innovation', 'Learning (artificial intelligence)', 'Microcomputers', 'Robots']","['Education', 'artificial intelligence', 'leaner']"
"The upcoming fifth generation (5G) of wireless networks is expected to lay a foundation of intelligent networks with the provision of some isolated artificial intelligence (AI) operations. However, fully intelligent network orchestration and management for providing innovative services will only be realized in Beyond 5G (B5G) networks. To this end, we envisage that the sixth generation (6G) of wireless networks will be driven by on-demand self-reconfiguration to ensure a many-fold increase in the network performance and service types. The increasingly stringent performance requirements of emerging networks may finally trigger the deployment of some interesting new technologies, such as large intelligent surfaces, electromagnetic-orbital angular momentum, visible light communications, and cell-free communications, to name a few. Our vision for 6G is a massively connected complex network capable of rapidly responding to the users' service calls through real-time learning of the network state as described by the network edge (e.g., base-station locations and cache contents), air interface (e.g., radio spectrum and propagation channel), and the user-side (e.g., battery-life and locations). The multi-state, multi-dimensional nature of the network state, requiring the real-time knowledge, can be viewed as a quantum uncertainty problem. In this regard, the emerging paradigms of machine learning (ML), quantum computing (QC), and quantum ML (QML) and their synergies with communication networks can be considered as core 6G enablers. Considering these potentials, starting with the 5G target services and enabling technologies, we provide a comprehensive review of the related state of the art in the domains of ML (including deep learning), QC, and QML and identify their potential benefits, issues, and use cases for their applications in the B5G networks. Subsequently, we propose a novel QC-assisted and QML-based framework for 6G communication networks while articulating its challenges and potential enabling technologies at the network infrastructure, network edge, air interface, and user end. Finally, some promising future research directions for the quantum- and QML-assisted B5G networks are identified and discussed.","['5G mobile communication', 'Communication networks', 'Quantum computing', 'Machine learning', 'Wireless networks', 'Parallel processing', 'Quantum communication']","['6G', 'B5G', 'machine learning', 'quantum communications', 'quantum machine learning']"
"The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.","['Image processing', 'Tomography', 'Data acquisition', 'Image reconstruction', 'Machine learning', 'Deep learning']","['Tomographic imaging', 'medical imaging', 'data acquisition', 'image reconstruction', 'image analysis', 'big data', 'machine learning', 'deep learning']"
"Internet of Things (IoT) is a network of all devices that can be accessed through the Internet. These devices can be remotely accessed and controlled using existing network infrastructure, thus allowing a direct integration of computing systems with the physical world. This also reduces human involvement along with improving accuracy and efficiency, resulting in economic benefit. The devices in IoT facilitate the day-to-day life of people. However, the IoT has an enormous threat to security and privacy due to its heterogeneous and dynamic nature. Authentication is one of the most challenging security requirements in the IoT environment, where a user (external party) can directly access information from the devices, provided the mutual authentication between user and devices happens. In this paper, we present a new signature-based authenticated key establishment scheme for the IoT environment. The proposed scheme is tested for security with the help of the widely used Burrows-Abadi-Needham logic, informal security analysis, and also the formal security verification using the broadly accepted automated validation of Internet security protocols and applications tool. The proposed scheme is also implemented using the widely accepted NS2 simulator, and the simulation results demonstrate the practicability of the scheme. Finally, the proposed scheme provides more functionality features, and its computational and communication costs are also comparable with other existing approaches.","['Authentication', 'Privacy', 'Elliptic curves', 'Protocols', 'Adaptation models', 'Sensors']","['Internet of things (IoT)', 'authentication', 'key establishment', 'Burrows-Abadi-Needham (BAN) logic', 'AVISPA', 'NS2 simulation', 'security']"
"With the development of technologies, such as big data, cloud computing, and the Internet of Things (IoT), digital twin is being applied in industry as a precision simulation technology from concept to practice. Further, simulation plays a very important role in the healthcare field, especially in research on medical pathway planning, medical resource allocation, medical activity prediction, etc. By combining digital twin and healthcare, there will be a new and efficient way to provide more accurate and fast services for elderly healthcare. However, how to achieve personal health management throughout the entire lifecycle of elderly patients, and how to converge the medical physical world and the virtual world to realize real smart healthcare, are still two key challenges in the era of precision medicine. In this paper, a framework of the cloud healthcare system is proposed based on digital twin healthcare (CloudDTH). This is a novel, generalized, and extensible framework in the cloud environment for monitoring, diagnosing and predicting aspects of the health of individuals using, for example, wearable medical devices, toward the goal of personal health management, especially for the elderly. CloudDTH aims to achieve interaction and convergence between medical physical and virtual spaces. Accordingly, a novel concept of digital twin healthcare (DTH) is proposed and discussed, and a DTH model is implemented. Next, a reference framework of CloudDTH based on DTH is constructed, and its key enabling technologies are explored. Finally, the feasibility of some application scenarios and a case study for real-time supervision are demonstrated.","['Medical services', 'Cloud computing', 'Senior citizens', 'Medical diagnostic imaging', 'Real-time systems', 'Computational modeling']","['Digital twin', 'elderly healthcare', 'personal health management', 'cloud computing', 'precision medicine', 'interaction', 'convergence']"
"Google Colaboratory (also known as Colab) is a cloud service based on Jupyter Notebooks for disseminating machine learning education and research. It provides a runtime fully configured for deep learning and free-of-charge access to a robust GPU. This paper presents a detailed analysis of Colaboratory regarding hardware resources, performance, and limitations. This analysis is performed through the use of Colaboratory for accelerating deep learning for computer vision and other GPU-centric applications. The chosen test-cases are a parallel tree-based combinatorial search and two computer vision applications: object detection/classification and object localization/segmentation. The hardware under the accelerated runtime is compared with a mainstream workstation and a robust Linux server equipped with 20 physical cores. Results show that the performance reached using this cloud service is equivalent to the performance of the dedicated testbeds, given similar resources. Thus, this service can be effectively exploited to accelerate not only deep learning but also other classes of GPU-centric applications. For instance, it is faster to train a CNN on Colaboratory's accelerated runtime than using 20 physical cores of a Linux server. The performance of the GPU made available by Colaboratory may be enough for several profiles of researchers and students. However, these free-of-charge hardware resources are far from enough to solve demanding real-world problems and are not scalable. The most significant limitation found is the lack of CPU cores. Finally, several strengths and limitations of this cloud service are discussed, which might be useful for helping potential users.","['Google', 'Machine learning', 'Hardware', 'Graphics processing units', 'Acceleration', 'Runtime', 'Computer vision']","['Deep learning', 'Colab', 'convolutional neural networks', 'Google colaboratory', 'GPU computing']"
"Machine learning (ML) based forecasting mechanisms have proved their significance to anticipate in perioperative outcomes to improve the decision making on the future course of actions. The ML models have long been used in many application domains which needed the identification and prioritization of adverse factors for a threat. Several prediction methods are being popularly used to handle forecasting problems. This study demonstrates the capability of ML models to forecast the number of upcoming patients affected by COVID-19 which is presently considered as a potential threat to mankind. In particular, four standard forecasting models, such as linear regression (LR), least absolute shrinkage and selection operator (LASSO), support vector machine (SVM), and exponential smoothing (ES) have been used in this study to forecast the threatening factors of COVID-19. Three types of predictions are made by each of the models, such as the number of newly infected cases, the number of deaths, and the number of recoveries in the next 10 days. The results produced by the study proves it a promising mechanism to use these methods for the current scenario of the COVID-19 pandemic. The results prove that the ES performs best among all the used models followed by LR and LASSO which performs well in forecasting the new confirmed cases, death rate as well as recovery rate, while SVM performs poorly in all the prediction scenarios given the available dataset.","['Predictive models', 'Forecasting', 'Linear regression', 'Support vector machines', 'Diseases', 'Machine learning', 'Prediction algorithms', 'COVID-19']","['COVID-19', 'exponential smoothing method', 'future forecasting', 'adjusted R² score', 'supervised machine learning']"
"Supporting high mobility in millimeter wave (mmWave) systems enables a wide range of important applications, such as vehicular communications and wireless virtual/augmented reality. Realizing this in practice, though, requires overcoming several challenges. First, the use of narrow beams and the sensitivity of mmWave signals to blockage greatly impact the coverage and reliability of highly-mobile links. Second, highly-mobile users in dense mmWave deployments need to frequently hand-off between base stations (BSs), which is associated with critical control and latency overhead. Furthermore, identifying the optimal beamforming vectors in large antenna array mmWave systems requires considerable training overhead, which significantly affects the efficiency of these mobile systems. In this paper, a novel integrated machine learning and coordinated beamforming solution is developed to overcome these challenges and enable highly-mobile mmWave applications. In the proposed solution, a number of distributed yet coordinating BSs simultaneously serve a mobile user. This user ideally needs to transmit only one uplink training pilot sequence that will be jointly received at the coordinating BSs using omni or quasi-omni beam patterns. These received signals draw a defining signature not only for the user location, but also for its interaction with the surrounding environment. The developed solution then leverages a deep learning model that learns how to use these signatures to predict the beamforming vectors at the BSs. This renders a comprehensive solution that supports highly mobile mmWave applications with reliable coverage, low latency, and negligible training overhead. Extensive simulation results based on accurate ray-tracing, show that the proposed deep-learning coordinated beamforming strategy approaches the achievable rate of the genie-aided solution that knows the optimal beamforming vectors with no training overhead. Compared with traditional beamforming solutions, the results show that the proposed deep learning-based strategy attains higher rates, especially in high-mobility large-array regimes.","['Array signal processing', 'Training', 'Machine learning', 'Channel estimation', 'Radio frequency', 'Sensors', 'Wireless communication']","['Millimeter wave', 'deep learning', 'machine learning', 'beamforming', 'channel estimation', 'vehicular communications', 'wireless virtual/augmented reality']"
"Compressive Sensing (CS) is a new sensing modality, which compresses the signal being acquired at the time of sensing. Signals can have sparse or compressible representation either in original domain or in some transform domain. Relying on the sparsity of the signals, CS allows us to sample the signal at a rate much below the Nyquist sampling rate. Also, the varied reconstruction algorithms of CS can faithfully reconstruct the original signal back from fewer compressive measurements. This fact has stimulated research interest toward the use of CS in several fields, such as magnetic resonance imaging, high-speed video acquisition, and ultrawideband communication. This paper reviews the basic theoretical concepts underlying CS. To bridge the gap between theory and practicality of CS, different CS acquisition strategies and reconstruction approaches are elaborated systematically in this paper. The major application areas where CS is currently being used are reviewed here. This paper also highlights some of the challenges and research directions in this field.","['Sensors', 'Transforms', 'Mathematical model', 'Sparse matrices', 'Compressed sensing', 'Reconstruction algorithms', 'Image reconstruction']","['Compressive sensing', 'sparsity', 'CS acquisition strategies', 'random demodulator', 'CS reconstruction algorithms', 'OMP', 'CS applications']"
"Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper, we show that the outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a BadNet) that has the state-of-the-art performance on the user's training and validation samples but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our U.S. street sign detector can persist even if the network is later retrained for another task and cause a drop in an accuracy of 25% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and-because the behavior of neural networks is difficult to explicate-stealthy. This paper provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.","['Training', 'Machine learning', 'Perturbation methods', 'Computational modeling', 'Biological neural networks', 'Security']","['Computer security', 'machine learning', 'neural networks']"
"Recent years have witnessed a paradigm shift in the storage of Electronic Health Records (EHRs) on mobile cloud environments, where mobile devices are integrated with cloud computing to facilitate medical data exchanges among patients and healthcare providers. This advanced model enables healthcare services with low operational cost, high flexibility, and EHRs availability. However, this new paradigm also raises concerns about data privacy and network security for e-health systems. How to reliably share EHRs among mobile users while guaranteeing high-security levels in the mobile cloud is a challenging issue. In this paper, we propose a novel EHRs sharing framework that combines blockchain and the decentralized interplanetary file system (IPFS) on a mobile cloud platform. Particularly, we design a trustworthy access control mechanism using smart contracts to achieve secure EHRs sharing among different patients and medical providers. We present a prototype implementation using Ethereum blockchain in a real data sharing scenario on a mobile app with Amazon cloud computing. The empirical results show that our proposal provides an effective solution for reliable data exchanges on mobile clouds while preserving sensitive health information against potential threats. The system evaluation and security analysis also demonstrate the performance improvements in lightweight access control design, minimum network latency with high security and data privacy levels, compared to the existing data sharing models.","['Blockchain', 'Cloud computing', 'Access control', 'Smart contracts', 'Medical services', 'Data privacy']","['Electronic health records (EHRs)', 'EHRs sharing', 'mobile cloud computing (MCC)', 'Internet of Medical Things (IoMT)', 'blockchain', 'smart contracts', 'access control', 'privacy', 'security']"
"In this paper, we propose a new metric to measure goodness-of-fit for classifiers: the Real World Cost function. This metric factors in information about a real world problem, such as financial impact, that other measures like accuracy or F1 do not. This metric is also more directly interpretable for users. To optimize for this metric, we introduce the Real-World-Weight Cross-Entropy loss function, in both binary classification and single-label multiclass classification variants. Both variants allow direct input of real world costs as weights. For single-label, multiclass classification, our loss function also allows direct penalization of probabilistic false positives, weighted by label, during the training of a machine learning model. We compare the design of our loss function to the binary cross-entropy and categorical cross-entropy functions, as well as their weighted variants, to discuss the potential for improvement in handling a variety of known shortcomings of machine learning, ranging from imbalanced classes to medical diagnostic error to reinforcement of social bias. We create scenarios that emulate those issues using the MNIST data set and demonstrate empirical results of our new loss function. Finally, we discuss our intuition about why this approach works and sketch a proof based on Maximum Likelihood Estimation.","['Training', 'Neural networks', 'Machine learning', 'Probabilistic logic', 'Measurement', 'Maximum likelihood estimation', 'Standards']","['Machine learning', 'class imbalance', 'oversampling', 'undersampling', 'ethnic stereotypes', 'social bias', 'maximum likelihood estimation', 'cross-entropy', 'softmax']"
"A hybrid antenna is proposed for future 4G/5G multiple input multiple output (MIMO) applications. The proposed antenna is composed of two antenna modules, namely, 4G antenna module and 5G antenna module. The 4G antenna module is a two-antenna array capable of covering the GSM850/900/1800/1900, UMTS2100, and LTE2300/2500 operating bands, while the 5G antenna module is an eight-antenna array operating in the 3.5-GHz band capable of covering the C-band (3400-3600 MHz), which could meet the demand of future 5G application. Compared with ideal uncorrelated antennas in an 8 × 8 MIMO system, the 5G antenna module has shown good ergodic channel capacity of ~40 b/s/Hz, which is only 6 b/s/Hz lower than ideal case. This multi-mode hybrid antenna is fabricated, and typically, experimental results such as S-parameter, antenna efficiency, radiation pattern, and envelope correlation coefficient are presented.","['MIMO', 'Channel capacity', '5G mobile communication', 'Smart phones', 'Antenna arrays', 'Antenna radiation', 'Channel capacity', 'Envelope correlation coefficient', 'Radiation patterns']","['MIMO', 'multi-antenna', 'multi-mode', 'channel capacity', '5G application']"
"Clustering is a fundamental problem in many data-driven application domains, and clustering performance highly depends on the quality of data representation. Hence, linear or non-linear feature transformations have been extensively used to learn a better data representation for clustering. In recent years, a lot of works focused on using deep neural networks to learn a clustering-friendly representation, resulting in a significant increase of clustering performance. In this paper, we give a systematic survey of clustering with deep learning in views of architecture. Specifically, we first introduce the preliminary knowledge for better understanding of this field. Then, a taxonomy of clustering with deep learning is proposed and some representative methods are introduced. Finally, we propose some interesting future opportunities of clustering with deep learning and give some conclusion remarks.","['Clustering methods', 'Machine learning', 'Clustering algorithms', 'Network architecture', 'Neural networks', 'Neurons', 'Gallium nitride']","['Clustering', 'deep learning', 'data representation', 'network architecture']"
"This paper aims to understand, identify, and mitigate the impacts of residential electric vehicle (EV) charging on distribution system voltages. A thorough literature review on the impacts of residential EV charging is presented, followed by a proposed method for evaluating the impacts of EV loads on the distribution system voltage quality. Practical solutions to mitigate EV load impacts are discussed as well, including infrastructural changes and indirect controlled charging with time-of-use (TOU) pricing. An optimal TOU schedule is also presented, with the aim of maximizing both customer and utility benefits. This paper also presents a discussion on implementing smart charging algorithms to directly control EV charging rates and EV charging starting times. Finally, a controlled charging algorithm is proposed to improve the voltage quality at the EV load locations while avoiding customer inconvenience. The proposed method significantly decreases the impacts of EV load charging on system peak load demand and feeder voltages.","['IEEE Standards', 'Electric vehicles', 'Pricing', 'Power system quality', 'Voltage control', 'Dynamic programming', 'Controlled charging']","['Electric vehicle charging', 'TOU pricing', 'controlled charging', 'power quality', 'voltage quality', 'distribution system', 'dynamic programming']"
"A binary version of the hybrid grey wolf optimization (GWO) and particle swarm optimization (PSO) is proposed to solve feature selection problems in this paper. The original PSOGWO is a new hybrid optimization algorithm that benefits from the strengths of both GWO and PSO. Despite the superior performance, the original hybrid approach is appropriate for problems with a continuous search space. Feature selection, however, is a binary problem. Therefore, a binary version of hybrid PSOGWO called BGWOPSO is proposed to find the best feature subset. To find the best solutions, the wrapper-based method K-nearest neighbors classifier with Euclidean separation matric is utilized. For performance evaluation of the proposed binary algorithm, 18 standard benchmark datasets from UCI repository are employed. The results show that BGWOPSO significantly outperformed the binary GWO (BGWO), the binary PSO, the binary genetic algorithm, and the whale optimization algorithm with simulated annealing when using several performance measures including accuracy, selecting the best optimal features, and the computational time.","['Feature extraction', 'Optimization', 'Classification algorithms', 'Particle swarm optimization', 'Search problems', 'Genetic algorithms', 'Data mining']","['Feature selection', 'hybrid binary optimization', 'grey wolf optimization', 'particle swarm optimization', 'classification']"
"Detecting COVID-19 early may help in devising an appropriate treatment plan and disease containment decisions. In this study, we demonstrate how transfer learning from deep learning models can be used to perform COVID-19 detection using images from three most commonly used medical imaging modes X-Ray, Ultrasound, and CT scan. The aim is to provide over-stressed medical professionals a second pair of eyes through intelligent deep learning image classification models. We identify a suitable Convolutional Neural Network (CNN) model through initial comparative study of several popular CNN models. We then optimize the selected VGG19 model for the image modalities to show how the models can be used for the highly scarce and challenging COVID-19 datasets. We highlight the challenges (including dataset size and quality) in utilizing current publicly available COVID-19 datasets for developing useful deep learning models and how it adversely impacts the trainability of complex models. We also propose an image pre-processing stage to create a trustworthy image dataset for developing and testing the deep learning models. The new approach is aimed to reduce unwanted noise from the images so that deep learning models can focus on detecting diseases with specific features from them. Our results indicate that Ultrasound images provide superior detection accuracy compared to X-Ray and CT scans. The experimental results highlight that with limited data, most of the deeper networks struggle to train well and provides less consistency over the three imaging modes we are using. The selected VGG19 model, which is then extensively tuned with appropriate parameters, performs in considerable levels of COVID-19 detection against pneumonia or normal for all three lung image modes with the precision of up to 86% for X-Ray, 100% for Ultrasound and 84% for CT scans.","['Computed tomography', 'Lung', 'X-ray imaging', 'Ultrasonic imaging', 'Machine learning', 'Diseases']","['COVID-19 detection', 'image processing', 'model comparison', 'CNN models', 'X-ray', 'ultrasound and CT based detection']"
"In this survey paper, we systematically summarize existing literature on bearing fault diagnostics with deep learning (DL) algorithms. While conventional machine learning (ML) methods, including artificial neural network, principal component analysis, support vector machines, etc., have been successfully applied to the detection and categorization of bearing faults for decades, recent developments in DL algorithms in the last five years have sparked renewed interest in both industry and academia for intelligent machine health monitoring. In this paper, we first provide a brief review of conventional ML methods, before taking a deep dive into the state-of-the-art DL algorithms for bearing fault applications. Specifically, the superiority of DL based methods are analyzed in terms of fault feature extraction and classification performances; many new functionalities enabled by DL techniques are also summarized. In addition, to obtain a more intuitive insight, a comparative study is conducted on the classification accuracy of different algorithms utilizing the open source Case Western Reserve University (CWRU) bearing dataset. Finally, to facilitate the transition on applying various DL algorithms to bearing fault diagnostics, detailed recommendations and suggestions are provided for specific application conditions. Future research directions to further enhance the performance of DL algorithms on health monitoring are also discussed.","['Induction motors', 'Vibrations', 'Sensors', 'Stators', 'Fault diagnosis', 'Machine learning algorithms', 'Monitoring']","['Bearing fault', 'deep learning', 'diagnostics', 'feature extraction', 'machine learning']"
"Various new national advanced manufacturing strategies, such as Industry 4.0, Industrial Internet, and Made in China 2025, are issued to achieve smart manufacturing, resulting in the increasing number of newly designed production lines in both developed and developing countries. Under the individualized designing demands, more realistic virtual models mirroring the real worlds of production lines are essential to bridge the gap between design and operation. This paper presents a digital twin-based approach for rapid individualized designing of the hollow glass production line. The digital twin merges physics-based system modeling and distributed real-time process data to generate an authoritative digital design of the system at pre-production phase. A digital twin-based analytical decoupling framework is also developed to provide engineering analysis capabilities and support the decision-making over the system designing and solution evaluation. Three key enabling techniques as well as a case study in hollow glass production line are addressed to validate the proposed approach.","['Optimization', 'Data models', 'Couplings', 'Glass', 'Conferences']","['Digital twin', 'individualized designing', 'mass individualization', 'multi-view synchronization', 'semi-physical simulation']"
"In this paper, we introduce an intelligent reflecting surface (IRS) to provide a programmable wireless environment for physical layer security. By adjusting the reflecting coefficients, the IRS can change the attenuation and scattering of the incident electromagnetic wave so that it can propagate in the desired way toward the intended receiver. Specifically, we consider a downlink multiple-input single-output (MISO) broadcast system, where the base station (BS) transmits independent data streams to multiple legitimate receivers and keeps them secret from multiple eavesdroppers. By jointly optimizing the beamformers at the BS and reflecting coefficients at the IRS, we formulate a minimum-secrecy-rate maximization problem under various practical constraints on the reflecting coefficients. The constraints capture the scenarios of both continuous and discrete reflecting coefficients of the reflecting elements. Due to the non-convexity of the formulated problem, we propose an efficient algorithm based on the alternating optimization and the path-following algorithm to solve it in an iterative manner. Besides, we show that the proposed algorithm can converge to a local (global) optimum. Furthermore, we develop two suboptimal algorithms with some forms of closed-form solutions to reduce computational complexity. Finally, the simulation results validate the advantages of the introduced IRS and the effectiveness of the proposed algorithms.","['Receivers', 'Signal to noise ratio', 'Wireless communication', 'MISO communication', 'Communication system security', 'Downlink', 'Optimization']","['Intelligent reflecting surface', 'programmable wireless environment', 'physical layer security', 'beamforming']"
"This paper considers a downlink cloud radio access network (C-RAN) in which all the base-stations (BSs) are connected to a central computing cloud via digital backhaul links with finite capacities. Each user is associated with a user-centric cluster of BSs; the central processor shares the user's data with the BSs in the cluster, which then cooperatively serve the user through joint beamforming. Under this setup, this paper investigates the user scheduling, BS clustering, and beamforming design problem from a network utility maximization perspective. Differing from previous works, this paper explicitly considers the per-BS backhaul capacity constraints. We formulate the network utility maximization problem for the downlink C-RAN under two different models depending on whether the BS clustering for each user is dynamic or static over different user scheduling time slots. In the former case, the user-centric BS cluster is dynamically optimized for each scheduled user along with the beamforming vector in each time-frequency slot, whereas in the latter case, the user-centric BS cluster is fixed for each user and we jointly optimize the user scheduling and the beamforming vector to account for the backhaul constraints. In both cases, the nonconvex per-BS backhaul constraints are approximated using the reweighted ℓ 1 -norm technique. This approximation allows us to reformulate the per-BS backhaul constraints into weighted per-BS power constraints and solve the weighted sum rate maximization problem through a generalized weighted minimum mean square error approach. This paper shows that the proposed dynamic clustering algorithm can achieve significant performance gain over existing naive clustering schemes. This paper also proposes two heuristic static clustering schemes that can already achieve a substantial portion of the gain.","['Array signal processing', 'Dynamic scheduling', 'Radio access networks', 'Downlink', 'Heuristic algorithms', 'Approximation methods', 'Cloud computing']","['Cloud radio access network (C-RAN)', 'network multiple-input multiple-output (MIMO)', 'multi-point (CoMP)', 'limited backhaul', 'user scheduling', 'base station clustering', 'beamforming', 'weighted sum rate', 'weighted minimum mean square error (WMMSE)']"
"The classification of electrocardiogram (ECG) signals is very important for the automatic diagnosis of heart disease. Traditionally, it is divided into two steps, including the step of feature extraction and the step of pattern classification. Owing to recent advances in artificial intelligence, it has been demonstrated that deep neural network, which trained on a huge amount of data, can carry out the task of feature extraction directly from the data and recognize cardiac arrhythmias better than professional cardiologists. This paper proposes an ECG arrhythmia classification method using two-dimensional (2D) deep convolutional neural network (CNN). The time domain signals of ECG, belonging to five heart beat types including normal beat (NOR), left bundle branch block beat (LBB), right bundle branch block beat (RBB), premature ventricular contraction beat (PVC), and atrial premature contraction beat (APC), were first transformed into time-frequency spectrograms by short-time Fourier transform. Subsequently, the spectrograms of the five arrhythmia types were utilized as input to the 2D-CNN such that the ECG arrhythmia types were identified and classified finally. Using ECG recordings from the MIT-BIH arrhythmia database as the training and testing data, the classification results show that the proposed 2D-CNN model can reach an averaged accuracy of 99.00%. On the other hand, in order to achieve optimal classification performances, the model parameter optimization was investigated. It was found when the learning rate is 0.001 and the batch size parameter is 2500, the classifier achieved the highest accuracy and the lowest loss. We also compared the proposed 2D-CNN model with a conventional one-dimensional CNN model. Comparison results show that the 1D-CNN classifier can achieve an averaged accuracy of 90.93%. Therefore, it is validated that the proposed CNN classifier using ECG spectrograms as input can achieve improved classification accuracy without additional manual pre-processing of the ECG signals.","['Electrocardiography', 'Feature extraction', 'Spectrogram', 'Convolutional neural networks', 'Heart beat', 'Diseases', 'Time-frequency analysis']","['Electrocardiogram (ECG)', 'arrhythmia classification', 'short-time Fourier transform (STFT)', 'convolutional neural network', 'model parameter optimization']"
"Prognostics and health management is an emerging discipline to scientifically manage the health condition of engineering systems and their critical components. It mainly consists of three main aspects: construction of health indicators, remaining useful life prediction, and health management. Construction of health indicators aims to evaluate the system's current health condition and its critical components. Given the observations of a health indicator, prediction of the remaining useful life is used to infer the time when an engineering systems or a critical component will no longer perform its intended function. Health management involves planning the optimal maintenance schedule according to the system's current and future health condition, its critical components and the replacement costs. Construction of health indicators is the key to predicting the remaining useful life. Bearings and gears are the most common mechanical components in rotating machines, and their health conditions are of great concern in practice. Because it is difficult to measure and quantify the health conditions of bearings and gears in many cases, numerous vibration-based methods have been proposed to construct bearing and gear health indicators. This paper presents a thorough review of vibration-based bearing and gear health indicators constructed from mechanical signal processing, modeling, and machine learning. This review paper will be helpful for designing further advanced bearing and gear health indicators and provides a basis for predicting the remaining useful life of bearings and gears. Most of the bearing and gear health indicators reviewed in this paper are highly relevant to simulated and experimental run-to-failure data rather than artificially seeded bearing and gear fault data. Finally, some problems in the literature are highlighted and areas for future study are identified.","['Gears', 'Vibrations', 'Degradation', 'Predictive models', 'Root mean square', 'Prognostics and health management', 'Signal processing']","['Ball bearings', 'condition monitoring', 'feature extraction', 'gears', 'prognostics and health management', 'signal processing algorithms', 'vibrations']"
"With the rapid development of Internet technology and social networks, a large number of comment texts are generated on the Web. In the era of big data, mining the emotional tendency of comments through artificial intelligence technology is helpful for the timely understanding of network public opinion. The technology of sentiment analysis is a part of artificial intelligence, and its research is very meaningful for obtaining the sentiment trend of the comments. The essence of sentiment analysis is the text classification task, and different words have different contributions to classification. In the current sentiment analysis studies, distributed word representation is mostly used. However, distributed word representation only considers the semantic information of word, but ignore the sentiment information of the word. In this paper, an improved word representation method is proposed, which integrates the contribution of sentiment information into the traditional TF-IDF algorithm and generates weighted word vectors. The weighted word vectors are input into bidirectional long short term memory (BiLSTM) to capture the context information effectively, and the comment vectors are better represented. The sentiment tendency of the comment is obtained by feedforward neural network classifier. Under the same conditions, the proposed sentiment analysis method is compared with the sentiment analysis methods of RNN, CNN, LSTM, and NB. The experimental results show that the proposed sentiment analysis method has higher precision, recall, and F1 score. The method is proved to be effective with high accuracy on comments.","['Sentiment analysis', 'Dictionaries', 'Neural networks', 'Semantics', 'Data mining', 'Internet', 'Task analysis']","['Sentiment analysis', 'artificial intelligence', 'social network', 'weighted word vectors', 'BiLSTM']"
"While 5G is being commercialized worldwide, research institutions around the world have started to look beyond 5G and 6G is expected to evolve into green networks, which deliver high Quality of Service and energy efficiency. To meet the demands of future applications, significant improvements need to be made in mobile network architecture. We envision 6G undergoing unprecedented breakthrough and integrating traditional terrestrial mobile networks with emerging space, aerial and underwater networks to provide anytime anywhere network access. This paper presents a detailed survey on wireless evolution towards 6G networks. In this survey, the prime focus is on the new architectural changes associated with 6G networks, characterized by ubiquitous 3D coverage, introduction of pervasive AI and enhanced network protocol stack. Along with this, we discuss related potential technologies that are helpful in forming sustainable and socially seamless networks, encompassing terahertz and visible light communication, new communication paradigm, blockchain and symbiotic radio. Our work aims to provide enlightening guidance for subsequent research of green 6G.","['5G mobile communication', 'Satellite broadcasting', 'Low earth orbit satellites', 'Wireless networks']","['6G', 'architecture', 'green networks', 'VLC', 'blockchain']"
"Network intrusion detection systems (NIDSs) provide a better solution to network security than other traditional network defense technologies, such as firewall systems. The success of NIDS is highly dependent on the performance of the algorithms and improvement methods used to increase the classification accuracy and decrease the training and testing times of the algorithms. We propose an effective deep learning approach, self-taught learning (STL)-IDS, based on the STL framework. The proposed approach is used for feature learning and dimensionality reduction. It reduces training and testing time considerably and effectively improves the prediction accuracy of support vector machines (SVM) with regard to attacks. The proposed model is built using the sparse autoencoder mechanism, which is an effective learning algorithm for reconstructing a new feature representation in an unsupervised manner. After the pre-training stage, the new features are fed into the SVM algorithm to improve its detection capability for intrusion and classification accuracy. Moreover, the efficiency of the approach in binary and multiclass classification is studied and compared with that of shallow classification methods, such as J48, naive Bayesian, random forest, and SVM. Results show that our approach has accelerated SVM training and testing times and performed better than most of the previous approaches in terms of performance metrics in binary and multiclass classification. The proposed STL-IDS approach improves network intrusion detection and provides a new research method for intrusion detection.","['Support vector machines', 'Machine learning', 'Intrusion detection', 'Feature extraction', 'Training', 'Computational modeling', 'Classification algorithms']","['Network security', 'network intrusion detection system', 'deep learning', 'sparse autoencoder', 'SVM', 'self-taught learning', 'NSL-KDD']"
"A centralized infrastructure system carries out existing data analytics and decision-making processes from our current highly virtualized platform of wireless networks and the Internet of Things (IoT) applications. There is a high possibility that these existing methods will encounter more challenges and issues in relation to network dynamics, resulting in a high overhead in the network response time, leading to latency and traffic. In order to avoid these problems in the network and achieve an optimum level of resource utilization, a new paradigm called edge computing (EC) is proposed to pave the way for the evolution of new age applications and services. With the integration of EC, the processing capabilities are pushed to the edge of network devices such as smart phones, sensor nodes, wearables, and on-board units, where data analytics and knowledge generation are performed which removes the necessity for a centralized system. Many IoT applications, such as smart cities, the smart grid, smart traffic lights, and smart vehicles, are rapidly upgrading their applications with EC, significantly improving response time as well as conserving network resources. Irrespective of the fact that EC shifts the workload from a centralized cloud to the edge, the analogy between EC and the cloud pertaining to factors such as resource management and computation optimization are still open to research studies. Hence, this paper aims to validate the efficiency and resourcefulness of EC. We extensively survey the edge systems and present a comparative study of cloud computing systems. After analyzing the different network properties in the system, the results show that EC systems perform better than cloud computing systems. Finally, the research challenges in implementing an EC system and future research directions are discussed.","['Cloud computing', 'Computer architecture', 'Edge computing', 'Ad hoc networks', 'Servers', 'Mobile computing', 'Resource management']","['IoT', 'cloud computing', 'edge computing', 'fog computing', 'multi-cloud']"
"The next-generation wireless networks are evolving into very complex systems because of the very diversified service requirements, heterogeneity in applications, devices, and networks. The network operators need to make the best use of the available resources, for example, power, spectrum, as well as infrastructures. Traditional networking approaches, i.e., reactive, centrally-managed, one-size-fits-all approaches, and conventional data analysis tools that have limited capability (space and time) are not competent anymore and cannot satisfy and serve that future complex networks regarding operation and optimization cost effectively. A novel paradigm of proactive, self-aware, self-adaptive, and predictive networking is much needed. The network operators have access to large amounts of data, especially from the network and the subscribers. Systematic exploitation of the big data dramatically helps in making the system smart, intelligent, and facilitates efficient as well as cost-effective operation and optimization. We envision data-driven next-generation wireless networks, where the network operators employ advanced data analytics, machine learning (ML), and artificial intelligence. We discuss the data sources and strong drivers for the adoption of the data analytics, and the role of ML, artificial intelligence in making the system intelligent regarding being self-aware, self-adaptive, proactive and prescriptive. A set of network design and optimization schemes are presented concerning data analytics. This paper concludes with a discussion of challenges and the benefits of adopting big data analytics, ML, and artificial intelligence in the next-generation communication systems.","['Big Data', 'Next generation networking', 'Optimization', 'Machine learning', 'Data analysis', 'Tools', 'Wireless communication']","['Big data analytics', 'machine learning', 'artificial intelligence', 'next-generation wireless']"
"Clean and environment-friendly energy harvesting are of prime interest today as it is one of the key enablers in achieving the Sustainable Development Goals (SDGs) as well as accelerates social progress and enhances living standards. India, the second-most populous nation with a population of 1.353 billion, is one of the largest consumers of fossil fuels in the world which is responsible for global warming. An ever-increasing population is projected until 2050, and consequently, the energy demand in the upcoming decades will be co-accelerated by the rapid industrial growth. The Ministry of New and Renewable Energy (MNRE) with the support of National Institution for Transforming India (NITI) Aayog is working to achieve the Indian Government's target of attaining 175 GW through renewable energy resources. Many Indian states are currently increasing their renewable energy capacity in an objective to meet future energy demand. The review paper discusses in-depth about the three Indian states, namely Karnataka, Gujarat, Tamil Nadu, which pioneers the renewable energy production in India. The global energy scenario was discussed in detail with Indian contrast. Further, the barriers to the development of renewable energy generation and policies of the Indian government are discussed in detail to promote renewable energy generation throughout India as well as globally since the challenges are similar for other nations. This study analyzed various prospects of the country in renewable energy which has been done in a purpose to help the scholars, researchers, and policymakers of the nation, as it gives an insight into the present renewable energy scenario of the country.","['Renewable energy sources', 'Production', 'Global warming', 'Fossil fuels', 'Meteorology', 'Electric potential', 'Sociology']","['Renewable energy potential', 'global energy scenario', 'Energy policy in India', 'renewable energy barriers', 'prospects of renewables in India', 'renewable energy in India']"
"Traditional power grids are being transformed into smart grids (SGs) to address the issues in the existing power system due to uni-directional information flow, energy wastage, growing energy demand, reliability, and security. SGs offer bi-directional energy flow between service providers and consumers, involving power generation, transmission, distribution, and utilization systems. SGs employ various devices for the monitoring, analysis, and control of the grid, deployed at power plants, distribution centers, and in consumers’ premises in a very large number. Hence, an SG requires connectivity, automation, and the tracking of such devices. This is achieved with the help of the Internet of Things (IoT). The IoT helps SG systems to support various network functions throughout the generation, transmission, distribution, and consumption of energy by incorporating the IoT devices (such as sensors, actuators, and smart meters), as well as by providing the connectivity, automation, and tracking for such devices. In this paper, we provide a comprehensive survey on the IoT-aided SG systems, which includes the existing architectures, applications, and prototypes of the IoT-aided SG systems. This survey also highlights the open issues, challenges, and future research directions for the IoT-aided SG systems.","['Internet of Things', 'Security', 'Power generation', 'Power grids', 'Monitoring']","['Home area network (HAN)', 'Internet of Things (IoT)', 'neighborhood area network (NAN)', 'smart grid (SG)', 'wide area network (WAN)']"
"In this paper, we investigate the feasibility of recognizing human hand gestures using micro-Doppler signatures measured by Doppler radar with a deep convolutional neural network (DCNN). Hand gesture recognition using radar can be applied to control electronic appliances. Compared with an optical recognition system, radar can work regardless of light conditions and it can be embedded in a case. We classify ten different hand gestures, with only micro-Doppler signatures on spectrograms without range information. The ten gestures, which included swiping from left to right, swiping from right to left, rotating clockwise, rotating counterclockwise, pushing, double pushing, holding, and double holding, were measured using Doppler radar and their spectrograms investigated. A DCNN was employed to classify the spectrograms, with 90% of the data utilized for training and the remaining 10% for validation. After five-fold validation, the classification accuracy of the proposed method was found to be 85.6%. With seven gestures, the accuracy increased to 93.1%.","['Doppler radar', 'Spectrogram', 'Laser radar', 'Neural networks', 'Gesture recognition', 'Spectrograms']","['Hand gesture', 'micro-Doppler signatures', 'Doppler radar', 'deep convolutional neural networks']"
"In recent decades, we have witnessed the evolution of information technologies from the development of VLSI technologies, to communication and networking infrastructure, to the standardization of multimedia compression and coding schemes, to effective multimedia content search and retrieval. As a result, multimedia devices and digital content have become ubiquitous. This path of technological evolution has naturally led to a critical issue that must be addressed next, namely, to ensure that content, devices, and intellectual property are being used by authorized users for legitimate purposes, and to be able to forensically prove with high confidence when otherwise. When security is compromised, intellectual rights are violated, or authenticity is forged, forensic methodologies and tools are employed to reconstruct what has happened to digital content in order to answer who has done what, when, where, and how. The goal of this paper is to provide an overview on what has been done over the last decade in the new and emerging field of information forensics regarding theories, methodologies, state-of-the-art techniques, major applications, and to provide an outlook of the future.","['Forensics', 'Forgery', 'Multimedia communication', 'Feature extraction', 'Fingerprint recognition', 'Computer Security', 'Information processing', 'Digital forensics', 'Very large scale integration', 'Data privacy', 'Intellectual property']","['Anti-forensics', 'information forensics', 'multimedia fingerprint', 'tampering detection']"
"Diabetic Retinopathy (DR) is an ophthalmic disease that damages retinal blood vessels. DR causes impaired vision and may even lead to blindness if it is not diagnosed in early stages. DR has five stages or classes, namely normal, mild, moderate, severe and PDR (Proliferative Diabetic Retinopathy). Normally, highly trained experts examine the colored fundus images to diagnose this fatal disease. This manual diagnosis of this condition (by clinicians) is tedious and error-prone. Therefore, various computer vision-based techniques have been proposed to automatically detect DR and its different stages from retina images. However, these methods are unable to encode the underlying complicated features and can only classify DR’s different stages with very low accuracy particularly, for the early stages. In this research, we used the publicly available Kaggle dataset of retina images to train an ensemble of five deep Convolution Neural Network (CNN) models (Resnet50, Inceptionv3, Xception, Dense121, Dense169) to encode the rich features and improve the classification for different stages of DR. The experimental results show that the proposed model detects all the stages of DR unlike the current methods and performs better compared to state-of-the-art methods on the same Kaggle dataset.","['Diabetes', 'Retinopathy', 'Training', 'Predictive models', 'Retina', 'Biomedical imaging']","['CNN', 'diabetic retinopathy', 'deep learning', 'ensemble model', 'fundus images', 'medical image analysis']"
"Emotion recognition from speech signals is an important but challenging component of Human-Computer Interaction (HCI). In the literature of speech emotion recognition (SER), many techniques have been utilized to extract emotions from signals, including many well-established speech analysis and classification techniques. Deep Learning techniques have been recently proposed as an alternative to traditional techniques in SER. This paper presents an overview of Deep Learning techniques and discusses some recent literature where these methods are utilized for speech-based emotion recognition. The review covers databases used, emotions extracted, contributions made toward speech emotion recognition and limitations related to it.","['Databases', 'Emotion recognition', 'Feature extraction', 'Speech recognition', 'Deep learning', 'Human computer interaction', 'Hidden Markov models']","['Speech emotion recognition', 'deep learning', 'deep neural network', 'deep Boltzmann machine', 'recurrent neural network', 'deep belief network', 'convolutional neural network']"
"The main vision of the Internet of Things (IoT) is to equip real-life physical objects with computing and communication power so that they can interact with each other for the social good. As one of the key members of IoT, Internet of Vehicles (IoV) has seen steep advancement in communication technologies. Now, vehicles can easily exchange safety, efficiency, infotainment, and comfort-related information with other vehicles and infrastructures using vehicular ad hoc networks (VANETs). We leverage on the cloud-based VANETs theme to propose cyber-physical architecture for the Social IoV (SIoV). SIoV is a vehicular instance of the Social IoT (SIoT), where vehicles are the key social entities in the machine-to-machine vehicular social networks. We have identified the social structures of SIoV components, their relationships, and the interaction types. We have mapped VANETs components into IoT-A architecture reference model to offer better integration of SIoV with other IoT domains. We also present a communication message structure based on automotive ontologies, the SAE J2735 message set, and the advanced traveler information system events schema that corresponds to the social graph. Finally, we provide the implementation details and the experimental analysis to demonstrate the efficacy of the proposed system as well as include different application scenarios for various user groups.","['Internet of things', 'Computer architecture', 'Ad hoc networks', 'Social network services', 'Vehicles', 'Intelligent vehicles']","['Social Network of Vehicles', 'Cyber-Physical Systems', 'Internet of Things', 'Internet of Vehicles', 'IoT Architecture Reference Model', 'Intelligent Transport Systems', 'SAE J2735']"
"Key generation from the randomness of wireless channels is a promising alternative to public key cryptography for the establishment of cryptographic keys between any two users. This paper reviews the current techniques for wireless key generation. The principles, performance metrics and key generation procedure are comprehensively surveyed. Methods for optimizing the performance of key generation are also discussed. Key generation applications in various environments are then introduced along with the challenges of applying the approach in each scenario. The paper concludes with some suggestions for future studies.","['Random processes', 'Physical layer', 'Network security', 'Wireless communication', 'Key generation', 'Cryptography']","['Physical layer security', 'key generation', 'wireless communication']"
"Electronic Health Records (EHRs) are entirely controlled by hospitals instead of patients, which complicates seeking medical advices from different hospitals. Patients face a critical need to focus on the details of their own healthcare and restore management of their own medical data. The rapid development of blockchain technology promotes population healthcare, including medical records as well as patient-related data. This technology provides patients with comprehensive, immutable records, and access to EHRs free from service providers and treatment websites. In this paper, to guarantee the validity of EHRs encapsulated in blockchain, we present an attribute-based signature scheme with multiple authorities, in which a patient endorses a message according to the attribute while disclosing no information other than the evidence that he has attested to it. Furthermore, there are multiple authorities without a trusted single or central one to generate and distribute public/private keys of the patient, which avoids the escrow problem and conforms to the mode of distributed data storage in the blockchain. By sharing the secret pseudorandom function seeds among authorities, this protocol resists collusion attack out of N from N -1 corrupted authorities. Under the assumption of the computational bilinear Diffie-Hellman, we also formally demonstrate that, in terms of the unforgeability and perfect privacy of the attribute-signer, this attribute-based signature scheme is secure in the random oracle model. The comparison shows the efficiency and properties between the proposed method and methods proposed in other studies.","['Privacy', 'Security', 'Hospitals', 'Electronic medical records', 'Standards']","['Attribute-based signature (ABS)', 'blockchain', 'electronic health records (EHRs)', 'multiple authorities', 'preserve privacy']"
"Twitter sentiment analysis technology provides the methods to survey public emotion about the events or products related to them. Most of the current researches are focusing on obtaining sentiment features by analyzing lexical and syntactic features. These features are expressed explicitly through sentiment words, emoticons, exclamation marks, and so on. In this paper, we introduce a word embeddings method obtained by unsupervised learning based on large twitter corpora, this method using latent contextual semantic relationships and co-occurrence statistical characteristics between words in tweets. These word embeddings are combined with n-grams features and word sentiment polarity score features to form a sentiment feature set of tweets. The feature set is integrated into a deep convolution neural network for training and predicting sentiment classification labels. We experimentally compare the performance of our model with the baseline model that is a word n-grams model on five Twitter data sets, the results indicate that our model performs better on the accuracy and F1-measure for twitter sentiment classification.","['Twitter', 'Neural networks', 'Convolution', 'Sentiment analysis', 'Terminology', 'Semantics']","['Twitter', 'sentiment analysis', 'word embeddings', 'convolution neural network']"
"Presently, educational institutions compile and store huge volumes of data, such as student enrolment and attendance records, as well as their examination results. Mining such data yields stimulating information that serves its handlers well. Rapid growth in educational data points to the fact that distilling massive amounts of data requires a more sophisticated set of algorithms. This issue led to the emergence of the field of educational data mining (EDM). Traditional data mining algorithms cannot be directly applied to educational problems, as they may have a specific objective and function. This implies that a preprocessing algorithm has to be enforced first and only then some specific data mining methods can be applied to the problems. One such preprocessing algorithm in EDM is clustering. Many studies on EDM have focused on the application of various data mining algorithms to educational attributes. Therefore, this paper provides over three decades long (1983-2016) systematic literature review on clustering algorithm and its applicability and usability in the context of EDM. Future insights are outlined based on the literature reviewed, and avenues for further research are identified.","['Clustering algorithms', 'Data mining', 'Partitioning algorithms', 'Classification algorithms', 'Clustering methods', 'Systematics', 'Big data']","['Data mining', 'clustering methods', 'educational technology', 'systematic review']"
"Blockchain have been an interesting research area for a long time and the benefits it provides have been used by a number of various industries. Similarly, the healthcare sector stands to benefit immensely from the blockchain technology due to security, privacy, confidentiality and decentralization. Nevertheless, the Electronic Health Record (EHR) systems face problems regarding data security, integrity and management. In this paper, we discuss how the blockchain technology can be used to transform the EHR systems and could be a solution of these issues. We present a framework that could be used for the implementation of blockchain technology in healthcare sector for EHR. The aim of our proposed framework is firstly to implement blockchain technology for EHR and secondly to provide secure storage of electronic records by defining granular access rules for the users of the proposed framework. Moreover, this framework also discusses the scalability problem faced by the blockchain technology in general via use of off-chain storage of the records. This framework provides the EHR system with the benefits of having a scalable, secure and integral blockchain-based solution.","['Blockchain', 'Peer-to-peer computing', 'Hospitals', 'Cryptography', 'Electronic medical records']","['Blockchain', 'health records', 'electronic health records', 'decentralization', 'and scalability']"
"The Internet of Things (IoT) is set to become one of the key technological developments of our times provided we are able to realize its full potential. The number of objects connected to IoT is expected to reach 50 billion by 2020 due to the massive influx of diverse objects emerging progressively. IoT, hence, is expected to be a major producer of big data. Sharing and collaboration of data and other resources would be the key for enabling sustainable ubiquitous environments, such as smart cities and societies. A timely fusion and analysis of big data, acquired from IoT and other sources, to enable highly efficient, reliable, and accurate decision making and management of ubiquitous environments would be a grand future challenge. Computational intelligence would play a key role in this challenge. A number of surveys exist on data fusion. However, these are mainly focused on specific application areas or classifications. The aim of this paper is to review literature on data fusion for IoT with a particular focus on mathematical methods (including probabilistic methods, artificial intelligence, and theory of belief) and specific IoT environments (distributed, heterogeneous, nonlinear, and object tracking environments). The opportunities and challenges for each of the mathematical methods and environments are given. Future developments, including emerging areas that would intrinsically benefit from data fusion and IoT, autonomous vehicles, deep learning for data fusion, and smart cities, are discussed.","['Data integration', 'Smart cities', 'Internet of Things', 'Big Data', 'Decision making', 'Computer science', 'Information technology']","['Internet of Things', 'big data', 'data fusion', 'computational and artificial intelligence', 'high performance computing', 'smart cities', 'smart societies', 'ubiquitous environments']"
"With the explosive growth of Internet of Things devices and massive data produced at the edge of the network, the traditional centralized cloud computing model has come to a bottleneck due to the bandwidth limitation and resources constraint. Therefore, edge computing, which enables storing and processing data at the edge of the network, has emerged as a promising technology in recent years. However, the unique features of edge computing, such as content perception, real-time computing, and parallel processing, has also introduced several new challenges in the field of data security and privacy-preserving, which are also the key concerns of the other prevailing computing paradigms, such as cloud computing, mobile cloud computing, and fog computing. Despites its importance, there still lacks a survey on the recent research advance of data security and privacy-preserving in the field of edge computing. In this paper, we present a comprehensive analysis of the data security and privacy threats, protection technologies, and countermeasures inherent in edge computing. Specifically, we first make an overview of edge computing, including forming factors, definition, architecture, and several essential applications. Next, a detailed analysis of data security and privacy requirements, challenges, and mechanisms in edge computing are presented. Then, the cryptography-based technologies for solving data security and privacy issues are summarized. The state-of-the-art data security and privacy solutions in edge-related paradigms are also surveyed. Finally, we propose several open research directions of data security in the field of edge computing.","['Edge computing', 'Cloud computing', 'Data privacy', 'Encryption', 'Computer architecture', 'Authentication']","['Edge computing', 'data security', 'cryptography', 'authentication', 'access control', 'privacy']"
"The Internet of Things (IoT) is one of the most promising technologies for the near future. Healthcare and well-being will receive great benefits with the evolution of this technology. This paper presents a review of techniques based on IoT for healthcare and ambient-assisted living, defined as the Internet of Health Things (IoHT), based on the most recent publications and products available in the market from industry for this segment. Also, this paper identifies the technological advances made so far, analyzing the challenges to be overcome and provides an approach of future trends. Through selected works, it is possible to notice that further studies are important to improve current techniques and that novel concept and technologies of IoHT are needed to overcome the identified challenges. The presented results aim to serve as a source of information for healthcare providers, researchers, technology specialists, and the general population to improve the IoHT.","['Medical services', 'Monitoring', 'Biomedical monitoring', 'Industries', 'Sensors', 'Internet of Things']","['Ambient assisted living', 'Internet of Things', 'Internet of Health Things', 'mobile health', 'remote healthcare monitoring', 'wearable']"
"Industry 4.0 can make a factory smart by applying intelligent information processing approaches, communication systems, future-oriented techniques, and more. However, the high complexity, automation, and flexibility of an intelligent factory bring new challenges to reliability and safety. Industrial big data generated by multisource sensors, intercommunication within the system and external-related information, and so on, might provide new solutions for predictive maintenance to improve system reliability. This paper puts forth attributes of industrial big data processing and actively explores industrial big data processing-based predictive maintenance. A novel framework is proposed for structuring multisource heterogeneous information, characterizing structured data with consideration of the spatiotemporal property, and modeling invisible factors, which would make the production process transparent and eventually implement predictive maintenance on facilities and energy saving in the industry 4.0 era. The effectiveness of the proposed scheme was verified by analyzing multisource heterogeneous industrial data for the remaining life prediction of key components of machining equipment.","['Big Data', 'Predictive maintenance', 'Data mining', 'Manufacturing processes', 'Industries', 'Feature extraction', 'Data models']","['Industrial big data', 'multisource heterogeneous data', 'structuralization and characterization', 'multiple invisible factors', 'predictive maintenance']"
"The whale optimization algorithm (WOA) has been shown to be powerful in searching for an optimal solution. This paper proposes an improvement to the whale optimization algorithm that is based on a Lévy flight trajectory and called the Lévy flight trajectory-based whale optimization algorithm (LWOA). The LWOA makes the WOA faster and more robust and avoids premature convergence. The Lévy flight trajectory is helpful for increasing the diversity of the population against premature convergence and enhancing the capability of jumping out of local optimal optima. This method helps obtaining a better tradeoff between the exploration and exploitation of the WOA. The proposed algorithm is characterized by quick convergence and high precision, and it can effectively get rid of a local optimum. The LWOA is further compared with other well-known nature-inspired algorithms on 23 benchmarks and solving infinite impulse response model identification. The statistical results on the benchmark functions show that the LWOA can significantly outperform others on a majority of the benchmark functions, especially in solving an optimization problem that has high dimensionality. Additionally, the superior identification capability of the proposed algorithm is evident from the results obtained through the simulation study compared with other algorithms. All the results prove the superiority of the LWOA.","['Whales', 'Optimization', 'Trajectory', 'Mathematical model', 'Convergence', 'Algorithm design and analysis', 'Spirals']","['Lévy flight trajectory', 'whale optimization algorithm', 'global optimization', 'IIR system']"
"Even after 16 years of existence, low energy adaptive clustering hierarchy (LEACH) protocol is still gaining the attention of the research community working in the area of wireless sensor network (WSN). This itself shows the importance of this protocol. Researchers have come up with various and diverse modifications of the LEACH protocol. Successors of LEACH protocol are now available from single hop to multi-hop scenarios. Extensive work has already been done related to LEACH and it is a good idea for a new research in the field of WSN to go through LEACH and its variants over the years. This paper surveys the variants of LEACH routing protocols proposed so far and discusses the enhancement and working of them. This survey classifies all the protocols in two sections, namely, single hop communication and multi-hop communication based on data transmission from the cluster head to the base station. A comparitive analysis using nine different parameters, such as energy efficiency, overhead, scalability complexity, and so on, has been provided in a chronological fashion. The article also discusses the strong and the weak points of each and every variants of LEACH. Finally the paper concludes with suggestions on future research domains in the area of WSN.","['Wireless sensor networks', 'Routing protocols', 'Routing', 'Time division multiple access', 'Schedules', 'Scalability']","['LEACH', 'Wireless Sensor Network', 'Clustering Protocol', 'Cluster Head', 'Routing']"
"The purpose of this paper is to survey and assess the state-of-the-art in automatic target recognition for synthetic aperture radar imagery (SAR-ATR). The aim is not to develop an exhaustive survey of the voluminous literature, but rather to capture in one place the various approaches for implementing the SAR-ATR system. This paper is meant to be as self-contained as possible, and it approaches the SAR-ATR problem from a holistic end-to-end perspective. A brief overview for the breadth of the SAR-ATR challenges is conducted. This is couched in terms of a single-channel SAR, and it is extendable to multi-channel SAR systems. Stages pertinent to the basic SAR-ATR system structure are defined, and the motivations of the requirements and constraints on the system constituents are addressed. For each stage in the SAR-ATR processing chain, a taxonomization methodology for surveying the numerous methods published in the open literature is proposed. Carefully selected works from the literature are presented under the taxa proposed. Novel comparisons, discussions, and comments are pinpointed throughout this paper. A two-fold benchmarking scheme for evaluating existing SAR-ATR systems and motivating new system designs is proposed. The scheme is applied to the works surveyed in this paper. Finally, a discussion is presented in which various interrelated issues, such as standard operating conditions, extended operating conditions, and target-model design, are addressed. This paper is a contribution toward fulfilling an objective of end-to-end SAR-ATR system design.","['Synthetic aperture radar', 'Classification', 'Feature recognition', 'Target recognition', 'System analysis and design', 'Benchmark testing', 'Radar tracking']","['SAR', 'radar', 'target', 'classification', 'recognition', 'features', 'model']"
"In recent decades, we have witnessed the evolution of biometric technology from the first pioneering works in face and voice recognition to the current state of development wherein a wide spectrum of highly accurate systems may be found, ranging from largely deployed modalities, such as fingerprint, face, or iris, to more marginal ones, such as signature or hand. This path of technological evolution has naturally led to a critical issue that has only started to be addressed recently: the resistance of this rapidly emerging technology to external attacks and, in particular, to spoofing. Spoofing, referred to by the term presentation attack in current standards, is a purely biometric vulnerability that is not shared with other IT security solutions. It refers to the ability to fool a biometric system into recognizing an illegitimate user as a genuine one by means of presenting a synthetic forged version of the original biometric trait to the sensor. The entire biometric community, including researchers, developers, standardizing bodies, and vendors, has thrown itself into the challenging task of proposing and developing efficient protection methods against this threat. The goal of this paper is to provide a comprehensive overview on the work that has been carried out over the last decade in the emerging field of antispoofing, with special attention to the mature and largely deployed face modality. The work covers theories, methodologies, state-of-the-art techniques, and evaluation databases and also aims at providing an outlook into the future of this very active field of research.","['Biometrics', 'Distance measurement', 'Computer security', 'Fingerprint recognition', 'Iris recognition', 'Access control', 'Speech recognition', 'Immune system', 'Biomedical monitoring', 'Authentication']","['Biometrics', 'Security', 'Anti-Spoofing']"
"The increasing demand for electricity and the emergence of smart grids have presented new opportunities for a home energy management system (HEMS) that can reduce energy usage. The HEMS incorporates a demand response (DR) tool that shifts and curtails demand to improve home energy consumption. This system commonly creates optimal consumption schedules by considering several factors, such as energy costs, environmental concerns, load profiles, and consumer comfort. With the deployment of smart meters, performing load control using the HEMS with DR-enabled appliances has become possible. This paper provides a comprehensive review on previous and current research related to the HEMS by considering various DR programs, smart technologies, and load scheduling controllers. The application of artificial intelligence for load scheduling controllers, such as artificial neural network, fuzzy logic, and adaptive neural fuzzy inference system, is also reviewed. Heuristic optimization techniques, which are widely used for optimal scheduling of various electrical devices in a smart home, are also discussed.","['Home appliances', 'Energy management', 'Energy consumption', 'Optimization', 'Schedules', 'Water heating']","['Home energy management system', 'demand response', 'smart technologies', 'integrated wireless technology', 'intelligent scheduling controller']"
"Industrial systems always prefer to reduce their operational expenses. To support such reductions, they need solutions that are capable of providing stability, fault tolerance, and flexibility. One such solution for industrial systems is cyber physical system (CPS) integration with the Internet of Things (IoT) utilizing cloud computing services. These CPSs can be considered as smart industrial systems, with their most prevalent applications in smart transportation, smart grids, smart medical and eHealthcare systems, and many more. These industrial CPSs mostly utilize supervisory control and data acquisition (SCADA) systems to control and monitor their critical infrastructure (CI). For example, WebSCADA is an application used for smart medical technologies, making improved patient monitoring and more timely decisions possible. The focus of the study presented in this paper is to highlight the security challenges that the industrial SCADA systems face in an IoT-cloud environment. Classical SCADA systems are already lacking in proper security measures; however, with the integration of complex new architectures for the future Internet based on the concepts of IoT, cloud computing, mobile wireless sensor networks, and so on, there are large issues at stakes in the security and deployment of these classical systems. Therefore, the integration of these future Internet concepts needs more research effort. This paper, along with highlighting the security challenges of these CI's, also provides the existing best practices and recommendations for improving and maintaining security. Finally, this paper briefly describes future research directions to secure these critical CPSs and help the research community in identifying the research gaps in this regard.","['Cloud computing', 'Security', 'SCADA systems', 'Power system stability', 'Fault tolerant systems', 'Wireless sensor networks', 'Internet of things', 'Stability analysis']","['APT', 'Industrial Control System', 'Internet of Things (IoT)', 'NIST', 'PRECYSE', 'Supervisory Control and Data Acquisition', 'SOA']"
"Traditional distance and density-based anomaly detection techniques are unable to detect periodic and seasonality related point anomalies which occur commonly in streaming data, leaving a big gap in time series anomaly detection in the current era of the IoT. To address this problem, we present a novel deep learning-based anomaly detection approach (DeepAnT) for time series data, which is equally applicable to the non-streaming cases. DeepAnT is capable of detecting a wide range of anomalies, i.e., point anomalies, contextual anomalies, and discords in time series data. In contrast to the anomaly detection methods where anomalies are learned, DeepAnT uses unlabeled data to capture and learn the data distribution that is used to forecast the normal behavior of a time series. DeepAnT consists of two modules: time series predictor and anomaly detector. The time series predictor module uses deep convolutional neural network (CNN) to predict the next time stamp on the defined horizon. This module takes a window of time series (used as a context) and attempts to predict the next time stamp. The predicted value is then passed to the anomaly detector module, which is responsible for tagging the corresponding time stamp as normal or abnormal. DeepAnT can be trained even without removing the anomalies from the given data set. Generally, in deep learning-based approaches, a lot of data are required to train a model. Whereas in DeepAnT, a model can be trained on relatively small data set while achieving good generalization capabilities due to the effective parameter sharing of the CNN. As the anomaly detection in DeepAnT is unsupervised, it does not rely on anomaly labels at the time of model generation. Therefore, this approach can be directly applied to real-life scenarios where it is practically impossible to label a big stream of data coming from heterogeneous sensors comprising of both normal as well as anomalous points. We have performed a detailed evaluation of 15 algorithms on 10 anomaly detection benchmarks, which contain a total of 433 real and synthetic time series. Experiments show that DeepAnT outperforms the state-of-the-art anomaly detection methods in most of the cases, while performing on par with others.","['Anomaly detection', 'Time series analysis', 'Clustering algorithms', 'Data models', 'Benchmark testing', 'Heuristic algorithms']","['Anomaly detection', 'artificial intelligence', 'convolutional neural network', 'deep neural networks', 'recurrent neural networks', 'time series analysis']"
"This paper provides a systematic review of the mutual coupling in multiple-input multiple-output (MIMO) systems, including the effects on performances of MIMO systems and various decoupling techniques. The mutual coupling changes the antenna characteristics in an array, and therefore, degrades the system performance of the MIMO system and causes the spectral regrowth. Although the system performance can be partially improved by calibrating out the mutual coupling in the digital domain, it is more effective to use decoupling techniques (from the antenna point) to overcome the mutual coupling effects. Some popular decoupling techniques for MIMO systems (especially for massive MIMO base station antennas) are also presented.","['Mutual coupling', 'MIMO communication', 'Antenna arrays', 'Signal to noise ratio', 'Interference', 'System performance']","['Capacity', 'error rate', 'MIMO antennas', 'mutual coupling']"
"This paper provides a comprehensive study of Federated Learning (FL) with an emphasis on enabling software and hardware platforms, protocols, real-life applications and use-cases. FL can be applicable to multiple domains but applying it to different industries has its own set of obstacles. FL is known as collaborative learning, where algorithm(s) get trained across multiple devices or servers with decentralized data samples without having to exchange the actual data. This approach is radically different from other more established techniques such as getting the data samples uploaded to servers or having data in some form of distributed infrastructure. FL on the other hand generates more robust models without sharing data, leading to privacy-preserved solutions with higher security and access privileges to data. This paper starts by providing an overview of FL. Then, it gives an overview of technical details that pertain to FL enabling technologies, protocols, and applications. Compared to other survey papers in the field, our objective is to provide a more thorough summary of the most relevant protocols, platforms, and real-life use-cases of FL to enable data scientists to build better privacy-preserving solutions for industries in critical need of FL. We also provide an overview of key challenges presented in the recent literature and provide a summary of related research work. Moreover, we explore both the challenges and advantages of FL and present detailed service use-cases to illustrate how different architectures and protocols that use FL can fit together to deliver desired results.","['Machine learning', 'Computer architecture', 'Protocols', 'Data models', 'Data privacy', 'Computational modeling', 'Industries']","['Federated learning', 'machine learning', 'collaborative AI', 'privacy', 'security', 'decentralized data', 'on-device AI', 'peer-to-peer network']"
"Early diagnosis of gear transmission has been a significant challenge, because gear faults occur primarily at microstructure or even material level but their effects can only be observed indirectly at a system level. The performance of a gear fault diagnosis system depends significantly on the features extracted and the classifier subsequently applied. Traditionally, fault-related features are extracted and identified based on domain expertise through data preprocessing which are system-specific and may not be easily generalized. On the other hand, although recently the deep neural networks based approaches featuring adaptive feature extractions and inherent classifications have attracted attention, they usually require a substantial set of training data. Aiming at tackling these issues, this paper presents a deep convolutional neural network-based transfer learning approach. The proposed transfer learning architecture consists of two parts; the first part is constructed with a pre-trained deep neural network that serves to extract the features automatically from the input, and the second part is a fully connected stage to classify the features that needs to be trained using gear fault experimental data. Case analyses using experimental data from a benchmark gear system indicate that the proposed approach not only entertains preprocessing free adaptive feature extractions, but also requires only a small set of training data.","['Gears', 'Feature extraction', 'Fault diagnosis', 'Convolution', 'Convolutional neural networks', 'Vibrations']","['Alexnet', 'deep convolutional neural network', 'gear fault diagnosis', 'transfer learning']"
"Modern technologies of mobile computing and wireless sensing prompt the concept of pervasive social network (PSN)-based healthcare. To realize the concept, the core problem is how a PSN node can securely share health data with other nodes in the network. In this paper, we propose a secure system for PSN-based healthcare. Two protocols are designed for the system. The first one is an improved version of the IEEE 802.15.6 display authenticated association. It establishes secure links with unbalanced computational requirements for mobile devices and resource-limited sensor nodes. The second protocol uses blockchain technique to share health data among PSN nodes. We realize a protocol suite to study protocol runtime and other factors. In addition, human body channels are proposed for PSN nodes in some use cases. The proposed system illustrates a potential method of using blockchain for PSN-based applications.","['Protocols', 'Medical services', 'Mobile handsets', 'Wireless communication', 'Body area networks', 'Security', 'IEEE 802.15 Standard']","['IEEE 802.15.6', 'blockchain', 'e-health', 'healthcare', 'human body channels']"
"Internet of Things (IoT) is a new technological paradigm that can connect things from various fields through the Internet. For the IoT connected healthcare applications, the wireless body area network (WBAN) is gaining popularity as wearable devices spring into the market. This paper proposes a wearable sensor node with solar energy harvesting and Bluetooth low energy transmission that enables the implementation of an autonomous WBAN. Multiple sensor nodes can be deployed on different positions of the body to measure the subject's body temperature distribution, heartbeat, and detect falls. A web-based smartphone application is also developed for displaying the sensor data and fall notification. To extend the lifetime of the wearable sensor node, a flexible solar energy harvester with an output-based maximum power point tracking technique is used to power the sensor node. Experimental results show that the wearable sensor node works well when powered by the solar energy harvester. The autonomous 24 h operation is achieved with the experimental results. The proposed system with solar energy harvesting demonstrates that long-term continuous medical monitoring based on WBAN is possible provided that the subject stays outside for a short period of time in a day.","['Wearable sensors', 'Wireless communication', 'Body area networks', 'Solar panels', 'Solar energy', 'Maximum power point trackers', 'Temperature measurement']","['Internet of Things', 'wireless body area network', 'energy harvesting', 'maximum power point tracking', 'Bluetooth']"
"Due to the monumental growth of Internet applications in the last decade, the need for security of information network has increased manifolds. As a primary defense of network infrastructure, an intrusion detection system is expected to adapt to dynamically changing threat landscape. Many supervised and unsupervised techniques have been devised by researchers from the discipline of machine learning and data mining to achieve reliable detection of anomalies. Deep learning is an area of machine learning which applies neuron-like structure for learning tasks. Deep learning has profoundly changed the way we approach learning tasks by delivering monumental progress in different disciplines like speech processing, computer vision, and natural language processing to name a few. It is only relevant that this new technology must be investigated for information security applications. The aim of this paper is to investigate the suitability of deep learning approaches for anomaly-based intrusion detection system. For this research, we developed anomaly detection models based on different deep neural network structures, including convolutional neural networks, autoencoders, and recurrent neural networks. These deep models were trained on NSLKDD training data set and evaluated on both test data sets provided by NSLKDD, namely NSLKDDTest+ and NSLKDDTest21. All experiments in this paper are performed by authors on a GPU-based test bed. Conventional machine learning-based intrusion detection models were implemented using well-known classification techniques, including extreme learning machine, nearest neighbor, decision-tree, random-forest, support vector machine, naive-bays, and quadratic discriminant analysis. Both deep and conventional machine learning models were evaluated using well-known classification metrics, including receiver operating characteristics, area under curve, precision-recall curve, mean average precision and accuracy of classification. Experimental results of deep IDS models showed promising results for real-world application in anomaly detection systems.","['Anomaly detection', 'Machine learning', 'Training', 'Intrusion detection', 'Measurement', 'Neural networks']","['Deep learning', 'convolutional neural networks', 'autoencoders', 'LSTM', 'k_NN', 'decision_tree', 'intrusion detection', 'convnets', 'information security']"
"The boom in the capabilities and features of mobile devices, like smartphones, tablets, and wearables, combined with the ubiquitous and affordable Internet access and the advances in the areas of cooperative networking, computer vision, and mobile cloud computing transformed mobile augmented reality (MAR) from science fiction to a reality. Although mobile devices are more constrained computationalwise from traditional computers, they have a multitude of sensors that can be used to the development of more sophisticated MAR applications and can be assisted from remote servers for the execution of their intensive parts. In this paper, after introducing the reader to the basics of MAR, we present a categorization of the application fields together with some representative examples. Next, we introduce the reader to the user interface and experience in MAR applications and continue with the core system components of the MAR systems. After that, we discuss advances in tracking and registration, since their functionality is crucial to any MAR application and the network connectivity of the devices that run MAR applications together with its importance to the performance of the application. We continue with the importance of data management in MAR systems and the systems performance and sustainability, and before we conclude this survey, we present existing challenging problems.","['Mobile communication', 'Mobile computing', 'Cloud computing', 'Augmented reality', 'Smart phones', 'Sensors']","['Mobile augmented reality', 'mobile computing', 'human computer interaction']"
"Due to recent advances in digital technologies, and availability of credible data, an area of artificial intelligence, deep learning, has emerged and has demonstrated its ability and effectiveness in solving complex learning problems not possible before. In particular, convolutional neural networks (CNNs) have demonstrated their effectiveness in the image detection and recognition applications. However, they require intensive CPU operations and memory bandwidth that make general CPUs fail to achieve the desired performance levels. Consequently, hardware accelerators that use application-specific integrated circuits, field-programmable gate arrays (FPGAs), and graphic processing units have been employed to improve the throughput of CNNs. More precisely, FPGAs have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism and their energy efficiency. In this paper, we review the recent existing techniques for accelerating deep learning networks on FPGAs. We highlight the key features employed by the various techniques for improving the acceleration performance. In addition, we provide recommendations for enhancing the utilization of FPGAs for CNNs acceleration. The techniques investigated in this paper represent the recent trends in the FPGA-based accelerators of deep learning networks. Thus, this paper is expected to direct the future advances on efficient hardware accelerators and to be useful for deep learning researchers.","['Deep learning', 'Field programmable gate arrays', 'Neural networks', 'Hardware', 'Acceleration', 'Convolution', 'Throughput']","['Adaptable architectures', 'convolutional neural networks (CNNs)', 'deep learning', 'dynamic reconfiguration', 'energy-efficient architecture', 'field programmable gate arrays (FPGAs)', 'hardware accelerator', 'machine learning', 'neural networks', 'optimization', 'parallel computer architecture', 'reconfigurable computing']"
"Neural architecture search (NAS) has significant progress in improving the accuracy of image classification. Recently, some works attempt to extend NAS to image segmentation which shows preliminary feasibility. However, all of them focus on searching architecture for semantic segmentation in natural scenes. In this paper, we design three types of primitive operation set on search space to automatically find two cell architecture DownSC and UpSC for semantic image segmentation especially medical image segmentation. Inspired by the U-net architecture and its variants successfully applied to various medical image segmentation, we propose NAS-Unet which is stacked by the same number of DownSC and UpSC on a U-like backbone network. The architectures of DownSC and UpSC updated simultaneously by a differential architecture strategy during the search stage. We demonstrate the good segmentation results of the proposed method on Promise12, Chaos, and ultrasound nerve datasets, which collected by magnetic resonance imaging, computed tomography, and ultrasound, respectively. Without any pretraining, our architecture searched on PASCAL VOC2012, attains better performances and much fewer parameters (about 0.8M) than U-net and one of its variants when evaluated on the above three types of medical image datasets.","['Computer architecture', 'Image segmentation', 'Magnetic resonance imaging', 'Medical diagnostic imaging', 'Task analysis', 'Microprocessors']","['Medical image segmentation', 'convolutional neural architecture search', 'deep learning']"
"An outbreak of a novel coronavirus disease (i.e., COVID-19) has been recorded in Wuhan, China since late December 2019, which subsequently became pandemic around the world. Although COVID-19 is an acutely treated disease, it can also be fatal with a risk of fatality of 4.03% in China and the highest of 13.04% in Algeria and 12.67% Italy (as of 8th April 2020). The onset of serious illness may result in death as a consequence of substantial alveolar damage and progressive respiratory failure. Although laboratory testing, e.g., using reverse transcription polymerase chain reaction (RT-PCR), is the golden standard for clinical diagnosis, the tests may produce false negatives. Moreover, under the pandemic situation, shortage of RT-PCR testing resources may also delay the following clinical decision and treatment. Under such circumstances, chest CT imaging has become a valuable tool for both diagnosis and prognosis of COVID-19 patients. In this study, we propose a weakly supervised deep learning strategy for detecting and classifying COVID-19 infection from CT images. The proposed method can minimise the requirements of manual labelling of CT images but still be able to obtain accurate infection detection and distinguish COVID-19 from non-COVID-19 cases. Based on the promising results obtained qualitatively and quantitatively, we can envisage a wide deployment of our developed technique in large-scale clinical studies.","['COVID-19', 'Computed tomography', 'Lung', 'Deep learning', 'Hospitals']","['COVID-19', 'deep learning', 'weakly supervision', 'CT~images', 'classification', 'convolutional neural network']"
"Extending the Technology Acceptance Model (TAM) for studying the e-learning acceptance is not a new research topic, and it has been tackled by many scholars. However, the development of a comprehensive TAM that could be able to examine the e-learning acceptance under any circumstances is regarded to be an essential research direction. To identify the most widely used external factors of the TAM concerning the e-learning acceptance, a literature review comprising of 120 significant published studies from the last twelve years was conducted. The review analysis indicated that computer self-efficacy, subjective/social norm, perceived enjoyment, system quality, information quality, content quality, accessibility, and computer playfulness were the most common external factors of TAM. Accordingly, the TAM has been extended by the aforementioned factors to examine the students' acceptance of e-learning in five different universities in the United Arab of Emirates (UAE). A total of 435 students participated in the study. The results indicated that system quality, computer self-efficacy, and computer playfulness have a significant impact on perceived ease of use of e-learning system. Furthermore, information quality, perceived enjoyment, and accessibility were found to have a positive influence on perceived ease of use and perceived usefulness of e-learning system.","['Electronic learning', 'Databases', 'Media', 'Bibliographies', 'Tools']","['E-learning', 'higher education', 'TAM', 'PLS-SEM']"
"Continuous practices, i.e., continuous integration, delivery, and deployment, are the software development industry practices that enable organizations to frequently and reliably release new features and products. With the increasing interest in the literature on continuous practices, it is important to systematically review and synthesize the approaches, tools, challenges, and practices reported for adopting and implementing continuous practices. This paper aimed at systematically reviewing the state of the art of continuous practices to classify approaches and tools, identify challenges and practices in this regard, and identify the gaps for future research. We used the systematic literature review method for reviewing the peer-reviewed papers on continuous practices published between 2004 and June 1, 2016. We applied the thematic analysis method for analyzing the data extracted from reviewing 69 papers selected using predefined criteria. We have identified 30 approaches and associated tools, which facilitate the implementation of continuous practices in the following ways: (1) reducing build and test time in continuous integration (CI); (2) increasing visibility and awareness on build and test results in CI; (3) supporting (semi-) automated continuous testing; (4) detecting violations, flaws, and faults in CI; (5) addressing security and scalability issues in deployment pipeline; and (6) improving dependability and reliability of deployment process. We have also determined a list of critical factors, such as testing (effort and time), team awareness and transparency, good design principles, customer, highly skilled and motivated team, application domain, and appropriate infrastructure that should be carefully considered when introducing continuous practices in a given organization. The majority of the reviewed papers were validation (34.7%) and evaluation (36.2%) research types. This paper also reveals that continuous practices have been successfully applied to both greenfield and maintenance projects. Continuous practices have become an important area of software engineering research and practice. While the reported approaches, tools, and practices are addressing a wide range of challenges, there are several challenges and gaps, which require future research work for improving the capturing and reporting of contextual information in the studies reporting different aspects of continuous practices; gaining a deep understanding of how software-intensive systems should be (re-) architected to support continuous practices; and addressing the lack of knowledge and tools for engineering processes of designing and running secure deployment pipelines.","['Software', 'Organizations', 'Software engineering', 'Systematics', 'Bibliographies', 'Testing', 'Production']","['Continuous integration', 'continuous delivery', 'continuous deployment', 'continuous software engineering', 'systematic literature review', 'empirical software engineering']"
"Electromagnetic radars have been shown potentially to be used for remote sensing of biosignals in a more comfortable and easier way than wearable and contact devices. While there is an increasing interest in using radars for health monitoring, their performance has not been tested and reported either in practical scenarios or with acceptable low errors. Therefore, we use a frequency modulated continuous wave (FMCW) radar operating at 77 GHz in a bedroom environment to extract the respiration and heart rates of a patient, who is used to lying down on the bed. Indeed, the proposed signal processing contains advanced phase unwrapping manipulation, which is unique. In addition, the results are compared with a reliable reference sensor. Our results show that the correlations between the reference sensor and the radar estimates are in 94% and 80% for breathing and heart rates, respectively.","['Heart rate', 'Phase noise', 'Monitoring', 'Radar remote sensing', 'Radar detection', 'Receivers']","['Breathing rate monitoring', 'FMCW radar', 'heart rate monitoring', 'Hexoskin', 'mm-wave', 'non-contact monitoring', 'phase analysis', 'remote sensing', 'vital signs', 'TI']"
"Shipbuilding companies are upgrading their inner workings in order to create Shipyards 4.0, where the principles of Industry 4.0 are paving the way to further digitalized and optimized processes in an integrated network. Among the different Industry 4.0 technologies, this paper focuses on augmented reality, whose application in the industrial field has led to the concept of industrial augmented reality (IAR). This paper first describes the basics of IAR and then carries out a thorough analysis of the latest IAR systems for industrial and shipbuilding applications. Then, in order to build a practical IAR system for shipyard workers, the main hardware and software solutions are compared. Finally, as a conclusion after reviewing all the aspects related to IAR for shipbuilding, it proposed an IAR system architecture that combines cloudlets and fog computing, which reduce latency response and accelerate rendering tasks while offloading compute intensive tasks from the cloud.","['Augmented reality', 'Industries', 'Task analysis', 'Software', 'Cameras', 'Companies', 'Hardware']","['Industry 4.0', 'augmented reality', 'industrial augmented reality', 'Internet of Things', 'cyber-physical systems', 'industrial operator support', 'smart factory', 'task execution', 'cloudlet', 'edge computing']"
"This paper conducts a comprehensive study on the application of big data and machine learning in the electrical power grid introduced through the emergence of the next-generation power system-the smart grid (SG). Connectivity lies at the core of this new grid infrastructure, which is provided by the Internet of Things (IoT). This connectivity, and constant communication required in this system, also introduced a massive data volume that demands techniques far superior to conventional methods for proper analysis and decision-making. The IoT-integrated SG system can provide efficient load forecasting and data acquisition technique along with cost-effectiveness. Big data analysis and machine learning techniques are essential to reaping these benefits. In the complex connected system of SG, cyber security becomes a critical issue; IoT devices and their data turning into major targets of attacks. Such security concerns and their solutions are also included in this paper. Key information obtained through literature review is tabulated in the corresponding sections to provide a clear synopsis; and the findings of this rigorous review are listed to give a concise picture of this area of study and promising future fields of academic and industrial research, with current limitations with viable solutions along with their effectiveness.","['Smart grids', 'Big Data', 'Machine learning', 'Security', 'Internet of Things']","['Big data analysis', 'cyber security', 'IoT', 'machine learning', 'smart grid']"
"Blockchain technology has attracted tremendous attention in both academia and capital market. However, overwhelming speculations on thousands of available cryptocurrencies and numerous initial coin offering scams have also brought notorious debates on this emerging technology. This paper traces the development of blockchain systems to reveal the importance of decentralized applications (dApps) and the future value of blockchain. We survey the state-of-the-art dApps and discuss the direction of blockchain development to fulfill the desirable characteristics of dApps. The readers will gain an overview of dApp research and get familiar with recent developments in the blockchain.","['Bitcoin', 'Games', 'Contracts', 'Peer-to-peer computing', 'Software systems']","['Blockchain', 'decentralized application', 'smart contract', 'software systems', 'survey']"
"Employing large intelligent surfaces (LISs) is a promising solution for improving the coverage and rate of future wireless systems. These surfaces comprise massive numbers of nearly-passive elements that interact with the incident signals, for example by reflecting them, in a smart way that improves the wireless system performance. Prior work focused on the design of the LIS reflection matrices assuming full channel knowledge. Estimating these channels at the LIS, however, is a key challenging problem. With the massive number of LIS elements, channel estimation or reflection beam training will be associated with (i) huge training overhead if all the LIS elements are passive (not connected to a baseband) or with (ii) prohibitive hardware complexity and power consumption if all the elements are connected to the baseband through a fully-digital or hybrid analog/digital architecture. This paper proposes efficient solutions for these problems by leveraging tools from compressive sensing and deep learning. First, a novel LIS architecture based on sparse channel sensors is proposed. In this architecture, all the LIS elements are passive except for a few elements that are active (connected to the baseband). We then develop two solutions that design the LIS reflection matrices with negligible training overhead. In the first approach, we leverage compressive sensing tools to construct the channels at all the LIS elements from the channels seen only at the active elements. In the second approach, we develop a deep-learning based solution where the LIS learns how to interact with the incident signal given the channels at the active elements, which represent the state of the environment and transmitter/receiver locations. We show that the achievable rates of the proposed solutions approach the upper bound, which assumes perfect channel knowledge, with negligible training overhead and with only a few active elements, making them promising for future LIS systems.","['Training', 'Deep learning', 'Wireless communication', 'Baseband', 'Tools', 'Reflection', 'Compressed sensing']","['Large intelligent surface', 'intelligent reflecting surfaces', 'reconfigurable intelligent surface', 'smart reflect-array', 'beamforming', 'millimeter wave', 'compressive sensing', 'deep learning']"
"Traditional machine learning algorithms have made great achievements in data-driven fault diagnosis. However, they assume that all the data must be in the same working condition and have the same distribution and feature space. They are not applicable for real-world working conditions, which often change with time, so the data are hard to obtain. In order to utilize data in different working conditions to improve the performance, this paper presents a transfer learning approach for fault diagnosis with neural networks. First, it learns characteristics from massive source data and adjusts the parameters of neural networks accordingly. Second, the structure of neural networks alters for the change of data distribution. In the same time, some parameters are transferred from source task to target task. Finally, the new model is trained by a small amount of target data in another working condition. The Case Western Reserve University bearing data set is used to validate the performance of the proposed transfer learning approach. Experimental results show that the proposed transfer learning approach can improve the classification accuracy and reduce the training time comparing with the conventional neural network method when there are only a small amount of target data.","['Employee welfare', 'Fault diagnosis', 'Data models', 'Training', 'Vibrations', 'Biological neural networks']","['Fault diagnosis', 'transfer learning', 'neural networks', 'machine learning']"
"For agricultural applications, regularized smart-farming solutions are being considered, including the use of unmanned aerial vehicles (UAVs). The UAVs combine information and communication technologies, robots, artificial intelligence, big data, and the Internet of Things. The agricultural UAVs are highly capable, and their use has expanded across all areas of agriculture, including pesticide and fertilizer spraying, seed sowing, and growth assessment and mapping. Accordingly, the market for agricultural UAVs is expected to continue growing with the related technologies. In this study, we consider the latest trends and applications of leading technologies related to agricultural UAVs, control technologies, equipment, and development. We discuss the use of UAVs in real agricultural environments. Furthermore, the future development of the agricultural UAVs and their challenges are presented.","['Cameras', 'Unmanned aerial vehicles', 'Agriculture', 'Wireless sensor networks', 'Wireless communication', 'Sensors', 'Rotors']","['Agricultural applications', 'agricultural UAV', 'control technology', 'smart farming', 'unmanned aerial vehicle', 'UAV platforms']"
"This paper investigates the coexistence between two key enabling technologies for fifth generation (5G) mobile networks, non-orthogonal multiple access (NOMA), and millimeter-wave (mmWave) communications. Particularly, the application of random beamforming to mmWave-NOMA systems is considered in order to avoid the requirement that the base station know all the users' channel state information. Stochastic geometry is used to characterize the performance of the proposed mmWave-NOMA transmission scheme by using key features of mmWave systems, i.e., that mmWave transmission is highly directional and potential blockages will thin the user distribution. Two random beamforming approaches that can further reduce the system overhead are also proposed, and their performance is studied analytically in terms of sum rates and outage probabilities. Simulation results are also provided to demonstrate the performance of the proposed schemes and verify the accuracy of the developed analytical results.","['Base stations', 'NOMA', 'Array signal processing', 'Precoding', '5G mobile communication', 'Interference']","['Non-orthogonal multiple access (NOMA)', 'millimeter wave (mmWave) networks', 'mmWave-NOMA communications', 'random beamforming']"
"The public key infrastructure-based authentication protocol provides basic security services for the vehicular ad hoc networks (VANETs). However, trust and privacy are still open issues due to the unique characteristics of VANETs. It is crucial to prevent internal vehicles from broadcasting forged messages while simultaneously preserving the privacy of vehicles against the tracking attacks. In this paper, we propose a blockchain-based anonymous reputation system (BARS) to establish a privacy-preserving trust model for VANETs. The certificate and revocation transparency is implemented efficiently with the proofs of presence and absence based on the extended blockchain technology. The public keys are used as pseudonyms in communications without any information about real identities for conditional anonymity. In order to prevent the distribution of forged messages, a reputation evaluation algorithm is presented relying on both direct historical interactions and indirect opinions about vehicles. A set of experiments is conducted to evaluate BARS in terms of security, validity, and performance, and the results show that BARS is able to establish a trust model with transparency, conditional anonymity, efficiency, and robustness for VANETs.","['Privacy', 'Public key', 'Bars', 'Data models', 'Authentication']","['Vehicular ad-hoc networks', 'blockchain', 'trust management', 'reputation system', 'privacy']"
"In recent years, the classification of breast cancer has been the topic of interest in the field of Healthcare informatics, because it is the second main cause of cancer-related deaths in women. Breast cancer can be identified using a biopsy where tissue is removed and studied under microscope. The diagnosis is based on the qualification of the histopathologist, who will look for abnormal cells. However, if the histopathologist is not well-trained, this may lead to wrong diagnosis. With the recent advances in image processing and machine learning, there is an interest in attempting to develop a reliable pattern recognition based systems to improve the quality of diagnosis. In this paper, we compare two machine learning approaches for the automatic classification of breast cancer histology images into benign and malignant and into benign and malignant sub-classes. The first approach is based on the extraction of a set of handcrafted features encoded by two coding models (bag of words and locality constrained linear coding) and trained by support vector machines, while the second approach is based on the design of convolutional neural networks. We have also experimentally tested dataset augmentation techniques to enhance the accuracy of the convolutional neural network as well as “handcrafted features + convolutional neural network”and “convolutional neural network features + classifier”configurations. The results show convolutional neural networks outperformed the handcrafted feature based classifier, where we achieved accuracy between 96.15% and 98.33% for the binary classification and 83.31% and 88.23% for the multi-class classification.","['Feature extraction', 'Convolutional neural networks', 'Breast cancer', 'Image coding', 'Machine learning']","['Histology images', 'convolutional neural networks', 'engineered features', 'bag of words', 'locality constrained linear coding']"
"Mobile edge computing (MEC) provides a promising approach to significantly reduce network operational cost and improve quality of service (QoS) of mobile users by pushing computation resources to the network edges, and enables a scalable Internet of Things (IoT) architecture for time-sensitive applications (e-healthcare, real-time monitoring, and so on.). However, the mobility of mobile users and the limited coverage of edge servers can result in significant network performance degradation, dramatic drop in QoS, and even interruption of ongoing edge services; therefore, it is difficult to ensure service continuity. Service migration has great potential to address the issues, which decides when or where these services are migrated following user mobility and the changes of demand. In this paper, two conceptions similar to service migration, i.e., live migration for data centers and handover in cellular networks, are first discussed. Next, the cutting-edge research efforts on service migration in MEC are reviewed, and a devisal of taxonomy based on various research directions for efficient service migration is presented. Subsequently, a summary of three technologies for hosting services on edge servers, i.e., virtual machine, container, and agent, is provided. At last, open research challenges in service migration are identified and discussed.","['Servers', 'Virtual machining', 'Edge computing', 'Cloud computing', 'Quality of service', 'Data centers', 'Handover']","['Mobile edge computing', 'service migration', 'live migration', 'migration path selection', 'cellular handover']"
"We investigate the application of non-orthogonal multiple access (NOMA) with successive interference cancellation (SIC) in downlink multiuser multiple-input multiple-output (MIMO) cellular systems, where the total number of receive antennas at user equipment (UE) ends in a cell is more than the number of transmit antennas at the base station (BS). We first dynamically group the UE receive antennas into a number of clusters equal to or more than the number of BS transmit antennas. A single beamforming vector is then shared by all the receive antennas in a cluster. We propose a linear beamforming technique in which all the receive antennas can significantly cancel the inter-cluster interference. On the other hand, the receive antennas in each cluster are scheduled on the power domain NOMA basis with SIC at the receiver ends. For inter-cluster and intra-cluster power allocation, we provide dynamic power allocation solutions with an objective to maximizing the overall cell capacity. An extensive performance evaluation is carried out for the proposed MIMO-NOMA system and the results are compared with those for conventional orthogonal multiple access (OMA)-based MIMO systems and other existing MIMO-NOMA solutions. The numerical results quantify the capacity gain of the proposed MIMO-NOMA model over MIMO-OMA and other existing MIMO-NOMA solutions.","['NOMA', 'Resource management', 'Receiving antennas', 'Downlink', 'Transmitting antennas', 'MIMO', 'Interference']","['5G cellular', 'non-orthogonal multiple access (NOMA)', 'multiple-input multiple-output (MIMO)', 'linear beamforming', 'dynamic user clustering', 'dynamic power allocation']"
"It is an exciting time for power systems as there are many ground-breaking changes happening simultaneously. There is a global consensus in increasing the share of renewable energy-based generation in the overall mix, transitioning to a more environmental-friendly transportation with electric vehicles as well as liberalizing the electricity markets, much to the distaste of traditional utility companies. All of these changes are against the status quo and introduce new paradigms in the way the power systems operate. The generation penetrates distribution networks, renewables introduce intermittency, and liberalized markets need more competitive operation with the existing assets. All of these challenges require using some sort of storage device to develop viable power system operation solutions. There are different types of storage systems with different costs, operation characteristics, and potential applications. Understanding these is vital for the future design of power systems whether it be for short-term transient operation or long-term generation planning. In this paper, the state-of-the-art storage systems and their characteristics are thoroughly reviewed along with the cutting edge research prototypes. Based on their architectures, capacities, and operation characteristics, the potential application fields are identified. Finally, the research fields that are related to energy storage systems are studied with their impacts on the future of power systems.","['Flywheels', 'Iron', 'Renewable energy sources', 'Production', 'Smart grids']","['Storage systems', 'electric vehicles', 'power system optimization', 'market liberalization', 'renewable energy', 'new operation schemes', 'power system planning']"
"With the development of satellite technology, up to date imaging mode of synthetic aperture radar (SAR) satellite can provide higher resolution SAR imageries, which benefits ship detection and instance segmentation. Meanwhile, object detectors based on convolutional neural network (CNN) show high performance on SAR ship detection even without land-ocean segmentation; but with respective shortcomings, such as the relatively small size of SAR images for ship detection, limited SAR training samples, and inappropriate annotations, in existing SAR ship datasets, related research is hampered. To promote the development of CNN based ship detection and instance segmentation, we have constructed a High-Resolution SAR Images Dataset (HRSID). In addition to object detection, instance segmentation can also be implemented on HRSID. As for dataset construction, under the overlapped ratio of 25%, 136 panoramic SAR imageries with ranging resolution from 1m to 5m are cropped to 800 × 800 pixels SAR images. To reduce wrong annotation and missing annotation, optical remote sensing imageries are applied to reduce the interferes from harbor constructions. There are 5604 cropped SAR images and 16951 ships in HRSID, and we have divided HRSID into a training set (65% SAR images) and test set (35% SAR images) with the format of Microsoft Common Objects in Context (MS COCO). 8 state-of-the-art detectors are experimented on HRSID to build the baseline; MS COCO evaluation metrics are applicated for comprehensive evaluation. Experimental results reveal that ship detection and instance segmentation can be well implemented on HRSID.","['Marine vehicles', 'Radar polarimetry', 'Image segmentation', 'Imaging', 'Synthetic aperture radar', 'Detectors', 'Image resolution']","['High-resolution SAR images dataset', 'ship detection', 'instance segmentation', 'deep learning', 'convolutional neural network']"
"Digital twin is a significant way to achieve smart manufacturing, and provides a new paradigm for fault diagnosis. Traditional data-based fault diagnosis methods mostly assume that the training data and test data are following the same distribution and can acquire sufficient data to train a reliable diagnosis model, which is unrealistic in the dynamic changing production process. In this paper, we present a two-phase digital-twin-assisted fault diagnosis method using deep transfer learning (DFDD), which realizes fault diagnosis both in the development and maintenance phases. At first, the potential problems that are not considered at design time can be discovered through front running the ultra-high-fidelity model in the virtual space, while a deep neural network (DNN)-based diagnosis model will be fully trained. In the second phase, the previously trained diagnosis model can be migrated from the virtual space to physical space using deep transfer learning for real-time monitoring and predictive maintenance. This ensures the accuracy of the diagnosis as well as avoids wasting time and knowledge. A case study about fault diagnosis using DFDD in a car body-side production line is presented. The results show the superiority and feasibility of our proposed method.","['Fault diagnosis', 'Data models', 'Manufacturing', 'Training data', 'Maintenance engineering', 'Predictive models', 'Solid modeling']","['Digital twin', 'deep transfer learning', 'fault diagnosis', 'smart manufacturing']"
"For task-scheduling problems in cloud computing, a multi-objective optimization method is proposed here. First, with an aim toward the biodiversity of resources and tasks in cloud computing, we propose a resource cost model that defines the demand of tasks on resources with more details. This model reflects the relationship between the user's resource costs and the budget costs. A multi-objective optimization scheduling method has been proposed based on this resource cost model. This method considers the makespan and the user's budget costs as constraints of the optimization problem, achieving multi-objective optimization of both performance and cost. An improved ant colony algorithm has been proposed to solve this problem. Two constraint functions were used to evaluate and provide feedback regarding the performance and budget cost. These two constraint functions made the algorithm adjust the quality of the solution in a timely manner based on feedback in order to achieve the optimal solution. Some simulation experiments were designed to evaluate this method's performance using four metrics: 1) the makespan; 2) cost; 3) deadline violation rate; and 4) resource utilization. Experimental results show that based on these four metrics, a multi-objective optimization method is better than other similar methods, especially as it increased 56.6% in the best case scenario.","['Scheduling', 'Optimization', 'Processor scheduling', 'Cloud computing', 'Ant colony optimization', 'Memory management', 'Resource management']","['Cloud computing', 'Ant colony', 'Task scheduling']"
"Texture analysis is used in a very broad range of fields and applications, from texture classification (e.g., for remote sensing) to segmentation (e.g., in biomedical imaging), passing through image synthesis or pattern recognition (e.g., for image inpainting). For each of these image processing procedures, first, it is necessary to extract—from raw images—meaningful features that describe the texture properties. Various feature extraction methods have been proposed in the last decades. Each of them has its advantages and limitations: performances of some of them are not modified by translation, rotation, affine, and perspective transform; others have a low computational complexity; others, again, are easy to implement; and so on. This paper provides a comprehensive survey of the texture feature extraction methods. The latter are categorized into seven classes: statistical approaches, structural approaches, transform-based approaches, model-based approaches, graph-based approaches, learning-based approaches, and entropy-based approaches. For each method in these seven classes, we present the concept, the advantages, and the drawbacks and give examples of application. This survey allows us to identify two classes of methods that, particularly, deserve attention in the future, as their performances seem interesting, but their thorough study is not performed yet.",[],[]
"Cell-free (CF) massive multiple-input-multiple-output (MIMO) systems have a large number of individually controllable antennas distributed over a wide area for simultaneously serving a small number of user equipments (UEs). This solution has been considered as a promising next-generation technology due to its ability to offer a similar quality of service to all UEs despite its low-complexity signal processing. In this paper, we provide a comprehensive survey of CF massive MIMO systems. To be more specific, the benefit of the so-called channel hardening and the favorable propagation conditions are exploited. Furthermore, we quantify the advantages of CF massive MIMO systems in terms of their energy- and cost-efficiency. Additionally, the signal processing techniques invoked for reducing the fronthaul burden for joint channel estimation and for transmit precoding are analyzed. Finally, the open research challenges in both its deployment and network management are highlighted.","['MIMO communication', 'Central Processing Unit', 'Power demand', 'Antenna arrays', 'Signal processing', 'Time-frequency analysis']","['Cell-free massive MIMO', 'B5G', 'signal processing', 'performance analysis']"
"The era of artificial neural network (ANN) began with a simplified application in many fields and remarkable success in pattern recognition (PR) even in manufacturing industries. Although significant progress achieved and surveyed in addressing ANN application to PR challenges, nevertheless, some problems are yet to be resolved like whimsical orientation (the unknown path that cannot be accurately calculated due to its directional position). Other problem includes; object classification, location, scaling, neurons behavior analysis in hidden layers, rule, and template matching. Also, the lack of extant literature on the issues associated with ANN application to PR seems to slow down research focus and progress in the field. Hence, there is a need for state-of-the-art in neural networks application to PR to urgently address the above-highlights problems for more successes. The study furnishes readers with a clearer understanding of the current, and new trend in ANN models that effectively addresses PR challenges to enable research focus and topics. Similarly, the comprehensive review reveals the diverse areas of the success of ANN models and their application to PR. In evaluating the performance of ANN models, some statistical indicators for measuring the performance of the ANN model in many studies were adopted. Such as the use of mean absolute percentage error (MAPE), mean absolute error (MAE), root mean squared error (RMSE), and variance of absolute percentage error (VAPE). The result shows that the current ANN models such as GAN, SAE, DBN, RBM, RNN, RBFN, PNN, CNN, SLP, MLP, MLNN, Reservoir computing, and Transformer models are performing excellently in their application to PR tasks. Therefore, the study recommends the research focus on current models and the development of new models concurrently for more successes in the field.","['Artificial neural networks', 'Task analysis', 'Fingerprint recognition', 'Computational modeling', 'Image recognition', 'Agriculture']","['Artificial neural networks', 'application to pattern recognition', 'feedforward neural networks', 'feedback neural networks', 'hybrid models']"
"Fog computing (FC) and Internet of Everything (IoE) are two emerging technological paradigms that, to date, have been considered standing-alone. However, because of their complementary features, we expect that their integration can foster a number of computing and network-intensive pervasive applications under the incoming realm of the future Internet. Motivated by this consideration, the goal of this position paper is fivefold. First, we review the technological attributes and platforms proposed in the current literature for the standing-alone FC and IoE paradigms. Second, by leveraging some use cases as illustrative examples, we point out that the integration of the FC and IoE paradigms may give rise to opportunities for new applications in the realms of the IoE, Smart City, Industry 4.0, and Big Data Streaming, while introducing new open issues. Third, we propose a novel technological paradigm, the Fog of Everything (FoE) paradigm, that integrates FC and IoE and then we detail the main building blocks and services of the corresponding technological platform and protocol stack. Fourth, as a proof-of-concept, we present the simulated energy-delay performance of a small-scale FoE prototype, namely, the V-FoE prototype. Afterward, we compare the obtained performance with the corresponding one of a benchmark technological platform, e.g., the V-D2D one. It exploits only device-to-device links to establish inter-thing “ad hoc” communication. Last, we point out the position of the proposed FoE paradigm over a spectrum of seemingly related recent research projects.","['Cloud computing', 'Big Data', 'Edge computing', 'Biological system modeling', 'Ecosystems', 'Smart cities']","['Fog of IoE', 'virtualized networked computing platforms for IoE', 'context-aware networking-plus-computing distributed resource management', 'Internet of Energy', 'Smart City', 'Industry 4.0', 'Big Data Streaming', 'future Internet']"
"Accurate prediction of remaining useful life (RUL) of lithium-ion battery plays an increasingly crucial role in the intelligent battery health management systems. The advances in deep learning introduce new data-driven approaches to this problem. This paper proposes an integrated deep learning approach for RUL prediction of lithium-ion battery by integrating autoencoder with deep neural network (DNN). First, we present a multi-dimensional feature extraction method with autoencoder model to represent battery health degradation. Then, the RUL prediction model-based DNN is trained for multi-battery remaining cycle life estimation. The proposed approach is applied to the real data set of lithium-ion battery cycle life from NASA, and the experiment results show that the proposed approach can improve the accuracy of RUL prediction.","['Lithium-ion batteries', 'Feature extraction', 'Predictive models', 'Voltage measurement', 'Temperature measurement', 'Machine learning']","['Lithium-ion battery', 'remaining useful life', 'RUL prediction model', 'deep learning', 'deep neural network']"
"Security breaches due to attacks by malicious software (malware) continue to escalate posing a major security concern in this digital age. With many computer users, corporations, and governments affected due to an exponential growth in malware attacks, malware detection continues to be a hot research topic. Current malware detection solutions that adopt the static and dynamic analysis of malware signatures and behavior patterns are time consuming and have proven to be ineffective in identifying unknown malwares in real-time. Recent malwares use polymorphic, metamorphic, and other evasive techniques to change the malware behaviors quickly and to generate a large number of new malwares. Such new malwares are predominantly variants of existing malwares, and machine learning algorithms (MLAs) are being employed recently to conduct an effective malware analysis. However, such approaches are time consuming as they require extensive feature engineering, feature learning, and feature representation. By using the advanced MLAs such as deep learning, the feature engineering phase can be completely avoided. Recently reported research studies in this direction show the performance of their algorithms with a biased training data, which limits their practical use in real-time situations. There is a compelling need to mitigate bias and evaluate these methods independently in order to arrive at a new enhanced method for effective zero-day malware detection. To fill the gap in the literature, this paper, first, evaluates the classical MLAs and deep learning architectures for malware detection, classification, and categorization using different public and private datasets. Second, we remove all the dataset bias removed in the experimental analysis by having different splits of the public and private datasets to train and test the model in a disjoint way using different timescales. Third, our major contribution is in proposing a novel image processing technique with optimal parameters for MLAs and deep learning architectures to arrive at an effective zero-day malware detection model. A comprehensive comparative study of our model demonstrates that our proposed deep learning architectures outperform classical MLAs. Our novelty in combining visualization and deep learning architectures for static, dynamic, and image processing-based hybrid approach applied in a big data environment is the first of its kind toward achieving robust intelligent zero-day malware detection. Overall, this paper paves way for an effective visual detection of malware using a scalable and hybrid deep learning framework for real-time deployments.","['Malware', 'Deep learning', 'Feature extraction', 'Computer architecture', 'Computer security']","['Cyber security', 'cybercrime', 'malware detection', 'static and dynamic analysis', 'artificial intelligence', 'machine learning', 'deep learning', 'image processing', 'scalable and hybrid framework']"
"Internet of Things (IoT) and smart computing technologies have revolutionized every sphere of 21 st century humans. IoT technologies and the data driven services they offer were beyond imagination just a decade ago. Now, they surround us and influence a variety of domains such as automobile, smart home, healthcare, etc. In particular, the Agriculture and Farming industries have also embraced this technological intervention. Smart devices are widely used by a range of people from farmers to entrepreneurs. These technologies are used in a variety of ways, from finding real-time status of crops and soil moisture content to deploying drones to assist with tasks such as applying pesticide spray. However, the use of IoT and smart communication technologies introduce a vast exposure to cybersecurity threats and vulnerabilities in smart farming environments. Such cyber attacks have the potential to disrupt the economies of countries that are widely dependent on agriculture. In this paper, we present a holistic study on security and privacy in a smart farming ecosystem. The paper outlines a multi layered architecture relevant to the precision agriculture domain and discusses the security and privacy issues in this dynamic and distributed cyber physical environment. Further more, the paper elaborates on potential cyber attack scenarios and highlights open research challenges and future directions.","['Privacy', 'Real-time systems', 'Computer security']","['Security', 'privacy', 'smart farming', 'precision agriculture', 'cloud computing', 'edge computing', 'cyber physical systems', 'IoT', 'artificial intelligence (AI)', 'machine learning', 'layered architecture']"
"Machine learning is one of the most prevailing techniques in computer science, and it has been widely applied in image processing, natural language processing, pattern recognition, cybersecurity, and other fields. Regardless of successful applications of machine learning algorithms in many scenarios, e.g., facial recognition, malware detection, automatic driving, and intrusion detection, these algorithms and corresponding training data are vulnerable to a variety of security threats, inducing a significant performance decrease. Hence, it is vital to call for further attention regarding security threats and corresponding defensive techniques of machine learning, which motivates a comprehensive survey in this paper. Until now, researchers from academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks. Thus, we revisit existing security threats and give a systematic survey on them from two aspects, the training phase and the testing/inferring phase. After that, we categorize current defensive techniques of machine learning into four groups: security assessment mechanisms, countermeasures in the training phase, those in the testing or inferring phase, data security, and privacy. Finally, we provide five notable trends in the research on security threats and defensive techniques of machine learning, which are worth doing in-depth studies in future.","['Security', 'Training', 'Machine learning algorithms', 'Training data', 'Support vector machines', 'Testing', 'Taxonomy']","['Machine learning', 'adversarial samples', 'security threats', 'defensive techniques']"
"In recent years, advanced threat attacks are increasing, but the traditional network intrusion detection system based on feature filtering has some drawbacks which make it difficult to find new attacks in time. This paper takes NSL-KDD data set as the research object, analyses the latest progress and existing problems in the field of intrusion detection technology, and proposes an adaptive ensemble learning model. By adjusting the proportion of training data and setting up multiple decision trees, we construct a MultiTree algorithm. In order to improve the overall detection effect, we choose several base classifiers, including decision tree, random forest, kNN, DNN, and design an ensemble adaptive voting algorithm. We use NSL-KDD Test+ to verify our approach, the accuracy of the MultiTree algorithm is 84.2%, while the final accuracy of the adaptive voting algorithm reaches 85.2%. Compared with other research papers, it is proved that our ensemble model effectively improves detection accuracy. In addition, through the analysis of data, it is found that the quality of data features is an important factor to determine the detection effect. In the future, we should optimize the feature selection and preprocessing of intrusion detection data to achieve better results.","['Intrusion detection', 'Feature extraction', 'Classification algorithms', 'Adaptation models', 'Machine learning algorithms', 'Neural networks', 'Prediction algorithms']","['Intrusion detection', 'ensemble learning', 'deep neural network', 'voting', 'MultiTree', 'NSL-KDD']"
"Nature computing has evolved with exciting performance to solve complex real-world combinatorial optimization problems. These problems span across engineering, medical sciences, and sciences generally. The Ebola virus has a propagation strategy that allows individuals in a population to move among susceptible, infected, quarantined, hospitalized, recovered, and dead sub-population groups. Motivated by the effectiveness of this strategy of propagation of the disease, a new bio-inspired and population-based optimization algorithm is proposed. This study presents a novel metaheuristic algorithm named Ebola Optimization Search Algorithm (EOSA) based on the propagation mechanism of the Ebola virus disease. First, we designed an improved SIR model of the disease, namely SEIR-HVQD: Susceptible (S), Exposed (E), Infected (I), Recovered (R), Hospitalized (H), Vaccinated (V), Quarantine (Q), and Death or Dead (D). Secondly, we represented the new model using a mathematical model based on a system of first-order differential equations. A combination of the propagation and mathematical models was adapted for developing the new metaheuristic algorithm. To evaluate the performance and capability of the proposed method in comparison with other optimization methods, two sets of benchmark functions consisting of forty-seven (47) classical and thirty (30) constrained IEEE-CEC benchmark functions were investigated. The results indicate that the performance of the proposed algorithm is competitive with other state-of-the-art optimization methods based on scalability, convergence, and sensitivity analyses. Extensive simulation results show that the EOSA outperforms popular metaheuristic algorithms such as the Particle Swarm Optimization Algorithm (PSO), Genetic Algorithm (GA), and Artificial Bee Colony Algorithm (ABC). Also, the algorithm was applied to address the complex problem of selecting the best combination of convolutional neural network (CNN) hyperparameters in the image classification of digital mammography. Results obtained showed the optimized CNN architecture successfully detected breast cancer from digital images at an accuracy of 96.0%. The source code of EOSA is publicly available at https://github.com/NathanielOy/EOSA_Metaheuristic .","['Optimization', 'Viruses (medical)', 'Diseases', 'Metaheuristics', 'Mathematical models', 'Heuristic algorithms', 'Statistics']","['Ebola virus', 'metaheuristic algorithm', 'optimization problems', 'constrained benchmark functions', 'image classification', 'convolutional neural network']"
"Wireless sensor networks (WSNs) will be integrated into the future Internet as one of the components of the Internet of Things, and will become globally addressable by any entity connected to the Internet. Despite the great potential of this integration, it also brings new threats, such as the exposure of sensor nodes to attacks originating from the Internet. In this context, lightweight authentication and key agreement protocols must be in place to enable end-to-end secure communication. Recently, Amin et al. proposed a three-factor mutual authentication protocol for WSNs. However, we identified several flaws in their protocol. We found that their protocol suffers from smart card loss attack where the user identity and password can be guessed using offline brute force techniques. Moreover, the protocol suffers from known session-specific temporary information attack, which leads to the disclosure of session keys in other sessions. Furthermore, the protocol is vulnerable to tracking attack and fails to fulfill user untraceability. To address these deficiencies, we present a lightweight and secure user authentication protocol based on the Rabin cryptosystem, which has the characteristic of computational asymmetry. We conduct a formal verification of our proposed protocol using ProVerif in order to demonstrate that our scheme fulfills the required security properties. We also present a comprehensive heuristic security analysis to show that our protocol is secure against all the possible attacks and provides the desired security features. The results we obtained show that our new protocol is a secure and lightweight solution for authentication and key agreement for Internet-integrated WSNs.","['Protocols', 'Wireless sensor networks', 'Internet', 'Authentication', 'Smart cards', 'Cryptography']","['Authentication', 'biometrics', 'key management', 'privacy', 'Rabin cryptosystem', 'smart card', 'wireless sensor networks']"
"The rapid development of blockchain technology and their numerous emerging applications has received huge attention in recent years. The distributed consensus mechanism is the backbone of a blockchain network. It plays a key role in ensuring the network's security, integrity, and performance. Most current blockchain networks have been deploying the proof-of-work consensus mechanisms, in which the consensus is reached through intensive mining processes. However, this mechanism has several limitations, e.g., energy inefficiency, delay, and vulnerable to security threats. To overcome these problems, a new consensus mechanism has been developed recently, namely proof of stake, which enables to achieve the consensus via proving the stake ownership. This mechanism is expected to become a cutting-edge technology for future blockchain networks. This paper is dedicated to investigating proof-of-stake mechanisms, from fundamental knowledge to advanced proof-of-stake-based protocols along with performance analysis, e.g., energy consumption, delay, and security, as well as their promising applications, particularly in the field of Internet of Vehicles. The formation of stake pools and their effects on the network stake distribution are also analyzed and simulated. The results show that the ratio between the block reward and the total network stake has a significant impact on the decentralization of the network. Technical challenges and potential solutions are also discussed.","['Blockchain', 'Cryptography', 'Games', 'Delays', 'Hash functions', 'Distributed databases']","['Blockchain', 'consensus mechanisms', 'energy', 'game theory', 'proof-of-stake', 'proof-of-work', 'security', 'and mining process']"
"In recent years, with the rapid development of Internet technology, online shopping has become a mainstream way for users to purchase and consume. Sentiment analysis of a large number of user reviews on e-commerce platforms can effectively improve user satisfaction. This paper proposes a new sentiment analysis model-SLCABG, which is based on the sentiment lexicon and combines Convolutional Neural Network (CNN) and attention-based Bidirectional Gated Recurrent Unit (BiGRU). In terms of methods, the SLCABG model combines the advantages of sentiment lexicon and deep learning technology, and overcomes the shortcomings of existing sentiment analysis model of product reviews. The SLCABG model combines the advantages of the sentiment lexicon and deep learning techniques. First, the sentiment lexicon is used to enhance the sentiment features in the reviews. Then the CNN and the Gated Recurrent Unit (GRU) network are used to extract the main sentiment features and context features in the reviews and use the attention mechanism to weight. And finally classify the weighted sentiment features. In terms of data, this paper crawls and cleans the real book evaluation of dangdang.com, a famous Chinese e-commerce website, for training and testing, all of which are based on Chinese. The scale of the data has reached 100000 orders of magnitude, which can be widely used in the field of Chinese sentiment analysis. The experimental results show that the model can effectively improve the performance of text sentiment analysis.","['Sentiment analysis', 'Analytical models', 'Feature extraction', 'Deep learning', 'Support vector machines']","['Attention mechanism', 'CNN', 'BiGRU', 'sentiment analysis', 'sentiment lexicon']"
"One of the biggest concerns of big data is privacy. However, the study on big data privacy is still at a very early stage. We believe the forthcoming solutions and theories of big data privacy root from the in place research output of the privacy discipline. Motivated by these factors, we extensively survey the existing research outputs and achievements of the privacy field in both application and theoretical angles, aiming to pave a solid starting ground for interested readers to address the challenges in the big data case. We first present an overview of the battle ground by defining the roles and operations of privacy systems. Second, we review the milestones of the current two major research categories of privacy: data clustering and privacy frameworks. Third, we discuss the effort of privacy study from the perspectives of different disciplines, respectively. Fourth, the mathematical description, measurement, and modeling on privacy are presented. We summarize the challenges and opportunities of this promising topic at the end of this paper, hoping to shed light on the exciting and almost uncharted land.","['Big data', 'Privacy', 'Clustering', 'Differential equations', 'Data privacy', 'Mathematical models']","['Big data', 'privacy', 'data clustering', 'differential privacy']"
"Driven by the demand to accommodate today's growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.","['Wireless communication', 'Training', '5G mobile communication', 'Supervised learning', 'Task analysis', 'Reinforcement learning']","['Machine learning', '5G mobile communication', 'B5G', 'wireless communication', 'mobile communication', 'artificial intelligence']"
"A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks. There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method, which we call smart augmentation and we show how to use it to increase the accuracy and reduce over fitting on a target network. Smart augmentation works, by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network. Smart augmentation has shown the potential to increase accuracy by demonstrably significant measures on all data sets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.","['Training', 'Biological neural networks', 'Informatics', 'Electronic mail', 'Machine learning', 'Data models']","['Artificial intelligence', 'artificial neural networks', 'machine learning', 'computer vision supervised learning', 'machine learning algorithms', 'image databases']"
"The concept of a digital twin has been used in some industries where an accurate digital model of the equipment can be used for predictive maintenance. The use of a digital twin for performance is critical, and for capital-intensive equipment such as jet engines it proved to be successful in terms of cost savings and reliability improvements. In this paper, we aim to study the expansion of the digital twin in including building life cycle management and explore the benefits and shortcomings of such implementation. In four rounds of experimentation, more than 25,000 sensor reading instances were collected, analyzed, and utilized to create and test a limited digital twin of an office building facade element. This is performed to point out the method of implementation, highlight the benefits gained from digital twin, and to uncover some of the technical shortcomings of the current Internet of Things systems for this purpose.","['Buildings', 'Solid modeling', 'Object oriented modeling', 'Three-dimensional displays', 'Real-time systems']","['Building information modeling', 'digital twin', 'life cycle management', 'Internet of Things', 'wireless sensor network']"
"The Internet of Energy (IoE) provides an effective networking technology for distributed green energy, which allows the connection of energy anywhere at any time. As an important part of the IoE, electric vehicles (EVs), and charging pile management are of great significance to the development of the IoE industry. Previous work has mainly focused on network performance optimization for its management, and few studies have considered the security of the management between EVs and charging piles. Therefore, this paper proposes a decentralized security model based on the lightning network and smart contract in the blockchain ecosystem; this proposed model is called the lightning network and smart contract (LNSC). The overall model involves registration, scheduling, authentication, and charging phases. The new proposed security model can be easily integrated with current scheduling mechanisms to enhance the security of trading between EVs and charging piles. Experimental results according to a realistic infrastructure are presented in this paper. These experimental results demonstrate that our scheme can effectively enhance vehicle security. Different performances of LNSC-based scheduling strategies are also presented.","['Contracts', 'Lightning', 'Charging stations', 'Electric vehicle charging']","['Blockchain', 'smart contract', 'vehicle charging', 'mutual authentication', 'Internet of Energy']"
"Detecting outliers is a significant problem that has been studied in various research and application areas. Researchers continue to design robust schemes to provide solutions to detect outliers efficiently. In this survey, we present a comprehensive and organized review of the progress of outlier detection methods from 2000 to 2019. First, we offer the fundamental concepts of outlier detection and then categorize them into different techniques from diverse outlier detection techniques, such as distance-, clustering-, density-, ensemble-, and learning-based methods. In each category, we introduce some state-of-the-art outlier detection methods and further discuss them in detail in terms of their performance. Second, we delineate their pros, cons, and challenges to provide researchers with a concise overview of each technique and recommend solutions and possible research directions. This paper gives current progress of outlier detection techniques and provides a better understanding of the different outlier detection methods. The open research issues and challenges at the end will provide researchers with a clear path for the future of outlier detection methods.","['Anomaly detection', 'Tools', 'Distributed databases', 'Trajectory', 'Deep learning']","['Outlier detection', 'distance-based', 'clustering-based', 'density-based', 'ensemble-based']"
"Bitcoin has recently attracted considerable attention in the fields of economics, cryptography, and computer science due to its inherent nature of combining encryption technology and monetary units. This paper reveals the effect of Bayesian neural networks (BNNs) by analyzing the time series of Bitcoin process. We also select the most relevant features from Blockchain information that is deeply involved in Bitcoin's supply and demand and use them to train models to improve the predictive performance of the latest Bitcoin pricing process. We conduct the empirical study that compares the Bayesian neural network with other linear and non-linear benchmark models on modeling and predicting the Bitcoin process. Our empirical studies show that BNN performs well in predicting Bitcoin price time series and explaining the high volatility of the recent Bitcoin price.","['Bitcoin', 'Predictive models', 'Neural networks', 'Time series analysis', 'Bayes methods', 'Biological system modeling']","['Bitcoin', 'blockchain', 'Bayesian neural network', 'time-series analysis', 'predictive model']"
"Today, the stability of the electric power grid is maintained through real time balancing of generation and demand. Grid scale energy storage systems are increasingly being deployed to provide grid operators the flexibility needed to maintain this balance. Energy storage also imparts resiliency and robustness to the grid infrastructure. Over the last few years, there has been a significant increase in the deployment of large scale energy storage systems. This growth has been driven by improvements in the cost and performance of energy storage technologies and the need to accommodate distributed generation, as well as incentives and government mandates. Energy management systems (EMSs) and optimization methods are required to effectively and safely utilize energy storage as a flexible grid asset that can provide multiple grid services. The EMS needs to be able to accommodate a variety of use cases and regulatory environments. In this paper, we provide a brief history of grid-scale energy storage, an overview of EMS architectures, and a summary of the leading applications for storage. These serve as a foundation for a discussion of EMS optimization methods and design.","['Energy management', 'Batteries', 'Frequency control', 'Optimization methods', 'Power system stability']","['Energy storage system (ESS)', 'energy management system (EMS)', 'battery energy storage system (BESS)', 'optimization methods', 'optimal control', 'linear programming (LP)', 'mixed integer linear programming (MILP)', 'battery management system (BMS)']"
"Although the Internet of Things (IoT) can increase efficiency and productivity through intelligent and remote management, it also increases the risk of cyber-attacks. The potential threats to IoT applications and the need to reduce risk have recently become an interesting research topic. It is crucial that effective Intrusion Detection Systems (IDSs) tailored to IoT applications be developed. Such IDSs require an updated and representative IoT dataset for training and evaluation. However, there is a lack of benchmark IoT and IIoT datasets for assessing IDSs-enabled IoT systems. This paper addresses this issue and proposes a new data-driven IoT/IIoT dataset with the ground truth that incorporates a label feature indicating normal and attack classes, as well as a type feature indicating the sub-classes of attacks targeting IoT/IIoT applications for multi-classification problems. The proposed dataset, which is named TON_IoT, includes Telemetry data of IoT/IIoT services, as well as Operating Systems logs and Network traffic of IoT network, collected from a realistic representation of a medium-scale network at the Cyber Range and IoT Labs at the UNSW Canberra (Australia). This paper also describes the proposed dataset of the Telemetry data of IoT/IIoT services and their characteristics. TON_IoT has various advantages that are currently lacking in the state-of-the-art datasets: i) it has various normal and attack events for different IoT/IIoT services, and ii) it includes heterogeneous data sources. We evaluated the performance of several popular Machine Learning (ML) methods and a Deep Learning model in both binary and multi-class classification problems for intrusion detection purposes using the proposed Telemetry dataset.","['Intrusion detection', 'Telemetry', 'Sensors', 'Internet of Things', 'Machine learning', 'Australia']","['Internet of Things (IoT)', 'Industrial Internet of Things (IIoT)', 'cybersecurity', 'intrusion detection systems (IDSs)', 'dataset']"
"The impending environmental issues and growing concerns for global energy crises are driving the need for new opportunities and technologies that can meet significantly higher demand for cleaner and sustainable energy systems. This necessitates the development of transportation and power generation systems. The electrification of the transportation system is a promising approach to green the transportation systems and to reduce the issues of climate change. This paper inspects the present status, latest deployment, and challenging issues in the implementation of Electric vehicles (EVs) infrastructural and charging systems in conjunction with several international standards and charging codes. It further analyzes EVs impacts and prospects in society. A complete assessment of charging systems for EVs with battery charging techniques is explained. Moreover, the beneficial and harmful impacts of EVs are categorized and thoroughly reviewed. Remedial measures for harmful impacts are presented and benefits obtained therefrom are highlighted. Bidirectional charging offers the fundamental feature of vehicle to grid technology. In this paper, the current challenging issues due to the massive deployment of EVs, and upcoming research trends are also presented. It is envisioned that the researchers interested in such areas can find this paper valuable and an informative one-stop source.","['Climate change', 'Electric vehicles', 'Batteries', 'Connectors', 'Fossil fuels', 'Power grids']","['Electric vehicles (EVs)', 'international standards', 'infrastructure of charging systems', 'plug-in hybrid electric vehicles (PHEVs)', 'impacts and challenging issues', 'vehicle to gird (V2G) technology']"
"In recent years, food safety issues have drawn growing concerns from society. In order to efficiently detect and prevent food safety problems and trace the accountability, building a reliable traceability system is indispensable. It is especially essential to accurately record, share, and trace the specific data within the whole food supply chain, including the process of production, processing, warehousing, transportation, and retail. The traditional traceability systems have issues, such as data invisibility, tampering, and sensitive information disclosure. The blockchain is a promising technology for the food safety traceability system because of the characteristics, such as the irreversible time vector, smart contract, and consensus algorithm. This paper proposes a food safety traceability system based on the blockchain and the EPC Information Services and develops a prototype system. The management architecture of on-chain & off-chain data is proposed as well, through which the traceability system can alleviate the data explosion issue of the blockchain for the Internet of Things. Furthermore, the enterprise-level smart contract is designed to prevent data tampering and sensitive information disclosure during information interaction among participants. The prototype system was implemented based on the Ethereum. According to the test results, the average time of information query response is around 2 ms, while the amount of on-chain data and query counts are 1 GB and 1000 times/s, respectively.","['Blockchain', 'Supply chains', 'Smart contracts', 'Explosions', 'Supply chain management', 'Prototypes']","['Food safety', 'traceability', 'blockchain', 'EPCIS', 'on-chain & off-chain', 'smart contract']"
"Agriculture plays a vital role in the economic growth of any country. With the increase of population, frequent changes in climatic conditions and limited resources, it becomes a challenging task to fulfil the food requirement of the present population. Precision agriculture also known as smart farming have emerged as an innovative tool to address current challenges in agricultural sustainability. The mechanism that drives this cutting edge technology is machine learning (ML). It gives the machine ability to learn without being explicitly programmed. ML together with IoT (Internet of Things) enabled farm machinery are key components of the next agriculture revolution. In this article, authors present a systematic review of ML applications in the field of agriculture. The areas that are focused are prediction of soil parameters such as organic carbon and moisture content, crop yield prediction, disease and weed detection in crops and species detection. ML with computer vision are reviewed for the classification of a different set of crop images in order to monitor the crop quality and yield assessment. This approach can be integrated for enhanced livestock production by predicting fertility patterns, diagnosing eating disorders, cattle behaviour based on ML models using data collected by collar sensors, etc. Intelligent irrigation which includes drip irrigation and intelligent harvesting techniques are also reviewed that reduces human labour to a great extent. This article demonstrates how knowledge-based agriculture can improve the sustainable productivity and quality of the product.","['Agriculture', 'Artificial intelligence', 'Internet of Things', 'Irrigation', 'Wireless sensor networks', 'Soil', 'Sensors']","['Agricultural engineering', 'machine learning', 'intelligent irrigation', 'IoT', 'prediction']"
"Fog computing is an architectural style in which network components between devices and the cloud execute application-specific logic. We present the first review on fog computing within healthcare informatics, and explore, classify, and discuss different application use cases presented in the literature. For that, we categorize applications into use case classes and list an inventory of application-specific tasks that can be handled by fog computing. We discuss on which level of the network such fog computing tasks can be executed, and provide tradeoffs with respect to requirements relevant to healthcare. Our review indicates that: 1) there is a significant number of computing tasks in healthcare that require or can benefit from fog computing principles; 2) processing on higher network tiers is required due to constraints in wireless devices and the need to aggregate data; and 3) privacy concerns and dependability prevent computation tasks to be completely moved to the cloud. These findings substantiate the need for a coherent approach toward fog computing in healthcare, for which we present a list of recommended research and development actions.","['Edge computing', 'Medical services', 'Wireless sensor networks', 'Cloud computing', 'Computer architecture', 'Wireless communication', 'Monitoring']","['Body sensor networks', 'fog computing', 'healthcare', 'health information management', 'internet of things', 'sensor devices', 'wireless sensor networks']"
"Multi-modality image fusion provides more comprehensive and sophisticated information in modern medical diagnosis, remote sensing, video surveillance, and so on. This paper presents a novel multi-modality medical image fusion method based on phase congruency and local Laplacian energy. In the proposed method, the non-subsampled contourlet transform is performed on medical image pairs to decompose the source images into high-pass and low-pass subbands. The high-pass subbands are integrated by a phase congruency-based fusion rule that can enhance the detailed features of the fused image for medical diagnosis. A local Laplacian energy-based fusion rule is proposed for low-pass subbands. The local Laplacian energy consists of weighted local energy and the weighted sum of Laplacian coefficients that describe the structured information and the detailed features of source image pairs, respectively. Thus, the proposed fusion rule can simultaneously integrate two key components for the fusion of low-pass subbands. The fused high-pass and low-pass subbands are inversely transformed to obtain the fused image. In the comparative experiments, three categories of multi-modality medical image pairs are used to verify the effectiveness of the proposed method. The experiment results show that the proposed method achieves competitive performance in both the image quantity and computational costs.","['Image fusion', 'Transforms', 'Laplace equations', 'Medical diagnostic imaging', 'Feature extraction', 'Medical diagnosis']","['Medical image fusion', 'multi-modality sensor fusion', 'NSCT', 'phase congruency', 'Laplacian energy']"
"The potential of blockchain has been extensively discussed in the literature and media mainly in finance and payment industry. One relatively recent trend is at the enterprise-level, where blockchain serves as the infrastructure for internet security and immutability. Emerging application domains include Industry 4.0 and Industrial Internet of Things (IIoT). Therefore, in this paper, we comprehensively review existing blockchain applications in Industry 4.0 and IIoT settings. Specifically, we present the current research trends in each of the related industrial sectors, as well as successful commercial implementations of blockchain in these relevant sectors. We also discuss industry-specific challenges for the implementation of blockchain in each sector. Further, we present currently open issues in the adoption of the blockchain technology in Industry 4.0 and discuss newer application areas. We hope that our findings pave the way for empowering and facilitating research in this domain, and assist decision-makers in their blockchain adoption and investment in Industry 4.0 and IIoT space.","['Industries', 'Medical services', 'Peer-to-peer computing', 'Internet of Things', 'Contracts', 'Production']","['Internet of Things', 'industry 40', 'industrial IoT', 'blockchain', 'smart contracts']"
"Approximate message passing (AMP) is a low-cost iterative signal recovery algorithm for linear system models. When the system transform matrix has independent identically distributed (IID) Gaussian entries, the performance of AMP can be asymptotically characterized by a simple scalar recursion called state evolution (SE). However, SE may become unreliable for other matrix ensembles, especially for ill-conditioned ones. This imposes limits on the applications of AMP. In this paper, we propose an orthogonal AMP (OAMP) algorithm based on de-correlated linear estimation (LE) and divergence-free non-linear estimation (NLE). The Onsager term in standard AMP vanishes as a result of the divergence-free constraint on NLE. We develop an SE procedure for OAMP and show numerically that the SE for OAMP is accurate for general unitarily-invariant matrices, including IID Gaussian matrices and partial orthogonal matrices. We further derive optimized options for OAMP and show that the corresponding SE fixed point coincides with the optimal performance obtained via the replica method. Our numerical results demonstrate that OAMP can be advantageous over AMP, especially for ill-conditioned matrices.","['Discrete cosine transforms', 'Estimation', 'Message passing', 'Algorithm design and analysis', 'Sparse matrices', 'Gaussian processes', 'Orthogonal matrices']","['Compressed sensing', 'approximate message passing (AMP)', 'replica method', 'state evolution', 'unitarily-invariant', 'IID Gaussian', 'partial orthogonal matrix']"
"With the accelerated development of Internet-of-Things (IoT), wireless sensor networks (WSNs) are gaining importance in the continued advancement of information and communication technologies, and have been connected and integrated with the Internet in vast industrial applications. However, given the fact that most wireless sensor devices are resource constrained and operate on batteries, the communication overhead and power consumption are therefore important issues for WSNs design. In order to efficiently manage these wireless sensor devices in a unified manner, the industrial authorities should be able to provide a network infrastructure supporting various WSN applications and services that facilitate the management of sensor-equipped real-world entities. This paper presents an overview of industrial ecosystem, technical architecture, industrial device management standards, and our latest research activity in developing a WSN management system. The key approach to enable efficient and reliable management of WSN within such an infrastructure is a cross-layer design of lightweight and cloud-based RESTful Web service.","['Urban areas', 'Wireless sensor networks', 'Monitoring', 'Industries', 'Standards', 'Security', 'Transportation']","['Internet-of-Things', 'device management', 'IEEE 802.15.4', 'RESTful', 'error correction coding (ECC)', 'cloud']"
"This paper presents the latest progress on cloud RAN (C-RAN) in the areas of centralization and virtualization. A C-RAN system centralizes the baseband processing resources into a pool and virtualizes soft base-band units on demand. The major challenges for C-RAN including front-haul and virtualization are analyzed with potential solutions proposed. Extensive field trials verify the viability of various front-haul solutions, including common public radio interface compression, single fiber bidirection and wavelength-division multiplexing. In addition, C-RANs facilitation of coordinated multipoint (CoMP) implementation is demonstrated with 50%-100% uplink CoMP gain observed in field trials. Finally, a test bed is established based on general purpose platform with assisted accelerators. It is demonstrated that this test bed can support multi-RAT, i.e., Time-Division Duplexing Long Term Evolution, Frequency-Division Duplexing Long Term Evolution, and Global System for Mobile Communications efficiently and presents similar performance to traditional systems.","['Optical fiber networks', 'Wavelength division multiplexing', 'Virtualization', 'Optical fiber devices', 'Radio access networks', 'Optical fiber testing', 'Mobile communication']","['C-RAN', 'CoMP', 'virtualization', 'cloud', 'front-haul']"
"Fog computing, an extension of cloud computing services to the edge of the network to decrease latency and network congestion, is a relatively recent research trend. Although both cloud and fog offer similar resources and services, the latter is characterized by low latency with a wider spread and geographically distributed nodes to support mobility and real-time interaction. In this paper, we describe the fog computing architecture and review its different services and applications. We then discuss security and privacy issues in fog computing, focusing on service and resource availability. Virtualization is a vital technology in both fog and cloud computing that enables virtual machines (VMs) to coexist in a physical server (host) to share resources. These VMs could be subject to malicious attacks or the physical server hosting it could experience system failure, both of which result in unavailability of services and resources. Therefore, a conceptual smart pre-copy live migration approach is presented for VM migration. Using this approach, we can estimate the downtime after each iteration to determine whether to proceed to the stop-and-copy stage during a system failure or an attack on a fog computing node. This will minimize both the downtime and the migration time to guarantee resource and service availability to the end users of fog computing. Last, future research directions are outlined.","['Edge computing', 'Cloud computing', 'Computer architecture', 'Real-time systems', 'Wireless sensor networks', 'Servers', 'Sensors']","['Cloud computing', 'edge computing', 'fog computing', 'live VM migration framework', 'virtualization']"
"Research on machine assisted text analysis follows the rapid development of digital media, and sentiment analysis is among the prevalent applications. Traditional sentiment analysis methods require complex feature engineering, and embedding representations have dominated leaderboards for a long time. However, the context-independent nature limits their representative power in rich context, hurting performance in Natural Language Processing (NLP) tasks. Bidirectional Encoder Representations from Transformers (BERT), among other pre-trained language models, beats existing best results in eleven NLP tasks (including sentence-level sentiment classification) by a large margin, which makes it the new baseline of text representation. As a more challenging task, fewer applications of BERT have been observed for sentiment classification at the aspect level. We implement three target-dependent variations of the BERT base model, with positioned output at the target terms and an optional sentence with the target built in. Experiments on three data collections show that our TD-BERT model achieves new state-of-the-art performance, in comparison to traditional feature engineering methods, embedding-based models and earlier applications of BERT. With the successful application of BERT in many NLP tasks, our experiments try to verify if its context-aware representation can achieve similar performance improvement in aspect-based sentiment analysis. Surprisingly, coupling it with complex neural networks that used to work well with embedding representations does not show much value, sometimes with performance below the vanilla BERT-FC implementation. On the other hand, incorporation of target information shows stable accuracy improvement, and the most effective way of utilizing that information is displayed through the experiment.","['Bit error rate', 'Task analysis', 'Neural networks', 'Sentiment analysis', 'Context modeling', 'Media']","['Deep learning', 'neural networks', 'sentiment analysis', 'BERT']"
"This paper presents end-to-end learning from spectrum data-an umbrella term for new sophisticated wireless signal identification approaches in spectrum monitoring applications based on deep neural networks. End-to-end learning allows to: 1) automatically learn features directly from simple wireless signal representations, without requiring design of hand-crafted expert features like higher order cyclic moments and 2) train wireless signal classifiers in one end-to-end step which eliminates the need for complex multi-stage machine learning processing pipelines. The purpose of this paper is to present the conceptual framework of end-to-end learning for spectrum monitoring and systematically introduce a generic methodology to easily design and implement wireless signal classifiers. Furthermore, we investigate the importance of the choice of wireless data representation to various spectrum monitoring tasks. In particular, two case studies are elaborated: 1) modulation recognition and 2) wireless technology interference detection. For each case study three convolutional neural networks are evaluated for the following wireless signal representations: temporal IQ data, the amplitude/phase representation, and the frequency domain representation. From our analysis, we prove that the wireless data representation impacts the accuracy depending on the specifics and similarities of the wireless signals that need to be differentiated, with different data representations resulting in accuracy variations of up to 29%. Experimental results show that using the amplitude/phase representation for recognizing modulation formats can lead to performance improvements up to 2% and 12% for medium to high SNR compared to IQ and frequency domain data, respectively. For the task of detecting interference, frequency domain representation outperformed amplitude/phase and IQ data representation up to 20%.","['Wireless communication', 'Wireless sensor networks', 'Monitoring', 'Machine learning', 'Interference', 'Modulation', 'Pipelines']","['Big spectrum data', 'spectrum monitoring', 'end-to-end learning', 'deep learning', 'convolutional neural networks', 'wireless signal identification', 'IoT']"
"We demonstrate use of iteratively pruned deep learning model ensembles for detecting pulmonary manifestations of COVID-19 with chest X-rays. This disease is caused by the novel Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) virus, also known as the novel Coronavirus (2019-nCoV). A custom convolutional neural network and a selection of ImageNet pretrained models are trained and evaluated at patient-level on publicly available CXR collections to learn modality-specific feature representations. The learned knowledge is transferred and fine-tuned to improve performance and generalization in the related task of classifying CXRs as normal, showing bacterial pneumonia, or COVID-19-viral abnormalities. The best performing models are iteratively pruned to reduce complexity and improve memory efficiency. The predictions of the best-performing pruned models are combined through different ensemble strategies to improve classification performance. Empirical evaluations demonstrate that the weighted average of the best-performing pruned models significantly improves performance resulting in an accuracy of 99.01% and area under the curve of 0.9972 in detecting COVID-19 findings on CXRs. The combined use of modality-specific knowledge transfer, iterative model pruning, and ensemble learning resulted in improved predictions. We expect that this model can be quickly adopted for COVID-19 screening using chest radiographs.","['Lung', 'COVID-19', 'Microorganisms', 'Computational modeling', 'National Institutes of Health', 'Predictive models']","['COVID-19', 'convolutional neural network', 'deep learning', 'ensemble', 'iterative pruning']"
"Nowadays synthetic aperture radar (SAR) and multiple-input-multiple-output (MIMO) antenna systems with the capability to radiate waves in more than one pattern and polarization are playing a key role in modern telecommunication and radar systems. This is possible with the use of antenna arrays as they offer advantages of high gain and beamforming capability, which can be utilized for controlling radiation pattern for electromagnetic (EM) interference immunity in wireless systems. However, with the growing demand for compact array antennas, the physical footprint of the arrays needs to be smaller and the consequent of this is severe degradation in the performance of the array resulting from strong mutual-coupling and crosstalk effects between adjacent radiating elements. This review presents a detailed systematic and theoretical study of various mutual-coupling suppression (decoupling) techniques with a strong focus on metamaterial (MTM) and metasurface (MTS) approaches. While the performance of systems employing antenna arrays can be enhanced by calibrating out the interferences digitally, however it is more efficient to apply decoupling techniques at the antenna itself. Previously various simple and cost-effective approaches have been demonstrated to effectively suppress unwanted mutual-coupling in arrays. Such techniques include the use of defected ground structure (DGS), parasitic or slot element, dielectric resonator antenna (DRA), complementary split-ring resonators (CSRR), decoupling networks, P.I.N or varactor diodes, electromagnetic bandgap (EBG) structures, etc. In this review, it is shown that the mutual-coupling reduction methods inspired By MTM and MTS concepts can provide a higher level of isolation between neighbouring radiating elements using easily realizable and cost-effective decoupling configurations that have negligible consequence on the array's characteristics such as bandwidth, gain and radiation efficiency, and physical footprint.","['Antenna arrays', 'MIMO communication', 'Mutual coupling', 'Synthetic aperture radar', 'Metamaterials']","['Decoupling methods', 'metamaterial (MTM)', 'metasurface (MTS)', 'multiple-input-multiple-output (MIMO)', 'synthetic aperture radar (SAR)', 'isolation enhancement', 'array antennas']"
"Providing reliable broadband wireless communications in high mobility environments, such as high-speed railway systems, remains one of the main challenges faced by the development of the next generation wireless systems. This paper provides a systematic review of high mobility communications. We first summarize a list of key challenges and opportunities in high mobility communication systems, then provide comprehensive reviews of techniques that can address these challenges and utilize the unique opportunities. The review covers a wide spectrum of communication operations, including the accurate modeling of high mobility channels, the transceiver structures that can exploit the properties of high mobility environments, the signal processing techniques that can harvest the benefits (e.g., Doppler diversity) and mitigate the impairments (e.g., carrier frequency offset, intercarrier interference, channel estimation errors) in high mobility systems, and the mobility management and network architectures that are designed specifically for high mobility systems. The survey focuses primarily on physical layer operations, which are affected the most by the mobile environment, with some additional discussions on higher layer operations, such as handover management and control-plane/user-plane decoupling, which are essential to high mobility operations. Future research directions on high mobility communications are summarized at the end of this paper.","['Doppler effect', 'Handover', 'Fading channels', 'OFDM', 'Wireless communication', 'Channel estimation', 'Mobility management']","['High mobility communications', 'fast time-varying fading', 'CFO', 'ICI', 'Doppler diversity', 'mobility management']"
"Internet of Things (IoT) connects sensing devices to the Internet for the purpose of exchanging information. Location information is one of the most crucial pieces of information required to achieve intelligent and context-aware IoT systems. Recently, positioning and localization functions have been realized in a large amount of IoT systems. However, security and privacy threats related to positioning in IoT have not been sufficiently addressed so far. In this paper, we survey solutions for improving the robustness, security, and privacy of location-based services in IoT systems. First, we provide an in-depth evaluation of the threats and solutions related to both global navigation satellite system (GNSS) and non-GNSS-based solutions. Second, we describe certain cryptographic solutions for security and privacy of positioning and location-based services in IoT. Finally, we discuss the state-of-the-art of policy regulations regarding security of positioning solutions and legal instruments to location data privacy in detail. This survey paper addresses a broad range of security and privacy aspects in IoT-based positioning and localization from both technical and legal points of view and aims to give insight and recommendations for future IoT systems providing more robust, secure, and privacy-preserving location-based services.","['Security', 'Robustness', 'Satellites', 'Receivers', 'Privacy', 'Law', 'Terrestrial atmosphere']","['Positioning', 'wireless localization', 'navigation', 'Internet of Things', 'GNSS', 'vulnerabilities', 'security', 'privacy', 'cryptography', 'trustworthiness', 'literature study']"
"Smart cities contain intelligent things which can intelligently automatically and collaboratively enhance life quality, save people's lives, and act a sustainable resource ecosystem. To achieve these advanced collaborative technologies such as drones, robotics, artificial intelligence, and Internet of Things (IoT) are required to increase the smartness of smart cities by improving the connectivity, energy efficiency, and quality of services (QoS). Therefore, collaborative drones and IoT play a vital role in supporting a lot of smart-city applications such as those involved in communication, transportation, agriculture,safety and security, disaster mitigation, environmental protection, service delivery, energy saving, e-waste reduction, weather monitoring, healthcare, etc. This paper presents a survey of the potential techniques and applications of collaborative drones and IoT which have recently been proposed in order to increase the smartness of smart cities. It provides a comprehensive overview highlighting the recent and ongoing research on collaborative drone and IoT in improving the real-time application of smart cities. This survey is different from previous ones in term of breadth, scope, and focus. In particular, we focus on the new concept of collaborative drones and IoT for improving smart-city applications. This survey attempts to show how collaborative drones and IoT improve the smartness of smart cities based on data collection, privacy and security, public safety, disaster management, energy consumption and quality of life in smart cities. It mainly focuses on the measurement of the smartness of smart cities, i.e., environmental aspects, life quality, public safety, and disaster management.","['Drones', 'Smart cities', 'Internet of Things', 'Collaboration', 'Monitoring', 'Safety', 'Security']","['ICT', 'smart city', 'energy consumption', 'smart drone', 'IoT', 'pollutions', 'gathering data', 'IoD', 'disaster', 'public safety', 'security and privacy', 'collaborative drone', 'IoT']"
"Deep learning is a branch of artificial intelligence. In recent years, with the advantages of automatic learning and feature extraction, it has been widely concerned by academic and industrial circles. It has been widely used in image and video processing, voice processing, and natural language processing. At the same time, it has also become a research hotspot in the field of agricultural plant protection, such as plant disease recognition and pest range assessment, etc. The application of deep learning in plant disease recognition can avoid the disadvantages caused by artificial selection of disease spot features, make plant disease feature extraction more objective, and improve the research efficiency and technology transformation speed. This review provides the research progress of deep learning technology in the field of crop leaf disease identification in recent years. In this paper, we present the current trends and challenges for the detection of plant leaf disease using deep learning and advanced imaging techniques. We hope that this work will be a valuable resource for researchers who study the detection of plant diseases and insect pests. At the same time, we also discussed some of the current challenges and problems that need to be resolved.","['Diseases', 'Deep learning', 'Feature extraction', 'Image recognition', 'Plants (biology)', 'Agriculture', 'Image color analysis']","['Deep learning', 'plant leaf disease detection', 'visualization', 'small sample', 'hyperspectral imaging']"
"Motor bearing is subjected to the joint effects of much more loads, transmissions, and shocks that cause bearing fault and machinery breakdown. A vibration signal analysis method is the most popular technique that is used to monitor and diagnose the fault of motor bearing. However, the application of the vibration signal analysis method for motor bearing is very limited in engineering practice. In this paper, on the basis of comparing fault feature extraction by using empirical wavelet transform (EWT) and Hilbert transform with the theoretical calculation, a new motor bearing fault diagnosis method based on integrating EWT, fuzzy entropy, and support vector machine (SVM) called EWTFSFD is proposed. In the proposed method, a novel signal processing method called EWT is used to decompose vibration signal into multiple components in order to extract a series of amplitude modulated-frequency modulated (AM-FM) components with supporting Fourier spectrum under an orthogonal basis. Then, fuzzy entropy is utilized to measure the complexity of vibration signal, reflect the complexity changes of intrinsic oscillation, and compute the fuzzy entropy values of AM-FM components, which are regarded as the inputs of the SVM model to train and construct an SVM classifier for fulfilling fault pattern recognition. Finally, the effectiveness of the proposed method is validated by using the simulated signal and real motor bearing vibration signals. The experiment results show that the EWT outperforms empirical mode decomposition for decomposing the signal into multiple components, and the proposed EWTFSFD method can accurately and effectively achieve the fault diagnosis of motor bearing.","['Fault diagnosis', 'Wavelet transforms', 'Entropy', 'Vibrations', 'Feature extraction', 'Support vector machines']","['Motor bearing', 'fault diagnosis', 'empirical wavelet transform', 'fuzzy entropy', 'support vector machine', 'Fourier spectrum segmentation', 'AM-FM components']"
"Fifth-generation (5G) cellular networks will almost certainly operate in the high-bandwidth, underutilized millimeter-wave (mmWave) frequency spectrum, which offers the potentiality of high-capacity wireless transmission of multi-gigabit-per-second (Gbps) data rates. Despite the enormous available bandwidth potential, mmWave signal transmissions suffer from fundamental technical challenges like severe path loss, sensitivity to blockage, directivity, and narrow beamwidth, due to its short wavelengths. To effectively support system design and deployment, accurate channel modeling comprising several 5G technologies and scenarios is essential. This survey provides a comprehensive overview of several emerging technologies for 5G systems, such as massive multiple-input multiple-output (MIMO) technologies, multiple access technologies, hybrid analog-digital precoding and combining, non-orthogonal multiple access (NOMA), cell-free massive MIMO, and simultaneous wireless information and power transfer (SWIPT) technologies. These technologies induce distinct propagation characteristics and establish specific requirements on 5G channel modeling. To tackle these challenges, we first provide a survey of existing solutions and standards and discuss the radio-frequency (RF) spectrum and regulatory issues for mmWave communications. Second, we compared existing wireless communication techniques like sub-6-GHz WiFi and sub-6 GHz 4G LTE over mmWave communications which come with benefits comprising narrow beam, high signal quality, large capacity data transmission, and strong detection potential. Third, we describe the fundamental propagation characteristics of the mmWave band and survey the existing channel models for mmWave communications. Fourth, we track evolution and advancements in hybrid beamforming for massive MIMO systems in terms of system models of hybrid precoding architectures, hybrid analog and digital precoding/combining matrices, with the potential antenna configuration scenarios and mmWave channel estimation (CE) techniques. Fifth, we extend the scope of the discussion by including multiple access technologies for mmWave systems such as non-orthogonal multiple access (NOMA) and space-division multiple access (SDMA), with limited RF chains at the base station. Lastly, we explore the integration of SWIPT in mmWave massive MIMO systems, with limited RF chains, to realize spectrally and energy-efficient communications.","['5G mobile communication', 'Millimeter wave communication', 'NOMA', 'MIMO communication', 'Wireless communication', 'Precoding', 'Radio frequency']","['Millimeter wave communications', 'propagation', 'channel measurements', 'channel models', 'MIMO', 'hybrid precoding', 'non-orthogonal multiple access (NOMA)', 'multiple access techniques', 'simultaneous wireless information and power transfer (SWIPT)', 'RF energy harvesting']"
"With the development of wireless communication technology, the need for bandwidth is increasing continuously, and the growing need makes wireless spectrum resources more and more scarce. Cognitive radio (CR) has been identified as a promising solution for the spectrum scarcity, and its core idea is the dynamic spectrum access. It can dynamically utilize the idle spectrum without affecting the rights of primary users, so that multiple services or users can share a part of the spectrum, thus achieving the goal of avoiding the high cost of spectrum resetting and improving the utilization of spectrum resources. In order to meet the critical requirements of the fifth generation (5G) mobile network, especially the Wider-Coverage , Massive-Capacity , Massive-Connectivity , and Low-Latency four application scenarios, the spectrum range used in 5G will be further expanded into the full spectrum era, possibly from 1 GHz to 100 GHz. In this paper, we conduct a comprehensive survey of CR technology and focus on the current significant research progress in the full spectrum sharing towards the four scenarios. In addition, the key enabling technologies that may be closely related to the study of 5G in the near future are presented in terms of full-duplex spectrum sensing, spectrum-database based spectrum sensing, auction based spectrum allocation, carrier aggregation based spectrum access. Subsequently, other issues that play a positive role for the development research and practical application of CR, such as common control channel, energy harvesting, non-orthogonal multiple access, and CR based aeronautical communication are discussed. The comprehensive overview provided by this survey is expected to help researchers develop CR technology in the field of 5G further.","['5G mobile communication', 'Sensors', 'Resource management', 'Internet', 'Cognitive radio']","['5G', 'cognitive radio', 'spectrum sharing', 'full-duplex spectrum sensing', 'carrier aggregation', 'energy harvesting']"
"A compact, high performance, and novel-shaped ultra-wideband (UWB) multiple-input multiple-output (MIMO) antenna with low mutual coupling is presented in this paper. The proposed antenna consists of two radiating elements with shared ground plane having an area of 50 x 30 mm 2 . F-shaped stubs are introduced in the shared ground plane of the proposed antenna to produce high isolation between the MIMO antenna elements. The designed MIMO antenna has very low mutual coupling of (S 21 <; -20 dB), low envelop correlation coefficient (ECC <; 0.04), high diversity gain (DG > 7.4 dB), high multiplexing efficiency (η Mux > -3.5), and high peak gain over the entire UWB frequencies. The antenna performance is studied in terms of S-Parameters, radiation properties, peak gain, diversity gain, envelop correlation coefficient, and multiplexing efficiency. A good agreement between the simulated and measured results is observed.","['MIMO communication', 'Mutual coupling', 'Ultra wideband antennas', 'Antenna measurements', 'Diversity methods', 'Multiplexing']","['MIMO antenna', 'diversity gain', 'multiplexing efficiency', 'microstrip patch']"
"Photovoltaic power generation forecasting is an important topic in the field of sustainable power system design, energy conversion management, and smart grid construction. Difficulties arise while the generated PV power is usually unstable due to the variability of solar irradiance, temperature, and other meteorological factors. In this paper, a hybrid ensemble deep learning framework is proposed to forecast short-term photovoltaic power generation in a time series manner. Two LSTM neural networks are employed working on temperature and power outputs forecasting, respectively. The forecasting results are flattened and combined with a fully connected layer to enhance forecasting accuracy. Moreover, we adopted the attention mechanism for the two LSTM neural networks to adaptively focus on input features that are more significant in forecasting. Comprehensive experiments are conducted with recently collected real-world photovoltaic power generation datasets. Three error metrics were adopted to compare the forecasting results produced by attention LSTM model with state-of-art methods, including the persistent model, the auto-regressive integrated moving average model with exogenous variable (ARIMAX), multi-layer perceptron (MLP), and the traditional LSTM model in all four seasons and various forecasting horizons to show the effectiveness and robustness of the proposed method.","['Forecasting', 'Predictive models', 'Logic gates', 'Power generation', 'Time series analysis', 'Neural networks', 'Deep learning']","['PV power generation', 'short-term forecasting', 'long short term memory', 'attention mechanism']"
"Since the early 1990s, a large number of chaos-based communication systems have been proposed exploiting the properties of chaotic waveforms. The motivation lies in the significant advantages provided by this class of non-linear signals. For this aim, many communication schemes and applications have been specially designed for chaos-based communication systems where energy, data rate, and synchronization awareness are considered in most designs. Recently, the major focus, however, has been given to the non-coherent chaos-based systems to benefit from the advantages of chaotic signals and non-coherent detection and to avoid the use of chaotic synchronization, which suffers from weak performance in the presence of additive noise. This paper presents a comprehensive survey of the entire wireless radio frequency chaos-based communication systems. First, it outlines the challenges of chaos implementations and synchronization methods, followed by comprehensive literature review and analysis of chaos-based coherent techniques and their applications. In the second part of the survey, we offer a taxonomy of the current literature by focusing on non-coherent detection methods. For each modulation class, this paper categorizes different transmission techniques by elaborating on its modulation, receiver type, data rate, complexity, energy efficiency, multiple access scheme, and performance. In addition, this survey reports on the analysis of tradeoff between different chaos-based communication systems. Finally, several concluding remarks are discussed.","['Chaotic communication', 'Synchronization', 'Coherent systems', 'Telecommunication services', 'Performance evaluation', 'Additive noise', 'Noise measurement', 'Nonlinear systems', 'Modulation']","['Chaos-based communication systems', 'chaos implementation', 'chaotic synchronization', 'coherent systems', 'noncoherent systems', 'applications', 'performance analysis']"
"An Internet of Vehicles (IoV) allows forming a self-organized network and broadcasting messages for the vehicles on roads. However, as the data are transmitted in an insecure network, it is essential to use an authentication mechanism to protect the privacy of vehicle users. Recently, Ying et al. proposed an authentication protocol for IoV and claimed that the protocol could resist various attacks. Unfortunately, we discovered that their protocol suffered from an offline identity guessing attack, location spoofing attack, and replay attack, and consumed a considerable amount of time for authentication. To resolve these shortcomings, we propose an improved protocol. In addition, we provide a formal proof to the proposed protocol to demonstrate that our protocol is indeed secure. Compared with previous methods, the proposed protocol performs better in terms of security and performance.","['Protocols', 'Authentication', 'Vehicular ad hoc networks', 'Smart cards', 'Cloud computing', 'Privacy']","['Internet of Vehicles', 'authentication', 'anonymity', 'smart card']"
"Internet of Things (IoT) is an emerging concept, which aims to connect billions of devices with each other. The IoT devices sense, collect, and transmit important information from their surroundings. This exchange of very large amount of information amongst billions of devices creates a massive energy need. Green IoT envisions the concept of reducing the energy consumption of IoT devices and making the environment safe. Inspired by achieving a sustainable environment for IoT, we first give the overview of green IoT and the challenges that are faced due to excessive usage of energy hungry IoT devices. We then discuss and evaluate the strategies that can be used to minimize the energy consumption in IoT, such as designing energy efficient datacenters, energy efficient transmission of data from sensors, and design of energy efficient policies. Moreover, we critically analyze the green IoT strategies and propose five principles that can be adopted to achieve green IoT. Finally, we consider a case study of very important aspect of IoT, i.e., smart phones and we provide an easy and concise view for improving the current practices to make the IoT greener for the world in 2020 and beyond.","['Green products', 'Air pollution', 'Energy efficiency', 'Radiofrequency identification', 'Internet', 'Energy consumption']","['Internet of things', 'green IoT', 'datacenter', 'green computing', 'smart phones']"
"In recent years, the emergence of blockchain technology (BT) has become a unique, most disruptive, and trending technology. The decentralized database in BT emphasizes data security and privacy. Also, the consensus mechanism in it makes sure that data is secured and legitimate. Still, it raises new security issues such as majority attack and double-spending. To handle the aforementioned issues, data analytics is required on blockchain based secure data. Analytics on these data raises the importance of arisen technology Machine Learning (ML). ML involves the rational amount of data to make precise decisions. Data reliability and its sharing are very crucial in ML to improve the accuracy of results. The combination of these two technologies (ML and BT) can provide highly precise results. In this paper, we present a detailed study on ML adoption for making BT-based smart applications more resilient against attacks. There are various traditional ML techniques, for instance, Support Vector Machines (SVM), clustering, bagging, and Deep Learning (DL) algorithms such as Convolutional Neural Network (CNN) and Long short-term memory (LSTM) can be used to analyse the attacks on a blockchain-based network. Further, we include how both the technologies can be applied in several smart applications such as Unmanned Aerial Vehicle (UAV), Smart Grid (SG), healthcare, and smart cities. Then, future research issues and challenges are explored. At last, a case study is presented with a conclusion.","['Blockchain', 'Security', 'Machine learning', 'Taxonomy', 'Databases', 'Prediction algorithms', 'Malware']","['Blockchain', 'machine learning', 'smart grid', 'data security and privacy', 'data analytics', 'smart applications']"
"Brain cancer classification is an important step that depends on the physician's knowledge and experience. An automated tumor classification system is very essential to support radiologists and physicians to identify brain tumors. However, the accuracy of current systems needs to be improved for suitable treatments. In this paper, we propose a hybrid feature extraction method with a regularized extreme learning machine (RELM) for developing an accurate brain tumor classification approach. The approach starts by preprocessing the brain images by using a min-max normalization rule to enhance the contrast of brain edges and regions. Then, the brain tumor features are extracted based on a hybrid method of feature extraction. Finally, a RELM is used for classifying the type of brain tumor. To evaluate and compare the proposed approach, a set of experiments is conducted on a new public dataset of brain images. The experimental results proved that the approach is more effective compared with the existing state-of-the-art approaches, and the performance in terms of classification accuracy improved from 91.51% to 94.233% for the experiment of the random holdout technique.","['Feature extraction', 'Tumors', 'Brain', 'Image segmentation', 'Gabor filters', 'Principal component analysis', 'Training']","['Brain tumor classification', 'hybrid feature extraction', 'NGIST features', 'PCA', 'regularized extreme learning machine']"
"Forecasting of fast fluctuated and high-frequency financial data is always a challenging problem in the field of economics and modelling. In this study, a novel hybrid model with the strength of fractional order derivative is presented with their dynamical features of deep learning, long-short term memory (LSTM) networks, to predict the abrupt stochastic variation of the financial market. Stock market prices are dynamic, highly sensitive, nonlinear and chaotic. There are different techniques for forecast prices in the time-variant domain and due to variability and uncertain behavior in stock prices, traditional methods, such as data mining, statistical approaches, and non-deep neural networks models are not suited for prediction and generalized forecasting stock prices. While autoregressive fractional integrated moving average (ARFIMA) model provides a flexible tool for classes of long-memory models. The advancement of machine learning-based deep non-linear modelling confirms that the hybrid model efficiently extracts profound features and model non-linear functions. LSTM networks are a special kind of recurrent neural network (RNN) that map sequences of input observations to output observations with capabilities of long-term dependencies. A novel ARFIMA-LSTM hybrid recurrent network is presented in which ARFIMA model-based filters having the linear tendencies better than ARIMA model in the data and passes the residual to the LSTM model that captures nonlinearity in the residual values with the help of exogenous dependent variables. The model not only minimizes the volatility problem but also overcome the over fitting problem of neural networks. The model is evaluated using PSX company data of the stock market based on RMSE, MSE and MAPE along with a comparison of ARIMA, LSTM model and generalized regression radial basis neural network (GRNN) ensemble method independently. The forecasting performance indicates the effectiveness of the proposed AFRIMA-LSTM hybrid model to improve around 80% accuracy on RMSE as compared to traditional forecasting counterparts.","['Data models', 'Predictive models', 'Forecasting', 'Time series analysis', 'Neural networks', 'Stock markets', 'Mathematical model']","['ARIMA model', 'ARFIMA model', 'GARCH model', 'RNN', 'LSTM model', 'RMSE', 'MSE', 'MAPE']"
"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper, we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.","['Task analysis', 'Feature extraction', 'Computational modeling', 'Data models', 'Machine learning', 'Learning systems', 'Natural language processing']","['Contrastive learning', 'representation learning', 'self-supervised learning', 'unsupervised learning', 'deep learning', 'machine learning']"
"Electroencephalogram (EEG) comprises valuable details related to the different physiological state of the brain. In this paper, a framework is offered for detecting the epileptic seizures from EEG data recorded from normal subjects and epileptic patients. This framework is based on a discrete wavelet transform (DWT) analysis of EEG signals using linear and nonlinear classifiers. The performance of the 14 different combinations of two-class epilepsy detection is studied using naïve Bayes (NB) and k-nearest neighbor (k-NN) classifiers for the derived statistical features from DWT. It has been found that the NB classifier performs better and shows an accuracy of 100% for the individual and combined statistical features derived from the DWT values of normal eyes open and epileptic EEG data provided by the University of Bonn, Germany. It has been found that the computation time of NB classifier is lesser than k-NN to provide better accuracy. So, the detection of an epileptic seizure based on DWT statistical features using NB classifiers is more suitable in real time for a reliable, automatic epileptic seizure detection system to enhance the patient's care and the quality of life.","['Discrete wavelet transforms', 'Electroencephalography', 'Feature extraction', 'Bayes methods', 'Physiology', 'Brain models', 'Seizures']","['Electroencephalograms (EEG)', 'epilepsy', 'discrete wavelet transform (DWT)', 'naïve Bayes (NB)', 'k-nearest neighbor (k-NN)']"
"Index modulation has become a promising technique in the context of orthogonal frequency division multiplexing (OFDM), whereby the specific activation of the frequency domain subcarriers is used for implicitly conveying extra information, hence improving the achievable throughput at a given bit error ratio (BER) performance. In this paper, a dual-mode OFDM technique (DM-OFDM) is proposed, which is combined with index modulation and enhances the attainable throughput of conventional index-modulation-based OFDM. In particular, the subcarriers are divided into several subblocks, and in each subblock, all the subcarriers are partitioned into two groups, modulated by a pair of distinguishable modem-mode constellations, respectively. Hence, the information bits are conveyed not only by the classic constellation symbols, but also implicitly by the specific activated subcarrier indices, representing the subcarriers' constellation mode. At the receiver, a maximum likelihood (ML) detector and a reduced-complexity near optimal log-likelihood ratio-based detector are invoked for demodulation. The minimum distance between the different legitimate realizations of the OFDM subblocks is calculated for characterizing the performance of DM-OFDM. Then, the associated theoretical analysis based on the pairwise error probability is carried out for estimating the BER of DM-OFDM. Furthermore, the simulation results confirm that at a given throughput, DM-OFDM achieves a considerably better BER performance than other OFDM systems using index modulation, while imposing the same or lower computational complexity. The results also demonstrate that the performance of the proposed low-complexity detector is indistinguishable from that of the ML detector, provided that the system's signal to noise ratio is sufficiently high.","['Indexes', 'OFDM', 'Detectors', 'Phase shift keying', 'Throughput', 'Signal to noise ratio']","['Orthogonal frequency division multiplexing (OFDM)', 'index modulation', 'index pattern', 'constellation', 'maximum likelihood detection', 'log-likelihood ratio based detection']"
"Heart disease is one of the complex diseases and globally many people suffered from this disease. On time and efficient identification of heart disease plays a key role in healthcare, particularly in the field of cardiology. In this article, we proposed an efficient and accurate system to diagnosis heart disease and the system is based on machine learning techniques. The system is developed based on classification algorithms includes Support vector machine, Logistic regression, Artificial neural network, K-nearest neighbor, Naïve bays, and Decision tree while standard features selection algorithms have been used such as Relief, Minimal redundancy maximal relevance, Least absolute shrinkage selection operator and Local learning for removing irrelevant and redundant features. We also proposed novel fast conditional mutual information feature selection algorithm to solve feature selection problem. The features selection algorithms are used for features selection to increase the classification accuracy and reduce the execution time of classification system. Furthermore, the leave one subject out cross-validation method has been used for learning the best practices of model assessment and for hyperparameter tuning. The performance measuring metrics are used for assessment of the performances of the classifiers. The performances of the classifiers have been checked on the selected features as selected by features selection algorithms. The experimental results show that the proposed feature selection algorithm (FCMIM) is feasible with classifier support vector machine for designing a high-level intelligent system to identify heart disease. The suggested diagnosis system (FCMIM-SVM) achieved good accuracy as compared to previously proposed methods. Additionally, the proposed system can easily be implemented in healthcare for the identification of heart disease.","['Feature extraction', 'Diseases', 'Machine learning', 'Heart', 'Prediction algorithms', 'Machine learning algorithms', 'Support vector machines']","['Heart disease classification', 'features selection', 'disease diagnosis', 'intelligent system', 'medical data analytics']"
"With the sharp increase in the number of intelligent devices, the Internet of Things (IoT) has gained more and more attention and rapid development in recent years. It effectively integrates the physical world with the Internet over existing network infrastructure to facilitate sharing data among intelligent devices. However, its complex and large-scale network structure brings new security risks and challenges to IoT systems. To ensure the security of data, traditional access control technologies are not suitable to be directly used for implementing access control in IoT systems because of their complicated access management and the lack of credibility due to centralization. In this paper, we proposed a novel attribute-based access control scheme for IoT systems, which simplifies greatly the access management. We use blockchain technology to record the distribution of attributes in order to avoid single point failure and data tampering. The access control process has also been optimized to meet the need for high efficiency and lightweight calculation for IoT devices. The security and performance analysis show that our scheme could effectively resist multiple attacks and be efficiently implemented in IoT systems.","['Blockchain', 'Internet of Things', 'Authorization', 'Authentication']","['Access control', 'attribute-based access control', 'blockchain', 'consortium blockchain', 'Internet of Things']"
This paper introduces a novel algorithm that increases the efficiency of the current cloud-based smart-parking system and develops a network architecture based on the Internet-of-Things technology. This paper proposed a system that helps users automatically find a free parking space at the least cost based on new performance metrics to calculate the user parking cost by considering the distance and the total number of free places in each car park. This cost will be used to offer a solution of finding an available parking space upon a request by the user and a solution of suggesting a new car park if the current car park is full. The simulation results show that the algorithm helps improve the probability of successful parking and minimizes the user waiting time. We also successfully implemented the proposed system in the real world.,"['Smart systems', 'Performance evaluation', 'Algorithm design and analysis', 'Cloud computing', 'Internet of things']","['Smart-parking system', 'performance metrics']"
"Twitter sentiment analysis offers organizations ability to monitor public feeling towards the products and events related to them in real time. The first step of the sentiment analysis is the text pre-processing of Twitter data. Most existing researches about Twitter sentiment analysis are focused on the extraction of new sentiment features. However, to select the pre-processing method is ignored. This paper discussed the effects of text pre-processing method on sentiment classification performance in two types of classification tasks, and summed up the classification performances of six pre-processing methods using two feature models and four classifiers on five Twitter datasets. The experiments show that the accuracy and F1-measure of Twitter sentiment classification classifier are improved when using the pre-processing methods of expanding acronyms and replacing negation, but barely changes when removing URLs, removing numbers or stop words. The Naive Bayes and Random Forest classifiers are more sensitive than Logistic Regression and support vector machine classifiers when various pre-processing methods were applied.",[],[]
"Crowdsensing applications utilize the pervasive smartphone users to collect large-scale sensing data efficiently. The quality of sensing data depends on the participation of highly skilled users. To motivate these skilled users to participate, they should receive enough rewards for compensating their resource consumption. Available incentive mechanisms mainly consider the truthfulness of the mechanism, but mostly ignore the issues of security and privacy caused by a “trustful” center. In this paper, we propose a privacy-preserving blockchain incentive mechanism in crowdsensing applications, in which a cryptocurrency built on blockchains is used as a secure incentive way. High quality contributors will get their payments that are recorded in transaction blocks. The miners will verify the transaction according to the sensing data assessment criteria published by the server. As the transaction information can disclose users’ privacy, a node cooperation verification approach is proposed to achieve k -anonymity privacy protection. Through theoretical analysis and simulation experiments, we show the feasibility and security of our incentive mechanism.","['Sensors', 'Servers', 'Task analysis', 'Data privacy', 'Privacy']","['Blockchain', 'crowdsensing', 'incentive mechanism', 'node cooperation', 'privacy-preserving', 'signcryption']"
"Diabetes, also known as chronic illness, is a group of metabolic diseases due to a high level of sugar in the blood over a long period. The risk factor and severity of diabetes can be reduced significantly if the precise early prediction is possible. The robust and accurate prediction of diabetes is highly challenging due to the limited number of labeled data and also the presence of outliers (or missing values) in the diabetes datasets. In this literature, we are proposing a robust framework for diabetes prediction where the outlier rejection, filling the missing values, data standardization, feature selection, K-fold cross-validation, and different Machine Learning (ML) classifiers (k-nearest Neighbour, Decision Trees, Random Forest, AdaBoost, Naive Bayes, and XGBoost) and Multilayer Perceptron (MLP) were employed. The weighted ensembling of different ML models is also proposed, in this literature, to improve the prediction of diabetes where the weights are estimated from the corresponding Area Under ROC Curve (AUC) of the ML model. AUC is chosen as the performance metric, which is then maximized during hyperparameter tuning using the grid search technique. All the experiments, in this literature, were conducted under the same experimental conditions using the Pima Indian Diabetes Dataset. From all the extensive experiments, our proposed ensembling classifier is the best performing classifier with the sensitivity, specificity, false omission rate, diagnostic odds ratio, and AUC as 0.789, 0.934, 0.092, 66.234, and 0.950 respectively which outperforms the state-of-the-art results by 2.00 % in AUC. Our proposed framework for the diabetes prediction outperforms the other methods discussed in the article. It can also provide better results on the same dataset which can lead to better performance in diabetes prediction. Our source code for diabetes prediction is made publicly available.","['Diabetes', 'Measurement', 'Maximum likelihood estimation', 'Machine learning', 'Standardization', 'Feature extraction', 'Predictive models']","['Diabetes prediction', 'ensembling classifier', 'machine learning', 'multilayer perceptron', 'missing values and outliers', 'Pima Indian Diabetic dataset']"
"The strengthening of electric energy security and the reduction of greenhouse gas emissions have gained enormous momentum in previous decades. The integration of large-scale intermittent renewable energy resources (RER) like wind energy into the existing electricity grids has increased significantly in the last decade. However, this integration poses many operational and control challenges that hamper the reliable and stable operation of the grids. This article aims to review the reported challenges caused by the integration of wind energy and the proposed solutions methodologies. Among the various challenges, the generation uncertainty, power quality issues, angular and voltage stability, reactive power support, and fault ride-through capability are reviewed and discussed. Besides, socioeconomic, environmental, and electricity market challenges due to the grid integration of wind power are also investigated. Many of the solutions used and proposed to mitigate the impact of these challenges, such as energy storage systems, wind energy policy, and grid codes, are also reviewed and discussed. This paper will assist the enthusiastic readers in seeing the full picture of wind energy integration challenges. It also puts in the hands of policymakers all aspects of the challenges so that they can adopt sustainable policies that support and overcome the difficulties facing the integration of wind energy into electricity grids.","['Power system stability', 'Wind turbines', 'Wind energy', 'Wind power generation', 'Reactive power', 'Wind forecasting', 'Mathematical model']","['Angular stability', 'energy storage system', 'fault ride-through capability', 'frequency response', 'grid codes', 'reactive power support', 'voltage stability', 'wind intermittency']"
"Healthcare is undergoing a rapid transformation from traditional hospital and specialist focused approach to a distributed patient-centric approach. Advances in several technologies fuel this rapid transformation of healthcare vertical. Among various technologies, communication technologies have enabled to deliver personalized and remote healthcare services. At present, healthcare widely uses the existing 4G network and other communication technologies for smart healthcare applications and are continually evolving to accommodate the needs of future intelligent healthcare applications. As the smart healthcare market expands the number of applications connecting to the network will generate data that will vary in size and formats. This will place complex demands on the network in terms of bandwidth, data rate, and latency, among other factors. As this smart healthcare market matures, the connectivity needs for a large number of devices and machines with sensor-based applications in hospitals will necessitate the need to implement Massive-Machine Type Communication. Further use cases such as remote surgeries and Tactile Internet will spur the need for Ultra Reliability and Low Latency Communications or Critical Machine Type Communication. The existing communication technologies are unable to fulfill the complex and dynamic need that is put on the communication networks by the diverse smart healthcare applications. Therefore, the emerging 5G network is expected to support smart healthcare applications, which can fulfill most of the requirements such as ultra-low latency, high bandwidth, ultra-high reliability, high density, and high energy efficiency. The future smart healthcare networks are expected to be a combination of the 5G and IoT devices which are expected to increase cellular coverage, network performance and address security-related concerns. This paper provides a state-of-the-art review of the 5G and IoT enabled smart healthcare, Taxonomy, research trends, challenges, and future research directions.","['Medical services', '5G mobile communication', 'Computer architecture', 'Internet of Things', 'Monitoring', 'Wireless communication', 'Biomedical monitoring']","['5G', 'smart healthcare', 'software-defined network', 'network function virtualization', 'the Internet of Things (IoT)', 'device-to-device (D2D)', 'ultra reliability and low latency communications']"
"Network slicing is born as an emerging business to operators by allowing them to sell the customized slices to various tenants at different prices. In order to provide better-performing and costefficient services, network slicing involves challenging technical issues and urgently looks forward to intelligent innovations to make the resource management consistent with users' activities per slice. In that regard, deep reinforcement learning (DRL), which focuses on how to interact with the environment by trying alternative actions and reinforcing the tendency actions producing more rewarding consequences, is assumed to be a promising solution. In this paper, after briefly reviewing the fundamental concepts of DRL, we investigate the application of DRL in solving some typical resource management for network slicing scenarios, which include radio resource slicing and priority-based core network slicing, and demonstrate the advantage of DRL over several competing schemes through extensive simulations. Finally, we also discuss the possible challenges to apply DRL in network slicing from a general perspective.","['Network slicing', 'Resource management', 'Neural networks', '5G mobile communication', 'Quality of experience']","['Deep reinforcement learning', 'network slicing', 'neural networks', 'Q-learning', 'resource management']"
"Synthetic aperture radar (SAR) images have been widely used for ship monitoring. The traditional methods of SAR ship detection are difficult to detect small scale ships and avoid the interference of inshore complex background. Deep learning detection methods have shown great performance on various object detection tasks recently but using deep learning methods for SAR ship detection does not show an excellent performance it should have. One of the important reasons is that there is no effective model to handle the detection of multiscale ships in multiresolution SAR images. Another important reason is it is difficult to handle multiscene SAR ship detection including offshore and inshore, especially it cannot effectively distinguish between inshore complex background and ships. In this paper, we propose a densely connected multiscale neural network based on faster-RCNN framework to solve multiscale and multiscene SAR ship detection. Instead of using a single feature map to generate proposals, we densely connect one feature map to every other feature maps from top to down and generate proposals from each fused feature map. In addition, we propose a training strategy to reduce the weight of easy examples in the loss function, so that the training process more focus on the hard examples to reduce false alarm. Experiments on expanded public SAR ship detection dataset, verify the proposed method can achieve an excellent performance on multiscale SAR ship detection in multiscene.","['Marine vehicles', 'Feature extraction', 'Synthetic aperture radar', 'Proposals', 'Object detection', 'Machine learning', 'Image resolution']","['Ship detection', 'multiscale', 'neural network', 'synthetic aperture radar (SAR)']"
"Nowadays, in the international scientific community of machine learning, there exists an enormous discussion about the use of black-box models or explainable models; especially in practical problems. On the one hand, a part of the community defends that black-box models are more accurate than explainable models in some contexts, like image preprocessing. On the other hand, there exist another part of the community alleging that explainable models are better than black-box models because they can obtain comparable results and also they can explain these results in a language close to a human expert by using patterns. In this paper, advantages and weaknesses for each approach are shown; taking into account a state-of-the-art review for both approaches, their practical applications, trends, and future challenges. This paper shows that both approaches are suitable for solving practical problems, but experts in machine learning need to understand the input data, the problem to solve, and the best way for showing the output data before applying a machine learning model. Also, we propose some ideas for fusing both, explainable and black-box, approaches to provide better solutions to experts in real-world domains. Additionally, we show one way to measure the effectiveness of the applied machine learning model by using expert opinions jointly with statistical methods. Throughout this paper, we show the impact of using explainable and black-box models on the security and medical applications.","['Machine learning', 'Mathematical model', 'Biological system modeling', 'Gallium nitride', 'Biological neural networks', 'Statistical analysis', 'Computational modeling']","['Black-box', 'white-box', 'explainable artificial intelligence', 'deep learning']"
"Credit card fraud is a serious problem in financial services. Billions of dollars are lost due to credit card fraud every year. There is a lack of research studies on analyzing real-world credit card data owing to confidentiality issues. In this paper, machine learning algorithms are used to detect credit card fraud. Standard models are first used. Then, hybrid methods which use AdaBoost and majority voting methods are applied. To evaluate the model efficacy, a publicly available credit card data set is used. Then, a real-world credit card data set from a financial institution is analyzed. In addition, noise is added to the data samples to further assess the robustness of the algorithms. The experimental results positively indicate that the majority voting method achieves good accuracy rates in detecting fraud cases in credit cards.","['Credit cards', 'Machine learning algorithms', 'Support vector machines', 'Radio frequency', 'Vegetation', 'Classification algorithms', 'Self-organizing feature maps']","['AdaBoost', 'classification', 'credit card', 'fraud detection', 'predictive modelling', 'voting']"
"With the rapid growth of social networks and microblogging websites, communication between people from different cultural and psychological backgrounds has become more direct, resulting in more and more “cyber”conflicts between these people. Consequently, hate speech is used more and more, to the point where it has become a serious problem invading these open spaces. Hate speech refers to the use of aggressive, violent or offensive language, targeting a specific group of people sharing a common property, whether this property is their gender (i.e., sexism), their ethnic group or race (i.e., racism) or their believes and religion. While most of the online social networks and microblogging websites forbid the use of hate speech, the size of these networks and websites makes it almost impossible to control all of their content. Therefore, arises the necessity to detect such speech automatically and filter any content that presents hateful language or language inciting to hatred. In this paper, we propose an approach to detect hate expressions on Twitter. Our approach is based on unigrams and patterns that are automatically collected from the training set. These patterns and unigrams are later used, among others, as features to train a machine learning algorithm. Our experiments on a test set composed of 2010 tweets show that our approach reaches an accuracy equal to 87.4% on detecting whether a tweet is offensive or not (binary classification), and an accuracy equal to 78.4% on detecting whether a tweet is hateful, offensive, or clean (ternary classification).","['Speech', 'Feature extraction', 'Twitter', 'Voice activity detection', 'Task analysis', 'Sentiment analysis']","['Twitter', 'hate speech', 'machine learning', 'sentiment analysis']"
"Iris recognition refers to the automated process of recognizing individuals based on their iris patterns. The seemingly stochastic nature of the iris stroma makes it a distinctive cue for biometric recognition. The textural nuances of an individual's iris pattern can be effectively extracted and encoded by projecting them onto Gabor wavelets and transforming the ensuing phasor response into a binary code - a technique pioneered by Daugman. This textural descriptor has been observed to be a robust feature descriptor with very low false match rates and low computational complexity. However, recent advancements in deep learning and computer vision indicate that generic descriptors extracted using convolutional neural networks (CNNs) are able to represent complex image characteristics. Given the superior performance of CNNs on the ImageNet large scale visual recognition challenge and a large number of other computer vision tasks, in this paper, we explore the performance of state-of-the-art pre-trained CNNs on iris recognition. We show that the off-the-shelf CNN features, while originally trained for classifying generic objects, are also extremely good at representing iris images, effectively extracting discriminative visual features and achieving promising recognition results on two iris datasets: ND-CrossSensor-2013 and CASIA-IrisThousand. We also discuss the challenges and future research directions in leveraging deep learning methods for the problem of iris recognition.","['Iris recognition', 'Machine learning', 'Computer architecture', 'Visualization', 'Computer vision', 'Feature extraction']","['Iris recognition', 'biometrics', 'deep learning', 'convolutional neural network']"
"The growing interest and recent breakthroughs in artificial intelligence and machine learning (ML) have actively contributed to an increase in research and development of new methods to estimate the states of electrified vehicle batteries. Data-driven approaches, such as ML, are becoming more popular for estimating the state of charge (SOC) and state of health (SOH) due to greater availability of battery data and improved computing power capabilities. This paper provides a survey of battery state estimation methods based on ML approaches such as feedforward neural networks (FNNs), recurrent neural networks (RNNs), support vector machines (SVM), radial basis functions (RBF), and Hamming networks. Comparisons between methods are shown in terms of data quality, inputs and outputs, test conditions, battery types, and stated accuracy to give readers a bigger picture view of the ML landscape for SOC and SOH estimation. Additionally, to provide insight into how to best approach with the comparison of different neural network structures, an FNN and long short-term memory (LSTM) RNN are trained fifty times each for 3000 epochs. The error is somewhat different for each training repetition due to the random initial values of the trainable parameters, demonstrating that it is important to train networks multiple times to achieve the best result. Furthermore, it is recommended that when performing a comparison among estimation techniques such as those presented in this review paper, the compared networks should have a similar number of learnable parameters and be trained and tested with identical data. Otherwise, it is difficult to make a general conclusion regarding the quality of a given estimation technique.","['Batteries', 'State of charge', 'Machine learning', 'Maximum likelihood estimation', 'Training', 'Temperature measurement']","['Machine learning', 'artificial intelligence', 'deep learning', 'battery management systems (BMS)', 'electric vehicles', 'state of charge', 'state of health']"
"This paper discusses the power quality issues for distributed generation systems based on renewable energy sources, such as solar and wind energy. A thorough discussion about the power quality issues is conducted here. This paper starts with the power quality issues, followed by discussions of basic standards. A comprehensive study of power quality in power systems, including the systems with dc and renewable sources is done in this paper. Power quality monitoring techniques and possible solutions of the power quality issues for the power systems are elaborately studied. Then, we analyze the methods of mitigation of these problems using custom power devices, such as D-STATCOM, UPQC, UPS, TVSS, DVR, etc., for micro grid systems. For renewable energy systems, STATCOM can be a potential choice due to its several advantages, whereas spinning reserve can enhance the power quality in traditional systems. At Last, we study the power quality in dc systems. Simpler arrangement and higher reliability are two main advantages of the dc systems though it faces other power quality issues, such as instability and poor detection of faults.","['Power quality', 'Voltage fluctuations', 'Harmonic analysis', 'Power system harmonics', 'Renewable energy sources', 'Voltage control']","['DC system', 'mitigation', 'monitor', 'power quality', 'renewable energy', 'spinning reserve', 'standards']"
"Fungal diseases not only influence the economic importance of the plants and its products but also abate their ecological prominence. Mango tree, specifically the fruits and the leaves are highly affected by the fungal disease named as Anthracnose. The main aim of this paper is to develop an appropriate and effective method for diagnosis of the disease and its symptoms, therefore espousing a suitable system for an early and cost-effective solution of this problem. Over the last few years, due to their higher performance capability in terms of computation and accuracy, computer vision, and deep learning methodologies have gained popularity in assorted fungal diseases classification. Therefore, for this paper, a multilayer convolutional neural network (MCNN) is proposed for the classification of the Mango leaves infected by the Anthracnose fungal disease. This paper is validated on a real-time dataset captured at the Shri Mata Vaishno Devi University, Katra, J&K, India consists of 1070 images of the Mango tree leaves. The dataset contains both healthy and infected leaf images. The results envisage the higher classification accuracy of the proposed MCNN model when compared to the other state-of-the-art approaches.","['Diseases', 'Deep learning', 'Convolutional neural networks', 'Agriculture', 'Training', 'Real-time systems', 'Biological system modeling']","['Convolutional neural network', 'image classification', 'plant pathology', 'precision agriculture']"
"The role-based access control (RBAC) framework is a mechanism that describes the access control principle. As a common interaction, an organization provides a service to a user who owns a certain role that was issued by a different organization. Such trans-organizational RBAC is common in face-toface communication but not in a computer network, because it is difficult to establish both the security that prohibits the malicious impersonation of roles and the flexibility that allows small organizations to participate and users to fully control their own roles. In this paper, we present an RBAC using smart contract (RBAC-SC), a platform that makes use of Ethereum's smart contract technology to realize a trans organizational utilization of roles. Ethereum is an open blockchain platform that is designed to be secure, adaptable, and flexible. It pioneered smart contracts, which are decentralized applications that serve as “autonomous agents”running exactly as programmed and are deployed on a blockchain. The RBAC-SC uses smart contracts and blockchain technology as versatile infrastructures to represent the trust and endorsement relationship that are essential in the RBAC and to realize a challenge-response authentication protocol that verifies a user's ownership of roles. We describe the RBAC-SC framework, which is composed of two main parts, namely, the smart contract and the challenge-response protocol, and present a performance analysis. A prototype of the smart contract is created and deployed on Ethereum's Testnet blockchain, and the source code is publicly available.","['Organizations', 'Contracts', 'Access control', 'Protocols', 'Standards organizations', 'Computer networks']","['Blockchain technology', 'role-based access control', 'smart contracts']"
"In the last century, the automotive industry has arguably transformed society, being one of the most complex, sophisticated, and technologically advanced industries, with innovations ranging from the hybrid, electric, and self-driving smart cars to the development of IoT-connected cars. Due to its complexity, it requires the involvement of many Industry 4.0 technologies, like robotics, advanced manufacturing systems, cyber-physical systems, or augmented reality. One of the latest technologies that can benefit the automotive industry is blockchain, which can enhance its data security, privacy, anonymity, traceability, accountability, integrity, robustness, transparency, trustworthiness, and authentication, as well as provide long-term sustainability and a higher operational efficiency to the whole industry. This review analyzes the great potential of applying blockchain technologies to the automotive industry emphasizing its cybersecurity features. Thus, the applicability of blockchain is evaluated after examining the state-of-the-art and devising the main stakeholders' current challenges. Furthermore, the article describes the most relevant use cases, since the broad adoption of blockchain unlocks a wide area of short- and medium-term promising automotive applications that can create new business models and even disrupt the car-sharing economy as we know it. Finally, after strengths, weaknesses, opportunities, and threats analysis, some recommendations are enumerated with the aim of guiding researchers and companies in future cyber-resilient automotive industry developments.","['Blockchain', 'Industries', 'Automotive engineering', 'Computer security', 'Biological system modeling', 'Automobiles']","['Blockchain', 'distributed ledger technology (DLT)', 'Industry 4.0', 'IIoT', 'cyber-physical system', 'cryptography', 'cybersecurity', 'tamper-proof data', 'privacy', 'traceability']"
"Sarcasm identification on text documents is one of the most challenging tasks in natural language processing (NLP), has become an essential research direction, due to its prevalence on social media data. The purpose of our research is to present an effective sarcasm identification framework on social media data by pursuing the paradigms of neural language models and deep neural networks. To represent text documents, we introduce inverse gravity moment based term weighted word embedding model with trigrams. In this way, critical words/terms have higher values by keeping the word-ordering information. In our model, we present a three-layer stacked bidirectional long short-term memory architecture to identify sarcastic text documents. For the evaluation task, the presented framework has been evaluated on three-sarcasm identification corpus. In the empirical analysis, three neural language models (i.e., word2vec, fastText and GloVe), two unsupervised term weighting functions (i.e., term-frequency, and TF-IDF) and eight supervised term weighting functions (i.e., odds ratio, relevance frequency, balanced distributional concentration, inverse question frequency-question frequency-inverse category frequency, short text weighting, inverse gravity moment, regularized entropy and inverse false negative-true positive-inverse category frequency) have been evaluated. For sarcasm identification task, the presented model yields promising results with a classification accuracy of 95.30%.","['Task analysis', 'Social networking (online)', 'Blogs', 'Long short term memory', 'Predictive models', 'Gravity', 'Analytical models']","['Sarcasm identification', 'term weighting', 'neural language model', 'bidirectional long shortterm memory']"
"Mobile edge computing (MEC) providing information technology and cloud-computing capabilities within the radio access network is an emerging technique in fifth-generation networks. MEC can extend the computational capacity of smart mobile devices (SMDs) and economize SMDs' energy consumption by migrating the computation-intensive task to the MEC server. In this paper, we consider a multi-mobile-users MEC system, where multiple SMDs ask for computation offloading to a MEC server. In order to minimize the energy consumption on SMDs, we jointly optimize the offloading selection, radio resource allocation, and computational resource allocation coordinately. We formulate the energy consumption minimization problem as a mixed interger nonlinear programming (MINLP) problem, which is subject to specific application latency constraints. In order to solve the problem, we propose a reformulation-linearization-technique-based Branch-and-Bound (RLTBB) method, which can obtain the optimal result or a suboptimal result by setting the solving accuracy. Considering the complexity of RTLBB cannot be guaranteed, we further design a Gini coefficient-based greedy heuristic (GCGH) to solve the MINLP problem in polynomial complexity by degrading the MINLP problem into the convex problem. Many simulation results demonstrate the energy saving enhancements of RLTBB and GCGH.","['Energy consumption', 'Servers', 'Resource management', 'Mobile communication', 'Computational modeling', 'Minimization', 'Radio spectrum management']","['Mobile edge computing', 'computation offloading', 'energy minimization', 'branch-and-bound method', 'reformulation-linearization-technique', 'Gini coefficient']"
"Herein, we focus on convergent 6G communication, localization and sensing systems by identifying key technology enablers, discussing their underlying challenges, implementation issues, and recommending potential solutions. Moreover, we discuss exciting new opportunities for integrated localization and sensing applications, which will disrupt traditional design principles and revolutionize the way we live, interact with our environment, and do business. Regarding potential enabling technologies, 6G will continue to develop towards even higher frequency ranges, wider bandwidths, and massive antenna arrays. In turn, this will enable sensing solutions with very fine range, Doppler, and angular resolutions, as well as localization to cm-level degree of accuracy. Besides, new materials, device types, and reconfigurable surfaces will allow network operators to reshape and control the electromagnetic response of the environment. At the same time, machine learning and artificial intelligence will leverage the unprecedented availability of data and computing resources to tackle the biggest and hardest problems in wireless communication systems. As a result, 6G will be truly intelligent wireless systems that will provide not only ubiquitous communication but also empower high accuracy localization and high-resolution sensing services. They will become the catalyst for this revolution by bringing about a unique new set of features and service capabilities, where localization and sensing will coexist with communication, continuously sharing the available resources in time, frequency, and space. This work concludes by highlighting foundational research challenges, as well as implications and opportunities related to privacy, security, and trust.","['Sensors', 'Location awareness', '6G mobile communication', '5G mobile communication', 'Robot sensing systems', 'Frequency measurement', 'Protocols']","['6G', 'beamforming', 'cmWave', 'context-aware', 'IRS', 'ML/AI', 'mmWave', 'radar', 'security', 'sensing', 'SLAM', 'THz']"
"While augment reality applications are becoming popular, more and more data-hungry and computation-intensive tasks are delay-sensitive. Mobile edge computing is expected to an effective solution to meet the low latency demand. In contrast to previous work on mobile edge computing, which mainly focus on computation offloading, this paper introduces a new concept of task caching. Task caching refers to the caching of completed task application and their related data in edge cloud. Then, we investigate the problem of joint optimization of task caching and offloading on edge cloud with the computing and storage resource constraint. We formulate this problem as mixed integer programming which is hard to solve. To solve the problem, we propose efficient algorithm, called task caching and offloading (TCO), based on alternating iterative algorithm. Finally, the simulation experimental results show that our proposed TCO algorithm outperforms others in terms of less energy cost.","['Task analysis', 'Cloud computing', 'Mobile handsets', 'Edge computing', 'Delays', 'Solid modeling', 'Streaming media']","['Caching', 'computation offloading', 'mobile edge computing', 'energy efficient']"
"Recently, multilevel inverters (MLIs) have gained lots of interest in industry and academia, as they are changing into a viable technology for numerous applications, such as renewable power conversion system and drives. For these high power and high/medium voltage applications, MLIs are widely used as one of the advanced power converter topologies. To produce high-quality output without the need for a large number of switches, development of reduced switch MLI (RS MLI) topologies has been a major focus of current research. Therefore, this review paper focuses on a number of recently developed MLIs used in various applications. To assist with advanced current research in this field and in the selection of suitable inverter for various applications, significant understanding on these topologies is clearly summarized based on the three categories, i.e., symmetrical, asymmetrical, and modified topologies. This review paper also includes a comparison based on important performance parameters, detailed technical challenges, current focus, and future development trends. By a suitable combination of switches, the MLI produces a staircase output with low harmonic distortion. For a better understanding of the working principle, a single-phase RS MLI topology is experimentally illustrated for different level generation using both fundamental and high switching frequency techniques which will help the readers to gain the utmost knowledge for advance research.","['Topology', 'Switches', 'Pulse width modulation', 'Inverters', 'Harmonic analysis', 'Power harmonic filters', 'Renewable energy sources']","['Control techniques', 'drives application', 'fundamental switching frequency', 'high switching frequency', 'multilevel inverter (MLI)', 'performance parameters', 'photovoltaic (PV) systems', 'reduced component count', 'renewable energy application']"
"In recent years, unmanned aerial vehicles (UAVs) have received considerable attention from regulators, industry and research community, due to rapid growth in a broad range of applications. Particularly, UAVs are being used to provide a promising solution to reliable and cost-effective wireless communications from the sky. The deployment of UAVs has been regarded as an alternative complement of existing cellular systems, to achieve higher transmission efficiency with enhanced coverage and capacity. However, heavily utilized microwave spectrum bands below 6 GHz utilized by legacy wireless systems are insufficient to attain remarkable data rate enhancement for numerous emerging applications. To resolve the spectrum crunch crisis and satisfy the requirements of 5G and beyond mobile communications, one potential solution is to use the abundance of unoccupied bandwidth available at millimeter wave (mmWave) frequencies. Inspired by the technique potentials, mmWave communications have also paved the way into the widespread use of UAVs to assist wireless networks for future 5G and beyond wireless applications. In this paper, we provide a comprehensive survey on current achievements in the integration of 5G mmWave communications into UAV-assisted wireless networks. More precisely, a taxonomy to classify the existing research issues is presented, by considering seven cutting-edge solutions. Subsequently, we provide a brief overview of 5G mmWave communications for UAV-assisted wireless networks from two aspects, i.e., key technical advantages and challenges as well as potential applications. Based on the proposed taxonomy, we further discuss in detail the state-of-the-art issues, solutions, and open challenges for this newly emerging area. Lastly, we complete this survey by pointing out open issues and shedding new light on future directions for further research on this area.","['Drones', '5G mobile communication', 'Wireless networks', 'Wireless sensor networks', 'Ad hoc networks', 'Millimeter wave communication']","['Millimeter wave (mmWave) communications', 'unmanned aerial vehicle (UAV)', 'mmWave UAV communications', 'UAV-assisted wireless networks', '5G and beyond']"
"Prosumer concept and digitilization offer the exciting potential of microgrid transactive energy systems at distribution level for reducing transmission losses, decreasing electric infrastructure expenditure, improving reliability, enhancing local energy use, and minimizing customers' electricity bills. Distributed energy resources, demand response, distributed ledger technologies, and local energy markets are integral parts of transaction energy system for emergence of decentralized smart grid system. Hence, this paper discusses transactive energy concept and proposes seven functional layers architecture for designing transactive energy system. The proposed architecture is compared with practical case study of Brooklyn microgrid. Moreover, this paper reviews the existing architectures and explains the widely known distributed ledger technologies (blockchain, directed acyclic graph, hashgraph, holochain, and tempo) alongwith their advantages and challenges. The local energy market concept is presented and critically analyzed for energy trade within a transactive energy system. This paper also reviews the potential and challenges of peer-to-peer and community-based energy markets. Proposed architecture and analytical review of distributed ledger technologies and local energy markets pave the way for advanced research and industrialization of transactive energy systems.","['Transactive energy', 'Microgrids', 'Distributed ledger', 'Load management', 'Blockchain', 'Computer architecture']","['Blockchain', 'decentralization', 'demand response', 'distributed ledger technologies', 'energy trading', 'local energy market', 'microgrid', 'peer-to-peer market', 'prosumer', 'renewable energy sources', 'smart grid', 'system architectures', 'transactive energy']"
"Advances in technology are not only changing the world around us but also driving the wireless industry to develop the next generation of network technology. There is a lot of buzz building over the advent of 5G that will facilitate the entire planet through continuous and ubiquitous communication connecting anybody to anything, anywhere, anytime, and anyhow regardless of the device, service, network, or geographical existence. 5G will also prove to be a paradigm shift including high carrier frequencies with massive bandwidths, having a large number of antennas, and with an extreme base station and device densities. In this paper, we investigate the potential beneficiaries of 5G and identify the use-cases, where 5G can make an impact. In particular, we consider three main use-cases: vehicle-to-everything (V2X) communication, drones, and healthcare. We explore and highlight the problems and deficiencies of current cellular technologies with respect to these use-cases and identify how 5G will overcome those deficiencies. We also identified the open research problems and provide possible future directions to cope with those issues.","['5G mobile communication', 'Vehicle-to-everything', 'Medical services', 'Drones', 'Reliability', 'Vehicular ad hoc networks', 'Wireless communication']","['5G', 'V2X communication', 'drones', 'healthcare', 'ultra-low-latency', 'ultra-high-reliability']"
This paper presents a complete approach to a successful utilization of a high-performance extreme learning machines (ELMs) Toolbox for Big Data. It summarizes recent advantages in algorithmic performance; gives a fresh view on the ELM solution in relation to the traditional linear algebraic performance; and reaps the latest software and hardware performance achievements. The results are applicable to a wide range of machine learning problems and thus provide a solid ground for tackling numerous Big Data challenges. The included toolbox is targeted at enabling the full potential of ELMs to the widest range of users.,"['Learning systems', 'Performance evaluation', 'Machine learning']","['Learning systems', 'Supervised learning', 'Machine learning', 'Prediction methods', 'Predictive models', 'Neural networks', 'Artificial neural networks', 'Feedforward neural networks', 'Radial basis function networks', 'Computer applications', 'Scientific computing', 'Performance analysis', 'High performance computing Software', 'Open source software', 'Utility programs']"
"Smart grid technology increases reliability, security, and efficiency of the electrical grids. However, its strong dependencies on digital communication technology bring up new vulnerabilities that need to be considered for efficient and reliable power distribution. In this paper, an unsupervised anomaly detection based on statistical correlation between measurements is proposed. The goal is to design a scalable anomaly detection engine suitable for large-scale smart grids, which can differentiate an actual fault from a disturbance and an intelligent cyber-attack. The proposed method applies feature extraction utilizing symbolic dynamic filtering (SDF) to reduce computational burden while discovering causal interactions between the subsystems. The simulation results on IEEE 39, 118, and 2848 bus systems verify the performance of the proposed method under different operation conditions. The results show an accuracy of 99%, true positive rate of 98%, and false positive rate of less than 2%","['Smart grids', 'Generators', 'Power system reliability', 'Reliability', 'Feature extraction', 'Security']","['Anomaly', 'cyber-attack', 'smart grid', 'statistical property', 'machine learning', 'unsupervised learning']"
"Unmanned aerial vehicles (UAVs) have stroke great interested both by the academic community and the industrial community due to their diverse military applications and civilian applications. Furthermore, UAVs are also envisioned to be part of future airspace traffic. The application functions delivery relies on information exchange among UAVs as well as between UAVs and ground stations (GSs), which further closely depends on aeronautical channels. However, there is a paucity of comprehensive surveys on aeronautical channel modeling in line with the specific aeronautical characteristics and scenarios. To fill this gap, this paper focuses on reviewing the air-to-ground (A2G), ground-to-ground (G2G), and air-to-air (A2A) channel measurements and modeling for UAV communications and aeronautical communications under various scenarios. We also provide the design guideline for managing the link budget of UAV communications taking account of link losses and channel fading effects. Moreover, we also analyze the receive/transmit diversity gain and spatial multiplexing gain achieved by multiple-antenna-aided UAV communications. Finally, we discuss the remaining challenge and open issues for the future development of UAV communication channel modeling.","['Atmospheric modeling', 'Solid modeling', 'Analytical models', 'Unmanned aerial vehicles', 'Airports', 'Sea measurements', 'Shadow mapping']","['UAV communication', 'aeronautical communication', 'channel characterization', 'statistical channel', 'air-to-ground', 'cellular networks', 'evaporation duct', 'shadowing', 'MIMO']"
"Massive multiple-input multiple-output is a promising physical layer technology for 5G wireless communications due to its capability of high spectrum and energy efficiency, high spatial resolution, and simple transceiver design. To embrace its potential gains, the acquisition of channel state information is crucial, which unfortunately faces a number of challenges, such as the uplink pilot contamination, the overhead of downlink training and feedback, and the computational complexity. In order to reduce the effective channel dimensions, researchers have been investigating the low-rank (sparse) properties of channel environments from different viewpoints. This paper then provides a general overview of the current low-rank channel estimation approaches, including their basic assumptions, key results, as well as pros and cons on addressing the aforementioned tricky challenges. Comparisons among all these methods are provided for better understanding and some future research prospects for these low-rank approaches are also forecasted.","['MIMO', 'Physical layer', '5G mobile communication', 'Energy efficiency', 'Spatial resolution', 'Transceivers']","['Massive MIMO', 'channel estimation', 'low-rank property', 'channel sparsity', 'angle reciprocity']"
"This paper focuses on bearing fault diagnosis with limited training data. A major challenge in fault diagnosis is the infeasibility of obtaining sufficient training samples for every fault type under all working conditions. Recently deep learning based fault diagnosis methods have achieved promising results. However, most of these methods require large amount of training data. In this study, we propose a deep neural network based few-shot learning approach for rolling bearing fault diagnosis with limited data. Our model is based on the siamese neural network, which learns by exploiting sample pairs of the same or different categories. Experimental results over the standard Case Western Reserve University (CWRU) bearing fault diagnosis benchmark dataset showed that our few-shot learning approach is more effective in fault diagnosis with limited data availability. When tested over different noise environments with minimal amount of training data, the performance of our few-shot learning model surpasses the one of the baseline with reasonable noise level. When evaluated over test sets with new fault types or new working conditions, few-shot models work better than the baseline trained with all fault types. All our models and datasets in this study are open sourced and can be downloaded from https://mekhub.cn/as/fault_diagnosis_with_few-shot_learning/.","['Fault diagnosis', 'Neural networks', 'Training', 'Employee welfare', 'Kernel', 'Mathematical model', 'Deep learning']","['Deep learning', 'few-shot learning', 'bearing fault diagnosis', 'limited data']"
"This study aims to explore the current status, potential applications, and future directions of blockchain technology in supply chain management. A literature survey, along with an analytical review, of blockchain-based supply chain research was conducted to better understand the trajectory of related research and shed light on the benefits, issues, and challenges in the blockchain-supply-chain paradigm. A selected corpus comprising 106 review articles was analyzed to provide an overview of the use of blockchain and smart contracts in supply chain management. The diverse industrial applications of these technologies in various sectors have increasingly received attention by researchers, engineers, and practitioners. Four major issues: traceability and transparency, stakeholder involvement and collaboration, supply chain integration and digitalization, and common frameworks on blockchain-based platforms, are critical for future orientation. Traditional supply chain activities involve several intermediaries, trust, and performance issues. The potential of blockchain can be leveraged to disrupt supply chain operations for better performance, distributed governance, and process automation. This study contributes to the comprehension of blockchain applications in supply chain management and provides a blueprint for these applications from the perspective of literature analysis. Future efforts regarding technical adoption/diffusion, block-supply chain integration, and their social impacts were highlighted to enrich the research scope.","['Blockchain', 'Smart contracts', 'Supply chains', 'Peer-to-peer computing', 'Distributed ledger', 'Systematics', 'Bibliographies']","['Blockchain', 'digital ledger', 'distributed ledger technology', 'logistics', 'shared ledger', 'smart contract', 'supply chain management', 'systematic literature review', 'value chain']"
"The proliferation of smartphones has significantly facilitated people's daily life, and diverse and powerful embedded sensors make smartphone a ubiquitous platform to acquire and analyze data, which may also provide great potential for efficient human activity recognition. This paper presents a systematic performance analysis of motion-sensor behavior for human activity recognition via smartphones. Sensory data sequences are collected via smartphones, when participants perform typical and daily human activities. A cycle detection algorithm is applied to segment the data sequence for obtaining the activity unit, which is then characterized by time-, frequency-, and wavelet-domain features. Then both personalized and generalized model using diverse classification algorithms are developed and implemented to perform activity recognition. Analyses are conducted using 27 681 sensory samples from 10 subjects, and the performance is measured in the form of F-score under various placement settings, and in terms of sensitivity to user space, stability to combination of motion sensors, and impact of data imbalance. Extensive results show that each individual has its own specific and discriminative movement patterns, and the F-score for personalized model and generalized model can reach 95.95% and 96.26%, respectively, which indicates our approach is accurate and efficient for practical implementation.","['Activity recognition', 'Smart phones', 'Accelerometers', 'Feature extraction', 'Legged locomotion', 'Gyroscopes']","['Smartphone', 'motion sensor', 'behavior analysis', 'human activity recognition', 'performance analysis']"
"Phasor measurement units (PMUs) are rapidly being deployed in electric power networks across the globe. Wide-area measurement system (WAMS), which builds upon PMUs and fast communication links, is consequently emerging as an advanced monitoring and control infrastructure. Rapid adaptation of such devices and technologies has led the researchers to investigate multitude of challenges and pursue opportunities in synchrophasor measurement technology, PMU structural design, PMU placement, miscellaneous applications of PMU from local perspectives, and various WAMS functionalities from the system perspective. Relevant research articles appeared in the IEEE and IET publications from 1983 through 2014 are rigorously surveyed in this paper to represent a panorama of research progress lines. This bibliography will aid academic researchers and practicing engineers in adopting appropriate topics and will stimulate utilities toward development and implementation of software packages.","['Phasor measurement', 'Synchronization', 'Bibliographies', 'Wide area measurement', 'Measurement']","['Phasor measurement unit (PMU)', 'synchrophasor measurement technology (SMT)', 'wide-area measurement system (WAMS)']"
"The network intrusion detection system is an important tool for protecting computer networks against threats and malicious attacks. Many techniques have recently been proposed; however, these face significant challenges due to the continuous emergence of new threats that are not recognized by existing systems. In this paper, we propose a novel two-stage deep learning (TSDL) model, based on a stacked auto-encoder with a soft-max classifier, for efficient network intrusion detection. The model comprises two decision stages: an initial stage responsible for classifying network traffic as normal or abnormal, using a probability score value. This is then used in the final decision stage as an additional feature, for detecting the normal state and other classes of attacks. The proposed model is able to learn useful feature representations from large amounts of unlabeled data and classifies them automatically and efficiently. To evaluate its effectiveness, several experiments are conducted on two public datasets, specifically the benchmark KDD99 and UNSW-NB15 datasets. Comparative simulation results demonstrate that our proposed model significantly outperforms existing approaches, achieving high recognition rates, up to 99.996% and 89.134%, for the KDD99 and UNSW-NB15 datasets respectively. We conclude that our model has the potential to serve as a future benchmark for the deep learning and network security research communities.","['Intrusion detection', 'Deep learning', 'Feature extraction', 'Computational modeling', 'Benchmark testing', 'Neural networks', 'Tools']","['Computational intelligence', 'two-stage deep learning model', 'feature representation', 'network intrusion detection', 'stacked auto-encoder']"
"Blockchain and other Distributed Ledger Technologies (DLTs) have evolved significantly in the last years and their use has been suggested for numerous applications due to their ability to provide transparency, redundancy and accountability. In the case of blockchain, such characteristics are provided through public-key cryptography and hash functions. However, the fast progress of quantum computing has opened the possibility of performing attacks based on Grover's and Shor's algorithms in the near future. Such algorithms threaten both public-key cryptography and hash functions, forcing to redesign blockchains to make use of cryptosystems that withstand quantum attacks, thus creating which are known as post-quantum, quantum-proof, quantum-safe or quantum-resistant cryptosystems. For such a purpose, this article first studies current state of the art on post-quantum cryptosystems and how they can be applied to blockchains and DLTs. Moreover, the most relevant post-quantum blockchain systems are studied, as well as their main challenges. Furthermore, extensive comparisons are provided on the characteristics and performance of the most promising post-quantum public-key encryption and digital signature schemes for blockchains. Thus, this article seeks to provide a broad view and useful guidelines on post-quantum blockchain security to future blockchain researchers and developers.","['Blockchain', 'Hash functions', 'Elliptic curve cryptography', 'Quantum computing']","['Blockchain', 'blockchain security', 'DLT', 'post-quantum', 'quantum-safe', 'quantum-resistant', 'quantum computing', 'cryptography', 'cryptosystem', 'cybersecurity']"
"According to the recent studies, malicious software (malware) is increasing at an alarming rate, and some malware can hide in the system by using different obfuscation techniques. In order to protect computer systems and the Internet from the malware, the malware needs to be detected before it affects a large number of systems. Recently, there have been made several studies on malware detection approaches. However, the detection of malware still remains problematic. Signature-based and heuristic-based detection approaches are fast and efficient to detect known malware, but especially signature-based detection approach has failed to detect unknown malware. On the other hand, behavior-based, model checking-based, and cloud-based approaches perform well for unknown and complicated malware; and deep learning-based, mobile devices-based, and IoT-based approaches also emerge to detect some portion of known and unknown malware. However, no approach can detect all malware in the wild. This shows that to build an effective method to detect malware is a very challenging task, and there is a huge gap for new studies and methods. This paper presents a detailed review on malware detection approaches and recent detection methods which use these approaches. Paper goal is to help researchers to have a general idea of the malware detection approaches, pros and cons of each detection approach, and methods that are used in these approaches.","['Computer viruses', 'Feature extraction', 'Encryption', 'Internet']","['Cyber security', 'malware classification', 'malware detection approaches', 'malware features']"
"Big data is considered to be the key to unlocking the next great waves of growth in productivity. The amount of collected data in our world has been exploding due to a number of new applications and technologies that permeate our daily lives, including mobile and social networking applications, and Internet of Thing-based smart-world systems (smart grid, smart transportation, smart cities, and so on). With the exponential growth of data, how to efficiently utilize the data becomes a critical issue. This calls for the development of a big data market that enables efficient data trading. Via pushing data as a kind of commodity into a digital market, the data owners and consumers are able to connect with each other, sharing and further increasing the utility of data. Nonetheless, to enable such an effective market for data trading, several challenges need to be addressed, such as determining proper pricing for the data to be sold or purchased, designing a trading platform and schemes to enable the maximization of social welfare of trading participants with efficiency and privacy preservation, and protecting the traded data from being resold to maintain the value of the data. In this paper, we conduct a comprehensive survey on the lifecycle of data and data trading. To be specific, we first study a variety of data pricing models, categorize them into different groups, and conduct a comprehensive comparison of the pros and cons of these models. Then, we focus on the design of data trading platforms and schemes, supporting efficient, secure, and privacy-preserving data trading. Finally, we review digital copyright protection mechanisms, including digital copyright identifier, digital rights management, digital encryption, watermarking, and others, and outline challenges in data protection in the data trading lifecycle.","['Pricing', 'Copyright protection', 'Data models', 'Data mining', 'Data analysis', 'Big Data applications']","['Big data', 'data pricing', 'privacy and digital copyright protection', 'data trading', 'data utilization', 'Internet of Things']"
"With the advent of the Internet of Things (IoT), the security of the network layer in the IoT is getting more and more attention. The traditional intrusion detection technologies cannot be well adapted in the complex Internet environment of IoT. For the deep learning algorithm of intrusion detection, a neural network structure may have fine detection accuracy for one kind of attack, but it may not have a good detection effect when facing other attacks. Therefore, it is urgent to design a self-adaptive model to change the network structure for different attack types. This paper presents an intrusion detection model based on improved genetic algorithm (GA) and deep belief network (DBN). Facing different types of attacks, through multiple iterations of the GA, the optimal number of hidden layers and number of neurons in each layer are generated adaptively, so that the intrusion detection model based on the DBN achieves a high detection rate with a compact structure. Finally, the NSL-KDD dataset was used to simulate and evaluate the model and algorithms. The experimental results show that the improved intrusion detection model combined with DBN can effectively improve the recognition rate of intrusion attacks and reduce the complexity of the neural network structure.","['Intrusion detection', 'Neurons', 'Genetic algorithms', 'Neural networks', 'Internet of Things', 'Deep learning']","['Internet of Things security', 'intrusion detection', 'deep belief network', 'genetic algorithm']"
"Tuberculosis (TB) is a chronic lung disease that occurs due to bacterial infection and is one of the top 10 leading causes of death. Accurate and early detection of TB is very important, otherwise, it could be life-threatening. In this work, we have detected TB reliably from the chest X-ray images using image pre-processing, data augmentation, image segmentation, and deep-learning classification techniques. Several public databases were used to create a database of 3500 TB infected and 3500 normal chest X-ray images for this study. Nine different deep CNNs (ResNet18, ResNet50, ResNet101, ChexNet, InceptionV3, Vgg19, DenseNet201, SqueezeNet, and MobileNet) were used for transfer learning from their pre-trained initial weights and were trained, validated and tested for classifying TB and non-TB normal cases. Three different experiments were carried out in this work: segmentation of X-ray images using two different U-net models, classification using X-ray images and that using segmented lung images. The accuracy, precision, sensitivity, F1-score and specificity of best performing model, ChexNet in the detection of tuberculosis using X-ray images were 96.47%, 96.62%, 96.47%, 96.47%, and 96.51% respectively. However, classification using segmented lung images outperformed that with whole X-ray images; the accuracy, precision, sensitivity, F1-score and specificity of DenseNet201 were 98.6%, 98.57%, 98.56%, 98.56%, and 98.54% respectively for the segmented lung images. The paper also used a visualization technique to confirm that CNN learns dominantly from the segmented lung regions that resulted in higher detection accuracy. The proposed method with state-of-the-art performance can be useful in the computer-aided faster diagnosis of tuberculosis.","['X-ray imaging', 'Lung', 'Image segmentation', 'Deep learning', 'Diseases', 'Medical diagnostic imaging']","['Tuberculosis detection', 'TB screening', 'deep learning', 'transfer learning', 'lungs segmentation', 'image processing']"
"Supply chains are evolving into automated and highly complex networks and are becoming an important source of potential benefits in the modern world. At the same time, consumers are now more interested in food product quality. However, it is challenging to track the provenance of data and maintain its traceability throughout the supply chain network. The traditional supply chains are centralized and they depend on a third party for trading. These centralized systems lack transparency, accountability and auditability. In our proposed solution, we have presented a complete solution for blockchain-based Agriculture and Food (Agri-Food) supply chain. It leverages the key features of blockchain and smart contracts, deployed over ethereum blockchain network. Although blockchain provides immutability of data and records in the network, it still fails to solve some major problems in supply chain management like credibility of the involved entities, accountability of the trading process and traceability of the products. Therefore, there is a need of a reliable system that ensures traceability, trust and delivery mechanism in Agri-Food supply chain. In the proposed system, all transactions are written to blockchain which ultimately uploads the data to Interplanetary File Storage System (IPFS). The storage system returns a hash of the data which is stored on blockchain and ensures efficient, secure and reliable solution. Our system provides smart contracts along with their algorithms to show interaction of entities in the system. Furthermore, simulations and evaluation of smart contracts along with the security and vulnerability analyses are also presented in this work.","['Supply chains', 'Blockchain', 'Supply chain management', 'Agricultural products', 'Storage management']","['Accountability', 'blockchain', 'credibility', 'reputation', 'supply chain', 'traceability', 'trust']"
"With the continued development of artificial intelligence (AI) technology, research on interaction technology has become more popular. Facial expression recognition (FER) is an important type of visual information that can be used to understand a human's emotional situation. In particular, the importance of AI systems has recently increased due to advancements in research on AI systems applied to AI robots. In this paper, we propose a new scheme for FER system based on hierarchical deep learning. The feature extracted from the appearance feature-based network is fused with the geometric feature in a hierarchical structure. The appearance feature-based network extracts holistic features of the face using the preprocessed LBP image, whereas the geometric feature-based network learns the coordinate change of action units (AUs) landmark, which is a muscle that moves mainly when making facial expressions. The proposed method combines the result of the softmax function of two features by considering the error associated with the second highest emotion (Top-2) prediction result. In addition, we propose a technique to generate facial images with neutral emotion using the autoencoder technique. By this technique, we can extract the dynamic facial features between the neutral and emotional images without sequence data. We compare the proposed algorithm with the other recent algorithms for CK+ and JAFFE dataset, which are typically considered to be verified datasets in the facial expression recognition. The ten-fold cross validation results show 96.46% of accuracy in the CK+ dataset and 91.27% of accuracy in the JAFFE dataset. When comparing with other methods, the result of the proposed hierarchical deep network structure shows up to about 3% of the accuracy improvement and 1.3% of average improvement in CK+ dataset, respectively. In JAFFE datasets, up to about 7% of the accuracy is enhanced, and the average improvement is verified by about 1.5%.","['Feature extraction', 'Face recognition', 'Face', 'Deep learning', 'Emotion recognition', 'Data mining']","['Artificial intelligence (AI)', 'facial expression recognition (FER)', 'emotion recognition', 'deep learning', 'LBP feature', 'geometric feature', 'convolutional neural network (CNN)']"
"Attention deficit hyperactivity disorder (ADHD) is one of the most common mental health disorders. As a neuro development disorder, neuroimaging technologies, such as magnetic resonance imaging (MRI), coupled with machine learning algorithms, are being increasingly explored as biomarkers in ADHD. Among various machine learning methods, deep learning has demonstrated excellent performance on many imaging tasks. With the availability of publically-available, large neuroimaging data sets for training purposes, deep learning-based automatic diagnosis of psychiatric disorders can become feasible. In this paper, we develop a deep learning-based ADHD classification method via 3-D convolutional neural networks (CNNs) applied to MRI scans. Since deep neural networks may utilize millions of parameters, even the large number of MRI samples in pooled data sets is still relatively limited if one is to learn discriminative features from the raw data. Instead, here we propose to first extract meaningful 3-D low-level features from functional MRI (fMRI) and structural MRI (sMRI) data. Furthermore, inspired by radiologists' typical approach for examining brain images, we design a 3-D CNN model to investigate the local spatial patterns of MRI features. Finally, we discover that brain functional and structural information are complementary, and design a multi-modality CNN architecture to combine fMRI and sMRI features. Evaluations on the hold-out testing data of the ADHD-200 global competition shows that the proposed multi-modality 3-D CNN approach achieves the state-of-the-art accuracy of 69.15% and outperforms reported classifiers in the literature, even with fewer training samples. We suggest that multi-modality classification will be a promising direction to find potential neuroimaging biomarkers of neuro development disorders.","['Feature extraction', 'Three-dimensional displays', 'Testing', 'Training', 'Neuroimaging', 'Biological neural networks']","['Attention deficit hyperactive disorder', '3D CNN', 'magnetic resonance imaging', 'multi-modality analysis']"
"With the popularity and development of network technology and the Internet, intrusion detection systems (IDSs), which can identify attacks, have been developed. Traditional intrusion detection algorithms typically employ mining association rules to identify intrusion behaviors. However, they fail to fully extract the characteristic information of user behaviors and encounter various problems, such as high false alarm rate (FAR), poor generalization capability, and poor timeliness. In this paper, we propose a network intrusion detection model based on a convolutional neural network-IDS (CNN-IDS). Redundant and irrelevant features in the network traffic data are first removed using different dimensionality reduction methods. Features of the dimensionality reduction data are automatically extracted using the CNN, and more effective information for identifying intrusion is extracted by supervised learning. To reduce the computational cost, we convert the original traffic vector format into an image format and use a standard KDD-CUP99 dataset to evaluate the performance of the proposed CNN model. The experimental results indicate that the AC, FAR, and timeliness of the CNN-IDS model are higher than those of traditional algorithms. Therefore, the model we propose has not only research significance but also practical value.","['Intrusion detection', 'Feature extraction', 'Convolution', 'Data models', 'Machine learning', 'Principal component analysis', 'Data mining']","['Communication technology', 'convolutional neural network', 'data dimensionality reduction', 'intrusion detection']"
"A smart factory is a highly digitized and connected production facility that relies on smart manufacturing. Additionally, artificial intelligence is the core technology of smart factories. The use of machine learning and deep learning algorithms has produced fruitful results in many fields like image processing, speech recognition, fault detection, object detection, or medical sciences. With the increment in the use of smart machinery, the faults in the machinery equipment are expected to increase. Machinery fault detection and diagnosis through various deep learning algorithms has increased day by day. Many types of research have been done and published using both open-source and closed-source datasets, implementing the deep learning algorithms. Out of many publicly available datasets, Case Western Reserve University (CWRU) bearing dataset has been widely used to detect and diagnose machinery bearing fault and is accepted as a standard reference for validating the models. This paper summarizes the recent works which use the CWRU bearing dataset in machinery fault detection and diagnosis employing deep learning algorithms. We have reviewed the published works and presented the working algorithm, result, and other necessary details in this paper. This paper, we believe, can be of good help for future researchers to start their work on machinery fault detection and diagnosis using the CWRU dataset.","['Fault detection', 'Machinery', 'Deep learning', 'Vibrations', 'Frequency-domain analysis', 'Wavelet transforms']","['Bearing', 'deep learning', 'machine learning', 'machinery fault detection and diagnosis', 'CWRU dataset']"
"In this paper, we propose a Blockchain-based infrastructure to support security- and privacy-oriented spatio-temporal smart contract services for the sustainable Internet of Things (IoT)-enabled sharing economy in mega smart cities. The infrastructure leverages cognitive fog nodes at the edge to host and process off loaded geo-tagged multimedia payload and transactions from a mobile edge and IoT nodes, uses AI for processing and extracting significant event information, produces semantic digital analytics, and saves results in Blockchain and decentralized cloud repositories to facilitate sharing economy services. The framework offers a sustainable incentive mechanism, which can potentially support secure smart city services, such as sharing economy, smart contracts, and cyber-physical interaction with Blockchain and IoT. Our unique contribution is justified by detailed system design and implementation of the framework.","['Sharing economy', 'Blockchain', 'Smart cities', 'Cognitive systems', 'Security', 'Business']","['Sharing economy', 'cognitive processing at the edge', 'mobile edge computing', 'Blockchain', 'smart city']"
"A feature of the Internet of Things (IoT) is that some users in the system need to be served quickly for small packet transmission. To address this requirement, a new multiple-input multiple-output non-orthogonal multiple access (MIMO-NOMA) scheme is designed in this paper, where one user is served with its quality of service requirement strictly met, and the other user is served opportunistically by using the NOMA concept. The novelty of this new scheme is that it confronts the challenge that the existing MIMO-NOMA schemes rely on the assumption that users' channel conditions are different, a strong assumption which may not be valid in practice. The developed precoding and detection strategies can effectively create a significant difference between the users' effective channel gains, and therefore, the potential of NOMA can be realized even if the users' original channel conditions are similar. Analytical and numerical results are provided to demonstrate the performance of the proposed MIMO-NOMA scheme.","['Internet of Things', 'MIMO', 'Precoding', 'Quality of service', 'Packet switching', 'NOMA']","['Non-orthogonal multiple access (NOMA)', 'multiple-input multiple-output (MIMO)', 'QR decomposition', 'MIMO precoding', 'power allocation']"
"The prognostic and health management (PHM) of lithium-ion batteries has received increasing attention in recent years. The remaining useful life (RUL) prediction and state of health (SOH) monitoring are two important parts in PHM of the lithium-ion battery. Nowadays, the development of signal processing technology and neural network technology introduces new data-driven methods to RUL prediction and SOH monitoring of the lithium-ion battery. This paper presents a neural-network-based method that combines long short-term memory (LSTM) network with particle swarm optimization and attention mechanism for RUL prediction and SOH monitoring of the lithium-ion battery. Before predicting RUL of the lithium-ion battery, the Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) is utilized for the raw data denoising, which can improve the accuracy of prediction. A real-life cycle dataset of lithium-ion batteries from NASA is used to evaluate the proposed method, and the experiment results show that when compared with traditional methods, the proposed method has higher accuracy.","['Lithium-ion batteries', 'Monitoring', 'Prognostics and health management', 'Support vector machines', 'Data models', 'Load modeling']","['Lithium-ion battery', 'prognostic and health management (PHM)', 'long short-term memory (LSTM)', 'attention mechanism']"
"Optimization of deep learning is no longer an imminent problem, due to various gradient descent methods and the improvements of network structure, including activation functions, the connectivity style, and so on. Then the actual application depends on the generalization ability, which determines whether a network is effective. Regularization is an efficient way to improve the generalization ability of deep CNN, because it makes it possible to train more complex models while maintaining a lower overfitting. In this paper, we propose to optimize the feature boundary of deep CNN through a two-stage training method (pre-training process and implicit regularization training process) to reduce the overfitting problem. In the pre-training stage, we train a network model to extract the image representation for anomaly detection. In the implicit regularization training stage, we re-train the network based on the anomaly detection results to regularize the feature boundary and make it converge in the proper position. Experimental results on five image classification benchmarks show that the two-stage training method achieves a state-of-the-art performance and that it, in conjunction with more complicated anomaly detection algorithm, obtains better results. Finally, we use a variety of strategies to explore and analyze how implicit regularization plays a role in the two-stage training process. Furthermore, we explain how implicit regularization can be interpreted as data augmentation and model ensemble.","['Training', 'Anomaly detection', 'Feature extraction', 'Image representation', 'Production', 'Principal component analysis', 'Machine learning']","['Deep CNN', 'image classification', 'overfitting', 'generalization', 'anomaly detection', 'implicit regularization']"
"Feature selection is a critical and prominent task in machine learning. To reduce the dimension of the feature set while maintaining the accuracy of the performance is the main aim of the feature selection problem. Various methods have been developed to classify the datasets. However, metaheuristic algorithms have achieved great attention in solving numerous optimization problem. Therefore, this paper presents an extensive literature review on solving feature selection problem using metaheuristic algorithms which are developed in the ten years (2009-2019). Further, metaheuristic algorithms have been classified into four categories based on their behaviour. Moreover, a categorical list of more than a hundred metaheuristic algorithms is presented. To solve the feature selection problem, only binary variants of metaheuristic algorithms have been reviewed and corresponding to their categories, a detailed description of them explained. The metaheuristic algorithms in solving feature selection problem are given with their binary classification, name of the classifier used, datasets and the evaluation metrics. After reviewing the papers, challenges and issues are also identified in obtaining the best feature subset using different metaheuristic algorithms. Finally, some research gaps are also highlighted for the researchers who want to pursue their research in developing or modifying metaheuristic algorithms for classification. For an application, a case study is presented in which datasets are adopted from the UCI repository and numerous metaheuristic algorithms are employed to obtain the optimal feature subset.","['Feature extraction', 'Search problems', 'Biomedical imaging', 'Task analysis', 'Particle swarm optimization', 'Measurement', 'Machine learning algorithms']","['Binary variants', 'classification', 'feature selection', 'literature review', 'metaheuristic algorithms']"
"This paper presents research challenges on security and privacy issues in the field of green IoT-based agriculture. We start by describing a four-tier green IoT-based agriculture architecture and summarizing the existing surveys that deal with smart agriculture. Then, we provide a classification of threat models against green IoT-based agriculture into five categories, including, attacks against privacy, authentication, confidentiality, availability, and integrity properties. Moreover, we provide a taxonomy and a side-by-side comparison of the state-of-the-art methods toward secure and privacy-preserving technologies for IoT applications and how they will be adapted for green IoT-based agriculture. In addition, we analyze the privacy-oriented blockchain-based solutions as well as consensus algorithms for IoT applications and how they will be adapted for green IoT-based agriculture. Based on the current survey, we highlight open research challenges and discuss possible future research directions in the security and privacy of green IoT-based agriculture.","['Internet of Things', 'Security', 'Privacy', 'Green products', 'Production', 'Agricultural products']","['Security', 'privacy', 'authentication', 'blockchain', 'smart agriculture', 'greenhouse']"
"Industry 4.0 is a concept devised for improving the way modern factories operate through the use of some of the latest technologies, like the ones used for creating the Industrial Internet of Things (IIoT), robotics, or Big Data applications. One of such technologies is blockchain, which is able to add trust, security, and decentralization to different industrial fields. This article focuses on analyzing the benefits and challenges that arise when using blockchain and smart contracts to develop Industry 4.0 applications. In addition, this paper presents a thorough review of the most relevant blockchain-based applications for Industry 4.0 technologies. Thus, its aim is to provide a detailed guide for the future Industry 4.0 developers that allows for determining how the blockchain can enhance the next generation of cybersecure industrial applications.","['Blockchain', 'Industries', 'Smart manufacturing', 'Production facilities', 'Next generation networking', 'Internet', 'Computer crime']","['Blockchain', 'Industry 4.0', 'cybersecurity', 'IIoT', 'smart factory', 'industrial augmented reality', 'cyber-physical system', 'fog and edge computing', 'cloud computing', 'Big Data']"
"Cloud Computing provides an effective platform for executing large-scale and complex workflow applications with a pay-as-you-go model. Nevertheless, various challenges, especially its optimal scheduling for multiple conflicting objectives, are yet to be addressed properly. The existing multi-objective workflow scheduling approaches are still limited in many ways, e.g., encoding is restricted by prior experts’ knowledge when handling a dynamic real-time problem, which strongly influences the performance of scheduling. In this paper, we apply a deep-Q-network model in a multi-agent reinforcement learning setting to guide the scheduling of multi-workflows over infrastructure-as-a-service clouds. To optimize multi-workflow completion time and user’s cost, we consider a Markov game model, which takes the number of workflow applications and heterogeneous virtual machines as state input and the maximum completion time and cost as rewards. The game model is capable of seeking for correlated equilibrium between make-span and cost criteria without prior experts’ knowledge and converges to the correlated equilibrium policy in a dynamic real-time environment. To validate our proposed approach, we conduct extensive case studies based on multiple well-known scientific workflow templates and Amazon EC2 cloud. The experimental results clearly suggest that our proposed approach outperforms traditional ones, e.g., non-dominated sorting genetic algorithm-II, multi-objective particle swarm optimization, and game-theoretic-based greedy algorithms, in terms of optimality of scheduling plans generated.","['Optimal scheduling', 'Games', 'Task analysis', 'Cloud computing', 'Reinforcement learning', 'Markov processes', 'Scheduling']","['Multi-objective workflow scheduling', 'deep-Q-network (DQN)', 'multi-agent reinforcement learning (MARL)', 'infrastructure-as-a-service (IaaS) cloud', 'quality-of-service (QoS)']"
"The volatility and uncertainty of wind power often affect the quality of electric energy, the security of the power grid, the stability of the power system, and the fluctuation of the power market. In this case, the research on wind power forecasting is of great significance for ensuring the better development of wind power grids and the higher quality of electric energy. Therefore, a lot of new forecasting methods have been put forward. In this paper, a new forecasting model based on a convolution neural network and LightGBM is constructed. The procedure is shown as follows. First, we construct new feature sets by analyzing the characteristics of the raw data on the time series from the wind field and adjacent wind field. Second, the convolutional neural network (CNN) is proposed to extract information from input data, and the network parameters are adjusted by comparing the actual results. Third, in consideration of the limitations of the single-convolution model in predicting wind power, we innovatively integrated the LightGBM classification algorithm at the model to improve the forecasting accuracy and robustness. Finally, compared with the existing support vector machines, LightGBM, and CNN, the fusion model has better performance in accuracy and efficiency.","['Forecasting', 'Wind power generation', 'Convolution', 'Feature extraction', 'Predictive models', 'Data mining', 'Kernel']","['Convolutional neural network', 'fusion model', 'LightGBM', 'ultra-short-term wind power forecasting', 'wind energy']"
"More and more network traffic data have brought great challenge to traditional intrusion detection system. The detection performance is tightly related to selected features and classifiers, but traditional feature selection algorithms and classification algorithms can't perform well in massive data environment. Also the raw traffic data are imbalanced, which has a serious impact on the classification results. In this paper, we propose a novel network intrusion detection model utilizing convolutional neural networks (CNNs). We use CNN to select traffic features from raw data set automatically, and we set the cost function weight coefficient of each class based on its numbers to solve the imbalanced data set problem. The model not only reduces the false alarm rate (FAR) but also improves the accuracy of the class with small numbers. To reduce the calculation cost further, we convert the raw traffic vector format into image format. We use the standard NSL-KDD data set to evaluate the performance of the proposed CNN model. The experimental results show that the accuracy, FAR, and calculation cost of the proposed model perform better than traditional standard algorithms. It is an effective and reliable solution for the intrusion detection of a massive network.","['Intrusion detection', 'Feature extraction', 'Training', 'Data models', 'Data preprocessing', 'Convolutional neural networks']","['Network intrusion detection', 'convolutional neural networks', 'image data format conversion', 'cost function weight', 'imbalanced dataset']"
"Nowadays, learning-based modeling system is adopted to establish an accurate prediction model for renewable energy resources. Computational Intelligence (CI) methods have become significant tools in production and optimization of renewable energies. The complexity of this type of energy lies in its coverage of large volumes of data and variables which have to be analyzed carefully. The present study discusses different types of Deep Learning (DL) algorithms applied in the field of solar and wind energy resources and evaluates their performance through a novel taxonomy. It also presents a comprehensive state-of-the-art of the literature leading to an assessment and performance evaluation of DL techniques as well as a discussion about major challenges and opportunities for comprehensive research. Based on results, differences on accuracy, robustness, precision values as well as the generalization ability are the most common challenges for the employment of DL techniques. In case of big dataset, the performance of DL techniques is significantly higher than that for other CI techniques. However, using and developing hybrid DL techniques with other optimization techniques in order to improve and optimize the structure of the techniques is preferably emphasized. In all cases, hybrid networks have better performance compared with single networks, because hybrid techniques take the advantages of two or more methods for preparing an accurate prediction. It is recommended to use hybrid methods in DL techniques.","['Wind energy', 'Solar energy', 'Renewable energy sources', 'Wind', 'Neural networks', 'Mathematical model', 'Computational modeling']","['Big dataset', 'deep learning', 'modeling', 'optimizing', 'solar energy', 'wind energy']"
"As the technique that determines the position of a target device based on wireless measurements, Wi-Fi localization is attracting increasing attention due to its numerous applications and the widespread deployment of Wi-Fi infrastructure. In this paper, we propose ConFi, the first convolutional neural network (CNN)-based Wi-Fi localization algorithm. Channel state information (CSI), which contains more position related information than traditional received signal strength, is organized into a time-frequency matrix that resembles image and utilized as the feature for localization. The ConFi models localization as a classification problem and addresses it with a five layer CNN that consists of three convolutional layers and two fully connected layers. The ConFi has a training stage and a localization stage. In the training stage, the CSI is collected at a number of reference points (RPs) and used to train the CNN via stochastic gradient descent algorithm. In the localization stage, the CSI of the target device is fed to the CNN and the localization result is calculated as the weighted centroid of the RPs with high output value. Extensive experiments are conducted to select appropriate parameters for the CNN and demonstrate the superior performance of the ConFi over existing methods.","['Antennas', 'Wireless fidelity', 'Artificial neural networks', 'Training', 'Antenna measurements', 'Feature extraction', 'Fading channels']","['Wi-Fi localization', 'channel state information', 'convolutional neural network', 'pattern recognition']"
"5G is the next generation cellular network that aspires to achieve substantial improvement on quality of service, such as higher throughput and lower latency. Edge computing is an emerging technology that enables the evolution to 5G by bringing cloud capabilities near to the end users (or user equipment, UEs) in order to overcome the intrinsic problems of the traditional cloud, such as high latency and the lack of security. In this paper, we establish a taxonomy of edge computing in 5G, which gives an overview of existing state-of-the-art solutions of edge computing in 5G on the basis of objectives, computational platforms, attributes, 5G functions, performance measures, and roles. We also present other important aspects, including the key requirements for its successful deployment in 5G and the applications of edge computing in 5G. Then, we explore, highlight, and categorize recent advancements in edge computing for 5G. By doing so, we reveal the salient features of different edge computing paradigms for 5G. Finally, open research issues are outlined.","['Edge computing', '5G mobile communication', 'Cloud computing', 'Real-time systems', 'Servers', 'Quality of service', 'Throughput']","['5G', 'cloud computing', 'edge computing', 'fog computing']"
"Emotion recognition represents the position and motion of facial muscles. It contributes significantly in many fields. Current approaches have not obtained good results. This paper aimed to propose a new emotion recognition system based on facial expression images. We enrolled 20 subjects and let each subject pose seven different emotions: happy, sadness, surprise, anger, disgust, fear, and neutral. Afterward, we employed biorthogonal wavelet entropy to extract multiscale features, and used fuzzy multiclass support vector machine to be the classifier. The stratified cross validation was employed as a strict validation model. The statistical analysis showed our method achieved an overall accuracy of 96.77±0.10%. Besides, our method is superior to three state-of-the-art methods. In all, this proposed method is efficient.","['Support vector machines', 'Wavelet transforms', 'Entropy', 'Feature extraction', 'Face recognition', 'Low-pass filters', 'Fuzzy logic', 'Emotion recognition']","['Facial emotion recognition', 'facial expression', 'biorthogonal wavelet entropy', 'support vector machine', 'fuzzy logic']"
"In the last few years, Internet of Things, Cloud computing, Edge computing, and Fog computing have gained a lot of attention in both industry and academia. However, a clear and neat definition of these computing paradigms and their correlation is hard to find in the literature. This makes it difficult for researchers new to this area to get a concrete picture of these paradigms. This work tackles this deficiency, representing a helpful resource for those who will start next. First, we show the evolution of modern computing paradigms and related research interest. Then, we address each paradigm, neatly delineating its key points and its relation with the others. Thereafter, we extensively address Fog computing, remarking its outstanding role as the glue between IoT, Cloud, and Edge computing. In the end, we briefly present open challenges and future research directions for IoT, Cloud, Edge, and Fog computing.","['Cloud computing', 'Edge computing', 'Internet of Things', 'Market research', 'Computer architecture', 'Libraries']","['Fog computing', 'cloud computing', 'edge computing', 'Internet of Things', 'mobile cloud computing', 'mobile edge computing']"
"The advancement of technologies over years has poised Internet of Things (IoT) to scoop out untapped information and communication technology opportunities. It is anticipated that IoT will handle the gigantic network of billions of devices to deliver plenty of smart services to the users. Undoubtedly, this will make our life more resourceful but at the cost of high energy consumption and carbon footprint. Consequently, there is a high demand for green communication to reduce energy consumption, which requires optimal resource availability and controlled power levels. In contrast to this, IoT devices are constrained in terms of resources-memory, power, and computation. Low power wide area (LPWA) technology is a response to the need for efficient utilization of power resource, as it evinces characteristics such as the capability to proffer low power connectivity to a huge number of devices spread over wide geographical areas at low cost. Various LPWA technologies, such as LoRa and SigFox, exist in the market, offering a proficient solution to the users. However, in order to abstain the need of new infrastructure (like base station) that is required for proprietary technologies, a new cellular-based licensed technology, narrowband IoT (NBIoT), is introduced by 3GPP in Rel-13. This technology presents a good candidature to handle LPWA market because of its characteristics like enhanced indoor coverage, low power consumption, latency insensitivity, and massive connection support towards NBIoT. This survey presents a profound view of IoT and NBIoT, subsuming their technical features, resource allocation, and energy-efficiency techniques and applications. The challenges that hinder the NBIoT path to success are also identified and discussed. In this paper, two novel energy-efficient techniques “zonal thermal pattern analysis” and energy-efficient adaptive health monitoring system have been proposed towards green IoT.","['Energy efficiency', 'Internet of Things', 'Power demand', 'Resource management', 'Agriculture', 'Computer architecture', 'Monitoring']","['Internet of Things (IoT)', 'narrowband Internet of Things (NBIoT)', 'low power wide area network (LPWAN)', 'green communication', 'smart agriculture', 'smart health']"
"Today, internet and device ubiquity are paramount in individual, formal and societal considerations. Next generation communication technologies, such as Blockchains (BC), Internet of Things (IoT), cloud computing, etc. offer limitless capabilities for different applications and scenarios including industries, cities, healthcare systems, etc. Sustainable integration of healthcare nodes (i.e. devices, users, providers, etc.) resulting in healthcare IoT (or simply IoHT) provides a platform for efficient service delivery for the benefit of care givers (doctors, nurses, etc.) and patients. Whereas confidentiality, accessibility and reliability of medical data are accorded high premium in IoHT, semantic gaps and lack of appropriate assets or properties remain impediments to reliable information exchange in federated trust management frameworks. Consequently, We propose a Blockchain Decentralised Interoperable Trust framework (DIT) for IoT zones where a smart contract guarantees authentication of budgets and Indirect Trust Inference System (ITIS) reduces semantic gaps and enhances trustworthy factor (TF) estimation via the network nodes and edges. Our DIT IoHT makes use of a private Blockchain ripple chain to establish trustworthy communication by validating nodes based on their inter-operable structure so that controlled communication required to solve fusion and integration issues are facilitated via different zones of the IoHT infrastructure. Further, C# implementation using Ethereum and ripple Blockchain are introduced as frameworks to associate and aggregate requests over trusted zones.","['Semantics', 'Ontologies', 'Internet of Things', 'Interoperability', 'Data models', 'Sensors', 'Medical services']","['Trustworthness', 'blockchain', 'security', 'interoperability', 'sustainable healthcare IoT systems']"
"In this paper, an edge computing system for IoT-based (Internet of Things) smart grids is proposed to overcome the drawbacks in the current cloud computing paradigm in power systems, where many problems have yet to be addressed such as fully realizing the requirements of high bandwidth with low latency. The new system mainly introduces edge computing in the traditional cloud-based power system and establishes a new hardware and software architecture. Therefore, a considerable amount of data generated in the electrical grid will be analyzed, processed, and stored at the edge of the network. Aided with edge computing paradigm, the IoT-based smart grids will realize the connection and management of substantial terminals, provide the real-time analysis and processing of massive data, and foster the digitalization of smart grids. In addition, we propose a privacy protection strategy via edge computing, data prediction strategy, and preprocessing strategy of hierarchical decision-making based on task grading (HDTG) for the IoT-based smart girds. The effectiveness of our proposed approaches has been demonstrated via the numerical simulations.","['Smart grids', 'Edge computing', 'Cloud computing', 'Industries', 'Sensors', 'Real-time systems']","['Edge computing', 'IoT-based smart grids', 'data prediction', 'artificial intelligence', 'data privacy protection', 'cloud computing']"
"Gesture recognition aims to recognize meaningful movements of human bodies, and is of utmost importance in intelligent human-computer/robot interactions. In this paper, we present a multimodal gesture recognition method based on 3-D convolution and convolutional long-short-term-memory (LSTM) networks. The proposed method first learns short-term spatiotemporal features of gestures through the 3-D convolutional neural network, and then learns long-term spatiotemporal features by convolutional LSTM networks based on the extracted short-term spatiotemporal features. In addition, fine-tuning among multimodal data is evaluated, and we find that it can be considered as an optional skill to prevent overfitting when no pre-trained models exist. The proposed method is verified on the ChaLearn LAP large-scale isolated gesture data set (IsoGD) and the Sheffield Kinect gesture (SKIG) data set. The results show that our proposed method can obtain the state-of-the-art recognition accuracy (51.02% on the validation set of IsoGD and 98.89% on SKIG).","['Gesture recognition', 'Three-dimensional displays', 'Spatiotemporal phenomena', 'Feature extraction', 'Neural networks', 'Convolution', 'Solid modeling']","['3-D convolution', 'convolutional LSTM', 'gesture recognition', 'multimodal']"
"Blockchains offer a decentralized, immutable and verifiable ledger that can record transactions of digital assets, provoking a radical change in several innovative scenarios, such as smart cities, eHealth or eGovernment. However, blockchains are subject to different scalability, security and potential privacy issues, such as transaction linkability, crypto-keys management (e.g. recovery), on-chain data privacy, or compliance with privacy regulations (e.g. GDPR). To deal with these challenges, novel privacy-preserving solutions for blockchain based on crypto-privacy techniques are emerging to empower users with mechanisms to become anonymous and take control of their personal data during their digital transactions of any kind in the ledger, following a Self-Sovereign Identity (SSI) model. In this sense, this paper performs a systematic review of the current state of the art on privacy-preserving research solutions and mechanisms in blockchain, as well as the main associated privacy challenges in this promising and disrupting technology. The survey covers privacy techniques in public and permissionless blockchains, e.g. Bitcoin and Ethereum, as well as privacy-preserving research proposals and solutions in permissioned and private blockchains. Diverse blockchain scenarios are analyzed, encompassing, eGovernment, eHealth, cryptocurrencies, Smart cities, and Cooperative ITS.","['Blockchain', 'Privacy', 'Data privacy', 'Proposals', 'Bitcoin']","['Blockchain', 'privacy', 'security', 'survey', 'bitcoin']"
"In recent years, multiple-input-multiple-output (MIMO) antennas with the ability to radiate waves in more than one pattern and polarization play a great role in modern telecommunication systems. This paper provides a theoretical review of different mutual coupling reduction techniques in MIMO antenna systems. The increase in the mutual coupling can affect the antenna characteristics drastically and therefore degrades the performance of the MIMO systems. It is possible to improve the performance partially by calibrating the mutual coupling in the digital domain. However, the simple and effective approach is to use the techniques, such as defected ground structure, parasitic or slot element, complementary split ring resonator, and decoupling networks which can overcome the mutual coupling effects by means of physical implementation. An extensive discussion on the basis of different mutual coupling reduction techniques, their examples, and comparative study is still rare in the literature. Therefore, in this paper, different MIMO antenna design techniques and all of their mutual coupling reduction techniques through various structures and mechanisms are presented with multiple examples and characteristics comparison.","['Mutual coupling', 'MIMO communication', 'Antenna radiation patterns', 'Correlation', 'Correlation coefficient', 'Topology']","['Diversity gain', 'ECC', 'MIMO', 'mutual coupling', 'PCB', 'UWB', 'WLAN']"
"The inherent flexibility of hierarchical structure scheme with main-servo loop control structure is proposed to the problem of integrated chassis control system for the vehicle. It includes both main loop, which calculates and allocates the aim force using the optimal robust control algorithm and servo loop control systems, which track and achieve the target force using the onboard independent brake actuators. In fact, for the brake actuator, the aim friction is obtained by tracking the corresponding slip ratio of target force. For the coefficient of tire-road friction varying with different road surface, to get the nonlinear time-varying target slip ratio, the most famous quasi-static magic formula is proposed to estimate and predict real-time coefficient of different road surface and the constrained hybrid genetic algorithm (GA) is used to identify the key parameters of the magic formula on-line. Then, a self-tuning longitudinal slip ratio controller (LSC) based on the nonsingular and fast terminal sliding mode (NFTSM) control method is designed to improve the tracking accuracy and response speed of the actuators. At last, the proposed integrated chassis control strategies and the self-tuning control strategies are verified by computer simulations.","['Friction', 'Force', 'Tires', 'Brakes', 'Genetic algorithms', 'Vehicle dynamics']","['Main-servo loop control structure', 'parameters identification', 'tire-road friction control', 'constrained hybrid genetic algorithm', 'nonlinear sliding mode control']"
"Industrial cyber-physical systems (ICPSs) are the backbones of Industry 4.0 and as such, have become a core transdisciplinary area of research, both in industry and academia. New challenges brought about by the growing scale and complexity of systems, insufficient information exchange, and the exploitation of knowledge available have started threatening the overall system safety and stability. This work is motivated by these challenges and the strategic and practical demands of developing ICPSs for safety critical systems such as the intelligent factory and the smart grid. It investigates the current status of research in ICPS monitoring and control, and reviews the recent advances in monitoring, fault diagnosis, and control approaches based on data-driven realization, which can take full advantage of the abundant data available from past observations and those collected online in realtime. The practical requirements in the typical ICPS applications are summarized as the major issues to be addressed for the monitoring and the safety control tasks. The key challenges and the research directions are proposed as references to the future work.","['Monitoring', 'Sensors', 'Iterative closest point algorithm', 'Observers', 'Smart grids', 'Fault diagnosis', 'Cyber-physical systems']","['Cyber-physical system (CPS)', 'data-driven', 'system monitoring', 'fault diagnosis', 'smart grid', 'plug-and-play control']"
"Q-learning is arguably one of the most applied representative reinforcement learning approaches and one of the off-policy strategies. Since the emergence of Q-learning, many studies have described its uses in reinforcement learning and artificial intelligence problems. However, there is an information gap as to how these powerful algorithms can be leveraged and incorporated into general artificial intelligence workflow. Early Q-learning algorithms were unsatisfactory in several aspects and covered a narrow range of applications. It has also been observed that sometimes, this rather powerful algorithm learns unrealistically and overestimates the action values hence abating the overall performance. Recently with the general advances of machine learning, more variants of Q-learning like Deep Q-learning which combines basic Q learning with deep neural networks have been discovered and applied extensively. In this paper, we thoroughly explain how Q-learning evolved by unraveling the mathematical complexities behind it as well its flow from reinforcement learning family of algorithms. Improved variants are fully described, and we categorize Q-learning algorithms into single-agent and multi-agent approaches. Finally, we thoroughly investigate up-to-date research trends and key applications that leverage Q-learning algorithms.","['Reinforcement learning', 'Mathematical model', 'Classification algorithms', 'Machine learning algorithms', 'Market research']","['Reinforcement learning', 'Q-learning', 'single-agent', 'multi-agent']"
"Intrusion detection systems (IDSs) play a pivotal role in computer security by discovering and repealing malicious activities in computer networks. Anomaly-based IDS, in particular, rely on classification models trained using historical data to discover such malicious activities. In this paper, an improved IDS based on hybrid feature selection and two-level classifier ensembles are proposed. A hybrid feature selection technique comprising three methods, i.e., particle swarm optimization, ant colony algorithm, and genetic algorithm, is utilized to reduce the feature size of the training datasets (NSL-KDD and UNSW-NB15 are considered in this paper). Features are selected based on the classification performance of a reduced error pruning tree (REPT) classifier. Then, a two-level classifier ensemble based on two meta learners, i.e., rotation forest and bagging, is proposed. On the NSL-KDD dataset, the proposed classifier shows 85.8% accuracy, 86.8% sensitivity, and 88.0% detection rate, which remarkably outperform other classification techniques recently proposed in the literature. The results regarding the UNSW-NB15 dataset also improve the ones achieved by several state-of-the-art techniques. Finally, to verify the results, a two-step statistical significance test is conducted. This is not usually considered by the IDS research thus far and, therefore, adds value to the experimental results achieved by the proposed classifier.","['Feature extraction', 'Internet of Things', 'Training', 'Intrusion detection', 'Bagging', 'Anomaly detection']","['Two-stage meta classifier', 'network anomaly detection', 'hybrid feature selection', 'intrusion detection system', 'statistical significance test']"
"Rotating machines have been widely used in industrial engineering. The fault diagnosis of rotating machines plays a vital important role to reduce the catastrophic failures and heavy economic loss. However, the measured vibration signal of rotating machinery often represents non-linear and non-stationary characteristics, resulting in difficulty in the fault feature extraction. As a statistical measure, entropy can quantify the complexity and detect dynamic change through taking into account the non-linear behavior of time series. Therefore, entropy can be served as a promising tool to extract the dynamic characteristics of rotating machines. Recently, many studies have applied entropy in fault diagnosis of rotating machinery. This paper aims to investigate the applications of entropy for the fault characteristics extraction of rotating machines. First, various entropy methods are briefly introduced. Its foundation, application, and some improvements are described and discussed. The review is divided into eight parts: Shannon entropy, Rényi entropy, approximate entropy, sample entropy, fuzzy entropy, permutation entropy, and other entropy methods. In each part, we will review the applications using the original entropy method and the improved entropy methods, respectively. In the end, a summary and some research prospects are given.","['Entropy', 'Time series analysis', 'Fault diagnosis', 'Complexity theory', 'Rotating machines', 'Vibrations']","['Entropy', 'fault diagnosis', 'fault feature extraction', 'rotating machinery', 'condition-based maintenance']"
"Federated learning is a newly emerged distributed machine learning paradigm, where the clients are allowed to individually train local deep neural network (DNN) models with local data and then jointly aggregate a global DNN model at the central server. Vehicular edge computing (VEC) aims at exploiting the computation and communication resources at the edge of vehicular networks. Federated learning in VEC is promising to meet the ever-increasing demands of artificial intelligence (AI) applications in intelligent connected vehicles (ICV). Considering image classification as a typical AI application in VEC, the diversity of image quality and computation capability in vehicular clients potentially affects the accuracy and efficiency of federated learning. Accordingly, we propose a selective model aggregation approach, where “fine” local DNN models are selected and sent to the central server by evaluating the local image quality and computation capability. Regarding the implementation of model selection, the central server is not aware of the image quality and computation capability in the vehicular clients, whose privacy is protected under such a federated learning framework. To overcome this information asymmetry, we employ two-dimension contract theory as a distributed framework to facilitate the interactions between the central server and vehicular clients. The formulated problem is then transformed into a tractable problem through successively relaxing and simplifying the constraints, and eventually solved by a greedy algorithm. Using two datasets, i.e., MNIST and BelgiumTSC, our selective model aggregation approach is demonstrated to outperform the original federated averaging (FedAvg) approach in terms of accuracy and efficiency. Meanwhile, our approach also achieves higher utility at the central server compared with the baseline approaches.","['Computational modeling', 'Servers', 'Image quality', 'Training', 'Contracts', 'Artificial intelligence', 'Data models']","['Federated learning', 'vehicular edge computing', 'model aggregation', 'contract theory']"
