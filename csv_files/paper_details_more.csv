abstracts,ieee_keywords,author_keywords
"The demand for wireless connectivity has grown exponentially over the last few decades. Fifth-generation (5G) communications, with far more features than fourth-generation communications, will soon be deployed worldwide. A new paradigm of wireless communication, the sixth-generation (6G) system, with the full support of artificial intelligence, is expected to be implemented between 2027 and 2030. Beyond 5G, some fundamental issues that need to be addressed are higher system capacity, higher data rate, lower latency, higher security, and improved quality of service (QoS) compared to the 5G system. This paper presents the vision of future 6G wireless communication and its network architecture. This article describes emerging technologies such as artificial intelligence, terahertz communications, wireless optical technology, free-space optical network, blockchain, three-dimensional networking, quantum communications, unmanned aerial vehicles, cell-free communications, integration of wireless information and energy transfer, integrated sensing and communication, integrated access-backhaul networks, dynamic network slicing, holographic beamforming, backscatter communication, intelligent reflecting surface, proactive caching, and big data analytics that can assist the 6G architecture development in guaranteeing the QoS. Besides, expected applications with 6G communication requirements and possible technologies are presented. We also describe potential challenges and research directions for achieving this goal.",[],[]
"As of today, the fifth generation (5G) mobile communication system has been rolled out in many countries and the number of 5G subscribers already reaches a very large scale. It is time for academia and industry to shift their attention towards the next generation. At this crossroad, an overview of the current state of the art and a vision of future communications are definitely of interest. This article thus aims to provide a comprehensive survey to draw a picture of the sixth generation (6G) system in terms of drivers, use cases, usage scenarios, requirements, key performance indicators (KPIs), architecture, and enabling technologies. First, we attempt to answer the question of “Is there any need for 6G?” by shedding light on its key driving factors, in which we predict the explosive growth of mobile traffic until 2030, and envision potential use cases and usage scenarios. Second, the technical requirements of 6G are discussed and compared with those of 5G with respect to a set of KPIs in a quantitative manner. Third, the state-of-the-art 6G research efforts and activities from representative institutions and countries are summarized, and a tentative roadmap of definition, specification, standardization, and regulation is projected. Then, we identify a dozen of potential technologies and introduce their principles, advantages, challenges, and open research issues. Finally, the conclusions are drawn to paint a picture of “What 6G may look like?.” This survey is intended to serve as an enlightening guideline to spur interests and further investigations for subsequent research and development of 6G communications systems.","['6G mobile communication', 'Industries', 'Wireless communication', 'Wireless sensor networks', '5G mobile communication', 'Artificial intelligence', 'Vehicles']","['5G', '6G', 'artificial intelligence', 'blockchain', 'cell-free MIMO', 'digital twin', 'edge computing', 'holographic-type communications', 'Internet of Everything', 'Internet of Things', 'machine learning', 'mobile networks', 'non-terrestrial networks', 'optical wireless communications', 'O-RAN', 'tactile Internet', 'terahertz', 'visible light communications', 'wireless communications']"
"Reconfigurable intelligent surfaces (RISs) have the potential of realizing the emerging concept of smart radio environments by leveraging the unique properties of metamaterials and large arrays of inexpensive antennas. In this article, we discuss the potential applications of RISs in wireless networks that operate at high-frequency bands, e.g., millimeter wave (30-100 GHz) and sub-millimeter wave (greater than 100 GHz) frequencies. When used in wireless networks, RISs may operate in a manner similar to relays. The present paper, therefore, elaborates on the key differences and similarities between RISs that are configured to operate as anomalous reflectors and relays. In particular, we illustrate numerical results that highlight the spectral efficiency gains of RISs when their size is sufficiently large as compared with the wavelength of the radio waves. In addition, we discuss key open issues that need to be addressed for unlocking the potential benefits of RISs for application to wireless communications and networks.","['Relays', 'Surface waves', 'Wireless communication', 'Radio transmitters', 'Antenna arrays']","['5G', '6G', 'reconfigurable intelligent surfaces', 'relays', 'smart radio environments']"
"Emerging applications such as Internet of Everything, Holographic Telepresence, collaborative robots, and space and deep-sea tourism are already highlighting the limitations of existing fifth-generation (5G) mobile networks. These limitations are in terms of data-rate, latency, reliability, availability, processing, connection density and global coverage, spanning over ground, underwater and space. The sixth-generation (6G) of mobile networks are expected to burgeon in the coming decade to address these limitations. The development of 6G vision, applications, technologies and standards has already become a popular research theme in academia and the industry. In this paper, we provide a comprehensive survey of the current developments towards 6G. We highlight the societal and technological trends that initiate the drive towards 6G. Emerging applications to realize the demands raised by 6G driving trends are discussed subsequently. We also elaborate the requirements that are necessary to realize the 6G applications. Then we present the key enabling technologies in detail. We also outline current research projects and activities including standardization efforts towards the development of 6G. Finally, we summarize lessons learned from state-of-the-art research and discuss technical challenges that would shed a new light on future research directions towards 6G.","['6G mobile communication', '5G mobile communication', 'Mobile communication', 'Internet of Things', 'Market research', 'Reliability', 'Broadband communication']","['Beyond 5G', '6G', 'mobile communication', 'emerging technologies', 'survey']"
"Ultra-high bandwidth, negligible latency and seamless communication are envisioned as milestones that will revolutionize the way by which societies create, distribute and consume information. The remarkable expansion of wireless data traffic has advocated the investigation of suitable regimes in the radio spectrum to satisfy users' escalating requirements and allow the exploitation of massive capacity and massive connectivity. To this end, the Terahertz (THz) frequency band (0.1-10 THz) has received noticeable attention in the research community as an ideal choice for scenarios involving high-speed transmission. As such, in this work, we present an up-to-date review paper to analyze key concepts associated with the THz system architecture. THz generation methods are first addressed by highlighting the recent progress in the devices technology. Moreover, the recently proposed channel models available for propagation at THz band frequencies are introduced. A comprehensive comparison is then presented between the THz wireless communication and its other contenders. In addition, several applications of THz communication are discussed taking into account various scales. Further, we highlight the milestones achieved regarding THz standardization activities. Finally, a future outlook is provided by presenting and envisaging several potential use cases and attempts to guide the deployment of the THz frequency band.","['Wireless communication', 'Bandwidth', 'Channel models', 'Standardization', 'Sensors', 'Radio frequency']","['Terahertz band', 'Terahertz communication', 'Terahertz transceivers', 'Terahertz channel model', 'high-speed transmission', 'Terahertz standardization']"
"The concept of reconfiguring wireless propagation environments using intelligent reflecting surfaces (IRS)s has recently emerged, where an IRS comprises of a large number of passive reflecting elements that can smartly reflect the impinging electromagnetic waves for performance enhancement. Previous works have shown promising gains assuming the availability of perfect channel state information (CSI) at the base station (BS) and the IRS, which is impractical due to the passive nature of the reflecting elements. This paper makes one of the preliminary contributions of studying an IRS-assisted multi-user multiple-input single-output (MISO) communication system under imperfect CSI. Different from the few recent works that develop least-squares (LS) estimates of the IRS-assisted channel vectors, we exploit the prior knowledge of the large-scale fading statistics at the BS to derive the Bayesian minimum mean squared error (MMSE) channel estimates under a protocol in which the IRS applies a set of optimal phase shifts vectors over multiple channel estimation sub-phases. The resulting mean squared error (MSE) is both analytically and numerically shown to be lower than that achieved by the LS estimates. Joint designs for the precoding and power allocation at the BS and reflect beamforming at the IRS are proposed to maximize the minimum user signal-to-interference-plus-noise ratio (SINR) subject to a transmit power constraint. Performance evaluation results illustrate the efficiency of the proposed system and study its susceptibility to channel estimation errors.","['Channel estimation', 'Protocols', 'Precoding', 'Signal to noise ratio', 'Array signal processing', 'Interference', 'MISO communication']","['Alternating optimization', 'channel estimation', 'intelligent reflecting surface', 'minimum mean squared error', 'multiple-input single-output system']"
"Non-orthogonal multiple access (NOMA) has been considered as a study-item in 3GPP for 5G new radio (NR). However, it was decided not to continue with it as a work-item, and to leave it for possible use in beyond 5G. In this paper, we first review the discussions that ended in such decision. Particularly, we present simulation comparisons between the Welch-bound equality spread multiple access (WSMA)-based NOMA and multi-user multiple-input-multiple-output (MU-MIMO), where the possible gain of WSMA-based NOMA, compared to MU-MIMO, is negligible. Then, we summarize the 3GPP discussions on NOMA, and propose a number of methods to reduce the implementation complexity and delay of both uplink (UL) and downlink (DL) NOMA-based transmission, as different ways to improve its efficiency. Here, particular attention is paid to reducing the receiver complexity, the cost of hybrid automatic repeat request as well as the user pairing complexity. As demonstrated, different smart techniques can be applied to improve the energy efficiency and the end-to-end transmission delay of NOMA-based systems.","['NOMA', 'Complexity theory', 'Correlation', '3GPP', 'Receivers', '5G mobile communication', 'Long Term Evolution']","['3GPP', '5G', 'HARQ', 'MU-MIMO', 'non-orthogonal multiple access (NOMA)', 'receiver design', 'user pairing', 'WSMA']"
"The use of large arrays might be the solution to the capacity problems in wireless communications. The signal-to-noise ratio (SNR) grows linearly with the number of array elements N when using Massive MIMO receivers and half-duplex relays. Moreover, intelligent reflecting surfaces (IRSs) have recently attracted attention since these can relay signals to achieve an SNR that grows as N 2 , which seems like a major benefit. In this article, we use a deterministic propagation model for a planar array of arbitrary size, to demonstrate that the mentioned SNR behaviors, and associated power scaling laws, only apply in the far-field. They cannot be used to study the regime where N → ∞. We derive an exact channel gain expression that captures three essential near-field behaviors and use it to revisit the power scaling laws. We derive new finite asymptotic SNR limits but also conclude that these are unlikely to be approached in practice. We further prove that an IRS-aided setup cannot achieve a higher SNR than an equal-sized Massive MIMO setup, despite its faster SNR growth. We quantify analytically how much larger the IRS must be to achieve the same SNR. Finally, we show that an optimized IRS does not behave as an “anomalous” mirror but can vastly outperform that benchmark.","['Signal to noise ratio', 'Relays', 'Receiving antennas', 'MIMO communication', 'Phased arrays', 'Gain']","['Intelligent reflecting surface', 'reconfigurable intelligent surface', 'software-controlled meta-surface', 'massive MIMO', 'regenerative MIMO relays', 'asymptotic limits', 'power scaling law', 'near-field', 'far-field']"
"Although the fifth generation (5G) wireless networks are yet to be fully investigated, the visionaries of the 6th generation (6G) echo systems have already come into the discussion. Therefore, in order to consolidate and solidify the security and privacy in 6G networks, we survey how security may impact the envisioned 6G wireless systems, possible challenges with different 6G technologies, and the potential solutions. We provide our vision on 6G security and security key performance indicators (KPIs) with the tentative threat landscape based on the foreseen 6G network architecture. Moreover, we discuss the security and privacy challenges that may encounter with the available 6G requirements and potential 6G applications. We also give the reader some insights into the standardization efforts and research-level projects relevant to 6G security. In particular, we discuss the security considerations with 6G enabling technologies such as distributed ledger technology (DLT), physical layer security, distributed AI/ML, visible light communication (VLC), THz, and quantum computing. All in all, this work intends to provide enlightening guidance for the subsequent research of 6G security and privacy at this initial phase of vision towards reality.","['6G mobile communication', 'Security', 'Privacy', '5G mobile communication', 'Computer architecture', 'Wireless communication', 'Distributed ledger']","['6G', 'security', 'security threats', 'AI/ML security', 'DLT', 'physical layer security', 'privacy', 'quantum computing']"
"This paper investigates the problem of detection and classification of unmanned aerial vehicles (UAVs) in the presence of wireless interference signals using a passive radio frequency (RF) surveillance system. The system uses a multistage detector to distinguish signals transmitted by a UAV controller from the background noise and interference signals. First, RF signals from any source are detected using a Markov models-based naïve Bayes decision mechanism. When the receiver operates at a signal-to-noise ratio (SNR) of 10 dB, and the threshold, which defines the states of the models, is set at a level 3.5 times the standard deviation of the preprocessed noise data, a detection accuracy of 99.8% with a false alarm rate of 2.8% is achieved. Second, signals from Wi-Fi and Bluetooth emitters, if present, are detected based on the bandwidth and modulation features of the detected RF signal. Once the input signal is identified as a UAV controller signal, it is classified using machine learning (ML) techniques. Fifteen statistical features extracted from the energy transients of the UAV controller signals are fed to neighborhood component analysis (NCA), and the three most significant features are selected. The performance of the NCA and five different ML classifiers are studied for 15 different types of UAV controllers. A classification accuracy of 98.13% is achieved by k-nearest neighbor classifier at 25 dB SNR. Classification performance is also investigated at different SNR levels and for a set of 17 UAV controllers which includes two pairs from the same UAV controller models.","['Radio frequency', 'Drones', 'Wireless fidelity', 'Feature extraction', 'RF signals', 'Interference']","['Interference', 'machine learning', 'Markov models', 'RF fingerprinting', 'unmanned aerial vehicles (UAVs)', 'UAV detection and classification']"
"In the past few years, a large body of literature has been created on downlink Non-Orthogonal Multiple Access (NOMA), employing superposition coding and Successive Interference Cancellation (SIC), in multi-antenna wireless networks. Furthermore, the benefits of NOMA over Orthogonal Multiple Access (OMA) have been highlighted. In this paper, we take a critical and fresh look at the downlink Next Generation Multiple Access (NGMA) literature. Instead of contrasting NOMA with OMA, we contrast NOMA with two other multiple access baselines. The first is conventional Multi-User Linear Precoding (MU-LP), as used in Space-Division Multiple Access (SDMA) and multi-user Multiple-Input Multiple-Output (MIMO) in 4G and 5G. The second, called Rate-Splitting Multiple Access (RSMA), is based on multi-antenna Rate-Splitting (RS). It is also a non-orthogonal transmission strategy relying on SIC developed in the past few years in parallel and independently from NOMA. We show that there is some confusion about the benefits of NOMA, and we dispel the associated misconceptions. First, we highlight why NOMA is inefficient in multi-antenna settings based on basic multiplexing gain analysis. We stress that the issue lies in how the NOMA literature, originally developed for single-antenna setups, has been hastily applied to multi-antenna setups, resulting in a misuse of spatial dimensions and therefore loss in multiplexing gains and rate. Second, we show that NOMA incurs a severe multiplexing gain loss despite an increased receiver complexity due to an inefficient use of SIC receivers. Third, we emphasize that much of the merits of NOMA are due to the constant comparison to OMA instead of comparing it to MU-LP and RS baselines. We then expose the pivotal design constraint that multi-antenna NOMA requires one user to fully decode the messages of the other users. This design constraint is responsible for the multiplexing gain erosion, rate and spectral efficiency loss, ineffectiveness to serve a large number of users, and inefficient use of SIC receivers in multi-antenna settings. Our analysis and simulation results confirm that NOMA should not be applied blindly to multi-antenna settings, highlight the scenarios where MU-LP outperforms NOMA and vice versa, and demonstrate the inefficiency, performance loss, and complexity disadvantages of NOMA compared to RSMA. The first takeaway message is that, while NOMA is suited for single-antenna settings (as originally intended), it is not efficient in most multi-antenna deployments. The second takeaway message is that another non-orthogonal transmission framework, based on RSMA, exists which fully exploits the multiplexing gain and the benefits of SIC to boost the rate and the number of users to serve in multi-antenna settings and outperforms both NOMA and MU-LP. Indeed, RSMA achieves higher multiplexing gains and rates, serves a larger number of users, is more robust to user deployments, network loads and inaccurate channel state information and has a lower receiver complexity than NOMA. Consequently, RSMA is a promising technology for NGMA and future networks such as 6G and beyond.","['Multiplexing', 'NOMA', 'Silicon carbide', 'Wireless networks', 'Receivers', 'Downlink', 'Complexity theory']","['Multiple antennas', 'downlink', 'non-orthogonal multiple access', 'superposition coding', 'rate-splitting multiple access', 'broadcast channel', 'multiuser linear precoding', 'multiuser multiple-input multiple-output', 'space division multiple access', 'next generation multiple access']"
"Millimeter-wave (mm-wave) communication is a key technology for future wireless networks. To combat significant path loss and exploit the abundant mm-wave spectrum, effective beamforming is crucial. Nevertheless, conventional fully digital beamforming techniques are inapplicable, as they demand a separate radio frequency (RF) chain for each antenna element, which is costly and consumes too much energy. Hybrid beamforming is a cost-effective alternative, which can significantly reduce the hardware cost and power consumption by employing a small number of RF chains. This paper presents a holistic view on hybrid beamforming for 5G and beyond mm-wave systems, based on a new taxonomy for different hardware structures. We take a pragmatic approach and compare different proposals from three key aspects: 1) hardware efficiency , i.e., the required hardware components; 2) computational efficiency of the associated beamforming algorithm; and 3) achievable spectral efficiency , a main performance indicator. Through systematic comparisons, the interplay and trade-off among these three design aspects are demonstrated, and promising candidates for hybrid beamforming in future wireless networks are identified.","['Array signal processing', 'Hardware', 'Radio frequency', 'Phase shifters', 'Spectral efficiency', '5G mobile communication', 'Complexity theory']","['Hybrid beamforming', 'millimeter-wave communications', '5G and beyond', 'wireless communications']"
"We propose two types of intelligent reflecting systems based on programmable metasurfaces and mirrors to focus the incident optical power towards a visible light communication receiver. We derive the required phase gradients for the metasurface array reflector and the required orientations of each mirror in the mirrors array reflector to achieve power focusing. Based on which, we derive the irradiance expressions for the two systems in the detector plane to characterize their performance in terms of aiming and focusing capabilities. We show analytically that the number of reflecting elements along with the relative source - reflector dimensions determine the system power focusing capability. Moreover, we quantify analytically the received power gain compared with reflector-free systems. In addition, we introduce a new simple metric to assess the relative reflectors' performance for a given source, detector, reflector layout. Finally, we verify the analytical findings regarding absolute and relative reflectors' performance via numerical simulations.","['Optical transmitters', 'Mirrors', 'Detectors', 'Solids', 'Radio transmitters', 'Optical receivers', 'Measurement']","['Channel modeling', 'intelligent reflecting surfaces', 'metasurfaces', 'mirror arrays', 'visible light communications']"
"Channel state information (CSI), which enables wireless systems to adapt their transmission parameters to instantaneous channel conditions and consequently achieve great performance boost, plays an increasingly vital role in mobile communications. However, getting accurate CSI is challenging due mainly to rapid channel variation caused by multi-path fading. The inaccuracy of CSI imposes a severe impact on the performance of a wide range of adaptive wireless systems, highlighting the significance of channel prediction that can combat outdated CSI effectively. The aim of this article is to shed light on the state of the art in this field and then go beyond by proposing a novel predictor that leverages the strong time-series prediction capability of deep recurrent neural networks incorporating long short-term memory or gated recurrent unit. In addition to an analytical comparison of computational complexity, performance evaluation in terms of prediction accuracy is carried out upon multi-antenna fading channels. Numerical results reveal that deep learning brings a notable performance gain compared with the conventional predictors built on shallow recurrent neural networks.","['Fading channels', 'MIMO communication', 'Delays', 'Channel estimation', 'Wireless communication', 'Recurrent neural networks', 'Deep learning']","['5G', 'artificial Intelligence', 'channel prediction', 'channel state information', 'deep learning', 'GRU', 'LSTM', 'machine learning', 'multi-antenna system', 'recurrent neural network']"
"Originally introduced in the early 2010’s, the idea of smart environments through reconfigurable intelligent surfaces (RIS) controlling the reflections of the electromagnetic waves has attracted much attention in recent years in preparation for the future 6G. Since reconfigurable intelligent surfaces are not based on increasing the number of sources, they could indeed pave the way to greener and potentially limitless wireless communications. In this paper, we design, model and demonstrate experimentally a millimeter wave reconfigurable intelligent surface based on an electronically tunable metasurface with binary phase modulation. We first study numerically the unit cell of the metasurface, based on a PIN diode, and obtain a good phase shift and return loss for both polarizations, over a wide frequency range around 28.5 GHz. We then fabricate and characterize the unit cell and verify its properties, before fabricating the whole 10 cm×10cm reconfigurable intelligent surface. We propose an analytical description of the use that can be done of the binary phase RIS, both in the near field (reflectarray configuration) and in the far field (access point extender). We finally verify experimentally that the designed RIS works as expected, performing laboratory experiments of millimeter wave beamforming both in the near field and far field configuration. Our experimental results demonstrate the high efficiency of our binary phase RIS to control millimeter waves in any kind of scenario and this at the sole cost of the energy dissipated by the PIN diodes used in our design.","['Resonant frequency', 'PIN photodiodes', 'Wireless communication', 'Surface waves', 'Metasurfaces', 'Millimeter wave technology', 'Millimeter wave communication']","['6G', 'binary tunable metasurface', 'mmWave', 'reflect array', 'RIS', 'smart environment']"
"Unmanned aerial vehicles (UAVs) are considered as one of the promising technologies for the next-generation wireless communication networks. Their mobility and their ability to establish line of sight (LOS) links with the users made them key solutions for many potential applications. In the same vein, artificial intelligence (AI) is growing rapidly nowadays and has been very successful, particularly due to the massive amount of the available data. As a result, a significant part of the research community has started to integrate intelligence at the core of UAVs networks by applying AI algorithms in solving several problems in relation to drones. In this article, we provide a comprehensive overview of some potential applications of AI in UAV-based networks. We also highlight the limits of the existing works and outline some potential future applications of AI for UAVs networks.","['Task analysis', 'Drones', 'Tutorials', 'Collaborative work', 'Reinforcement learning', 'Classification algorithms', 'Training']","['Artificial intelligence', 'deep learning', 'federated learning', 'machine learning', 'reinforcement learning', 'UAVs']"
"In this article, we introduce and study the potentials and challenges of integrated access and backhaul (IAB) as one of the promising techniques for evolving 5G networks. We study IAB networks from different perspectives. We summarize the recent Rel-16 as well as the upcoming Rel-17 3GPP discussions on IAB, and highlight the main IAB-specific agreements on different protocol layers. Also, concentrating on millimeter wave-based communications, we evaluate the performance of IAB networks in both dense and suburban areas. Using a finite stochastic geometry model, with random distributions of IAB nodes as well as user equipments (UEs) in a finite region, we study the service coverage rate defined as the probability of the event that the UEs' minimum rate requirements are satisfied. We present comparisons between IAB and hybrid IAB/fiber-backhauled networks where a part or all of the small base stations are fiber-connected. Finally, we study the robustness of IAB networks to weather and various deployment conditions and verify their effects, such as blockage, tree foliage, rain as well as antenna height/gain on the coverage rate of IAB setups, as the key differences between the fiber-connected and IAB networks. As we show, IAB is an attractive approach to enable the network densification required by 5G and beyond.","['Wireless communication', '3GPP', '5G mobile communication', 'Microwave communication', 'Protocols', 'Resource management']","['Integrated access and backhaul', 'IAB', 'densification', 'millimeter wave (mmWave) communications', '3GPP', 'stochastic geometry', 'Poisson point process', 'coverage probability', 'germ-grain model', 'ITU-R', 'FITU-R', 'wireless backhaul', '5G NR', 'rain', 'tree foliage', 'blockage', 'relay']"
"The future 5G networks are expected to use millimeter wave (mmWave) frequency bands to take advantage of the large unused spectrum. However, due to the high path loss at mmWave frequencies, coverage of mmWave signals can get severely reduced, especially for non-line-of-sight (NLOS) scenarios as mmWave signals are severely attenuated when going through obstructions. In this work, we study the use of passive metallic reflectors of different shapes/sizes to improve 28 GHz mmWave signal coverage for both indoor and outdoor NLOS scenarios. We quantify the gains that can be achieved in the link quality with metallic reflectors using measurements, analytical expressions, and ray tracing simulations. In particular, we provide an analytical model for the end-to-end received power in an NLOS scenario using reflectors of different shapes and sizes. For a given size of the flat metallic sheet reflector approaching to the size of the incident beam, we show that the reflected received power for the NLOS link is the same as line-of-sight (LOS) free space received power of the same link distance. Extensive results are provided to study the impact of environmental features and reflector characteristics on NLOS link quality.","['Reflection', 'Receivers', 'Shape', 'Antenna measurements', 'Transmitting antennas', 'Repeaters']","['Coverage', 'indoor', 'mmWave', 'non-line-of-sight (NLOS)', 'outdoor', 'PXI', 'ray tracing (RT)', 'reflector']"
"Harvesting data from distributed Internet of Things (IoT) devices with multiple autonomous unmanned aerial vehicles (UAVs) is a challenging problem requiring flexible path planning methods. We propose a multi-agent reinforcement learning (MARL) approach that, in contrast to previous work, can adapt to profound changes in the scenario parameters defining the data harvesting mission, such as the number of deployed UAVs, number, position and data amount of IoT devices, or the maximum flying time, without the need to perform expensive recomputations or relearn control policies. We formulate the path planning problem for a cooperative, non-communicating, and homogeneous team of UAVs tasked with maximizing collected data from distributed IoT sensor nodes subject to flying time and collision avoidance constraints. The path planning problem is translated into a decentralized partially observable Markov decision process (Dec-POMDP), which we solve through a deep reinforcement learning (DRL) approach, approximating the optimal UAV control policy without prior knowledge of the challenging wireless channel characteristics in dense urban environments. By exploiting a combination of centered global and local map representations of the environment that are fed into convolutional layers of the agents, we show that our proposed network architecture enables the agents to cooperate effectively by carefully dividing the data collection task among themselves, adapt to large complex environments and state spaces, and make movement decisions that balance data collection goals, flight-time efficiency, and navigation constraints. Finally, learning a control policy that generalizes over the scenario parameter space enables us to analyze the influence of individual parameters on collection performance and provide some intuition about system-level benefits.","['Path planning', 'Robot sensing systems', 'Training', 'Data collection', 'Wireless communication', 'Task analysis', 'Navigation']","['Internet of Things (IoT)', 'map-based planning', 'multi-agent reinforcement learning (MARL)', 'trajectory planning', 'unmanned aerial vehicle (UAV)']"
"We consider multi-antenna wireless systems aided by reconfigurable intelligent surfaces (RIS). RIS presents a new physical layer technology for improving coverage and energy efficiency by intelligently controlling the propagation environment. In practice however, achieving the anticipated gains of RIS requires accurate channel estimation. Recent attempts to solve this problem have considered the least-squares (LS) approach, which is simple but also sub-optimal. The optimal channel estimator, based on the minimum mean-squared-error (MMSE) criterion, is challenging to obtain and is non-linear due to the non-Gaussianity of the effective channel seen at the receiver. Here we present approaches to approximate the optimal MMSE channel estimator. As a first approach, we analytically develop the best linear estimator, the LMMSE, together with a corresponding majorization-minimization-based algorithm designed to optimize the RIS phase shift matrix during the training phase. This estimator is shown to yield improved accuracy over the LS approach by exploiting second-order statistical properties of the wireless channel and the noise. To further improve performance and better approximate the globally-optimal MMSE channel estimator, we propose data-driven non-linear solutions based on deep learning. Specifically, by posing the MMSE channel estimation problem as an image denoising problem, we propose two convolutional neural network (CNN)-based methods to perform the denoising and approximate the optimal MMSE channel estimation solution. Our numerical results show that these CNN-based estimators give superior performance compared with linear estimation approaches. They also have low computational complexity requirements, thereby motivating their potential use in future RIS-aided wireless communication systems.","['Channel estimation', 'Wireless communication', 'Training', 'MISO communication', 'Noise reduction', 'Discrete Fourier transforms', 'Physical layer']","['Reconfigurable intelligent surface', 'MISO', 'LMMSE', 'MMSE', 'majorization-minimization', 'deep learning', 'convolutional neural network', 'channel estimation', 'achievable rate']"
"In this article, we study two novel massive multiple-input multiple-output (MIMO) transmitter architectures for millimeter wave (mmWave) communications which comprise few active antennas, each equipped with a dedicated radio frequency (RF) chain, that illuminate a nearby large intelligent reflecting/transmitting surface (IRS/ITS). The IRS (ITS) consists of a large number of low-cost and energy-efficient passive antenna elements which are able to reflect (transmit) a phase-shifted version of the incident electromagnetic field. Similar to lens array (LA) antennas, IRS/ITS-aided antenna architectures are energy efficient due to the almost lossless over-the-air connection between the active antennas and the intelligent surface. However, unlike for LA antennas, for which the number of active antennas has to linearly grow with the number of passive elements (i.e., the lens aperture) due to the non-reconfigurablility (i.e., non-intelligence) of the lens, for IRS/ITS-aided antennas, the reconfigurablility of the IRS/ITS facilitates scaling up the number of radiating passive elements without increasing the number of costly and bulky active antennas. We show that the constraints that the precoders for IRS/ITS-aided antennas have to meet differ from those of conventional MIMO architectures. Taking these constraints into account and exploiting the sparsity of mmWave channels, we design two efficient precoders; one based on maximizing the mutual information and one based on approximating the optimal unconstrained fully digital (FD) precoder via the orthogonal matching pursuit algorithm. Furthermore, we develop a power consumption model for IRS/ITS-aided antennas that takes into account the impacts of the IRS/ITS imperfections, namely the spillover loss, taper loss, aperture loss, and phase shifter loss. Moreover, we study the effect that the various system parameters have on the achievable rate and show that a proper positioning of the active antennas with respect to the IRS/ITS leads to a considerable performance improvement. Our simulation results reveal that unlike conventional MIMO architectures, IRS/ITS-aided antennas are both highly energy efficient and fully scalable in terms of the number of transmitting (passive) antennas. Therefore, IRS/ITS-aided antennas are promising candidates for realizing the potential of mmWave ultra massive MIMO communications in practice.","['Radio frequency', 'Transmitting antennas', 'Scattering', 'Computer architecture', 'Massive MIMO', 'Reflector antennas', 'Energy efficiency']","['Intelligent reflecting/transmitting surfaces', 'reflect/transmit array', 'lens array', 'hybrid MIMO', 'mmWave communications', 'scalability', 'energy efficiency']"
"With the emergence of the Internet of Things (IoT) technology, wireless connectivity should be more ubiquitous than ever. In fact, the availability of wireless connection everywhere comes with security threats that, unfortunately, cannot be handled by conventional cryptographic solutions alone, especially in heterogeneous and decentralized future wireless networks. In general, physical layer security (PLS) helps in bridging this gap by taking advantage of the fading propagation channel. Moreover, the adoption of reconfigurable intelligent surfaces (RIS) in wireless networks makes the PLS techniques more efficient by involving the channel into the design loop. In this article, we conduct a comprehensive literature review on the RIS-assisted PLS for future wireless communications. We start by introducing the basic concepts of RISs and their different applications in wireless communication networks and the most common PLS performance metrics. Then, we focus on the review and classification of RIS-assisted PLS applications, exhibiting multiple scenarios, system models, objectives, and methodologies. In fact, most of the works in this field formulate an optimization problem to maximize the secrecy rate (SR) or secrecy capacity (SC) at a legitimate user by jointly optimizing the beamformer at the transmitter and the RIS’s coefficients, while the differences are in the adopted methodology to optimally/sub-optimally approach the solution. We finalize this survey by presenting some insightful recommendations and suggesting open problems for future research extensions.","['Measurement', 'Array signal processing', 'Security', 'Reliability', 'Communication system security', 'Wireless networks']","['Physical layer security (PLS)', 'reconfigurable intelligent surface (RIS)', 'secrecy outage probability', 'secrecy rate']"
"In this work, we investigate the probability distribution function of the channel fading between a base station, an array of intelligent reflecting elements, known as large intelligent surfaces (LIS), and a single-antenna user. We assume that both fading channels, i.e., the channel between the base station and the LIS, and the channel between the LIS and the single user are Nakagami-m distributed. Additionally, we derive the exact bit error probability considering quadrature amplitude (M-QAM) and binary phase-shift keying (BPSK) modulations when the number of LIS elements, n, is equal to 2 and 3. We assume that the LIS can perform phase adjustment, but there is a residual phase error modeled by a Von Mises distribution. Based on the central limit theorem, and considering a large number of reflecting elements, we also present an accurate approximation and upper bounds for the bit error rate. Through several Monte Carlo simulations, we demonstrate that all derived expressions perfectly match the simulated results.","['Fading channels', 'Base stations', 'Signal to noise ratio', 'Error probability', 'Channel estimation', 'Probability density function', 'Optimization']","['Bit error rate', 'large intelligent surfaces', 'massive MIMO', 'Nakagami-m fading', 'von Mises circular distribution']"
"The fifth-generation (5G) of cellular networks and beyond requires massive connectivity, high data rates, and low latency. Millimeter-wave (mmWave) communications is a key 5G enabling technology to meet these requirements thanks to its technical potentials that can be integrated with other 5G enablers such as ultra-dense networks (UDNs) and massive multiple-input-multiple-output (massive MIMO) systems. However, some technical challenges, which are mainly related to specific characteristics of mmWave propagation, must be addressed. All the aforementioned points will be discussed in this article before presenting the different existing architectures of massive MIMO mmWave systems. This survey mainly aims at presenting a comprehensive state-of-the-art review of the channel estimation techniques associated with the different mmWave system architectures. Subsequently, we will provide a comparison among existing solutions in terms of their respective benefits and shortcomings. Finally, some open directions of research are discussed, and challenges that wait to be met are pointed out.","['Channel estimation', 'Antenna arrays', 'MIMO communication', 'Matching pursuit algorithms', '5G mobile communication', 'Array signal processing', 'Lenses']","['Millimeter-wave communications', 'massive MIMO', 'channel estimation', '5G', 'cellular systems', 'spatial channel model', 'lens antenna array', 'hybrid architecture', 'few-bit ADCs', 'compressive sensing', 'beamforming']"
"The concept of Smart Cities has been introduced as a way to benefit from the digitization of various ecosystems at a city level. To support this concept, future communication networks need to be carefully designed with respect to the city infrastructure and utilization of resources. Recently, the idea of ‘smart’ environment, which takes advantage of the infrastructure in order to enable better performance of wireless networks, has been proposed. This idea is aligned with the recent advances in design of reconfigurable intelligent surfaces (RISs), which are planar structures with the capability to reflect impinging electromagnetic waves toward preferred directions. Thus, RISs are expected to provide the necessary flexibility for the design of the ‘smart’ communication environment, which can be optimally shaped to enable cost- and energy-efficient signal transmissions where needed. Upon deployment of RISs, the ecosystem of the Smart Cities would become even more controllable and adaptable, which would subsequently ease the implementation of future communication networks in urban areas and boost the interconnection among private households and public services. In this article, we provide our vision on RIS integration into future Smart Cities by pointing out some forward-looking new application scenarios and use cases and by highlighting the potential advantages of RIS deployment. To this end, we identify the most promising research directions and opportunities. The respective design problems are formulated mathematically. Moreover, we focus the discussion on the key enabling aspects for RIS-assisted Smart Cities, which require substantial research efforts such as pilot decontamination, precoding for large multiuser networks, distributed operation and control of RISs. These contributions pave the road to a systematic design of RIS-assisted communication networks for Smart Cities in the years to come.","['Smart cities', 'Precoding', 'Decontamination', 'Wireless networks', 'Ecosystems', 'Reconfigurable intelligent surfaces', 'Communication networks']","['Reconfigurable intelligent surfaces', 'smart cities', 'channel estimation', 'Internet of Things', 'precoding', 'relaying', 'research challenges', 'unmanned aerial vehicles', 'vehicular communications']"
"In the present work, we investigate the impact of transceiver hardware imperfection on reconfigurable intelligent surface (RIS)-assisted wireless systems. In this direction, first, we present a general model that accommodates the impact of the transmitter (TX) and receiver (RX) radio frequency impairments. Next, we derive novel closed-form expressions for the instantaneous end-to-end signal-to-noise-plus-distortion-ratio (SNDR). Building upon these expressions, we extract an exact closed-form expression for the system's outage probability, which allows us not only to quantify RIS-assisted systems' outage performance but also reveals that the maximum allowed spectral efficiency of the transmission scheme is limited by the levels of the transceiver hardware imperfection. Likewise, a diversity analysis is provided. Moreover, in order to characterize the capacity of RIS-assisted systems, we report a new upper-bound for the ergodic capacity, which takes into account the number of the RIS's reflective units (RUs), the level of TX and RX hardware imperfection, as well as the transmission signal-to-noise-ratio (SNR). Finally, two insightful ergodic capacity ceilings are extracted for the high-SNR and high-RUs regimes. Our results highlight the importance of accurately modeling the transceiver hardware imperfection and reveals that they significantly limit the RIS-assisted wireless system performance.","['Hardware', 'Wireless communication', 'Probability', 'Transceivers', 'Power system reliability', 'Signal to noise ratio', 'Distortion']","['Diversity order', 'ergodic capacity', 'outage probability', 'performance analysis', 'reconfigurable intelligent surfaces']"
"We study support for unmanned aerial vehicle (UAV) communications through a cell-free massive MIMO architecture, wherein a large number of access points (APs) is deployed in place of large co-located massive MIMO arrays. We consider also a variation of the pure cell-free architecture by applying a user-centric association approach, where each user is served only from a subset of APs in the network. Under the general assumption that the propagation channel between the mobile stations, either UAVs or ground users (GUEs), and the APs follows a Ricean distribution, we derive closed form spectral efficiency lower bounds for uplink and downlink with linear minimum mean square error channel estimation. We consider several power allocation and user scheduling strategies for such a system, and, among these, also minimum-rate maximizing power allocation strategies to improve the system fairness. Our numerical results reveal that cell-free massive MIMO architecture and its low-complexity user-centric alternative may provide better performance than a traditional multi-cell massive MIMO network deployment.","['Massive MIMO', 'Resource management', 'Channel estimation', 'Downlink', 'Uplink', 'Fading channels', 'Antenna arrays']","['Cell-free massive MIMO', 'Ricean fading channel', 'spectral efficiency', 'power allocation', 'UAV communications', 'user-centric']"
"Reconfigurable intelligent surfaces (RISs) have promising coverage and data-rate gains for wireless communication systems in 5G and beyond. Prior work has mainly focused on analyzing the performance of these surfaces using simulations or lab-level prototypes. To draw accurate insights about the actual performance of these systems, this paper develops an RIS proof-of-concept prototype and extensively evaluates its potential gains in the field under realistic wireless communication settings. In particular, a 160-element RIS, operating at a 5.8 GHz band, is designed, fabricated, and accurately measured in the anechoic chamber. This surface is then integrated into a wireless communication system and the beamforming gains, pathloss, and coverage improvements are evaluated in realistic outdoor communication scenarios. When both the transmitter and receiver employ directional antennas, the developed RIS achieves 15–20 dB gain in the signal-to-noise ratio (SNR) in a range of ±60° beamforming angles. In terms of coverage, and considering a far-field experiment with a blockage between a basestation and a grid of mobile users and with an average signal path of 35 m, the RIS provides an average SNR improvement of 6 dB (max 8 dB) within an area > 75 m 2 . Thanks to the scalable RIS design, these SNR gains can be directly increased with larger RIS areas. For example, a single 1,600-element RIS with the same design is expected to provide 26 dB SNR gain for a similar deployment (theoretical estimation). These results draw useful insights into the design and performance of RIS systems and provide an important proof for their potential gains in real-world far-field wireless communication environments.","['Wireless communication', 'Surface waves', 'Array signal processing', 'Gain', 'Topology', 'Switches', 'Wireless sensor networks']","['Reconfigurable intelligent surfaces', 'prototype', 'beamforming', 'coverage', 'sub-6GHz']"
"The deployment of the fifth-generation (5G) wireless communication services requires the installation of 5G next-generation Node-B Base Stations (gNBs) over the territory and the wide adoption of 5G User Equipment (UE). In this context, the population is concerned about the potential health risks associated with the Radio Frequency (RF) emissions from 5G equipment, with several communities actively working toward stopping the 5G deployment. To face these concerns, in this work, we analyze the health risks associated with 5G exposure by adopting a new and comprehensive viewpoint, based on the communications engineering perspective. By exploiting our background, we investigate the alleged health effects of 5G exposure and critically review the latest works that are often referenced to support the health concerns from 5G. We then precisely examine the up-to-date metrics, regulations, and assessment of compliance procedures for 5G exposure, by evaluating the latest guidelines from the Institute of Electrical and Electronics Engineers (IEEE), the International Commission on Non-Ionizing Radiation Protection (ICNIRP), the International Telecommunication Union (ITU), the International Electrotechnical Commission (IEC), and the United States Federal Communications Commission (FCC), as well as the national regulations in more than 220 countries. We also thoroughly analyze the main health risks that are frequently associated with specific 5G features (e.g., multiple-input multiple-output (MIMO), beamforming, cell densification, adoption of millimeter waves, and connection of millions of devices). Finally, we examine the risk mitigation techniques based on communications engineering that can be implemented to reduce the exposure from 5G gNB and UE. Overall, we argue that the widely perceived health risks that are attributed to 5G are not supported by scientific evidence from communications engineering. In addition, we explain how the solutions to minimize the health risks from 5G (including currently unknown effects) are already mature and ready to be implemented. Finally, future works, e.g., aimed at evaluating long-term impacts of 5G exposure, as well as innovative solutions to further reduce the RF emissions, are suggested.","['5G mobile communication', 'Regulation', 'Statistics', 'Sociology', 'Risk management', 'Measurement', 'ITU']","['5G', 'health risks', 'health effects', 'EMF exposure', 'EMF regulations', 'EMF metrics', 'assessment of compliance', '5G features', 'risk mitigation']"
"Massive and ubiquitous deployment of devices in networks of fifth generation (5G) and beyond wireless has necessitated the development of ultra-low-power wireless communication paradigms. Recently, wireless-powered networks with backscatter communications (WPN-BCs) has been emerged as a most prominent technology for enabling large-scale self-sustainable wireless networks with the capabilities of RF energy harvesting (EH) and of extreme low power consumption. Therefore, we provide a comprehensive literature review on the fundamentals, challenges and the on-going research efforts in the domain of WPN-BCs. Our emphasis is on large-scale networks. In particular, we discuss signal processing aspects, network design issues and efficient communication techniques. Moreover, we review emerging technologies for WPN-BCs to bring about the best use of resources. Some applications of this innovative technology are also highlighted. Finally, we address some open research problems and future research directions.","['Backscatter', 'Radio frequency', 'Wireless communication', 'Receivers', 'RF signals', 'Wireless sensor networks']","['Backscatter communication systems', 'wireless-powered networks', 'large-scale communication systems', 'IoT networks', 'massive machine type communications']"
"Unmanned aerial vehicle (UAV)-based communications is a promising new technology that can add a wide range of new capabilities to the current network infrastructure. Given the flexibility, cost-efficiency, and convenient use of UAVs, they can be deployed as temporary base stations (BSs) for on-demand situations like BS overloading or natural disasters. In this work, a UAV-based communication system with radio frequency (RF) access links to the mobile users (MUs) and a free-space optical (FSO) backhaul link to the ground station (GS) is considered. In particular, the RF and FSO channels in this network depend on the UAV's positioning and (in)stability. The relative position of the UAV with respect to the MUs impacts the likelihood of a line-of-sight (LOS) connection in the RF link and the instability of the hovering UAV affects the quality of the FSO channel. Thus, taking these effects into account, we analyze the end-to-end system performance of networks employing UAVs as buffer-aided (BA) and non-buffer-aided (non-BA) relays in terms of the ergodic sum rate. Simulation results validate the accuracy of the proposed analytical derivations and reveal the benefits of buffering for compensation of the random fluctuations caused by the UAV's instability. Our simulations also show that the ergodic sum rate of both BA and non-BA UAV-based relays can be enhanced considerably by optimizing the positioning of the UAV. We further study the impact of the MU density and the weather conditions on the end-to-end system performance.","['Radio frequency', 'Unmanned aerial vehicles', 'Relay networks (telecommunications)', 'Optical buffering', 'Laser beams']","['Unmanned aerial vehicle (UAV)', 'free-space optical (FSO)', 'buffer-aided (BA) relays', 'non-buffer-aided (non-BA) relays', 'instability', 'positioning']"
"The sixth generation (6G) of mobile network will be composed by different nodes, from macro-devices (satellite) to nano-devices (sensors inside the human body), providing a full connectivity fabric all around us. These heterogeneous nodes constitute an ultra dense network managing tons of information, often very sensitive. To trust the services provided by such network, security is a mandatory feature by design. In this scenario, physical-layer security (PLS) can act as a first line of defense, providing security even to low-resourced nodes in different environments. This paper discusses challenges, solutions and visions of PLS in beyond-5G networks.","['6G mobile communication', 'Satellites', 'Fabrics', 'Sensors', 'Security', 'Communication networks']","['6G', 'physical-layer security', 'MIMO', 'IRS', 'visible light communications', 'authentication', 'key distribution']"
"Open Radio Access Network (O-RAN) alliance was recently launched to devise a new RAN architecture featuring open, software-driven, virtual, and intelligent radio access architecture. O-RAN architecture is based on (1) disaggregated RAN functions that run as Virtual Network Function (VNF) and Physical Network Function (PNF); (2) the notion of RAN controller that runs centrally RAN applications such as mobility management, users’ scheduling, radio resources allocation, etc. The RAN controller is in charge of enforcing the application decisions by using open interfaces with the RAN functions. One important feature introduced by O-RAN is the heavy usage of Machine Learning (ML) techniques, particularly Deep Learning (DL), to foster innovation and ease the deployment of intelligent RAN applications that are able to fulfill the Quality of Service (QoS) requirements of the envisioned 5G and beyond network services. In this work, we first give an overview of the evolution of RAN architectures toward 5G and beyond, namely C-RAN, vRAN, and O-RAN. We also compare them based on various perspectives, such as edge support, virtualization, control and management, energy consumption, and AI support. Then, we review existing DL-based solutions addressing the RAN part. We also show how they can be integrated/mapped to the O-RAN architecture since these works were not initially adapted to the O-RAN architecture. In addition, we present two case studies for DL techniques deployment in O-RAN. Furthermore, we describe how the main steps of deployed DL models in O-RAN can be automated, to ensure stable performance of these models, introducing ML system operations (MLOps) concept in O-RAN. Finally, we identify key technical challenges, open issues, and future research directions related to the Artificial Intelligence (AI)-enabled O-RAN architecture.","['Computer architecture', '5G mobile communication', 'Radio access networks', 'Cloud computing', 'Deep learning', 'Resource management', 'Industries']","['B5G networks', 'RAN', 'open RAN architecture', 'RAN intelligent controller', 'deep learning', 'MLOps']"
"This paper surveys the literature on point-to-point (P2P) links for integrated satellite-aerial networks, which are envisioned to be among the key enablers of the sixth-generation (6G) of wireless networks vision. The paper first outlines the unique characteristics of such integrated large-scale complex networks, often denoted by spatial networks, and focuses on two particular space-air infrastructures, namely, satellites networks and high-altitude platforms (HAPs). The paper then classifies the connecting P2P communications links as satellite-to-satellite links at the same layer (SSLL), satellite-to-satellite links at different layers (SSLD), and HAP-to-HAP links (HHL). The paper surveys each layer of such spatial networks separately, and highlights the possible natures of the connecting links (i.e., radio-frequency or free-space optics) with a dedicated survey of the existing link-budget results. The paper, afterwards, presents the prospective merit of realizing such an integrated satellite-HAP network towards providing broadband services in under-served and remote areas. Finally, the paper sheds light on several future research directions in the context of spatial networks, namely large-scale network optimization, intelligent offloading, smart platforms, energy efficiency, multiple access schemes, distributed spatial networks, and routing.","['Satellites', 'Radio frequency', 'Satellite broadcasting', 'Earth', 'Computer architecture', 'Nonhomogeneous media', 'Low earth orbit satellites']","['Integrated satellite-aerial networks', 'spatial networks', 'satellites', 'high-altitude platforms', 'broadband services']"
"Internet of Things (IoT) is an emerging paradigm that is turning and revolutionizing worldwide cities into smart cities. However, this emergence is accompanied with several cybersecurity concerns due mainly to the data sharing and constant connectivity of IoT networks. To address this problem, multiple Intrusion Detection Systems (IDSs) have been designed as security mechanisms, which showed their efficiency in mitigating several IoT-related attacks, especially when using deep learning (DL) algorithms. Indeed, Deep Neural Networks (DNNs) significantly improve the detection rate of IoT-related intrusions. However, DL-based models are becoming more and more complex, and their decisions are hardly interpreted by users, especially companies’ executive staff and cybersecurity experts. Hence, the corresponding users cannot neither understand and trust DL models decisions, nor optimize their decisions (users) based on DL models outputs. To overcome these limits, Explainable Artificial Intelligence (XAI) is an emerging paradigm of Artificial Intelligence (AI), that provides a set of techniques to help interpreting and understanding predictions made by DL models. Thus, XAI enables to explain the decisions of DL-based IDSs to make them interpretable by cybersecurity experts. In this paper, we design a new XAI-based framework to give explanations to any critical DL-based decisions for IoT-related IDSs. Our framework relies on a novel IDS for IoT networks, that we also develop by leveraging deep neural network, to detect IoT-related intrusions. In addition, our framework uses three main XAI techniques ( $i.e.$ , RuleFit, Local Interpretable Model-Agnostic Explanations (LIME), and SHapley Additive exPlanations (SHAP)), on top of our DNN-based model. Our framework can provide both local and global explanations to optimize the interpretation of DL-based decisions. The local explanations target a single/particular DL output, while global explanations focus on deducing the most important features that have conducted to each made decision (e.g., intrusion detection). Thus, our proposed framework introduces more transparency and trust between the decisions made by our DL-based IDS model and cybersecurity experts. Both NSL-KDD and UNSW-NB15 datasets are used to validate the feasibility of our XAI framework. The experimental results show the efficiency of our framework to improve the interpretability of the IoT IDS against well-known IoT attacks, and help the cybersecurity experts get a better understanding of IDS decisions.","['Internet of Things', 'Deep learning', 'Computer security', 'Computer architecture', 'Artificial intelligence', 'Intrusion detection', 'Predictive models']","['Internet of Things', 'intrusion detection system', 'deep learning', 'explainable artificial intelligence', 'local and global explanations']"
"Communication system's performance is sensitive to bandwidth, power, cost etc. There have been various solutions to improve the performance, out of them, one of the fundamental solutions over the years is design of optimum modulation schemes. As the research on beyond 5G heats up, we survey and explore power and bandwidth efficient modulation schemes for the next generation communication systems in details. In the existing literature, initially square quadrature amplitude modulation (SQAM) was considered. However, only square constellations are not sufficient for varying channel conditions and rate requirements, thus, efficient odd power of 2 constellations were introduced. For odd power of 2 constellations, rectangular QAM (RQAM) is most commonly used. However, RQAM is not a good choice and modified cross QAM (XQAM) constellation is preferred which provides improved power efficiency over RQAM due to its energy efficient two dimensional (2D) structure. The increasing demand for high data-rates has further encouraged research towards more compact 2D constellations which leads to hexagonal lattice structure based hexagonal QAM (HQAM) constellations. In this work, various QAM constellations are discussed and detailed study of star QAM, XQAM, and HQAM is presented. Generation, peak and average energies, peak-to-average-power ratio, symbol-error-rate, decision boundaries, bit mapping, Gray code penalty, and bit-error-rate of star QAM, XQAM, and HQAM constellations for different constellation orders are presented. Finally, a comparative study of various QAM constellations is presented which justifies the supremacy of HQAM over other QAM constellations for various wireless communication systems and a potential modulation scheme for future standards.","['Quadrature amplitude modulation', 'Modulation', 'Bandwidth', 'Wireless communication', 'Phase modulation', 'Frequency shift keying', 'Signal to noise ratio']","['Bit mapping', 'cross QAM', 'decision region', 'gray code penalty', 'hexagonal QAM', 'star QAM', 'symbol-error-rate']"
"The conceptualisation of the sixth generation of mobile wireless networks (6G) has already started with some potential disruptive technologies resonating as enablers for driving the emergence of a number of innovative applications. Particularly, 6G will be a prominent supporter for the evolution towards a truly Intelligent Transportation System and the realization of the Smart City concept by fulfilling the limitations of 5G, once vehicular networks are becoming highly dynamic and complex with stringent requirements on ultra-low latency, high reliability, and massive connections. More importantly, providing security and privacy to such critical systems should be a top priority as vulnerabilities can be catastrophic, thus there are huge concerns regarding data collected from sensors, people and their habits. In this paper, we provide a timely deliberation of the role that promissory 6G enabling technologies such as artificial intelligence, network softwarisation, network slicing, blockchain, edge computing, intelligent reflecting surfaces, backscatter communications, terahertz links, visible light communications, physical layer authentication, and cell-free massive multiple-input multiple-output (MIMO) will play on providing the expected level of security and privacy for the Internet of Vehicles.","['Security', '6G mobile communication', '5G mobile communication', 'Privacy', 'Vehicle-to-everything', 'Long Term Evolution', 'Data privacy']","['6G networks', 'Internet of Vehicles', 'privacy', 'security', 'vehicle-to-everything communications']"
"Reconfigurable Intelligent Surface (RIS) has emerged as a promising technology in wireless networks to achieve high spectrum and energy efficiency. RIS typically comprises a large number of low-cost nearly passive elements that can smartly interact with the impinging electromagnetic waves for performance enhancement. However, optimally configuring massive number of RIS elements remains a challenge. In this article, we present a machine learning (ML) based modeling approach that learns the interactions between the phase shifts of the RIS elements and receiver (Rx) location attributes and uses them to predict the achievable rate directly without using channel state information (CSI) or received pilots. Once learned, our model can be used to predict optimal RIS configuration for any new receiver locations in the same wireless network. We leverage deep learning (DL) techniques to build our model and study its performance and robustness. Simulation results demonstrate that the proposed DL model can recommend near-optimal RIS configurations for test receiver locations which achieved close to an upper bound performance that assumes perfect channel knowledge. Our DL model was trained using less than 2% of the total receiver locations. This promising result represents great potential in developing a practical solution for the optimal phase shifts of RIS elements without requesting CSI from the wireless network infrastructure.","['Optimization', 'Channel estimation', 'Array signal processing', 'Wireless networks', 'Receivers', 'Predictive models', 'Channel models']","['Reconfigurable intelligent surface', 'large intelligent surface', 'deep learning', 'channel estimation']"
"Reconfigurable intelligent surface (RIS)-assisted transmission and space shift keying (SSK) appear as promising candidates for future energy-efficient wireless systems. In this article, two RIS-based SSK schemes are proposed to efficiently improve the error and throughput performance of conventional SSK systems, respectively. The first one, termed RIS-SSK with passive beamforming (RIS-SSK-PB), employs an RIS for beamforming and targets the maximization of the minimum squared Euclidean distance between any two decision points. The second one, termed RIS-SSK with Alamouti space-time block coding (RIS-SSK-ASTBC), employs an RIS for ASTBC and enables the RIS to transmit its own Alamouti-coded information while reflecting the incident SSK signals to the destination. A low-complexity beamformer and an efficient maximum-likelihood (ML) detector are designed for RIS-SSK-PB and RIS-SSK-ASTBC, respectively. Approximate expressions for the average bit error probabilities of the source and/or the RIS are derived in closed-form assuming ML detection. Extensive computer simulations are conducted to verify the performance analysis. Results show that RIS-SSK-PB significantly outperforms the existing RIS-free and RIS-based SSK schemes, and RIS-SSK-ASTBC enables highly reliable transmission with throughput improvement.","['Array signal processing', 'Euclidean distance', 'Detectors', 'Reconfigurable intelligent surfaces', 'Throughput', 'Energy efficiency', 'Performance analysis']","['Alamouti code', 'passive beamforming', 'performance analysis', 'reconfigurable intelligent surface', 'space shift keying']"
"In recent years, with the rapid enhancement of computing power, deep learning methods have been widely applied in wireless networks and achieved impressive performance. To effectively exploit the information of graph-structured data as well as contextual information, graph neural networks (GNNs) have been introduced to address a series of optimization problems of wireless networks. In this overview, we first illustrate the construction method of wireless communication graph for various wireless networks and simply introduce the progress of several classical paradigms of GNNs. Then, several applications of GNNs in wireless networks such as resource allocation and several emerging fields, are discussed in detail. Finally, some research trends about the applications of GNNs in wireless communication systems are discussed.","['Wireless networks', 'Data models', 'Deep learning', 'Resource management', 'Network topology', 'Ions', 'Transceivers']","['Wireless networks', 'graph neural networks', 'resource management']"
"Electromagnetic wave control using the concept of a reflecting surface is first studied as a near-field and a far-field problem. Using a secondary source present in a wireless communication environment, such as a backscatter tag, it is possible to leverage the incoming radiation from the source as a reference-wave to synthesize the desired wavefront across the reflecting surface, radiating a field of interest. In this geometry, the phase grating, which is synthesized using an array of sub-wavelength unit cells, is calculated by interacting the incident reference-wave with the desired wavefront, similar to a hologram. When illuminated by the reference-wave, the reflected wavefront from the calculated phase grating is guaranteed to constructively add in the direction of the desired radiation (beam-steering in the far-field) and also focus at the intended depth (beam-focusing in the radiative near-field). Next, leveraging a dynamic modulation mechanism in the context of an intelligent reflective surface (IRS) illuminated by a backscatter tag, we demonstrate that one can selectively focus and defocus at an arbitrarily positioned receiver within the 3D field of view of the reflecting surface. This enables the control of the amplitude of the radiated electric field at the receiver location, paving the way for a spatial modulation mechanism by means of reconfiguring the reflecting surface in a backscattered wireless communication environment. In addition to the phase modification approach on a unit cell level to reconfigure the aperture radiated wavefronts, we finally present a time varying IRS concept making use of a time-delay based approach relying on a delay adjustment between the reflection coefficients of the IRS' unit cell lattices.","['Surface waves', 'Apertures', 'Electromagnetics', 'Surface treatment', 'Backscatter', 'Focusing', 'Modulation']","['Backscattering', 'beam-focusing', 'beam-steering', 'electromagnetic wave control', 'far-field', 'intelligent reflecting surfaces', 'near-field', 'spatial modulation']"
"This work discusses the optimal reconfigurable intelligent surface placement in highly-directional millimeter wave links. In particular, we present a novel system model that takes into account the relationship between the transmission beam footprint at the RIS plane and the RIS size. Subsequently, based on the model we derive the end-to-end expression of the received signal power and, furthermore, provide approximate closed-form expressions in the case that the RIS size is either much smaller or at least equal to the transmission beam footprint. Moreover, building upon the expressions, we derive the optimal RIS placement that maximizes the end-to-end signal-to-noise ratio. Finally, we substantiate the analytical findings by means of simulations, which reveal important trends regarding the optimal RIS placement according to the system parameters.","['Signal to noise ratio', 'Aperture antennas', 'Wireless communication', 'Antenna radiation patterns', 'Surface waves', 'Transmitting antennas', 'Relays']","['surfaces', 'signal-to-noise ratio analysis']"
"This paper considers the joint impact of non-linear hardware impairments at the base station (BS) and user equipments (UEs) on the uplink performance of single-cell massive MIMO (multiple-input multiple-output) in practical Rician fading environments. First, Bussgang decomposition-based effective channels and distortion characteristics are analytically derived and the spectral efficiency (SE) achieved by several receivers are explored for third-order non-linearities. Next, two deep feedforward neural networks are designed and trained to estimate the effective channels and the distortion variance at each BS antenna, which are used in signal detection. We compare the performance of the proposed methods with state-of-the-art distortion-aware and -unaware Bayesian linear minimum mean-squared error (LMMSE) estimators. The proposed deep learning approach improves the estimation quality by exploiting impairment characteristics, while LMMSE methods treat distortion as noise. Using the data generated by the derived effective channels for general order of non-linearities at both the BS and UEs, it is shown that the deep learning-based estimator provides better estimates of the effective channels also for non-linearities more than order three.","['Nonlinear distortion', 'Hardware', 'Channel estimation', 'Massive MIMO', 'Uplink', 'Deep learning']","['Deep learning', 'hardware impairments', 'uplink spectral efficiency', 'distortion-aware receiver', 'channel estimation', 'Rician fading']"
"Next-generation communication networks are expected to integrate newly-used technologies in a smart way to ensure continuous connectivity in rural areas and to alleviate the traffic load in dense regions. The prospective access network in 6G should hinge on satellite systems to take advantage of their wide coverage and high capacity. However, adopting satellites in 6G could be hindered because of the additional latency introduced, which is not tolerable by all traffic types. Therefore, we propose a traffic offloading scheme that integrates both the satellite and terrestrial networks to smartly allocate the traffic between them while satisfying different traffic requirements. Specifically, the proposed scheme offloads the Ultra-Reliable Low Latency Communication (URLLC) traffic to the terrestrial backhaul to satisfy its stringent latency requirement. However, it offloads the enhanced Mobile Broadband (eMBB) traffic to the satellite since eMBB needs high data rates but is not always sensitive to delay. Our scheme is shown to reduce the transmission delay of URLLC packets, decrease the number of dropped eMBB packets, and hence improve the network’s availability. Our findings highlight that the inter-working between satellite and terrestrial networks is crucial to mitigate the expected high load on the limited terrestrial capacity.","['Satellites', 'Base stations', 'Delays', 'Low earth orbit satellites', 'Satellite broadcasting', 'Multiplexing', '5G mobile communication']","['Enhanced mobile broadband (eMBB)', 'integrated satellite terrestrial networks (ISTN)', 'traffic offloading', 'ultra-reliable low latency communication (URLLC)']"
"Increasing incidents of cyber attacks and evolution of quantum computing poses challenges to secure existing information and communication technologies infrastructure. In recent years, quantum key distribution (QKD) is being extensively researched, and is widely accepted as a promising technology to realize secure networks. Optical fiber networks carry a huge amount of information, and are widely deployed around the world in the backbone terrestrial, submarine, metro, and access networks. Thus, instead of using separate dark fibers for quantum communication, integration of QKD with the existing classical optical networks has been proposed as a cost-efficient solution, however, this integration introduces new research challenges. In this paper, we do a comprehensive survey of the state-of-the-art QKD secured optical networks, which is going to shape communication networks in the coming decades. We elucidate the methods and protocols used in QKD secured optical networks, and describe the process of key establishment. Various methods proposed in the literature to address the networking challenges in QKD secured optical networks, specifically, routing, wavelength and time-slot allocation (RWTA), resiliency, trusted repeater node (TRN) placement, QKD for multicast service, and quantum key recycling are described and compared in detail. This survey begins with the introduction to QKD and its advantages over conventional encryption methods. Thereafter, an overview of QKD is given including quantum bits, basic QKD system, QKD schemes and protocol families along with the detailed description of QKD process based on the Bennett and Brassard-84 (BB84) protocol as it is the most widely used QKD protocol in the literature. QKD system are also prone to some specific types of attacks, hence, we describe the types of quantum hacking attacks on the QKD system along with the methods used to prevent them. Subsequently, the process of point-to-point mechanism of QKD over an optical fiber link is described in detail using the BB84 protocol. Different architectures of QKD secured optical networks are described next. Finally, major findings from this comprehensive survey are summarized with highlighting open issues and challenges in QKD secured optical networks.","['Optical fiber networks', 'Security', 'Protocols', 'Repeaters', 'Quantum networks', 'Photonics', 'Optical pulses']","['Quantum-classical coexistence', 'quantum key distribution', 'lightpath attacks', 'optical networks', 'routing', 'wavelength and time-slot allocation', 'trusted repeater nodes']"
"Non-orthogonal Multiple Access (NOMA) has been envisioned as one of the key enabling techniques to fulfill the requirements of future wireless networks. The primary benefit of NOMA is higher spectrum efficiency compared to Orthogonal Multiple Access (OMA). This paper presents an error rate comparison of two distinct NOMA schemes, i.e., power domain NOMA (PD-NOMA) and Sparse Code Multiple Access (SCMA). In a typical PD-NOMA system, successive interference cancellation (SIC) is utilized at the receiver, which however may lead to error propagation. In comparison, message passing decoding is employed in SCMA. To attain the best error rate performance of PD-NOMA, we optimize the power allocation with the aid of pairwise error probability and then carry out the decoding using generalized sphere decoder (GSD). Our extensive simulation results show that SCMA system with “ 5\times 10” setting (i.e., ten users communicate over five subcarriers, each active over two subcarriers) achieves better uncoded BER and coded BER performance than both typical “ 1\times 2” and “ 2\times 4” PD-NOMA systems in uplink Rayleigh fading channel. Finally, the impacts of channel estimation error on SCMA, SIC and GSD based PD-NOMA and the complexity of multiuser detection schemes are also discussed.","['NOMA', 'Silicon carbide', 'Uplink', 'Bit error rate', 'Multiuser detection', 'Complexity theory', 'Resource management']","['Non-orthogonal multiple access (NOMA)', 'power domain NOMA (PD-NOMA)', 'successive interference cancellation (SIC)', 'generalized sphere decoder (GSD)', 'sparse code multiple access (SCMA)', 'bit error rate (BER)']"
"We address channel characterization and modeling for medical wireless body-area networks (WBANs) based on the optical wireless technology. We focus on the intra-WBAN communication links, i.e., between a set of medical sensors and a coordination node, placed on the patient's body. We consider a realistic mobility model, e.g., inside a hospital room, which takes into account the effect of shadowing due to body parts movements and the variations of the underlying channels. To take into account the global and local user mobility, we consider a dynamic model based on a three-dimensional animation of a walk cycle, as well as walk trajectories based on an improved random way-point mobility model. Then, Monte Carlo ray-tracing simulations are performed to obtain the channel impulse responses for different link configurations at different instants of the walk scenarios. We then derive first-and second-order statistics of the channel parameters such as the channel DC gain, delay spread, and coherence time, and furthermore, propose best fit statistical models to describe the distribution of these parameters for a general scenario.","['Wireless communication', 'Optical sensors', 'Biomedical optical imaging', 'Solid modeling', 'Wireless sensor networks', 'Body area networks']","['Wireless body-area networks', 'optical wireless communications', 'channel modeling', 'channel characterization', 'ray tracing', 'user mobility']"
"Nowadays, deep neural networks (DNNs) are the core enablers for many emerging edge AI applications. Conventional approaches for training DNNs are generally implemented at central servers or cloud centers for centralized learning, which is typically time-consuming and resource-demanding due to the transmission of a large number of data samples from the edge device to the remote cloud. To overcome these disadvantages, we consider accelerating the learning process of DNNs on the Mobile-Edge-Cloud Computing (MECC) paradigm. In this paper, we propose HierTrain, a hierarchical edge AI learning framework, which efficiently deploys the DNN training task over the hierarchical MECC architecture. We develop a novel hybrid parallelism method, which is the key to HierTrain, to adaptively assign the DNN model layers and the data samples across the three levels of the edge device, edge server and cloud center. We then formulate the problem of scheduling the DNN training tasks at both layer-granularity and sample-granularity. Solving this optimization problem enables us to achieve the minimum training time. We further implement a hardware prototype consisting of an edge device, an edge server and a cloud server, and conduct extensive experiments on it. Experimental results demonstrate that HierTrain can achieve up to6.9×speedups compared to the cloud-based hierarchical training approach.","['Training', 'Parallel processing', 'Cloud computing', 'Data models', 'Computational modeling', 'Artificial intelligence', 'Servers']","['Edge AI', 'deep learning', 'fast model training', 'mobile-edge-cloud computing']"
"The current development of 5G technology is flourishing with widespread deployment across the world at a rapid pace. However, there is still a demand concerning 5G research for service and performance improvement. Research tasks include but are not limited to quality-of-service (QoS), energy efficiency, massive connectivity, reliable communications, and security. Due to the advancement of deep learning, numerous such research has utilized this technique. This article provides a comprehensive review of 5G communications research using deep learning. Specifically, we address the issues of low-density parity-check (LDPC) coding, massive multiple-input multiple-output (MIMO), non-orthogonal multiple access (NOMA), resource allocation, and security.","['Deep learning', 'NOMA', '5G mobile communication', 'Wireless networks', 'Massive MIMO', 'Quality of service', 'Parity check codes']","['Deep learning (DL)', 'machine learning (ML)', 'fifth generation (5G)', 'massive multiple-input multiple-output (MIMO)', 'low-density parity-check coding (LDPC)', 'non-orthogonal multiple access (NOMA)', 'resource allocation', 'security']"
"An intelligent reflecting surface (IRS) is an emerging technology in the next-generation (B5G and 6G) wireless communications with the aim of improving the spectral/energy efficiency of the wireless networks. In this paper, we focus on multiple IRS-aided multiuser multiple-input single-output (MISO) downlink network, where each IRS is deployed near to the cell boundary of the cellular network in order to help the downlink transmission to the cell-edge users. To achieve the significant performance benefits provided by the rate-splitting (RS) transmission in multiuser scenario, two-layer hierarchical rate splitting (2L-HRS) technique is deployed at the base station (BS) to serve all the users. Furthermore, we propose an On-Off scheme for controlling the IRSs with practical phase shifts. In order to analyze the performance of the users, we derive the closed-form expressions of outage probability for both cell-edge users and near users. By extensive Monte Carlo simulations, we demonstrate that the proposed design framework outperforms the corresponding one-layer RS (1L-RS), the multiuser linear precoding (MU-LP) and 2L-HRS without IRS. In addition, we reveal the advantages of introducing RS-based IRS in improving the cell-users' performance. The impact of channel estimation errors due to imperfect channel state information, and the various network's parameters, such as the number of reflecting elements and the number of cell-users, on the network performance is demonstrated.","['Monte Carlo methods', 'Closed-form solutions', 'Channel estimation', 'MISO communication', 'Probability', 'Downlink', 'Power system reliability']","['Intelligent reflecting surface (IRS)', 'rate splitting (RS)', 'multiple-input single-output (MISO)', 'downlink', 'multiuser']"
"The recent research trends for achieving ultra-reliable and low-latency communication networks are largely driven by smart manufacturing and industrial Internet-of-Things applications. Such applications are being realized through Tactile Internet that allows users to control remote things and involve the bidirectional transmission of video, audio, and haptic data. However, the end-to-end propagation latency presents a stubborn bottleneck, which can be alleviated by using various artificial intelligence-based application layer and network layer prediction algorithms, e.g., forecasting and preempting haptic feedback transmission. In this paper, we study the experimental data on traffic characteristics of control signals and haptic feedback samples obtained through virtual reality-based human-to-machine teleoperation. Moreover, we propose the installation of edge-intelligence servers between master and slave devices to implement the preemption of haptic feedback from control signals. Harnessing virtual reality-based teleoperation experiments, we further propose a two-stage artificial intelligence-based module for forecasting haptic feedback samples. The first-stage unit is a supervised binary classifier that detects if haptic sample forecasting is necessary and the second-stage unit is a reinforcement learning unit that ensures haptic feedback samples are forecasted accurately when different types of material are present. Furthermore, by evaluating analytical expressions, we show the feasibility of deploying remote human-to-machine teleoperation over fiber backhaul by using our proposed artificial intelligence-based module, even under heavy traffic intensity.","['Haptic interfaces', 'Servers', 'Master-slave', 'Forecasting', '5G mobile communication', 'Ultra reliable low latency communication', 'Robots']","['Human-to-machine applications', 'reinforcement learning', 'supervised learning', 'ultra-low latency communication']"
"If Beyond-5G(B5G)/6G is to support critical infrastructure worldwide, there must be an effort to jointly integrate low latency and privacy into wireless protocols. This research analyzes new schemes for securing applications at low latency by extending Physical Layer Security (PLS) algorithms to B5G/6G systems. We design protocols that advance a specific form of Physical Layer Security, known as Key-based Physical Layer Security, from the theoretical realm into the practical. One form of a Key-based algorithm obscures information from eavesdroppers by mapping it to cleverly rotated reference signals. For such a scheme, prior work has found that the two-way exchange of completely private information involves six OFDM symbol times, not including synchronization. By developing a protocol that makes the most optimum use of time-frequency resources, we show that it is possible to achieve two-way private exchange and synchronization in only four symbol times - the least latency possible. A significant simulation result indicates that our improved protocol achieves sub-1 ms two-way latency, including re-transmissions. We also illustrate a protocol that shows how Key-based Physical Layer Security can privatize critical PHY layer functions such as cell search. Lastly, additional results show the performance of the PLS algorithm in terms of Key Bit Error Rates and secret key transmission rates.","['Security', 'Physical layer security', 'Protocols', '5G mobile communication', 'Privacy', 'Communication system security', 'Signal to noise ratio']","['Physical layer security', '6G security', 'beyond 5G security', 'ultra secure low latency communications', 'security at layer-1']"
"Bluetooth Low Energy (BLE) has become the de facto communication protocol for the Internet of Things (IoT) and smart wearable devices for its ultra-low energy consumption, ease of development, good enough network coverage, and data transfer speed. Due to the simplified design of this protocol, there have been lots of security and privacy vulnerabilities. As billions of health care, personal fitness wearable, smart lock, industrial automation devices adopt this technology for communication, its vulnerabilities should be dealt with high priority. Some segregated works on BLE were performed focusing on various vulnerabilities, such as the insecure implementation of encryption, device authentication, user privacy, etc. However, there has been no comprehensive survey on the security vulnerabilities of this protocol. In this survey paper, we present a comprehensive taxonomy for the security and privacy issues of BLE. We present possible attack scenarios for different types of vulnerabilities, classify them according to their severity, and list possible mitigation techniques. We also provide case studies regarding how different vulnerabilities can be exploited in real BLE devices.","['Security', 'Bluetooth', 'Internet of Things', 'Privacy', 'Protocols', 'Wearable computers', 'Taxonomy']","['Bluetooth Low Energy (BLE)', 'BLE vulnerabilities', 'passive eavesdropping', 'device fingerprinting', 'privacy attack', 'IoT', 'wearable device', 'security tools']"
"Reliability is becoming increasingly important for many applications envisioned for future wireless systems. A technology that could improve reliability in these systems is massive MIMO (Multiple-Input Multiple-Output). One reason for this is a phenomenon called channel hardening, which means that as the number of antennas in the system increases, the variations of channel gain decrease in both the time- and frequency domain. Our analysis of channel hardening is based on a joint comparison of theory, measurements and simulations. Data from measurement campaigns including both indoor and outdoor scenarios, as well as cylindrical and planar base station arrays, are analyzed. The simulation analysis includes a comparison with the COST 2100 channel model with its massive MIMO extension. The conclusion is that the COST 2100 model is well suited to represent real scenarios, and provides a reasonable match to actual measurements up to the uncertainty of antenna patterns and user interaction. Also, the channel hardening effect in practical massive MIMO channels is less pronounced than in complex independent and identically distributed (i.i.d.) Gaussian channels, which are often considered in theoretical work.","['Antenna measurements', 'MIMO communication', 'Channel models', 'Base stations', 'Antenna arrays', 'Frequency measurement']","['Channel hardening', 'channel model', 'COST 2100 channel model', 'massive MIMO', 'measurements', 'reliability']"
"Millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) systems have been considered as one of the primary candidates for the fifth generation (5G) and beyond 5G wireless communication networks to satisfy the ever-increasing capacity demands. Full-duplex technology can further enhance the advantages of mmWave massive MIMO systems. However, the strong self-interference (SI) is the major limiting factor in the full-duplex technology. Hence, this paper proposes a novel angular-based joint hybrid precoding/combining (AB-JHPC) technique for the full-duplex mmWave massive-MIMO systems. Our primary goals are listed as: (i) improving the self-interference cancellation (SIC), (ii) increasing the intended signal power, (iii) decreasing the channel estimation overhead, (iv) designing the massive MIMO systems with a low number of RF chains. First, the RF-stage of AB-JHPC is developed via slow time-varying angle-of-departure (AoD) and angle-of-arrival (AoA) information. A joint transmit/receive RF beamformer design is proposed for covering (excluding) the AoD/AoA support of intended (SI) channel. Second, the BB-stage of AB-JHPC is constructed via the reduced-size effective intended channel. After using the well-known singular value decomposition (SVD) approach at the BB-stage, we also propose a new semi-blind minimum mean square error (S-MMSE) technique to further suppress the residual SI power by using AoD/AoA parameters. Thus, the instantaneous SI channel knowledge is not needed in the proposed AB-JHPC technique. Finally, we consider a transfer block architecture to minimize the number of RF chains. The numerical results demonstrate that the SI signal is remarkably canceled via the proposed AB-JHPC technique. It is shown that AB-JHPC achieves 85.7 dB SIC and the total amount of SIC almost linearly increases via antenna isolation techniques. We observe that the proposed full-duplex mmWave massive MIMO systems double the achievable rate capacity compared to its half-duplex counterpart as the antenna array size increases and the transmit/receive antenna isolation improves. Moreover, the proposed S-MMSE algorithm provides considerably high capacity than the conventional SVD approach.","['Radio frequency', 'Wireless communication', 'Interference cancellation', '5G mobile communication', 'Transmitting antennas', 'Channel estimation', 'Massive MIMO']","['Full-duplex', 'massive MIMO', 'millimeter wave communications', 'hybrid precoding', 'hybrid combining', 'low CSI overhead', 'RF chain', 'semi-blind MMSE', 'energy efficiency', 'imperfect angle information']"
"Free-space optical (FSO) links are considered as a cost-efficient way to fill the backhaul/fronthaul connectivity gap between millimeter wave (mmWave) access networks and optical fiber based central networks. In this paper, we investigate the end-to-end performance of dual-hop mixed FSO/mmWave systems to address this combined use. The FSO link is modeled as a Gamma-Gamma fading channel using both heterodyne detection and indirect modulation/direct detection with pointing error impairments, while the mmWave link experiences the fluctuating two-ray fading. Under the assumption of both amplify-and-forward and decode-and-forward relaying, we derive novel closed-form expressions for the outage probability, average bit error probability (BER), ergodic capacity, effective capacity in terms of bivariate Fox's H-functions. Additionally, we discuss the diversity gain and provide other important engineering insights based on the high signal-to-noise-ratio analysis of the outage probability and the average BER. Finally, all our analytical results are verified using Monte Carlo simulations.","['Radio frequency', 'Optical mixing', 'Capacity planning', 'Relays', 'Rayleigh channels', 'Adaptive optics']","['Free-space optical', 'Gamma-Gamma', 'fluctuating two-ray', 'relay']"
"An accurate and simple analytical model for the computation of the reflection amplitude and phase of Reconfigurable Intelligent Surfaces is presented. The model is based on a transmission-line circuit representation of the RIS which takes into account the physics behind the structure including the effect of all relevant geometrical and electrical parameters. The proposed representation of the RIS allows to take into account the effect of incidence angle, mutual coupling among elements and the effect of the interaction of the periodic surface with the RIS ground plane. It is shown that the proposed approach allows to design a physically realisable RIS without recurring to onerous electromagnetic simulations. The proposed model aims at filling the gap between RIS assisted communications algorithms and physical implementation issues which determine realistic performance of these surfaces.","['Surface waves', 'Surface impedance', 'Analytical models', 'Metasurfaces', 'Varactors', 'Integrated circuit modeling', 'Computational modeling']","['Equivalent circuit model (ECM)', 'reconfigurable intelligent surfaces (RIS)', 'intelligent reflective surfaces (IRS)', 'metasurfaces', 'transmission line (TL)']"
"In this article, we investigate intelligent anti-jamming communication method for wireless sensor networks. The stochastic game framework is introduced to model and analyze the multi-user anti-jamming problem, and a joint multi-agent anti-jamming algorithm (JMAA) is proposed to obtain the optimal anti-jamming strategy. In intelligent multi-channel blocking jamming environment, the proposed JMAA adopts multi-agent reinforcement learning to make online channel selection, which can effectively tackle the external malicious jamming and avoid the internal mutual interference among sensor nodes. The simulation results show that, the proposed JMAA is superior to the frequency-hopping method, the sensing-based method and the independent reinforcement learning. Specifically, the proposed JMAA has the higher average packet receive ratio than both the frequency-hopping method and the sensing-based method. Compared with the independent reinforcement learning, JMAA has faster convergence rate when reaching the same performance of average packet receive ratio. In addition, since the JMAA does not need to model the jamming patterns, it can be widely used for combating other malicious jamming such as sweep jamming and probabilistic jamming.","['Jamming', 'Wireless sensor networks', 'Reinforcement learning', 'Heuristic algorithms', 'Wireless communication', 'Interference', 'Sensors']","['Communication anti-jamming', 'channel selection', 'multi-agent reinforcement learning', 'Q-learning', 'wireless sensor networks']"
"Fifth generation (5G)-industrial Internet of things (IIoT) is the integration of IIoT and a private 5G network. The IIoT is a concept that involves incorporating smart objects, gadgets, and solutions into cutting-edge industrial operations to increase reliability, efficiency, and over-production costs. Furthermore, the integration of IIoT and 5G/beyond 5G (B5G) provides the potential for ubiquitous and instantaneous connectivity. The 5G architecture can handle the IIoT’s stringent ultra-low latency, realtime processing, high data rate, nearby storage, and reliability requirements. A new era of economic growth is predicted for IIoT-assisted 5G/B5G wireless networks. It should be noted that the majority of the work in IIoT is focused on the architecture, with reliability and throughput being largely ignored. This paper provides a comprehensive review of B5G assisted IIoT wireless networks, with a focus on enhanced mobile broadband (eMBB) and ultra-reliable low latency communication (URLLC) services. Furthermore, it provides insights into various applications and key enabling technologies from the perspective of URLLC, eMBB, and their tradeoff.","['Industrial Internet of Things', 'Ultra reliable low latency communication', '5G mobile communication', 'Internet of Things', 'Reliability', 'Industries', 'Wireless networks']","['Fifth generation (5G)', 'beyond 5G (B5G)', 'sixth generation (6G)', 'industrial Internet of things (IIoT)', 'ultra-reliable low latency communication (URLLC)', 'enhanced mobile broadband (eMBB)']"
"5G is a promising technology that has the potential to support verticals and applications such as Industrial Internet of Things IoT (IIoT), smart cities, autonomous vehicles, remote surgeries, virtual and augmented realities, and so on. These verticals have a diverse set of network connectivity requirements, and it is challenging to deliver customized services for each by using a common 5G infrastructure. Thus, the operation of Local 5G operator (L5GO) networks or private 5G networks are a viable option to tackle this challenge. A L5GO network is a localized small cell network which can offer tailored service delivery. The adaptation of network softwarization in 5G allows vertical owners to deploy and operate L5GO networks. However, the deployment of L5GOs raises various issues related to management of subscribers, roaming users, spectrum, security, and also the infrastructure. This paper proposes a blockchain-based platform to address these issues. The paper introduces a set of blockchain-based modularized functions such as service rating systems, bidding techniques, and selection functions, which can be used to deploy different services for L5GOs. Exploitation of blockchain technology ensures availability, non-reliance on trusted third parties, secure transfer payments, and stands to gain many more advantages. The performance and the viability of the proposed platform are analyzed by using simulations and a prototype implementation.","['5G mobile communication', 'Blockchain', 'Smart contracts', 'Ecosystems', 'Quality of service', 'Base stations']","['5G', 'local 5G operators', 'blockchain', 'smart contracts']"
"Deep learning (DL) has proven its unprecedented success in diverse fields such as computer vision, natural language processing, and speech recognition by its strong representation ability and ease of computation. As we move forward to a thoroughly intelligent society with 6G wireless networks, new applications and use cases have been emerging with stringent requirements for next-generation wireless communications. Therefore, recent studies have focused on the potential of DL approaches in satisfying these rigorous needs and overcoming the deficiencies of existing model-based techniques. The main objective of this article is to unveil the state-of-the-art advancements in the field of DL-based physical layer methods to pave the way for fascinating applications of 6G. In particular, we have focused our attention on four promising physical layer concepts foreseen to dominate next-generation communications, namely massive multiple-input multiple-output systems, sophisticated multi-carrier waveform designs, reconfigurable intelligent surface-empowered communications, and physical layer security. We examine up-to-date developments in DL-based techniques, provide comparisons with state-of-the-art methods, and introduce a comprehensive guide for future directions. We also present an overview of the underlying concepts of DL, along with the theoretical background of well-known DL techniques. Furthermore, this article provides programming examples for a number of DL techniques and the implementation of a DL-based multiple-input multiple-output by sharing user-friendly code snippets, which might be useful for interested readers.","['6G mobile communication', '5G mobile communication', 'Modulation', 'Artificial intelligence', 'Wireless networks', 'Wireless communication', 'Millimeter wave communication']","['Deep learning', '6G', 'massive multiple-input multiple-output (MIMO)', 'multi-carrier (MC) waveform designs', 'reconfigurable intelligent surfaces (RIS)', 'physical layer (PHY) security']"
"Cell-free Massive MIMO systems consist of a large number of geographically distributed access points (APs) that serve the users by coherent joint transmission. The spectral efficiency (SE) achieved by each user depends on the power allocation: which APs that transmit to which users and with what power. In this article, we revisit the max-min and sum-SE power allocation policies, which have previously been approached using high-complexity general-purpose solvers. We develop and compare several different high-performance low-complexity power allocation algorithms that are appropriate for use in large systems. We propose two new algorithms for sum-SE power optimization inspired by weighted minimum mean square error (WMMSE) minimization and fractional programming (FP). Further, one new FP-based algorithm is proposed for max-min fair power allocation. The alternating direction method of multipliers (ADMM) is used to solve specific convex subproblems in the proposed algorithms. Our ADMM reformulations lead to multiple small-sized subproblems with closed-form solutions. The proposed algorithms find global or local optimal power allocation solutions for large-scale systems but with reduced computational time compared to previous work.","['Resource management', 'Downlink', 'Optimization', 'Programming', 'Interference', 'Signal to noise ratio', 'Massive MIMO']","['Cell-free massive MIMO', 'power allocation', 'max-min fairness', 'sum-SE maximization', 'WMMSE', 'fractional programming', 'ADMM']"
"Mobile edge computing (MEC) brings a breakthrough for Internet of Things (IoT) for its ability of offloading tasks from user equipments (UEs) to nearby servers which have rich computation resource. 5G network brings a huge breakthrough on transmission rate. Together with MEC and 5G, both execution delay of tasks and time delay from downloading would be shorter and the quality of experience (QoE) of UEs can be improved. Considering practical conditions, the computation resource of an MEC server is finite to some extent. Therefore, how to prevent the abuse of MEC resource and further allocate the resource reasonably becomes a key point for an MEC system. In this paper, an MEC system with multi-user is considered where a base station (BS) with an MEC server, which can not only provide computation offloading service but also data cache service. Especially, we take the charge for both data transmission and task computation as one part of total cost of UEs, and then explore a joint optimization for downlink resource allocation, offloading decision and computation resource allocation to minimize the total cost in terms of the time delay and the charge to UEs. The proposed problem is formulated as a mixed integer programming (MIP) one which is NP-hard. Therefore, we decouple the original problem into two subproblems which are downlink resource allocation problem and joint offloading decision and computation resource allocation problem. Then we address these two subproblems by using convex and nonconvex optimization techniques, respectively. An iterative algorithm is proposed to obtain a suboptimal solution in polynomial time. Simulation results show that our proposed algorithm performs better than benchmark algorithms.","['Task analysis', 'Servers', 'Resource management', 'Delays', 'Energy consumption', 'Bandwidth', 'Minimization']","['Mobile edge computing', 'offloading decision', 'resource allocation', 'charge to UEs']"
"Non-terrestrial networks, including Unmanned Aerial Vehicles (UAVs), High Altitude Platform Station (HAPS) nodes and Low Earth Orbiting (LEO) satellites, are expected to have a pivotal role in sixth-generation wireless networks. With inherent features such as flexible placement, wide footprints, and preferred channel conditions, they can tackle several challenges faced by current terrestrial networks. However, their successful and widespread adoption relies on energy-efficient on-board communication systems. In this context, the integration of Reconfigurable Smart Surfaces (RSS) into aerial platforms is envisioned as a key enabler of energy-efficient and cost-effective aerial platform deployments. RSS consist of low-cost reflectors capable of smartly directing signals in a nearly passive way. In this paper, we investigate the link budget of RSS-assisted communications for two RSS reflection paradigms discussed in the literature, namely “specular” and “scattering” paradigms. Specifically, we analyze the characteristics of RSS-equipped aerial platforms and compare their communication performance with that of RSS-assisted terrestrial networks using standardized channel models. In addition, we derive the optimal aerial platform placements for both reflection paradigms. Our results provide important insights for the design of RSS-assisted communications. For instance, given that a HAPS has a large area for RSS, it provides superior link budget performance in most studied scenarios. In contrast, the limited RSS area on UAVs and the large propagation loss in LEO satellite communications make them unfavorable candidates for supporting terrestrial users. Finally, the optimal location of an RSS-equipped platform may depend on the platform's altitude, coverage footprint, and type of environment","['Low earth orbit satellites', 'Scattering', 'Satellites', 'Receivers', 'Electrical engineering', 'Propagation losses', 'Optical transmitters']","['Reconfigurable smart surfaces (RSS)', 'reconfigurable intelligent surfaces (RIS)', 'aerial platform', 'unmanned aerial vehicle (UAV)', 'high altitude platform station (HAPS)', 'low earth orbit (LEO) satellite']"
"Lately, the deployment of heterogeneous wireless networks has emerged, as part of the 5G vision, to cope with the users' exaggerated service demands. In this context, the application of Non-Orthogonal Multiple Access (NOMA) technique constitutes a promising solution to facilitate a balance between spectral efficiency and system complexity. In this article, we consider the problem of joint user association and uplink power allocation, in heterogeneous 5G wireless networks, employing NOMA technology. The coupled problem is treated under an incomplete information scenario, where the Base Stations (BSs) have statistical only knowledge of the users' channel conditions. To deal with the incompleteness of Channel State Information (CSI), a Contract Theory (CT) based approach is introduced. A Reinforcement Learning (RL) based methodology, capitalizing on the provided feedback from the communication environment is initially adopted, in order to achieve the users to BS association in an iterative and distributed manner. The problem of uplink power allocation is subsequently formulated as a contract between each BS and its corresponding users. The optimal power is thus obtained as the solution of the optimization of each BS's utility function, while ensuring the optimality of the utility function of each associated user, given the unique communications characteristics and type of each user. Detailed numerical evaluation of the performance of the proposed unified user association and power allocation framework is provided, via modeling and simulation, illustrating its operation, features and benefits, under densely deployed heterogeneous environments.","['Resource management', 'NOMA', 'Contracts', 'Wireless networks', 'Uplink', 'Interference', 'Optimization']","['Contract theory (CT)', 'heterogeneous networks', 'incomplete information', 'non-orthogonal multiple access (NOMA)', 'power allocation', 'reinforcement learning (RL)', 'user association']"
"Fifth-generation (5G) of wireless networks are expected to accommodate different services with contrasting quality of service (QoS) requirements within a common physical infrastructure in an efficient way. In this article, we address the radio access network (RAN) slicing problem and focus on the three 5G primary services, namely, enhanced mobile broadband (eMBB), ultra-reliable and low-latency communications (URLLC) and massive machine-type communications (mMTC). In particular, we formulate the joint allocation of power and resource blocks to the heterogeneous users in the downlink targeting the transmit power minimization and by considering mixed numerology-based frame structures. Most importantly, the proposed scheme does not only consider the heterogeneous QoS requirements of each service, but also the queue status of each user during the scheduling of resource blocks. In addition, imperfect Channel State Information (CSI) is considered by including an outage probabilistic constraint into the formulation. The resulting non-convex problem is converted to a more tractable problem by exploiting Big-M formulation, probabilistic to non-probabilistic transformation, binary relaxation and successive convex approximation (SCA). The proposed solution is evaluated for different mixed-numerology resource grids within the context of strict slice-isolation and slice-aware radio resource management schemes via extensive numerical simulations.","['Resource management', 'Ultra reliable low latency communication', 'Quality of service', '5G mobile communication', 'Optimization', 'Radio access networks', 'Reliability']","['RAN resource slicing', 'power minimization', 'resource block allocation', 'mixednumerologies', 'eMBB', 'URLLC and mMTC']"
"This paper aims to make a comparison between decode-and-forward (DF) relays and reconfigurable intelligent surfaces (RISs) in the case where only one relay or RIS is selected based on the maximization of the signal-to-noise-ratio (SNR). Our study accounts for the spatial distribution of RISs and relays, which is assumed to follow a Poisson point process (PPP). It considers two different path loss models corresponding to RIS/relays randomly located in the near-field and the far-field of the transmitter. Based on the Gamma distribution moment matching method and tools from stochastic geometry, we derive approximations for the outage probability (OP) as well as the energy efficiency (EE) of the RISs-assisted system in the near-field and the far-field scenarios separately. Under the same conditions as RIS, the expressions for OP and EE of the half-duplex and the full-duplex DF relays-assisted systems are also derived. Simulation results are presented to corroborate the proposed analysis and compare between the three technologies. Our results show that RIS is the best choice in the near-field case, regardless of the OP or EE criterion. Compared to half-duplex relays and full-duplex relays, the RIS based system is the most energy-efficient solution to assist communication. Moreover, RIS allows for an improvement in both OP and EE when equipped with more reflecting elements or more densely deployed.","['Relays', 'Signal to noise ratio', 'Rayleigh channels', 'Power system reliability', 'Power demand', 'Closed-form solutions', 'Channel estimation']","['Decode-and-forward relays', 'energy efficiency', 'far-field', 'near-filed', 'outage probability', 'reconfigurable intelligent surfaces', 'Poisson point process']"
"The next generation (6G) wireless systems aim to cater to the Internet of Everything (IoE) and revolutionize customer services and applications to a fully intelligent and autonomous system. To achieve this, the digital twin edge network (DITEN) is proposed to combine mobile/multi-access edge computing (MEC) and digital twin (DT), thereby improving the network performance such as throughput and security, and reducing the cost of communication, computation, and caching. In DITENs, the network status can be continuously monitored, and based on the obtained network states, the networking schemes, such as routing and resource management, can be studied in the established DITENs from a centralized perspective. In this survey, we present a comprehensive overview of DITEN for 6G. First, we present the fundamental aspects of DITEN, including concept, framework, and potential. Second, a comprehensive design of DITEN is devised, including the DT modeling/updating, DT deployment, key issues, and enabling technologies. Then, the typical applications of DITEN towards 6G are provided, including the Internet of Things (IoT), vehicular network, space-air-ground integrated network (SAGIN), healthcare, wireless systems, and other applications, along with the design of DITEN in each application, such as DT modeling, DT association, incentive mechanisms, and so on. Finally, challenges and open issues are discussed.","['Digital twins', 'Wireless communication', 'Internet of Things', 'Computational modeling', 'Monitoring', 'Cloud computing', 'Task analysis']","['Digital twin edge networks (DITEN)', '6G', 'DT modeling', 'DT deployment', 'applications', 'challenges']"
"Deep learning (DL) has seen great success in the computer vision (CV) field, and related techniques have been used in security, healthcare, remote sensing, and many other areas. As a parallel development, visual data has become universal in daily life, easily generated by ubiquitous low-cost cameras. Therefore, exploring DL-based CV may yield useful information about objects, such as their number, locations, distribution, motion, etc. Intuitively, DL-based CV can also facilitate and improve the designs of wireless communications, especially in dynamic network scenarios. However, so far, such work is rare in the literature. The primary purpose of this article, then, is to introduce ideas about applying DL-based CV in wireless communications to bring some novel degrees of freedom to both theoretical research and engineering applications. To illustrate how DL-based CV can be applied in wireless communications, an example of using a DL-based CV with a millimeter-wave (mmWave) system is given to realize optimal mmWave multiple-input and multiple-output (MIMO) beamforming in mobile scenarios. In this example, we propose a framework to predict future beam indices from previously observed beam indices and images of street views using ResNet, 3-dimensional ResNext, and a long short-term memory network. The experimental results show that our frameworks achieve much higher accuracy than the baseline method, and that visual data can significantly improve the performance of the MIMO beamforming system. Finally, we discuss the opportunities and challenges of applying DL-based CV in wireless communications.","['Wireless communication', 'Visualization', 'Cameras', 'MIMO communication', 'Array signal processing', 'Channel estimation', 'Wireless sensor networks']","['Computer vision', 'deep learning', 'multiple-input and multiple-output', 'beamforming', 'beam tracking', 'long short-term memory', 'wireless communications']"
"In this work, we study age-optimal scheduling with stability constraints in a multiple access channel with two heterogeneous source nodes transmitting to a common destination. The first node is connected to a power grid and it has randomly arriving data packets. Another energy harvesting (EH) sensor monitors a stochastic process and sends status updates to the destination. We formulate an optimization problem that aims at minimizing the average age of information (AoI) of the EH node subject to the queue stability condition of the grid-connected node. First, we consider a Probabilistic Random Access (PRA) policy where both nodes make independent transmission decisions based on some fixed probability distributions. We show that with this policy, the average AoI is equal to the average peak AoI, if the EH node only sends freshly generated samples. In addition, we derive the optimal solution in closed form, which reveals some interesting properties of the considered system. Furthermore, we consider a Drift-Plus-Penalty (DPP) policy and develop AoI-optimal and peak-AoI-optimal scheduling algorithms using the Lyapunov optimization theory. Simulation results show that the DPP policy outperforms the PRA policy in various scenarios, especially when the destination node has low multi-packet reception capabilities.","['Optimization', 'Batteries', 'Signal to noise ratio', 'Power system stability', 'Wireless sensor networks', 'Receivers', 'Queueing analysis']","['Age of information', 'energy harvesting', 'Lyapunov optimization', 'multiple access channel', 'random access', 'scheduling']"
"Next generation mobile networks need to expand towards uncharted territories in order to enable the digital transformation of society. In this context, aerial devices such as unmanned aerial vehicles (UAVs) are expected to address this gap in hard-to-reach locations. However, limited battery-life is an obstacle for the successful spread of such solutions. Reconfigurable intelligent surfaces (RISs) represent a promising solution addressing this challenge since on-board passive and lightweight controllable devices can efficiently reflect the signal propagation from the ground BSs towards specific target areas. In this paper, we focus on air-to-ground networks where UAVs equipped with RIS can fly over selected areas to provide connectivity. In particular, we study how to optimally compensate flight effects and propose RiFe as well as its practical implementation Fair-RiFe that automatically configure RIS parameters accounting for undesired UAV oscillations due to adverse atmospheric conditions. Our results show that both algorithms provide robustness and reliability while outperforming state-of-the-art solutions in the multiple conditions studied.","['Perturbation methods', 'Optimization', 'Signal to noise ratio', 'Receivers', 'Oscillators', 'Drones', 'Trajectory']","['UAV', 'robust communication', 'drones', 'RIS', 'smart surfaces', 'optimization']"
"Using stochastic geometry tools, we develop a systematic framework to characterize the meta distributions of the downlink SIR/SNR and data rate of the typical device in a cellular network with coexisting sub-6GHz and millimeter wave (mm-wave) spectrums. Macro base-stations (MBSs) transmit on sub-6GHz channels (which we term “microwave” channels), whereas small base-stations (SBSs) communicate with devices on mm-wave channels. The SBSs are connected to MBSs via a microwave (μwave) wireless backhaul. The μwave channels are interference limited and mm-wave channels are noise limited; therefore, we have the meta distribution of SIR and SNR in μwave and mm-wave channels, respectively. To model the line-of-sight (LOS) nature of mm-wave channels, we use Nakagami-m fading model. To derive the meta distribution of SIR/SNR, we characterize the conditional success probability (CSP) (or equivalently reliability) and its b th moment for the typical device (a) when it associates to a μwave MBS for direct transmission, and (b) when it associates to a mm-wave SBS for dual-hop transmission (backhaul and access transmission). Performance metrics such as the mean and variance of the local delay (network jitter), mean of the CSP (coverage probability), and variance of the CSP are derived. Closedform expressions are presented for special scenarios. The extensions of the developed framework to the μwave-only network or mm-wave only networks where SBSs have mm-wave backhauls are discussed. Numerical results validate the analytical results. Insights are extracted related to the reliability, coverage probability, and latency of the considered network.","['Cellular networks', 'Fading channels', 'Reliability', 'Delays', 'Performance evaluation', 'Antennas', 'Gain']","['5G cellular networks', 'millimeter wave', 'meta distribution', 'reliability', 'latency', 'wireless backhaul', 'Nakagami fading', 'stochastic geometry']"
"A new composite fading model is introduced. This shadowed Beaulieu-Xie model is developed to characterize wireless communication in an environment with an arbitrary number of line-of-sight and non-line-of-sight signals, in contrast to the existing Rayleigh, Ricean, generalized Ricean (i.e., κ - μ), and Nakagami-m models. The proposed model benefits from four parameters that characterize a wide range of fading conditions, unlike existing composite models such as the shadowed Ricean model, the two-wave with diffuse power model, and the fluctuating two-ray model. The proposed shadowed Beaulieu-Xie model is used here to characterize experimental data obtained from fading measurements in 28 GHz outdoor millimeter-wave channels, and it is found to describe the communication environment accurately. We conclude that the proposed composite fading model is particularly useful for characterizing emerging (millimeter-wave and terahertz) wireless communication systems.","['Rayleigh channels', 'Probability density function', 'Wireless communication', 'Shadow mapping', 'Numerical models', 'Adaptation models']","['Beaulieu-Xie distribution', 'bit-error rate', 'channel model', 'emerging systems', 'fading models', 'mmwave', 'non-central chi distribution', 'outage probability', 'terahertz']"
"Intelligent Reflecting Surfaces (IRSs) have recently emerged as a promising solution for realizing the smart radio environment concept by leveraging the characteristics of metamaterials and large antenna arrays. However, the existing works on IRS do not account for the transceiver impairments while analyzing the performance of IRS assisted wireless systems. This paper investigates the impact of transceiver hardware imperfections on the performance of IRS-assisted wireless systems. Specifically, we have analyzed the spectral efficiency (SE), the energy efficiency (EE) and the outage probability of IRS-assisted wireless systems by deriving their closed-form expressions and verifying them through simulation results. The results show the importance of modeling and compensating for hardware impairment as they significantly restrict the performance of such systems.","['Wireless communication', 'Closed-form solutions', 'Probability', 'Distortion', 'Transceivers', 'Hardware', 'Power system reliability']","['Intelligent reflecting surfaces', 'performance analysis', 'spectral efficiency', 'energy efficiency', 'outage', 'hardware impairments', 'relay']"
"The statistical characterization of a sum of random variables (RVs) is useful for investigating the performance of wireless communication systems. We derive exact closed-form expressions for the probability density function (PDF) and cumulative distribution function (CDF) of a sum of independent but not identically distributed (i.n.i.d.) Fisher-Snedecor F RVs. Both PDF and CDF are expressed in terms of the multivariate Fox's H-function. Besides, a simple and accurate approximation to the sum of i.n.i.d. Fisher-Snedecor F variates is presented using the moment matching method. The obtained PDF and CDF are used to evaluate the performance of wireless communication applications including the outage probability, the effective capacity, and the channel capacities under four different adaptive transmission strategies. Moreover, the corresponding approximate expressions are obtained to provide useful insights for the design and deployment of wireless communication systems. In addition, we derive simple asymptotic expressions for the proposed mathematical analysis in the high signal-to-noise ratio regime. Finally, the numerical results demonstrate the accuracy of the derived expressions.","['Probability density function', 'Fading channels', 'Random variables', 'Channel capacity', 'Wireless communication', 'Signal to noise ratio', 'Performance analysis']","['Channel capacity', 'effective capacity', 'Fisher-Snedecor F-distribution', 'sum of random variables']"
"Dynamic and Adaptive Load Balancing (DALB) and Controller Adaption and Migration Decision (CAMD) frameworks are the recently developed efficient controller selection frameworks that solved the challenge of load-imbalance in Software-Defined Networking (SDN). While CAMD framework was established to be efficient over DALB framework yet it was not efficient when the incoming-traffic load was elephant flow, hence, leading to a significant reduction in the overall system performance. This study had proposed an Improved Switch Migration Decision Algorithm (ISMDA) that solved the network challenge when the incoming load is elephant flow. The balancing module of the switch migration framework, which runs on each controller, is initiated during the controller load imbalance phase. The improved framework used the controller variance and controller average load status to determine the set of underloaded controllers in the network. The constructed efficient migration model was used to, simultaneously, identify both the migration cost and load-balancing variation for the optimal selection of controller among the set of underloaded controllers. The controller throughput, response time, number of migration space and packet loss were used as the performance comparison metrics. The average controller throughput of ISMDA increased with 7.4% over CAMD framework while average response time of the proposed algorithm improved over CAMD framework with 5.7%. Similarly, the proposed framework had 5.6% average improved migration space over CAMD framework and the packet-loss of ISMDA had average 6.4% performance over the CAMD framework. It was concluded that ISMDA was efficient over CAMD framework when the incoming traffic load is elephant flow.","['Switches', 'Load management', 'Time factors', 'Throughput', 'Process control', 'Protocols']","['SDN', 'load balancing', 'switch migration', 'algorithm', 'distributed controllers']"
"Ever-increasing energy consumption, the depletion of non-renewable resources, the climate impact associated with energy generation, and finite energy-production capacity are important concerns that drive the urgent creation of new solutions for energy management. In this regard, by leveraging the massive connectivity provided by emerging 5G communications, this paper proposes a long-term sustainable Demand-Response (DR) architecture for the efficient management of available energy consumption for Internet of Things (IoT) infrastructures. The proposal uses Network Functions Virtualization (NFV) and Software Defined Networking (SDN) technologies as enablers and promotes the primary use of energy from renewable sources. Associated with architecture, this paper presents a novel consumption model conditioned on availability and in which the consumers are part of the management process. To efficiently use the energy from renewable and non-renewable sources, several management strategies are herein proposed, such as prioritization of the energy supply and workload scheduling using time-shifting capabilities. The complexity of the proposal is analyzed in order to present an appropriate architectural framework. The energy management solution is modeled as an Integer Linear Programming (ILP) and, to verify the improvements in energy utilization, an algorithmic solution and its evaluation are presented. Finally, open research problems and application scenarios are discussed.","['Proposals', 'Energy management', 'Computer architecture', '5G mobile communication', 'Energy consumption', 'Internet of Things', 'Network function virtualization']","['Energy efficiency', 'energy management', 'demand response', 'IoT', 'green energy', 'NFV', 'renewable energy', 'SDN']"
"In this paper, we study the problem of topology optimization and routing in integrated access and backhaul (IAB) networks, as one of the promising techniques for evolving 5G networks. We study the problem from different perspectives. We develop efficient genetic algorithm-based schemes for both IAB node placement and non-IAB backhaul link distribution, and evaluate the effect of routing on bypassing temporal blockages. Here, concentrating on millimeter wave-based communications, we study the service coverage probability, defined as the probability of the event that the user equipments’ (UEs) minimum rate requirements are satisfied. Moreover, we study the effect of different parameters such as the antenna gain, blockage, and tree foliage on the system performance. Finally, we summarize the recent Rel-16 as well as the upcoming Rel-17 3GPP discussions on routing in IAB networks, and discuss the main challenges for enabling mesh-based IAB networks. As we show, with a proper network topology, IAB is an attractive approach to enable the network densification required by 5G and beyond.","['Routing', 'Wireless communication', 'Optimization', 'Network topology', '3GPP', 'Topology', 'Resource management']","['Integrated access and backhaul', 'IAB', 'genetic algorithm', 'node selection', 'topology optimization', 'densification', 'millimeter wave', '(mmWave) communications', '3GPP', 'stochastic geometry', 'poisson point process', 'coverage probability', 'germ-grain model', 'wireless backhaul', '5G NR', 'blockage', 'relay', 'routing', 'tree foliage', 'machine learning']"
"This paper considers the design of beamforming for orthogonal time frequency space modulation assisted non-orthogonal multiple access (OTFS-NOMA) networks, in which a high-mobility user is sharing the spectrum with multiple low-mobility NOMA users. In particular, the beamforming design is formulated as an optimization problem whose objective is to maximize the low-mobility NOMA users’ data rates while guaranteeing that the high-mobility user’s targeted data rate can be met. Both the cases with and without channel state information errors are considered, where low-complexity solutions are developed by applying successive convex approximation and semidefinite relaxation. Simulation results are also provided to show that the use of the proposed beamforming schemes can yield a significant performance gain over random beamforming.","['NOMA', 'Array signal processing', 'Time-frequency analysis', 'Base stations', 'Signal to noise ratio', 'Interference']","['Non-orthogonal multiple access', 'orthogonal time frequency space modulation', 'beamforming', 'MIMO']"
"In this work, we propose a novel sidechain structure via an optimized two-way peg protocol for device authentication in the smart community in order to overcome the limitations of existing approaches. The proposed scheme uses private side blockchains to distribute and manage the local registration and authentication processes, in addition to a local mainchain block to circulate the information record with other smart systems. More importantly, we propose the optimized two-way peg protocol in the proposed sidechain system in order to prevent the worthless information injection attack during the authentication information sharing procedure between the main chain and side blockchains. The optimized two-way peg protocol supervises the availability of the required information by dynamically evaluating the trustworthiness of each smart device. The evaluation is based on numerous criteria, such as the authentication method, previous authentication information sharing history, and local authentication results. Consequently, the simulation results prove the superiority of the proposed scheme in terms of reducing authentication time, improving information management efficiency and decreasing storage consumption as compared to existing works, and the applicability and feasibility of the optimized two-way peg protocol have been approved. It is noteworthy that the proposed sidechain-based method shows its superiority in reducing the cost of authentication time compared with the blockchain-based method when using blockchain structure. The reflected savings are 33.33%, 34.29%, and 36.36% when in comparison to the conventional authentication process without applying any additional method, the authentication process using the proposed sidechain based method, and the authentication process using the blockchain based method, respectively.","['Authentication', 'Protocols', 'Logic gates', 'Smart cities', 'Information management']","['Smart community', 'blockchain', 'information sharing', 'device authentication', 'sidechain', 'two-way peg protocol']"
"Predominant network intrusion detection systems (NIDS) aim to identify malicious traffic patterns based on a handcrafted dataset of rules. Recently, the application of machine learning in NIDS helps alleviate the enormous effort of human observation. Federated learning (FL) is a collaborative learning scheme concerning distributed data. Instead of sharing raw data, it allows a participant to share only a trained local model. Despite the success of existing FL solutions, in NIDS, a network’s traffic data distribution does not always fit into the single global model of FL; some networks have similarities with each other but other networks do not. We propose Segmented-Federated Learning (Segmented-FL), where by employing periodic local model evaluation and network segmentation, we aim to bring similar network environments to the same group. A comparison between FL and our method was conducted against a range of metrics including the weighted precision, recall, and F1 score, using a collected dataset from 20 massively distributed networks within 60 days. By studying the optimized hyperparameters of Segmented-FL and employing three evaluation methods, it shows that Segmented-FL has better performance in all three types of intrusion detection tasks, achieving validation weighted F1 scores of 0.964, 0.803, and 0.912 with Method A, Method B, and Method C respectively. For each method, this scheme shows a gain of 0.1%, 4.0% and 1.1% in performance compared with FL.","['Operating systems', 'Traffic control', 'Telecommunication traffic', 'Intrusion detection']","['Cybersecurity', 'deep learning', 'intrusion detection', 'segmented-federated learning', 'LAN', 'convolutional neural network']"
"Increased data rates and very low-latency requirements place strict constraints on the computational complexity of channel decoders in the new 5G communications standard. Practical low-density parity-check (LDPC) decoder implementations use message-passing decoding with finite precision, which becomes coarse as complexity is more severely constrained. In turn, performance degrades as the precision becomes more coarse. Recently, the information bottleneck (IB) method was used to design mutual-information-maximizing mappings that replace conventional finite-precision node computations. As a result, the exchanged messages in the IB approach can be represented with a very small number of bits. 5G LDPC codes have the so-called protograph-based raptor-like (PBRL) structure which offers inherent rate-compatibility and excellent performance. This paper extends the IB principle to the flexible class of PBRL LDPC codes as standardized in 5G. The extensions include IB decoder design for puncturing and rate-compatibility. In contrast to existing IB decoder design techniques, the proposed decoder can be used for a large range of code rates with a static set of optimized mappings. The proposed construction approach is evaluated for a typical range of code rates and bit resolutions ranging from 3 bit to 5 bit. Frame error rate simulations show that the proposed scheme always outperforms min-sum decoding algorithms and operates close to double-precision sum-product belief propagation decoding. Furthermore, alternatives to the lookup table implementations of the mutual-information-maximizing mappings are investigated.","['Decoding', 'Parity check codes', 'Quantization (signal)', 'Random variables', '5G mobile communication', 'Belief propagation']","['LDPC codes', '5G', 'message-passing decoding', 'mutual-information based signal processing', 'information bottleneck method', 'machine learning']"
"Integrated sensing and communication (ISAC) has been widely recognized as a key technology in future sixth-generation (6G) wireless networks, especially for emerging applications and scenarios demanding both high-performance sensing and communication functionalities. The ISAC technique has the potential of enabling sensing and communication functionalities in a single hardware platform, even sharing the same waveform, thus, achieving the reduced cost of hardware implementation and the efficient use of spectrum resources. In this paper, we commence with the discussion on sensing and communication at the current stage from the view of motivations, applications, and challenges. Then, we provide a comprehensive survey of the state-of-the-art approaches to the ISAC technique from the waveform design perspective. To be specific, we classify the waveform design methods into three categories, namely, communication-centric waveform design, sensing-centric waveform design, and joint waveform optimization and design. In addition, potential research directions and challenges for future ISAC waveform design are outlined.","['Sensors', 'OFDM', 'Symbols', 'Optimization', 'Ions', 'Codes', '6G mobile communication']","['6G', 'integrated sensing and communication', 'communication-centric waveform design', 'sensing-centric waveform design', 'joint waveform optimization and design']"
"Wireless traffic usage forecasting methods can help to facilitate proactive resource allocation solutions in cloud managed wireless networks. In this paper, we present temporal and spatial analysis of network traffic using real traffic data of an enterprise network comprising 470 access points (APs). We classify and separate APs into different groups according to their traffic usage patterns. We study various statistical properties of traffic data, such as auto-correlations and cross-correlations within and across different groups of APs. Our analysis shows that the group of APs with high traffic utilization have strong seasonality patterns. However, there are also APs with no such seasonal patterns. We also study the relation between number of connected users and traffic generated, and show that more connected users do not always mean more traffic data, and vice versa. We use Holt-Winters, seasonal auto-regressive integrated moving average (SARIMA), long short-term memory (LSTM), gated recurrent unit (GRU) and convolutional neural network (CNN) methods for forecasting traffic usage. Our results show that there is no single universal best method that can forecast traffic usage of every AP in an enterprise wireless network. The combined models such as CNN-LSTM and CNN-GRU are also used for spatio-temporal forecasting of a single AP traffic usage. The results show that considering spatial dependencies of neighboring APs can improve the forecasting performance of a single AP if it has significant spatial correlations.","['Forecasting', 'Time series analysis', 'Wireless networks', 'Data analysis', 'Machine learning', 'Resource management']","['5G', 'CNN', 'CNN-GRU', 'CNN-LSTM', 'forecasting', 'GRU', 'holt-winters', 'LSTM', 'neural network', 'real network data', 'SARIMA', 'spatio-temporal', 'temporal', 'time series analysis', 'WLAN']"
"In the beyond 5G networks, extraordinary demands for data rates and capacity are to be met. A possible candidate to address these challenges is Non-Orthogonal Multiple Access (NOMA) technique, which leads to higher diversity gains and massive connectivity. One caveat of NOMA is the increased receiver complexity to nullify the Inter User Interference (IUI) through Successive Interference Cancellation (SIC). In this paper, a cooperative relaying scheme is employed to improve the overall diversity gain and data rates of the NOMA system. Extrinsic Information Transfer (EXIT) charts are employed to examine the cooperative NOMA system’s user fairness as well as its performance while implementing the IRregular Convolutional Code (IRCC). The EXIT chart using IRCC evaluates the convergence analysis for the proposed system. Furthermore, the implementation of EXIT charts for optimization convergence is exploited by using power optimization and the SIC for the joint rate to evaluate the fairness of the system. Simulation results shows that cooperative NOMA helps to achieve higher diversity gain and improved data rate for the cooperative NOMA system.","['NOMA', 'Silicon carbide', 'Resource management', 'Multiuser detection', 'Interference', 'Relays', 'Receivers']","['Cooperative communications', 'extrinsic information transfer', 'multiple access', 'non-orthogonal multiple access', 'orthogonal multiple access', 'successive interference cancellation']"
"Driven by the limited radio spectrum resources and the high energy consumption of wireless devices, symbiotic radio (SR) has recently been proposed to support passive Internet-of-Things (IoT) networks, where a primary transmitter (PT) transmits information to a primary reader (PR), while passive backscatter devices (BDs) modulate their own information on the received primary signal and backscatter the modulated signal to the same PR by adjusting their reflection coefficients. Existing works on SR have mainly studied the case of a single BD while without considering the BD's energy harvesting (EH) ability. In this paper, we aim to maximize the energy efficiency (EE) of an SR system that includes multiple BDs each being able to harvest energy while backscattering, by jointly optimizing the PT transmission power and the BDs' reflection coefficients and time division multiple access (TDMA) time slot durations for both the parasitic SR (PSR) and commensal SR (CSR) cases. To solve the formulated non-convex optimization problems, we propose a Dinkelbach-based iterative algorithm that builds on the block coordinated decent (BCD) method and the successive convex programming (SCP) technique. Simulation results show that the proposed algorithm converges fast, and the system EE is maximized when the BD that can provide the highest EE is allocated the maximum allowed time for backscattering while guaranteeing the throughput requirements for both the primary link and the other backscatter links.","['Backscatter', 'Throughput', 'Time division multiple access', 'Radio frequency', 'Symbiosis', 'Simulation', 'Radio transmitters']","['Backscatter communication', 'energy efficiency', 'resource allocation', 'symbiotic radio', 'wireless power transfer']"
"Though visible-light communication (VLC) channels are contained by opaque boundaries, they present unique challenges in the development of multi-user/multi-cell scenarios. In this paper, two hybrid transmission schemes are proposed for managing multiple users in multi-cell VLC networks. The proposed schemes are based on using non-orthogonal multiple access (NOMA) in the network access points (APs), while applying zero-forcing (ZF) pre-coding to the cell edge users' signals, which are cooperatively broadcast from the APs. The proposed approach allows a reduction of the inter-cell interference affecting the cell-edge users thanks to ZF pre-coding, while dealing with inter-user interference for cell-center users via NOMA signaling. Considering different transmission scenarios, we show the improvement in the network total achievable data rate as well as fairness, as compared to conventional NOMA. For example, for a typical scenario considered, an improvement of up to 39% in total achievable rate and up to 112% in the network fairness is achieved. The proposed approach also presents a clear advantage over the conventional ZF pre-coding, for which the maximum number of users is constrained to the number of APs.","['NOMA', 'Interference', 'Light emitting diodes', 'Computer architecture', 'MISO communication', 'Silicon carbide', 'Microprocessors']","['Visible light communications', 'multi-user networks', 'non-orthogonal multiple access', 'pre-coding']"
"As virtual reality (VR) applications become popular, the desire to enable high-quality, lightweight, and mobile VR can potentially be achieved by performing the VR rendering and encoding computations at the edge and streaming the rendered video to the VR glasses. However, if the rendering has to be performed after the edge gets to know of the user’s new head and body position, the ultra-low latency requirements of VR will not be met by the roundtrip delay. In this article, we introduce edge intelligence, wherein the edge can predict, pre-render and cache the VR video in advance, to be streamed to the user VR glasses as soon as needed. The edge-based predictive pre-rendering approach can address the challenging six Degrees of Freedom (6DoF) VR content. Compared to 360-degree videos and 3DoF (head motion only) VR, 6DoF VR supports both head and body motion, thus not only viewing direction but also viewing position can change. Hence, our proposed VR edge intelligence comprises of predicting both the head and body motions of a user accurately using past head and body motion traces. In this article, we develop a multi-task long short-term memory (LSTM) model for body motion prediction and a multi-layer perceptron (MLP) model for head motion prediction. We implement the deep learning-based motion prediction models and validate their accuracy and effectiveness using a dataset of over 840,000 samples for head and body motion.","['Rendering (computer graphics)', 'Predictive models', 'Streaming media', 'Edge computing', 'Resists', 'Encoding', 'Decoding']","['Virtual reality', 'video streaming', 'six degrees of freedom (6DoF)', 'edge computing', 'edge intelligence', 'motion prediction']"
"Illumination LEDs, but also infrared LEDs have limited bandwidth. To achieve high throughput, one needs to modulate the LED significantly above its 3 dB bandwidth. Orthogonal Frequency Division Multiplexing (OFDM) is a popular modulation technique to cope with the frequency selectivity of the LED channel. In this article, we challenge whether its large Peak-to-Average-Power Ratio (PAPR) and resulting large DC bias are justified. We compare systems using the same power and derive how PAM and OFDM variants reach their optimum throughput at different bandwidths and differently shaped spectral densities, thus at very different Signal to Noise Ratio (SNR) profiles but nonetheless the same transmit power.When corrected for the path loss and normalized to the noise power in the 3 dB bandwidth of the LED, we call this the Normalized Power Budget (NPB). OFDM can exploit the low-pass LED response using a waterfilling approach. This is attractive if the NPB exceeds 60 dB. OFDM will then have to spread its signal over more than ten times the LED bandwidth and requires a DC bias of more than 4 times the rms modulation depth. Second-order distortion and LED droop may then become a limitation, if not compensated. At lower power (NPB between 30 and 60 dB), DCO-OFDM outperforms PAM, provided that it significantly reduces its bias and only if it uses an appropriate adaptive bit and power loading. Without adaptive bit loading, thus using a frequency–constant modulation order, for instance made feasible by a pre-emphasis, OFDM always shows lower performance than PAM; about 2.5 dB at a NPB around 60 dB. Below 30 dB of NPB, even waterfilling cannot outweigh the need for a larger bias in OFDM, and PAM should be preferred. We argue that a mobile system that has to operate seamlessly in wide coverage and short–range high–throughput regimes, needs to adapt not only its bandwidth and its bit–loading profile, but also its DCO-OFDM modulation depth, and preferably falls back from OFDM to PAM.","['OFDM', 'Bandwidth', 'Light emitting diodes', 'Optical modulation', 'Optimization', 'Throughput']","['LED', 'VLC', 'IR', 'PAM', 'OFDM', 'waterfilling', 'pre-emphasis', 'optical wireless communication']"
"To support multiple users in an indoor multiple-input multiple-output visible light communication (MIMO-VLC) system adopting spatial multiplexing, orthogonal frequency division multiple access (OFDMA) is usually adopted, where the overall modulation bandwidth is shared by all the users. In this paper, by fully exploiting the spatial distributions of light-emitting diode (LED) transmitters in the ceiling and end users around the receiving plane, we propose a space division multiple access (SDMA) technique for indoor spatial multiplexing-based MIMO-VLC systems. When applying SDMA, users within the MIMO-VLC system are divided into different user groups (UGs) based on their spatial locations with respect to different LED transmitters. Specifically, each UG can use the overall modulation bandwidth of the system. For efficient implementation of SDMA, two distributed user grouping (DUG) approaches are proposed, including basic DUG and transmit diversity-enhanced DUG (TD-DUG), and a two-step resource allocation algorithm is further designed. The achievable rates of the MIMO-VLC system employing SDMA with both basic DUG and TD-DUG are derived accordingly. To verify the superiority of SDMA over conventional OFDMA, detailed analytical and simulation results are presented. Moreover, a proof-of-concept experiment is conducted to demonstrate the advantage of SDMA in a practical spatial multiplexing-based MIMO-VLC system.","['Multiaccess communication', 'Modulation', 'Bandwidth', 'MIMO communication', 'Light emitting diodes', 'Lighting', 'Optical transmitters']","['Distributed user grouping (DUG)', 'multiple-input multiple-output (MIMO)', 'space division multiple access (SDMA)', 'visible light communication (VLC)']"
"The massive connectivity is among other unprecedented requirements which are expected to be satisfied in order to follow the perpetual increase of connected devices in the era of Internet of Things. In contrast to the family of conventional orthogonal multiple access schemes, the key distinguishing feature of non-orthogonal multiple access (NOMA) is its capacity to support the massive connectivity. Sparse code multiple access (SCMA) is one of the powerful schemes of code-domain NOMA (CD-NOMA) and is among the promising candidates of multiple access techniques to be employed in future generations of wireless communication systems thanks to the sparsity pattern of its codebooks. This technique has been actively investigated in recent years. In this paper, we provide a comprehensive survey of the state-of-the-art of SCMA. First, we will pinpoint SCMA place in the NOMA landscape including power-domain NOMA and CD-NOMA with the aim of justifying why SCMA is prominent. Then, its system architecture is highlighted and its basic principles are presented, afterwards a review of exiting codebook designs and available SCMA detectors is provided, before showing how resources are expected to be assigned, and how SCMA can be combined with other existing and emerging technologies. Finally, we present a range of future research trends and challenging open issues that should be addressed to optimize SCMA performance.","['Wireless communication', 'NOMA', 'Systems architecture', 'Prototypes', 'Detectors', 'Market research', 'Resource management']","['SCMA', 'NOMA', 'code-domain', 'codebook design', 'multi-dimensional constellations', 'message-passing algorithms', 'SCMA detector', 'IoT']"
"The problem of efficient ultra-massive multiple-input multiple-output (UM-MIMO) data detection in terahertz (THz)-band non-orthogonal multiple access (NOMA) systems is considered. We argue that the most common THz NOMA configuration is power-domain superposition coding over quasi-optical doubly-massive MIMO channels. We propose spatial tuning techniques that modify antenna subarray arrangements to enhance channel conditions. Towards recovering the superposed data at the receiver side, we propose a family of data detectors based on low-complexity channel matrix puncturing, in which higher-order detectors are dynamically formed from lower-order component detectors. The proposed solutions are first detailed for the case of superposition coding of multiple streams in point-to-point THz MIMO links. Then, the study is extended to multi-user NOMA, in which randomly distributed users get grouped into narrow cell sectors and are allocated different power levels depending on their proximity to the base station. Successive interference cancelation is shown to be carried with minimal performance and complexity costs under spatial tuning. Approximate bit error rate (BER) equations are derived, and an architectural design is proposed to illustrate complexity reductions. Under typical THz conditions, channel puncturing introduces more than an order of magnitude reduction in BER at high signal-to-noise ratios while reducing complexity by approximately 90%.","['NOMA', 'Detectors', 'MIMO communication', 'Array signal processing', 'Silicon carbide', 'Resource management', 'Baseband']","['THz communications', 'NOMA', 'UM-MIMO', 'subspace detectors', 'channel puncturing']"
"The diverse requirements of next-generation communication systems necessitate awareness, flexibility, and intelligence as essential building blocks of future wireless networks. The awareness can be obtained from the radio signals in the environment using wireless sensing and radio environment mapping (REM) methods. This is, however, accompanied by threats such as eavesdropping, manipulation, and disruption posed by malicious attackers. To this end, this work analyzes the wireless sensing and radio environment awareness mechanisms, highlighting their vulnerabilities and provides solutions for mitigating them. As an example, the different threats to REM and its consequences in a vehicular communication scenario are described. Furthermore, the use of REM for securing communications is discussed and future directions regarding sensing/REM security are highlighted.","['Sensors', 'Communication system security', 'Wireless sensor networks', 'Robot sensing systems', 'Wireless networks', 'Jamming', 'Hash functions']","['5G', '6G', 'cryptography', 'joint radar and communication (JRC)', 'physical layer security', 'radio environment mapping (REM)', 'REM security', 'sensing security', 'vehicle-to-everything (V2X) communication', 'wireless sensing', 'WLAN sensing']"
"The 6th Generation (6G) radio access technology is expected to support extreme communication requirements in terms of throughput, latency and reliability, which can only be achieved by providing capillary wireless coverage. In this paper, we present our vision for short-range low power 6G ‘in-X’ subnetworks, with the ‘X’ standing for the entity in which the cell in which the subnetwork is deployed, e.g., a production module, a robot, a vehicle, a house or even a human body. Such cells can support services that can be life-critical and that traditionally relied on wired systems. We discuss potential deployment options, as well as candidate air interface components and spectrum bands. Interference management is identified as a major challenge in dense deployments, which needs to handle also non-cellular types of interference like jamming attacks and impulsive noise. A qualitative example of interference-robust system design is also presented.","['6G mobile communication', 'Reliability', 'Interference', '5G mobile communication', 'Wireless communication', 'Jamming', 'Wireless sensor networks']","['6G', 'in-X', 'subnetworks', 'industrial automation', 'in-vehicle communications', 'in-body communications', 'interference management', 'jamming']"
"The conflation of cognitive radio (CR) and non-orthogonal multiple access (NOMA) concepts is a promising approach to fulfill the massive connectivity goals of future networks given the spectrum scarcity. Accordingly, this paper investigates the performance of a cooperative CR-NOMA network in the presence of system impairments and interference. Our analysis is involved with the derivation of the end-to-end outage probability for primary and secondary networks by accounting for channel state information (CSI), hardware imperfections, and residual interference caused by successive interference cancellation errors as well as coexisting primary/secondary users. Moreover, a mathematically tractable upper bound on spectral efficiency (SE) with its high-SNR approximations are derived. Besides, we propose an optimal power allocation scheme for CR-NOMA users to guarantee their outage and SE fairness. The numerical results validated by Monte Carlo simulations show that the CR-NOMA network provides a superior outage performance over orthogonal multiple access. Furthermore, the higher level of system imperfections leads to the performance degradation of the CR-NOMA networks. As imperfections become more severe, the CR-NOMA is observed to deliver relatively inferior outage and SE performance as compared to the perfect system scenario.","['NOMA', 'Silicon carbide', 'Hardware', 'Numerical models', 'Mathematical model', 'Channel estimation', 'Probability']","['Cognitive radio', 'cooperative non-orthogonal multiple access', 'outage probability (OP)', 'spectral efficiency', 'hardware impairment']"
"Free-space optical (FSO) communication provides wireless optical connectivity with high data rates and low-cost implementation; however, its performance is strongly influenced by the power attenuation due to the infrequent adverse weather conditions. This article proposes a novel highly sensitive dual-mode receiver comprising an array of single-photon avalanche diodes (SPADs) and a PIN photodiode (PD) to enhance the availability of FSO links. In adverse weather conditions, the receiver operates in the SPAD-mode; whereas, in the clear weather conditions, the receiver works in the PD-mode. A hybrid receiver controller is employed in the proposed receiver to adaptively control the switching process based on the received light levels. The adaptive controller also adjusts the incident photon rate of the SPAD array using a variable optical attenuator (VOA) to optimize the performance of the SPAD unit. Our extensive performance analysis illustrates the superior achievable data rates of the proposed receiver under various weather conditions compared to the traditional FSO receivers with either PD or SPAD array.","['Optical attenuators', 'Optical receivers', 'Meteorology', 'Optical switches', 'Optical sensors', 'Attenuation']","['Optical wireless communications', 'free-space optical communication', 'optical receivers', 'single-photon avalanche diode']"
"This article uses a stochastic geometry model-based approach to analyze the downlink performance of an indoor visible light communication (VLC) system with human blockages. The system performance is analyzed for a regular placement of light emitting diodes (LEDs) in a rectangular configuration. The proposed analysis is divided into two parts. In the first part, it is assumed that human blockages are static. The closed-form expression of the signal-to-noise ratio (SNR) at the receiver with static human blockages is derived. Further, the analysis is extended for the case where blockage mobility has also been considered. We compare the system performance in terms of received SNR for 4 and 8 LED configurations under predefined total power constraint. It is observed that at a lower density of human blockages in the room, the 4-LED configuration outperforms the 8-LED configuration. However, at a higher density of human blockages in the room, the 8-LED configuration outperforms the 4-LED configuration for the same total power constraint. Hence the proposed analysis will be useful for designing a VLC based indoor communication system wherein the system designer can switch between these two configurations depending on the number of blockages.","['Light emitting diodes', 'Visible light communication', 'Wireless communication', 'Stochastic processes', 'Signal to noise ratio', 'Optical receivers', 'Optical transmitters']","['Visible light communication (VLC)', 'human blockage', 'matern hardcore point process (MHCP)', 'random waypoint model (RWP)']"
"In this article, we investigate the performance of a piece-wise linear model of energy harvesting-based multiuser overlay spectrum sharing system. Herein, an energy-constrained secondary node acts as a cooperative relay to assist the information transmission between a primary transmitter and multiple primary receivers. In return for the cooperation, the secondary node enjoys access to the primary user's spectrum for its own information transfer. Specifically, by employing a time-switching based receiver, the secondary node harvests energy from the received radio-frequency signal of primary transmitter during a dedicated energy harvesting phase. In the subsequent information transfer phase, the secondary node splits the harvested power to forward the primary data as well as its own information intended for another secondary user. We analyze the impact of decoding primary's information at the secondary receiver on the performance of secondary network. Importantly, we propose an improved energy harvesting-based relaying scheme which makes an efficient use of available degrees of freedom and thereby enhances the performance of both primary and secondary networks significantly. For this analytical framework, we derive the expressions of outage probability for primary and secondary networks. Numerical and simulation results are obtained to extract various useful insights and to validate our theoretical developments.","['Wireless communication', 'Radio frequency', 'Simulation', 'Radio transmitters', 'Receivers', 'Information processing', 'Relays']","['Cooperative relaying', 'non-linear energy harvesting', 'outage probability', 'performance analysis', 'spectrum sharing', 'simultaneous wireless information and power transfer (SWIPT)']"
"In this paper, we develop a unified theoretical framework for analyzing the outage performance of reconfigurable intelligent surfaces (RISs)-assisted communication systems over generalized fading channels and in the presence of phase noise. Fox’s H function theory is then utilized to derive the outage probability for various channel fading and phase noise distributions in closed-form. We further conduct an asymptotic outage analysis to obtain insightful findings. In particular, we present the maximum diversity order achievable over such channels and demonstrate the performance variation in comparison to conventional Rayleigh channels. Then, based on upper bounds and lower bounds, we propose a design criteria for RISs to achieve the maximum diversity order in the presence of phase noise. More specifically, we show that if the absolute difference between pairs of phase errors is less than \pi /2, RIS-assisted communications achieve the full diversity order over independent fading channels, even in the presence of phase noise. The theoretical frameworks and findings are validated with the aid of Monte Carlo simulations.","['Phase noise', 'Rayleigh channels', 'Probability', 'Power system reliability', 'Signal to noise ratio', 'Cultural differences', 'Wireless networks']","['Reconfigurable intelligent surface', 'Fox’s H-distribution', 'rice fading', 'phase noise', 'outage probability', 'diversity order']"
"Overcoming the link blockage challenges is essential for enhancing the reliability and latency of millimeter wave (mmWave) and sub-terahertz (sub-THz) communication networks. Previous approaches relied mainly on either (i) multiple-connectivity, which under-utilizes the network resources, or on (ii) the use of out-of-band and non-RF sensors to predict link blockages, which is associated with increased cost and system complexity. In this paper, we propose a novel solution that relies only on in-band mmWave wireless measurements to proactively predict future dynamic line of sight (LOS) link blockages. The proposed solution utilizes deep neural networks and special patterns of received signal power, that we call pre-blockage wireless signatures to infer future blockages. Specifically, the developed machine learning models attempt to predict: (i) If a future blockage will occur? (ii) When will this blockage happen? (iii) What is the type of the blockage? And (iv) what is the direction of the moving blockage? To evaluate our proposed approach, we build a large-scale real-world dataset comprising nearly 0.5 million data points (mmWave measurements) for both indoor and outdoor blockage scenarios. The results, using this dataset, show that the proposed approach can successfully predict the occurrence of future dynamic blockages with more than 85% accuracy. Further, for the outdoor scenario with highly-mobile vehicular blockages, the proposed model can predict the exact time of the future blockage with less than 100 ms error for blockages happening within the future 600 ms. These results, among others, highlight the promising gains of the proposed proactive blockage prediction solution which could potentially enhance the reliability and latency of future wireless networks.","['Millimeter wave communication', 'Machine learning', 'Predictive models', 'Wireless networks']","['Millimeter wave', 'machine learning', 'dynamic blockage prediction', 'wireless signatures']"
"In this work, we examine an intelligent reflecting surface (IRS) assisted downlink non-orthogonal multiple access (NOMA) scenario intending to maximize the sum-rate of users. The optimization problem at the IRS is quite complicated, and non-convex since it requires the tuning of the phase shift reflection matrix. Driven by the rising deployment of deep reinforcement learning (DRL) techniques that are capable of coping with solving non-convex optimization problems, we employ DRL to predict and optimally tune the IRS phase shift matrices. Simulation results reveal that the IRS-assisted NOMA system based on our utilized DRL scheme achieves a high sum-rate compared to OMA-based one, and as the transmit power increases, the capability of serving more users increases. Furthermore, results show that imperfect successive interference cancellation (SIC) has a deleterious impact on the data rate of users performing SIC. As the imperfection increases by ten times, the rate decreases by more than 10%.","['NOMA', 'Wireless communication', 'Resource management', 'Downlink', 'Optimization', 'Interference cancellation', '6G mobile communication']","['Intelligent reflecting surfaces (IRS)', 'non-orthogonal multiple access (NOMA)', 'deep reinforcement learning (DRL)', '5G and beyond', '6G', 'phase shift design']"
"The concept of digital twin (DT) is constantly revealing as a key enabling technology for the deployment of mobile communication services envisaged for the sixth-generation (6G) Internet-of-Things (IoT). This paper aims at providing a comprehensive review of the current state-of-the-art DT-enabled 6G oriented network services. The main characteristics of this new key enabling technology and its critical aspects are highlighted. An overview of the 6G network requirements for the deployment of its innovative envisioned services is firstly provided, emphasizing how the DT concept represents a complementary key enabling technology for them. This is followed by a brief introduction of the DT technology. Subsequently, a comprehensive classification and analysis of the research advancements on DT-enabled 6G services currently available in literature is provided. This paper is concluded by highlighting the most representative challenges and future directions necessary for the deployment of this promising and innovative technology.","['6G mobile communication', 'Real-time systems', 'Internet of Things', 'Artificial intelligence', '5G mobile communication', 'Communications technology', 'Big Data']","['Beyond fifth-generation (5G) network', 'digital twin (DT)', 'Internet of Things (IoT)', 'Internet of Vehicles (IoV)', 'sixth-generation (6G) network']"
"This paper describes and investigates a novel concept of frequency-domain spectral shaping (FDSS) with spectral extension for the uplink (UL) coverage enhancement in 5G New Radio (NR), building on discrete Fourier transform spread orthogonal frequency-domain multiplexing (DFT-s-OFDM). The considered FDSS concept is shown to have large potential for reducing the peak-to-average-power ratio (PAPR) of the signal, which directly impacts the feasible maximum transmit power under practical nonlinear power amplifiers (PAs) while still meeting the radio frequency (RF) emission requirements imposed by the regulations. To this end, the FDSS scheme with spectral extension is formulated, defining filter windows that fit to the 5G NR spectral flatness requirements. The PAPR reduction capabilities and the corresponding maximum achievable transmit powers are evaluated for a variety of bandwidth allocations in the supported 5G NR frequency ranges 1 and 2 (FR1 and FR2) and compared to those of the currently supported waveforms in 5G NR, particularly π/2-BPSK with FDSS without spectral extension and QPSK without FDSS. Furthermore, an efficient receiver structure capable of reducing the noise enhancement in the equalization phase is proposed. Finally, by evaluating the link-level performance, together with the transmit power gain, the overall coverage enhancement gains of the method are analyzed and provided. The obtained results show that the spectrally-extended FDSS method is a very efficient solution to improve the 5G NR UL coverage clearly outperforming the state-of-the-art, while being also simple in terms of computational complexity such that the method is implementation feasible in practical 5G NR terminals.","['5G mobile communication', 'Peak to average power ratio', 'Frequency-domain analysis', 'Resource management', '3GPP', 'Receivers', 'Uplink']","['5G new radio evolution', 'coverage', 'DFT-s-OFDM', 'energy-efficiency', 'frequency-domain spectral shaping', 'peak-to-average-power ratio', 'radio link performance', 'transmitter requirements']"
"Cell-free massive MIMO systems consist of many distributed access points with simple components that jointly serve the users. In millimeter wave bands, only a limited set of predetermined beams can be supported. In a network that consolidates these technologies, downlink analog beam selection stands as a challenging task for the network sum-rate maximization. Low-cost digital filters can improve the network sum-rate further. In this work, we propose low-cost joint designs of analog beam selection and digital filters. The proposed joint designs achieve significantly higher sum-rates than the disjoint design benchmark. Supervised machine learning (ML) algorithms can efficiently approximate the input-output mapping functions of the beam selection decisions of the joint designs with low computational complexities. Since the training of ML algorithms is performed off-line, we propose a well-constructed joint design that combines multiple initializations, iterations, and selection features, as well as beam conflict control, i.e., the same beam cannot be used for multiple users. The numerical results indicate that ML algorithms can retain 99-100% of the original sum-rate results achieved by the proposed well-constructed designs.","['Radio frequency', 'Array signal processing', 'Training', 'Classification algorithms', 'Computational complexity', 'Transmitting antennas', 'Millimeter wave technology']","['Cell-free', 'millimeter wave', 'hybrid architecture', 'analog beamforming', 'digital beamforming', 'beam training']"
"We target the problem of performing a large set of measurements over the territory to characterize the exposure from a 5G deployment. Since using a single Spectrum Analyzer (SA) is not practically feasible (due to the limited battery duration), in this work we adopt an integrated approach, based on the massive measurement of 5G metrics with a 5G smartphone, followed by a detailed analysis done with the SA and an ElectroMagnetic Field (EMF) meter in selected locations. Results, obtained over a real territory covered by 5G signal, reveal that 5G exposure is overall very limited for most of measurement locations, both in terms of field strength (up to 0.7 [V/m]) and as share w.r.t. other wireless technologies (typically lower than 15%). Moreover, our approach allows easily spotting measurement outliers, e.g., due to the exploitation of Dynamic Spectrum Sharing (DSS) techniques between 4G and 5G. In addition, the exposure metrics collected with the smartphone are overall a good proxy of the total exposure measured over the whole 5G channel. Moreover, the sight conditions and the distance from 5G base station play a great role in determining the level of exposure. Finally, a maximum of 130 [W] of power radiated by a 5G base station is estimated in the scenario under consideration.","['5G mobile communication', 'Power measurement', 'Particle measurements', 'Measurement', 'Atmospheric measurements', 'Time measurement', 'Meters']","['5G cellular networks', '5G EMF measurements', 'coverage analysis']"
"Wireless powered communication networks (WPCNs) are commonly analyzed by using the linear energy harvesting (EH) model. However, since practical EH circuits are non-linear, the use of the linear EH model gives rise to distortions and mismatches. To overcome these issues, we propose a more realistic, nonlinear EH model. The model is based upon the error function and has three parameters. Their values are determined to best fit with measured data. We also develop the asymptotic version of this model. For comparative evaluations, we consider the linear and rational EH models. With these four EH models, we investigate the performance of a WPCN. It contains a multiple-antenna power station (PS), a signal-antenna wireless device (WD), and a multiple-antenna information receiving station (IRS). The WD harvests the energy broadcast by the PS in the PS-WD link, and then it uses the energy in the WD-IRS link to transfer information. We analyze the average throughput of delay-limited and delay-tolerant transmission modes as well as the average bit error rate (BER) of binary phase-shift keying (BPSK) and binary differential phase-shift keying (BDPSK) over the four EH modes. As well, we derive the asymptotic expressions for the large PS antenna case and the effects of transmit power control. Furthermore, for the case of multiple WDs, we optimize energy beamforming and time allocation to maximize the minimum rate of the WDs. Finally, the performances of four EH models are validated by Monte-Carlo simulations.","['Integrated circuit modeling', 'Resource management', 'Wireless communication', 'Throughput', 'Power generation', 'Array signal processing', 'Analytical models']","['Nonlinear energy harvesting', 'wireless powered communications network', 'average throughput', 'bit error rate', 'resource allocation']"
"Next generation wireless networks are expected to be extremely complex due to their massive heterogeneity in terms of the types of network architectures they incorporate, the types and numbers of smart IoT devices they serve, and the types of emerging applications they support. In such large-scale and heterogeneous networks (HetNets), radio resource allocation and management (RRAM) becomes one of the major challenges encountered during system design and deployment. In this context, emerging Deep Reinforcement Learning (DRL) techniques are expected to be one of the main enabling technologies to address the RRAM in future wireless HetNets. In this paper, we conduct a systematic in-depth, and comprehensive survey of the applications of DRL techniques in RRAM for next generation wireless networks. Towards this, we first overview the existing traditional RRAM methods and identify their limitations that motivate the use of DRL techniques in RRAM. Then, we provide a comprehensive review of the most widely used DRL algorithms to address RRAM problems, including the value- and policy-based algorithms. The advantages, limitations, and use-cases for each algorithm are provided. We then conduct a comprehensive and in-depth literature review and classify existing related works based on both the radio resources they are addressing and the type of wireless networks they are investigating. To this end, we carefully identify the types of DRL algorithms utilized in each related work, the elements of these algorithms, and the main findings of each related work. Finally, we highlight important open challenges and provide insights into several future research directions in the context of DRL-based RRAM. This survey is intentionally designed to guide and stimulate more research endeavors towards building efficient and fine-grained DRL-based RRAM schemes for future wireless networks.","['Wireless communication', 'Wireless networks', 'Optimization', 'Resource management', 'Quality of service', 'Next generation networking', 'Reinforcement learning']","['Radio resource allocation and management', 'deep reinforcement learning', 'next generation wireless networks', 'HetNets', 'power', 'bandwidth', 'rate', 'access control']"
"Non-orthogonal multiple access (NOMA) is envisioned as a promising technology that enhances the spectrum efficiency for future cellular networks. On the other hand, there is a growing interest in wireless powered sensor networks toward the sustainable realization of Internet of Things (IoT), as they may not require any battery replacement in principle. Moreover, the use of unmanned aerial vehicle (UAV) as a moving access point or base station has emerged as a potential solution to the high traffic demands of wireless networks. This article focuses on the UAV-aided data collection from multiple wireless powered sensor nodes, where a single-antenna UAV supplies all the sensor nodes located in the coverage area with energy for data transmission, with which the sensor nodes send back their information to the UAV. We study several transmission schemes including NOMA as well as cooperative relaying, together with two representative sensor node pairing strategies. Based on the theoretical analysis in terms of the achievable outage probabilities as well as the corresponding numerical results, we elucidate suitable node pairing strategies that depend on selected transmission schemes.","['NOMA', 'Unmanned aerial vehicles', 'Wireless sensor networks', 'Resource management', 'Optimization', 'Probability density function', 'Data collection']","['Internet of Things (IoT)', 'non-orthogonal multiple access (NOMA)', 'unmanned aerial vehicle (UAV)', 'wireless powered sensor network (WPSN)']"
"The autoencoder concept has fostered the reinterpretation and the design of modern communication systems. It consists of an encoder, a channel and a decoder block that modify their internal neural structure in an end-to-end learning fashion. However, the current approach to train an autoencoder relies on the use of the cross-entropy loss function. This approach can be prone to overfitting issues and often fails to learn an optimal system and signal representation (code). In addition, less is known about the autoencoder ability to design channel capacity-approaching codes, i.e., codes that maximize the input-output mutual information under a certain power constraint. The task being even more formidable for an unknown channel for which the capacity is unknown and therefore it has to be learnt. In this paper, we address the challenge of designing capacity-approaching codes by incorporating the presence of the communication channel into a novel loss function for the autoencoder training. In particular, we exploit the mutual information between the transmitted and received signals as a regularization term in the cross-entropy loss function, with the aim of controlling the amount of information stored. By jointly maximizing the mutual information and minimizing the cross-entropy, we propose a theoretical approach that a) computes an estimate of the channel capacity and b) constructs an optimal coded signal approaching it. Theoretical considerations are made on the choice of the cost function and the ability of the proposed architecture to mitigate the overfitting problem. Simulation results offer an initial evidence of the potentiality of the proposed method.","['Mutual information', 'Decoding', 'Channel capacity', 'Training', 'Task analysis', 'Cost function', 'Receivers']","['Digital communications', 'physical layer', 'statistical learning', 'autoencoders', 'coding theory', 'mutual information', 'channel capacity', 'explainable machine learning']"
"In this paper, we investigate the problem of scheduling transmissions for spatially scattered nodes that contribute to a collaborative federated learning (FL) algorithm via wireless links provided by a drone. In the considered system, the drone acts as an orchestrator, coordinating the transmissions and the learning schedule within a predefined deadline. The actual schedule is reflected in a planned path: as the drone traverses it, it controls the distance and thereby the data rate to each node. Hence, the model is structured such that the drone orchestrator uses the path (trajectory) as its only tool to achieve fairness in terms of learning staleness , which reflects the learning time discrepancy among the nodes. Using the number of learning epochs performed at each learner as a performance indicator, we combine the average number of epochs computed and staleness into a balanced optimization criterion that is agnostic to the underlying FL implementation. We consider two methods for solving the complex trajectory planning optimization problem for static nodes: (1) successive convex programming (SCP) and (2) deep reinforcement learning (RL). Considering the proposed criterion, both methods are compared in three specific scenarios with few nodes. The results show that drone-orchestrated FL outperforms an immobile deployment by providing improvements in the range of 57% to 87.7%. Additionally, RL-guided trajectories are generally superior to SCP provided ones for complex node arrangements.","['Drones', 'Robots', 'Wireless communication', 'Schedules', 'Robot sensing systems', 'Trajectory optimization', 'Task analysis']","['Drone trajectory optimization', 'wireless communications', 'federated learning', 'drone small cells', 'staleness minimization', 'reinforcement learning', 'convex approximation', 'unmanned aerial vehicles', 'edge computing']"
"The edge computing paradigm has become extremely popular over the past years, as a means of offloading computationally intensive tasks by users of resource and battery-constrained devices. Nevertheless, the edge networks’ overexploitation by the ever-increasing number of task-offloading users, gradually leads to their performance degradation. In this paper, we leverage on the different levels of available computing capabilities across the network, and we design an incentive mechanism that aims to shift the selfish users’ preference from the edge to the upper fog computing layer, accounting for their level of delay tolerance. To deal with the users’ heterogeneity in terms of their applications’ multi-dimensional distinctive features (including their delay tolerance/sensitivity), a multi-dimensional contract theory modeling is adopted, according to which the edge server determines the bundles of the users’ provided efforts and corresponding offered rewards. In this respect, each user’s effort represents the amount of its initially offloaded task at the edge that is allowed to be further forwarded and processed at the fog. Considering that the users-to-edge server offloading is performed under Non-Orthogonal Multiple Access (NOMA), the problem of joint computation task offloading and uplink transmission power allocation is subsequently addressed via a Stackelberg game, where the edge server and the users are treated as leader and followers, respectively. The aim of the game is to minimize the end-to-end network’s energy consumption and increase its resource utilization efficiency. The incentive mechanism and resource allocation framework is evaluated via modeling and simulation regarding its operation and efficiency under different scenarios.","['Task analysis', 'Resource management', 'Contracts', 'Servers', 'Computational modeling', 'Edge computing', 'Delays']","['Computation offloading', 'edge-fog networks', 'game theory', 'incentive mechanism', 'multi-dimensional contract theory']"
"In this paper, we analyze the performance of an energy harvesting (EH)-assisted overlay cognitive non-orthogonal multiple access (NOMA) system. The underlying system consists of a primary transmitter-receiver pair accompanied by an energy-constrained secondary transmitter (ST) with its intended receiver. Accordingly, ST employs a time switching (TS) based receiver architecture to harvest energy from radio-frequency signals of the primary transmissions, and thereby uses this energy to relay the primary information and to transmit its own information simultaneously using the NOMA principle. For this, we propose two cooperative spectrum sharing (CSS) schemes based on incremental relaying (IR) protocol using amplify-and-forward (AF) and decode-and-forward (DF) strategies, viz., CSS-IAF and CSS-IDF, and compare their performance with the competitive fixed relaying based schemes. The proposed IR-based schemes adeptly avail the degrees-of-freedom to boost the system performance. Thereby, considering the realistic assumption of the NOMA-based imperfect successive interference cancellation, we derive the expressions of outage probability for the primary and secondary networks under both CSS-IAF and CSS-IDF schemes subject to the Nakagami-m fading. In addition, we quantify the throughput and energy efficiency for the considered system. The obtained theoretical findings are finally validated through numerous analytical and simulation results to reveal the advantages of the proposed CSS schemes over the baseline direct link transmission and orthogonal multiple access schemes.","['NOMA', 'Relays', 'Receivers', 'Protocols', 'Wireless sensor networks', 'Wireless networks', 'Silicon carbide']","['Amplify-and-forward', 'cognitive radio', 'decode-and-forward', 'energy harvesting', 'incremental relaying', 'non-orthogonal multiple access (NOMA)', 'overlay spectrum sharing', 'simultaneous wireless information and power transfer (SWIPT)']"
"The forthcoming communication networks for public safety authorities rely on the fifth generation (5G) of mobile networking technologies. Police officers, paramedics, border guards, as well as fire and rescue personnel, will connect through commercial operator's access network and rapidly deployable tactical bubbles. This transition from closed and dedicated infrastructure to hybrid architecture will expand the threat surface and expose mission-critical applications and sensitive information to cyber and physical adversaries. We explore and survey security architecture and enablers for prioritized public safety communication in 5G networks. We identify security threat scenarios and analyze enabling vulnerabilities, threat actors, attacks vectors, as well as risk levels. Security enablers are surveyed for tactical access and core networks, commercial infrastructure, and mission-critical applications, starting from push-to-talk and group video communication and leading to situational-awareness and remote-controlled systems. Two solutions are trialed and described in more detail: remote attestation enhanced access control for constrained devices, and securing of satellite backhauls. We also discuss future research directions highlighting the need for enablers to automate security of rapid deployments, for military-grade cost-effective customizations of commercial network services to ensure robustness, and for hardening of various types of public safety equipment.","['Security', 'Safety', '5G mobile communication', 'Mission critical systems', '3GPP', 'Long Term Evolution', 'Next generation networking']","['5G', 'cybersecurity', 'hybrid architecture', 'mobile network', 'public safety', 'security', 'survey', 'tactical bubble', 'trials']"
"In underwater wireless optical communications (UWOC), the random obstruction of light propagation by air bubbles can cause fluctuations in the incoming light intensity of a receiver. In this paper, we propose a statistical model for determining the received power by a receiver in the presence of air bubbles. First, based on real experiments of the behavior of air bubbles underwater, we propose statistical models for the generation, size, and horizontal distribution of each air bubble. Second, we mathematically derive the obstruction caused by the shadow of each bubble as it passes over the beam area. We then compute the combined obstruction of all generated air bubbles to determine the total obstructed power, which is a random variable due to the randomness of bubble behavior. Next, we find the first and second moments of the total obstructed power to model the statistical distribution of the obstructed received power by using the method of moments, which shows that the Weibull distribution suitably matches the simulation data. We also estimate the shape and scale parameters by using two derived moments. Furthermore, we also construct a statistical model of the received power with complete blockage in the presence of air bubbles and we derive the distribution of the composite channel model combining the proposed bubble-obstruction model with a Gamma-Gamma turbulence model. Finally, we obtain and verify the analytic forms of the average bit error rate and the capacity of UWOC systems under this newly proposed composite channel model.","['Atmospheric modeling', 'Optical beams', 'Receivers', 'Laser beams', 'Mathematical model', 'Wireless communication', 'Shape']","['Underwater wireless optical communications', 'air bubbles', 'turbulence', 'performance analysis']"
"Terahertz (THz) communications with a frequency band 0.1 – 10 THz are envisioned as a promising solution to future high-speed wireless communication. Although with tens of gigahertz available bandwidth, THz signals suffer from severe free-spreading loss and molecular-absorption loss, which limit the wireless transmission distance. To compensate for the propagation loss, the ultra-massive multiple-input-multiple-output (UM-MIMO) can be applied to generate a high-gain directional beam by beamforming technologies. In this paper, a review of beamforming technologies for THz UM-MIMO systems is provided. Specifically, we first present the system model of THz UM-MIMO and identify its channel parameters and architecture types. Then, we illustrate the basic principles of beamforming via UM-MIMO and discuss the far-field and near-field assumptions in THz UM-MIMO. Moreover, an important beamforming strategy in THz band, i.e., beam training, is introduced wherein the beam training protocol and codebook design approaches are summarized. The intelligent-reflecting-surface (IRS)-assisted joint beamforming and multi-user beamforming in THz UM-MIMO systems are studied, respectively. The spatial-wideband effect and frequency-wideband effect in the THz beamforming are analyzed and the corresponding solutions are provided. Further, we present the corresponding fabrication techniques and illuminate the emerging applications benefiting from THz beamforming. Open challenges and future research directions on THz UM-MIMO systems are finally highlighted.","['Wireless communication', 'Tutorials', 'Array signal processing', 'Millimeter wave communication', 'MIMO communication', 'Antenna arrays', 'Propagation losses']","['Terahertz communications', 'ultra-massive MIMO', 'terahertz channel model', 'wideband beamforming', 'multi-user MIMO', 'intelligent reflecting surface', 'terahertz antenna array']"
"The current broadband coverage area requisites and the expected user demand is satisfied by the state of the art satellite industry by using multiple spot beams of high throughput satellites with fixed multi-beam pattern and footprint planning. However, in recent years, new mobile broadband users with dynamic traffic demand are requesting for services in remote geographical locations such as air (aeroplanes) and water (ships). Furthermore, the expected demand varies with time and geographical location of the users. Hence, a practical approach to meet such heterogeneous demand is to plan adaptive beams to the satellites equipped with beamforming capabilities. In this paper, we study the state of the art fixed multi-beam pattern and footprint plan and show its drawbacks to support the non-uniformly distributed user terminals and varying traffic demands. To end this, we propose an adaptive multi-beam pattern and footprint plan where we design spot beams with flexible size and position based on the spatial clustering of the users in order to increase the flexibility of the high throughput satellite system. Numerical simulations demonstrate the high system performance of the proposed methodology.","['Satellites', 'Throughput', 'Satellite antennas', 'Broadband antennas', 'Satellite broadcasting', 'Receiving antennas', 'Broadband communication']","['Multi-beam high throughput satellite systems', 'beam pattern', 'beam footprint', 'DVB-S2X', 'precoding']"
"Increasing traffic demands are causing network operators to adopt disaggregated and open networking solutions to better exploit optical transmission capacity, and consequently enable a software-defined networking (SDN) approach to control and management that encompasses the WDM data transport layer. In these frameworks, a quality of transmission estimator (QoT-E) that gives the generalized signal-to-noise ratio (GSNR) is commonly used to compute the feasibility of transparent lightpaths (LP)s, taking into account the amplified spontaneous emission (ASE) noise and the nonlinear interference (NLI). In general, the ASE noise is the main contributor to the GSNR and is also the most challenging noise component to evaluate in a scenario with varying spectral loads, due to fluctuations in the optical amplifier responses. In this work, we propose a machine learning (ML) algorithm that is trained using different ASE-shaped spectral loads in order to predict the OSNR component of the GSNR; this methodology is subsequently used in combination with a QoT-E in the lightpath computation engine (L-PCE). We present an experiment on a point-to-point optical line system (OLS), including 9 commercial erbium-doped fiber amplifiers (EDFA)s used as black-boxes, each with variable gain and tilt values, and 8 fibers that are characterized by distinct physical parameters. Within this experiment, we receive the signal at the end of the OLS, measuring the bit-error-rate (BER) and the power spectrum, over 2520 different spectral loads. From this dataset, we extract the expected GSNRs and their linear and nonlinear components. Through joint application of a ML algorithm and the open-source GNPy library, we obtain a complete QoT-E, demonstrating that a reliable and accurate LP feasibility predictor may be implemented.","['Optical fiber networks', 'Optical noise', 'Signal to noise ratio', 'Wavelength division multiplexing', 'Prediction algorithms', 'Optical scattering', 'Predictive models']","['Machine learning', 'optical communications', 'erbium doped fiber amplifier (EDFA)', 'Raman scattering', 'quality of transmission (QoT)']"
"Cell-free (CF) massive multiple-input-multiple-output (mMIMO) deployments are usually investigated with half-duplex nodes and high-capacity fronthaul links. To leverage the possible gains in throughput and energy efficiency (EE) of full-duplex (FD) communications, we consider a FD CF mMIMO system with practical limited-capacity fronthaul links . We derive closed-form spectral efficiency (SE) lower bounds for this system with maximum-ratio combining/maximum-ratio transmission processing and optimal uniform quantization. We then optimize the weighted sum EE (WSEE) via downlink and uplink power control by using a two-layered approach: the first layer formulates the optimization as a generalized convex program, while the second layer solves the optimization decentrally using the alternating direction method of multipliers. We analytically show that the proposed two-layered formulation yields a Karush-Kuhn-Tucker point of the original WSEE optimization. We numerically show the influence of weights on the individual EE of the users, which demonstrates the utility of the WSEE metric to incorporate heterogeneous EE requirements of users. We show that low fronthaul capacity reduces the number of users each AP can support, and the cell-free system, consequently, becomes user-centric.","['Uplink', 'Downlink', 'Optimization', 'Quantization (signal)', 'Fading channels', 'Antennas', 'Power control']","['Decentralized optimization', 'energy efficiency', 'full-duplex (FD)', 'limited-capacity fronthaul']"
"Multi-access edge computing (MEC) has been proposed as an approach capable of addressing latency and bandwidth issues in application computation offloading to extend the capabilities beyond the computational and storage limitations of mobile devices. However, there is a critical challenge in MEC to maintain the service continuity between the offloaded user application that is running on the MEC host and the mobile device when a user is moving from radio node to radio node. Furthermore, energy consumption of application computation offloading is an important consideration for MEC service providers in terms of operational costs. Therefore, we formulate the MEC host selection and user application migration problem as a shortest path problem of network energy minimization. We simulate the problem in a hierarchical MEC network deployment environment. We also propose the metric, computational intensity (CI), that can be used by MEC service providers to address the MEC host selection problem. Our results show that with the increment of CI, the selection of MEC hosts tends to move toward level 3 (central deployment) due to energy efficiency and then return to the deployment at level 1 (radio node level) due to latency constraint of the user application. We show that with high accuracy in predicting the user mobility and the available resources in the MEC network, latency- and mobility-aware MEC host selection and user application migration can be pre-calculated to improve response time and energy efficiency.","['Mobile handsets', 'Task analysis', 'Energy efficiency', 'Mobile applications', 'Servers', 'Computational modeling', 'Edge computing']","['Application computation offloading', 'energy efficiency', 'multi-access edge computing', 'MEC host selection', 'user application migration', 'user mobility']"
"The key to developing future generations of wireless communication systems lies in the expansion of extant methodologies, which ensures the coexistence of a variety of devices within a system. In this paper, we assume several multicasting (MC) groups comprising three types of heterogeneous users including Information Decoding (ID), Energy Harvesting (EH) and both ID and EH. We present a novel framework to investigate the multi-group (MG) - MC precoder designs for three different scenarios, namely, Separate Multicast and Energy Precoding Design (SMEP), Joint Multicast and Energy Precoding Design (JMEP), and Per-User Information and/or Energy Precoding Design (PIEP). In the considered system, a multi-antenna source transmits the relevant information and/or energy to the groups of corresponding receivers using more than one MC streams. The data processing users employ the conventional ID receiver architectures, the EH users make use of a non-linear EH module for energy acquisition, while the users capable of performing both ID and EH utilize the separated architecture with disparate ID and non-linear EH units. Our contribution is threefold. Firstly, we propose an optimization framework to i) minimize the total transmit power and ii) to maximize the sum harvested energy, the two key performance metrics of MG-MC systems. The proposed framework allows the analysis of the system under arbitrary given quality of service and harvested energy requirements. Secondly, to deal with the non-convexity of the formulated problems, we transform the original problems respectively into equivalent forms, which can be effectively solved by semi-definite relaxation (SDR) and alternating optimization. The convergence of the proposed algorithms is analytically guaranteed. Thirdly, a comparative study between the proposed schemes is conducted via extensive numerical results, wherein the benefits of adopting SMEP over JMEP and PIEP models are discussed.","['Precoding', 'Receivers', 'Optimization', 'MISO communication', 'Wireless communication', 'Wireless sensor networks', 'Minimization']","['Energy optimization', 'multi-group multicast systems', 'precoding', 'simultaneous wireless information and power transmission (SWIPT)']"
"Unmanned-aerial-vehicles (UAVs) are intended to be a vital part of beyond 5G (B5G) and 6G communication networks. UAV-to-ground communications in urban and populated areas are usually exposed to highly variable propagation conditions that can be often characterized by composite fading channels. This paper provides mathematical framework for the performance evaluation of UAV-to-ground communications over double-scattered single-shadowed (DS-SS), and double scattered double shadowed (DS-DS) fading channels. To analyse in details we provide probability density function (PDF), cumulative distribution function (CDF), average fade duration (AFD) and level crossing rate (LCR) of the product of double Nakagami-m (DN) and single inverse Gamma (SIG) random processes (RPs), as well as the product of DN and double inverse Gamma (DIG) RPs. Furthermore, the derived integral-form formulas for the second order (SO) statistical measures are approximated by Laplace integration (LI) and exponential LI in order to provide closed-form expressions. The impact of DS-SS and DS-DS fading types on the SO statistics of UAV-to-ground propagation scenario are thoroughly examined. Moreover, the impact of different values of DS-SS and DS-DS fading severities on the SO statistics are also taken into investigation. Lastly, the proposed UAV-to-ground model is extended to include the SO performance analysis of L-number of UAVs. All the analytical results for the SO statistics are confirmed by Monte-Carlo simulations.","['Fading channels', 'Monte Carlo methods', '5G mobile communication', 'System performance', 'Probability density function', 'Random processes', 'Shadow mapping']","['5G communications', 'average fade duration (AFD)', 'composite fading channels', 'level crossing rate (LCR)', 'unmanned aerial vehicle (UAV)']"
"Programmable data plane (PDP) is an emerging technology for programming packet processing tasks by means of a domain-specific high-level language (e.g., programming protocol-independent packet processor (P4)) and programmable switch chips. Recently, several PDP virtualization schemes have been introduced to enable more flexible and elastic network management. In this article, we first give an overview PDP and P4. After that, existing PDP virtualization schemes are classified into hypervisor- and compiler-based approaches and their pros and cons are analyzed in detail. Finally, open challenges for PDP virtualization are identified and future research directions are presented.","['Virtualization', 'Switches', 'Pipelines', 'Merging', 'Programming', 'Program processors', 'Data mining']","['Programmable data plane', 'P4', 'virtualization']"
"Hybrid terrestrial-satellite (HTS) communication systems have gained a tremendous amount of interest recently due to the high demand for global high data rates. Conventional satellite communications operate in the conventional Ku (12 GHz) and Ka (26.5-40 GHz) radio-frequency bands for assessing the feeder link, between the ground gateway and the satellite. Nevertheless, with the aim to provide hundreds of Mbps of throughput per each user, free-space optical (FSO) feeder links have been proposed to fulfill these high data rates requirements. In this paper, we investigate the physical layer security performance for a hybrid very high throughput satellite communication system with an FSO feeder link. In particular, the satellite receives the incoming optical wave from an appropriate optical ground station, carrying the data symbols of N users through various optical apertures and combines them using the selection combining technique. Henceforth, the decoded and regenerated information signals of the N users are zero-forcing (ZF) precoded in order to cancel the interbeam interference at the end-users. The communication is performed under the presence of malicious eavesdroppers nodes at both hops. Statistical properties of the signal-to-noise ratio of the legitimate and wiretap links at each hop are derived, based on which the intercept probability metric is evaluated. The derived results show that above a certain number of optical apertures, the secrecy level is not improved further. Also, the system's secrecy is improved using ZF precoding compared to the no-precoding scenario for some specific nodes' positions. All the derived analytical expressions are validated through Monte Carlo simulations.","['Optical fiber communication', 'High-temperature superconductors', 'Satellites', 'Adaptive optics', 'Satellite broadcasting', 'Optical beams', 'Radio frequency']","['Free-space optics', 'high-throughput communications', 'hybrid terrestrial-satellite systems', 'intercept probability', 'optical feeder links', 'physical layer security', 'zero-forcing precoding']"
"This paper investigates a multi-user massive multiple-input multiple-output (MU-mMIMO) hybrid precoding (HP) scheme using low-resolution phase shifters (PSs) and digital-to-analog converters (DACs). The proposed HP approach involves two stages: RF beamforming based on the slowly time-varying channel second-order correlation matrix, and baseband MU precoding based on the instantaneous effective baseband channel to mitigate MU-interference by a regularized zero-forcing (RZF) technique. We consider three HP design architectures: (i) HP using full-resolution PSs and DACs, with a baseband transfer block for constant-modulus RF beamformer, (ii) HP using b-bit PSs and full-resolution DACs, with an orthogonal matching pursuit (OMP) based algorithm that can approach the optimal unconstrained RF beamformer, and (iii) HP using b-bit PSs and q-bit DACs, taking into account also DAC quantization noise. Illustrative results show that the proposed HP schemes with low-resolution PSs can approach the sum-rate of full-resolution PSs by using only 2-bit PSs, while offering higher energy efficiency. Furthermore, a study of sum-rate results for various PS and DAC quantization levels reveals that HP can achieve near-optimal performance with only 2-bit PSs and 5-bit DACs. Moreover, a comparison of the different array configurations, namely, uniform linear array (ULA), uniform circular array (UCA), uniform rectangular array (URA), and concentric circular array (CCA), indicates that URA and CCA outperform UCA and ULA in terms of spectral and energy efficiencies.","['Phased arrays', 'Radio frequency', 'Baseband', 'Quantization (signal)', 'Digital-analog conversion', 'Precoding', 'Phase shifters']","['Massive multiple-input multiple-output (mMIMO)', 'hybrid precoding', 'energy efficiency', 'low-resolution digital-to-analog converters (DACs)', 'low-resolution phase shifters (PSs)', 'uniform linear array (ULA)', 'uniform circular array (UCA)', 'uniform rectangular array (URA)', 'concentric circular array (CCA)']"
"Generalized frequency division multiplexing (GFDM), an enabler of beyond-5G wireless networks, can be critically impaired due to radio frequency (RF) phase noise. However, joint channel estimation and phase noise compensation for GFDM systems have not been addressed before. Hence, we tackle this problem. To this end, we propose an iterative algorithm for joint channel and phase noise estimation and two algorithms for joint data detection and phase noise compensation. These algorithms use linear and non-linear least-squares (NLS) methods and employ block-type and comb-type pilots. The complexity of these algorithms is also analyzed. Moreover, to reduce their complexity, interpolation techniques are deployed to decrease the number of unknowns. We also analyze the signal-to-interference-plus noise ratio (SINR) and sum-rate of GFDM contaminated with phase noise. Furthermore, the accuracy of the channel and phase noise estimates is established via Cramér-Rao lower bounds (CRLBs). The simulation results illustrate that the mean-squared error (MSE) performance of the proposed joint channel and phase noise estimator reaches the CRLB. Moreover, the proposed joint data symbol detection and phase noise compensation algorithms nearly eliminate the impacts of phase noise in GFDM systems.","['Phase noise', 'Channel estimation', 'Estimation', 'OFDM', 'Receivers', 'Interference', 'Signal to noise ratio']","['GFDM', 'phase noise', 'non-linear least-squares (NLS)', 'Cramér-Rao lower bound (CRLB)']"
"Wireless body area networks (WBANs) are characterized by large fluctuations in channel losses due to body shadowing. These fluctuations follow the patterns of the user's body movements. For example, in the case of walking and running, channel losses follow cyclical patterns. This paper presents an algorithm for transmission power control (TPC) and dynamic routing in WBANs when the user performs periodic body movements. The objective of the algorithm is to decrease the average power consumption to deliver packets to a common sink provided that a desired packet delivery rate (PDR) is guaranteed. This problem is important in WBANs given that replacing batteries is detrimental to several applications of WBANs, especially when sensors of the WBAN are implanted on the user's body. To the best of our knowledge, the proposed algorithm is the first to consider the joint problem of TPC and dynamic routing while not relying on non-local data (i.e., measurements of received power). This characteristic is important because traditional algorithms rely on data not local to transmitters, so these data have to be transmitted, consuming power unnecessarily. Traditional algorithms are also limited to the star topology only, so routing is not considered, which decreases network connectivity and transmission-power savings. The proposed algorithm is implemented on a WBAN of Shimmer wireless sensors. Experimental results show a reduction in power consumption of 23.4% to 50.4% when compared against transmissions at maximum power and a PDR within 5.6% of the desired value. The power consumption of the overhead of the proposed algorithm can be as small as 11% of that one of traditional algorithms. The algorithm's complexity is shown to beO(N3), whereNis the number of sensors. Finally, the algorithm is compared with traditional algorithms which reduce power consumption by 39.0% on average at most.","['Sensors', 'Wireless communication', 'Routing', 'Heuristic algorithms', 'Power demand', 'Power measurement', 'Wireless sensor networks']","['Transmission power control', 'dynamic routing', 'wireless sensor', 'inertial measurement unit']"
"The physical-layer radio access of 5G New Radio (NR) and other modern wireless networks builds on the cyclic prefix (CP) orthogonal frequency-division multiplexing (OFDM), known to suffer from the high peak-to-average power ratio (PAPR) challenge. In this article, novel PAPR reduction methods are developed, referred to as the iterative clipping and weighted error filtering (ICWEF) approach. To this end, clipping noise is separated from the data signal in frequency domain and properly tailored frequency-selective clipping noise filtering is adopted to control the tradeoff between PAPR reduction and transmitted signal quality. Furthermore, as 5G NR networks support adopting different OFDM numerologies at different bandwidth parts within one channel bandwidth, the ICWEF approach is also extended to take into account and suppress the resulting inter-numerology interference-something that most existing state-of-the-art methods do not consider. To facilitate comprehensive performance evaluations, a software-defined radio based prototyping testbed including a high-power base station power amplifier is also developed and used for assessing the performance of PAPR reduction solutions. The proposed ICWEF-based PAPR reduction concept is thereon thoroughly validated with extensive numerical and experimental results and shown to outperform the existing state-of-the-art reference solutions.","['Peak to average power ratio', 'OFDM', '5G mobile communication', 'Frequency-domain analysis', 'Resource management', 'Iterative methods', 'Time-domain analysis']","['5G new radio (NR)', 'clipping', 'error vector magnitude (EVM)', 'filtering', 'mixed numerology', 'orthogonal frequency-division multiplexing (OFDM)', 'peak-to-average-power ratio (PAPR)', 'prototyping', 'software-defined radio (SDR)', 'waveform', 'wireless communications']"
"The Electric Smart Grid (ESG) is referred to as the next generation electricity power network. The ESG is an intelligent critical infrastructure subject to various security vulnerabilities and especially data privacy breaches. This study presents a comprehensive overview of the latest privacy-preserving mechanisms and policies in the ESG, while promoting the employment of modern access control techniques towards preventing personal data disclosure, affecting both utility companies and consumers. Efficient categorization is provided regarding the proposed privacy preserving methods and characteristics, while focus is also given on the use of the Blockchain technology and the Multi Authority Access Control paradigm in the ESG infrastructure. The study concludes with a discussion upon privacy and access control challenges, as well as future work concerning the ESG.","['Privacy', 'Data privacy', 'Security', 'Authentication', 'Access control', 'Standards', 'Smart meters']","['Electric smart grid', 'AMI', 'privacy', 'access control', 'blockchain', 'data anomymization', 'encryption']"
"Recently, reconfigurable intelligent surfaces (RISs) have been identified as one potential solution to avoid performance degradation of using millimeter wave (mmWave) frequencies in vehicular communications. In this paper, we investigate the use of an RIS in a mmWave vehicular communication network. The problem of weighted sum-rate maximization in the uplink is considered, where an RIS is used to assist the communication. We focus on both single-user and multi-user cases. Single user case is solved using successive refinement algorithm, where two phase-optimization schemes that help reducing the channel estimation overhead are considered. In multi-user case, fractional programming technique is used to reformulate the original problem into a more convenient form, and an algorithm based on alternating optimization is proposed. The validity of the proposed methods is confirmed by numerical simulations. A significant performance increase is seen when utilizing an RIS in both cases. Considered phase optimization schemes are shown to result in a significant reduction in channel estimation at a cost of small performance degradation compared to the full channel state information beamforming scenario. We perform simulations to investigate the effects of mobility, and the results demonstrate the ability of an RIS to mitigate the effects of mobility to some extent. Furthermore, to get practical insights into vehicular communications aided by an RIS, we use a commercial ray-tracing tool to evaluate the performance.","['Array signal processing', 'Channel estimation', 'Optimization', 'Wireless communication', 'Uplink', 'Sensors', 'Communication networks']","['Alternating optimization', 'fractional programming', 'mmWave communications', 'passive beamforming', 'ray tracing', 'reconfigurable intelligent surfaces', 'vehicular communications']"
"To deliver high performance and reliability to the mobile users in accessing mobile cloud services, the major interest is currently given to the integration of centralized cloud computing and distributed edge computing infrastructures. In such a heterogeneous network ecosystem, multiple cloudlets from different service providers coexist. However, to meet the stringent latency requirements of computation-intensive and mission-critical applications, overloaded cloudlets can offload some of the incoming job requests to their relatively under-loaded neighboring cloudlets. In this paper, we propose a novel economic and non-cooperative game-theoretic model for load balancing among competitive cloudlets. This model aims to maximize the utilities of all the competing cloudlets while meeting the end-to-end latency of the users. We characterize the problem as a generalized Nash equilibrium problem and investigate the existence and uniqueness of a pure-strategy Nash equilibrium. We design a variational inequality based algorithm to compute the pure-strategy Nash equilibrium. We show that all the competing cloudlets are able to maximize their utilities by employing our proposed Nash equilibrium computation offload strategy in both under- and overloaded conditions. We also show through numerical evaluations that our load balancing model outperforms some of the existing game-theoretic load balancing frameworks, especially in a highly overloaded condition.","['Load management', 'Cloud computing', 'Load modeling', 'Games', 'Quality of service', 'Computational modeling', 'Nash equilibrium']","['Cloudlet computing', 'non-cooperative load balancing', 'generalized Nash equilibrium', 'variational inequality']"
"Maritime activities represent a major domain of economic growth with several emerging maritime Internet of Things use cases, such as smart ports, autonomous navigation, and ocean monitoring systems. The major enabler for this exciting ecosystem is the provision of broadband, low-delay, and reliable wireless coverage to the ever-increasing number of vessels, buoys, platforms, sensors, and actuators. Towards this end, the integration of unmanned aerial vehicles (UAVs) in maritime communications introduces an aerial dimension to wireless connectivity going above and beyond current deployments, which are mainly relying on shore-based base stations with limited coverage and satellite links with high latency. Considering the potential of UAV-aided wireless communications, this survey presents the state-of-the-art in UAV-aided maritime communications, which, in general, are based on both conventional optimization and machine-learning-aided approaches. More specifically, relevant UAV-based network architectures are discussed together with the role of their building blocks. Then, physical-layer, resource management, and cloud/edge computing and caching UAV-aided solutions in maritime environments are discussed and grouped based on their performance targets. Moreover, as UAVs are characterized by flexible deployment with high re-positioning capabilities, studies on UAV trajectory optimization for maritime applications are thoroughly discussed. In addition, aiming at shedding light on the current status of real-world deployments, experimental studies on UAV-aided maritime communications are presented and implementation details are given. Finally, several important open issues in the area of UAV-aided maritime communications are given, related to the integration of sixth generation (6G) advancements. These future challenges include physical-layer aspects, non-orthogonal multiple access schemes, radical learning paradigms for swarms of UAVs and unmanned surface and underwater vehicles, as well as UAV-aided edge computing and caching.","['Maritime communications', 'Network architecture', 'Satellite broadcasting', 'Internet of Things', 'Autonomous aerial vehicles', 'Wireless sensor networks', 'Broadband communication']","['Maritime communications', 'maritime Internet of Things (IoT)', 'sixth-generation (6G) mobile communication networks', 'space-air-ground-sea integrated networks', 'underwater IoT', 'unmanned aerial vehicles (UAVs)']"
"The demand for mobile data is likely to grow at a pace more than envisaged in the coming years. Further, as applications such as the Internet of Things (IoT) come to fruition, there will be increased diversity in the types of devices demanding Internet connectivity and their requirements. Significant increase in data rate requirements is also expected due to services such as Ultra High Definition (UHD) video streaming and cloud computing. To meet all these demands, physical layer waveform candidates for future generations of communications need to be robust and inherently capable of extending into multiple domains (space, time, frequency, users, transmission media, code etc.) to ensure efficient utilization of resources. Multiple domains can be innately integrated into the design process of modulation schemes by using tensors, which are multi-way arrays. This paper introduces a unified tensor framework, providing a foundation for multi-domain communication systems that can be used to represent, design and analyse schemes that span several domains. Transmitted signals are represented by N$ th order time function tensors which are coupled, using a system tensor of order N+M, with the received signals which are represented by another tensor of order M through the contracted convolution. We begin with the continuous time representation of the tensor system model and present both the strict multi-domain generalization of the Nyquist criterion for zero interference (inter-tensor and intra-tensor interference) as well as a relaxation. We present an equivalent discrete time system model, and as an example of using the tensor framework we derive tensor based linear equalization methods to combat multi-domain interference. An application to multi-user MIMO-GFDM illustrates the utility of this novel framework for derivation of joint domain signal processing techniques.","['Tensile stress', 'MIMO communication', 'Wireless communication', 'Interference', 'Multiplexing', 'Encoding']","['Linear equalization', 'MIMO', 'multi-domain communication systems', 'tensor modelling', 'wireless communication systems']"
"Semantic communication has witnessed a great progress with the development of natural language processing (NLP) and deep learning (DL). Although existing semantic communication technologies can effectively reduce errors in semantic interpretation, most of these solutions adopt a fixed bit length structure, along with a rigid transmission scheme, which is inefficient and lacks scalability faced with different meanings and signal-to-noise ratio (SNR) conditions. In this paper, we explore the impact of adaptive bit lengths on semantic coding (SC) under various channel conditions. First, we propose progressive semantic hybrid automatic repeat request (HARQ) schemes that utilize incremental knowledge (IK) to simultaneously reduce the communication cost and semantic error. On top of this, we design a novel semantic encoding solution with multi-bit length selection. In this fashion, the transmitter employs a policy network to decide the appropriate coding rate, so as to secure the correct information delivery at the cost of minimal bits. Moreover, a specific denoiser is further introduced to reduce the semantic errors encountered in the transmission process according to the semantic characteristics of context. Extensive simulation results have been conducted to verify the effectiveness of the proposed solution.","['Semantics', 'Decoding', 'Encoding', 'Bit rate', 'Transformers', 'Noise reduction', 'Channel coding']","['Semantic communication', 'semantic coding', 'joint source channel coding', 'deep learning', 'neural network', 'transformer', 'end-to-end communication', 'HARQ']"
"Estimating the power consumption and computational complexity of various digital signal processing (DSP) algorithms used in wireless communications systems is critical to assess the feasibility of implementing such algorithms in hardware, and for designing energy-constrained communications systems. Therefore, this paper presents a novel approach, based on practical system measurements using field programmable gate array (FPGA) and application-specific integrated circuit (ASIC), to evaluate the power consumption and the associated computational complexity of the most common mathematical operations performed within various DSP algorithms. Using the proposed approach, a new metric is developed for mapping the computational complexity to the computational power consumed by the mathematical operation in wireless transceivers. This allows combining the commonly used computational complexity metrics that are typically computed for each mathematical operation separately. Consequently, a single unified metric can be used to describe the entire algorithm. Therefore, the comparison and trade-offs between different algorithms become easier and more informative. The developed approach is used to evaluate the computational power of several DSP algorithms used in wireless communications systems, and perform thorough computational complexity comparisons. The obtained results reveal that computational complexity comparisons using different mathematical operations can be highly misleading in several scenarios. The power consumption evaluation of the considered DSP algorithms show that some algorithms may require a prohibitively high power, which makes such algorithms unsuitable for power-constrained wireless communications systems. The results also show that the proposed methodology can be adopted for various hardware implementation, however, some calibration might be required based on the adopted platform.","['Field programmable gate arrays', 'Power measurement', 'Power demand', 'Wireless communication', 'Receivers', 'Signal processing algorithms', 'Transmitters']","['Power', 'complexity', 'power optimization', 'computational power', 'computational complexity', 'FPGA', 'ASIC', 'PAPR', 'CFO', 'channel estimation']"
"Pilot symbol assisted modulation (PSAM) is widely used to obtain the channel state information (CSI) needed for coherent demodulation. It allows the density of pilot symbols to be dynamically chosen depending on the channel conditions. However, the insertion of pilots reduces the spectral efficiency, more severely when the channel is highly time-variant and/or frequency-selective. In these cases a significant amount of pilots is required to properly track the channel variations in both time and frequency dimensions. Alternatively, non-coherent demodulation does not require any CSI for the demodulation independently of the channel conditions. For the particular case of up-link (UL) based on massive single input - multiple output (SIMO) combined with orthogonal frequency division multiplexing (OFDM), we propose to replace the traditional reference signals of PSAM by a new differentially-encoded data stream that can be non-coherently detected. The latter can be demodulated without the knowledge of the CSI and subsequently used for the channel estimation. We denote our proposal as hybrid demodulation scheme (HDS) because it exploits both the benefits of a coherent demodulation scheme (CDS) and a non-coherent demodulation scheme (NCDS) to increase the spectral efficiency. The mean squared error (MSE) of the channel estimation, bit error rate (BER), achieved throughput and complexity are analyzed to highlight the benefits of this differential data-aided channel estimation as compared to other approaches. We show that the channel estimation is almost as good as PSAM, while the BER performance and throughput are improved for different channel conditions with a very small complexity increase.","['Channel estimation', 'OFDM', 'Demodulation', 'Complexity theory', 'Throughput', 'Antennas', 'Time-frequency analysis']","['Channel estimation', 'differential modulation', 'high-mobility', 'massive SIMO', 'non-coherent']"
"Visible light communication (VLC) is being envisioned as an enabling technology to provide the much-needed spectral relief for the ever-increasing demand for Internet connectivity and data consumption. Since VLC uses illumination sources for lighting as well as communication, it is required to provide dimming control for proper lighting and enhanced error performance for reliable data communication. In this paper, we address both these issues holistically. We formulate and study the power spectral densities of dimming-based modulation schemes, namely variable on-off keying (VOOK) and variable pulse position modulation (VPPM), and hence, derive their bandwidth requirements and spectral efficiencies. Moreover, the capacity of VLC systems is severely limited by the inter-symbol interference (ISI) occurring as a result of the multipath propagation of light signals in VLC. We propose to ameliorate the error performance of VLC systems by using channel equalization for ISI mitigation, thereby enhancing the system capacity. We develop the analytical model of a dimmable VLC system employing channel equalization and use this model to study the effect of dimming and data rate on the error performance of VOOK and VPPM schemes. We present simulation and analytical results to show that the performance of dimming-based modulation schemes is significantly improved using channel equalization.","['Modulation', 'Equalizers', 'Visible light communication', 'OFDM', 'Lighting', 'Spectral efficiency', 'Optical filters']","['Dimming', 'ISI mitigation', 'channel equalization', 'visible light communication']"
"Orthogonal time-frequency space (OTFS) scheme, which transforms a time and frequency selective channel into an almost non-selective channel in the delay-Doppler domain, establishes reliable wireless communication for high-speed moving devices. This work designs and analyzes low-complexity zero-forcing (LZ) and minimum mean square error (LM) receivers for multiple-input multiple-output (MIMO)-OTFS systems with perfect and imperfect receive channel state information (CSI). The proposed receivers provide exactly the same solution as that of their conventional counterparts, and reduce the complexity by exploiting the doubly-circulant nature of the MIMO-OTFS channel matrix, the block-wise inverse, and Schur complement. We also derive, by exploiting the Taylor expansion and results from random matrix theory, a tight approximation of the post-processing signal-to-noise-plus-interference-ratio (SINR) expressions in closed-form for both LZ and LM receivers. We show that the derived SINR expressions, when averaged over multiple channel realizations, accurately characterize their respective bit error rate (BER) with both perfect and imperfect receive CSI. We numerically show the lower BER and lower complexity of the proposed designs over state-of-the-art exiting solutions.","['Receivers', 'Complexity theory', 'Time-frequency analysis', 'Signal to noise ratio', 'Interference', 'Wireless communication', 'Bit error rate']","['Message passing (MP)', 'orthogonal time-frequency space (OTFS)', 'linear low-complexity receivers']"
"The number of disasters has increased over the past decade where these calamities significantly affect the functionality of communication networks. In the context of 6G, airborne and spaceborne networks offer hope in disaster recovery to serve the underserved and to be resilient in calamities. Therefore, our paper reviews the state-of-the-art literature on post-disaster wireless communication networks and provides insights for the future establishment of such networks. In particular, we first give an overview of the works investigating the general procedures and strategies for facing any large-scale disaster. Then, we present technological solutions for post-disaster communications, such as the recovery of the terrestrial infrastructure, installing aerial networks, and using spaceborne networks. Afterwards, we shed light on the technological aspects of post-disaster networks, primarily the physical and networking issues. We present the literature on channel modeling, coverage and capacity, radio resource management, localization, and energy efficiency in the physical layer part, and discuss the integrated space-air-ground architectures, routing, delay-tolerant/software-defined networks, and edge computing in the networking layer part. This paper also includes interesting simulation results which can provide practical guidelines about the deployment of ad hoc network architectures in emergency scenarios. Finally, we present several promising research directions, namely backhauling, cache-enabled and intelligent reflective surface-enabled networks, placement optimization of aerial base stations (ABSs), and the mobility-related aspects that come into play when deploying aerial networks, such as planning their trajectories and the consequent handovers (HOs).","['Computer architecture', 'Wireless communication', 'Satellites', 'Ad hoc networks', 'Routing', 'Mesh networks']","['Coverage', 'stochastic geometry', 'non-terrestrial networks', 'resilience', 'backhaul', '6G']"
"Progress in optical wireless communication (OWC) has unleashed the potential to transmit data in an ultra-fast manner without incurring large investments and bulk infrastructure. OWC includes wireless data transmissions in three optical sub-bands; ultraviolet, visible, and infrared. This paper discusses installing infrared OWC, known as free space optics (FSO), systems on top of installed radio frequency (RF) networks for outdoor applications to benefit from the reliability of RF links and the unlicensed broad optical spectrum, and the large data rates carried by laser beams propagating in free space. We equally review commercially available solutions and the hardware requirements for RF and FSO technology co-existence. The potential of hybrid RF/FSO for space communication is further discussed. Finally, open problems and future research directions are presented.","['Radio frequency', 'Optical switches', 'Wireless communication', 'Laser beams', 'Optical transmitters', 'Bandwidth', 'Safety']","['Free space optics', 'RF systems', 'heterogeneous links', 'switching', 'diversity', 'atmospheric turbulence', 'propagation effects', 'digital divide', '6G', 'satellite and deep space communication']"
"Faster-than-Nyquist (FTN) signaling is a promising non-orthogonal pulse modulation technique that can improve the spectral efficiency (SE) of next generation communication systems at the expense of higher detection complexity to remove the introduced inter-symbol interference (ISI). In this paper, we investigate the detection problem of ultra high-order quadrature-amplitude modulation (QAM) FTN signaling where we exploit a mathematical programming technique based on the alternating directions multiplier method (ADMM). The proposed ADMM sequence estimation (ADMMSE) FTN signaling detector demonstrates an excellent trade-off between performance and computational effort enabling successful detection and SE gains for QAM modulation orders as high as 64K (65,536). The complexity of the proposed ADMMSE detector is polynomial in the length of the transmit symbols sequence and its sensitivity to the modulation order increases only logarithmically. Simulation results show that for 16-QAM, the proposed ADMMSE FTN signaling detector achieves comparable SE gains to the generalized approach semidefinite relaxation-based sequence estimation (GASDRSE) FTN signaling detector, but at an experimentally evaluated much lower computational time. Simulation results additionally show SE gains for modulation orders starting from 4-QAM, or quadrature phase shift keying (QPSK), up to and including 64K-QAM when compared to conventional Nyquist signaling. The very low computational effort required makes the proposed ADMMSE detector a practically promising FTN signaling detector for both low order and ultra high-order QAM FTN signaling systems.","['Detectors', 'Quadrature amplitude modulation', 'Modulation', 'Complexity theory', 'Estimation', 'Phase shift keying', 'Receivers']","['ADMM', 'faster-than-Nyquist (FTN) signaling', 'intersymbol interference (ISI)', 'sequence estimation', 'ultra high-order QAM']"
"Carrier Aggregation (CA) is an integral part of current cellular networks. Its ability to enhance the peak data rate, to efficiently utilize the limited available spectrum resources and to satisfy the demand for data-hungry applications has drawn large attention from different wireless network communities. Given the benefits of CA in the terrestrial wireless environment, it is of great interest to analyze and evaluate the potential impact of CA in the satellite domain. In this article, we provide a brief introduction to the possible CA configurations, their deployment scenarios/use cases as well as their advantages, disadvantages, and challenges. Next, the problem of multiuser aggregation and access control design for CA in multi-beam high throughput satellite systems under practical system constraints is presented. In particular, we propose an efficient CA design solution that gives the optimal carrier-user assignment along with the percentage that each user exploits each carrier assuming that multiple users can be multiplexed in each carrier. Both inter-transponder and intra-transponder CA at the satellite payload level of the communication stack are considered. We propose a flexible carrier allocation approach for a CA-enabled multi-beam satellite system targeting a proportionally fair user demand satisfaction. Simulation results and analysis shed some light on this rather unexplored scenario and demonstrate the feasibility of the CA in satellite communication systems.","['Satellite broadcasting', 'Satellites', 'Logic gates', 'Transponders', 'Throughput', 'Resource management']","['High throughput satellite', 'carrier aggregation', 'flexible resource allocation', 'channel bonding', 'multi-beam satellite', 'dual connectivity']"
"This paper provides a comprehensive survey on the coexistence of cellular and IEEE 802.11 standards from a holistic viewpoint that takes into account the coexistence of all existing and future cellular and IEEE 802.11 standards in all the available unlicensed spectrum bands. Unlike existing survey works focusing mostly on any unlicensed band and/or standard, we start by giving an overview of unlicensed spectrum bands, including 2.4 GHz, 5 GHz, 6 GHz, and 60 GHz. We then review the operation of cellular technologies, namely Long-Term Evolution Unlicensed (LTE-U), Licensed Assisted Access (LAA), and New Radio Unlicensed (NR-U), worldwide in the unlicensed spectrum bands. Further, we summarize scenarios and categories of coexistence mechanisms, conditions for a fair coexistence, and coexistence-related features. An extensive study on the coexistence mechanisms, deployment scenarios, as well as standardization efforts for the coexistence between cellular and IEEE 802.11 standards, is carried out. Finally, we highlight the coexistence challenges and open problems, the convergence of the Third Generation Partnership Project (3GPP) and IEEE standards, as well as future research directions. Moreover, to provide insights on the relative measures, we also carry out comparative studies of several key concerns with regard to the coexistence, namely unlicensed spectrum band, regulatory requirement, coexistence mechanism, and cellular standardization effort. Each study presents a comparison among potential features of one of these concerns in tabular forms. Finally, we summarize key lessons that are learned and discussed throughout the paper.","['Wireless fidelity', 'IEEE 802.11 Standard', 'Standards', 'Long Term Evolution', 'Europe', 'Convergence', 'Cellular networks']","['Unlicensed band', 'survey', 'cellular network', 'millimeter-wave', 'coexistence', 'LTE-U', 'LAA', 'NR-U', 'WiFi', 'IEEE 802.11']"
"Dual-Functional Radar-Communication systems enhance the benefits of communications and radar sensing by jointly implementing these on the same hardware platform and using the common RF resources. An important and latest concern to be addressed in designing such systems is maximizing the energy-efficiency. In this paper, we consider a Dual-Functional Radar-Communication system performing simultaneous multi-user communications and radar sensing, and investigate the energy-efficiency behaviour with respect to active transmission elements. Specifically, we formulate a problem to find the optimal precoders and the number of active RF chains for maximum energy-efficiency by taking into consideration the power consumption of low-resolution Digital-to-Analog Converters on each RF chain under communications and radar performance constraints. We consider Rate-Splitting Multiple Access to perform multi-user communications with perfect and imperfect Channel State Information at Transmitter. The formulated non-convex optimization problem is solved by means of a novel algorithm. We demonstrate by numerical results that Rate Splitting Multiple Access achieves an improved energy-efficiency by employing a smaller number of RF chains compared to Space Division Multiple Access, owing to its generalized structure and improved interference management capabilities.","['Radar', 'Radio frequency', 'Interference', 'Transmitters', 'Hardware', 'Sensors', 'Power demand']","['Digital-to-analog converters', 'dual-functional radar-communication', 'energy-efficiency', 'rate-splitting multiple access', 'RF chain optimization']"
"Due to the dramatic increase in wireless data traffic and the associated increase in energy consumption, designing energy-efficient wireless networks with improved spectral efficiency is a pressing concern. The focus of this article is the design of a green, highly energy-efficient cellular heterogeneous network (HetNet) by taking advantage of multiple-input-multiple-output (MIMO) structure and deployment of small cells. We consider the downlink of a two-tier HetNet, in which multiple-antenna small cells are coordinated to serve users. Even though the deployment of MIMO together with small cells improves the communication system's performance in terms of data rate and reliability, circuit energy consumption in such a network is a critical issue. To address this, an energy-efficient antenna selection and radio resource block assignment algorithm is proposed for the small cells, and a single radio-frequency (RF) chain structure is considered for the massive MIMO macro base station. Then, while coordinating transmissions between cells subject to user-centric clustering, an energy-efficient beamforming design and power allocation optimization problem with respect to the quality of service requirement of users, transmit power budget of base stations, and fronthaul capacity is formulated; the problem is solved using the Dinkelbach method. Simulation results demonstrate the performance potential of our proposed algorithm in terms of energy efficiency and spectral efficiency.","['Resource management', 'Interference', 'Optimization', 'Massive MIMO', '5G mobile communication', 'Quality of service']","['Multiple-input-multiple-output (MIMO) system', 'small cells', 'energy efficiency', 'interference management', 'radio resource allocation', 'heterogeneous cellular networks (HetNets)', 'coordinated transmission']"
"In this article, we propose an orthogonal frequency-division multiplexing system supported by the compressed sensing assisted index modulation, termed as (OFDM-CSIM), applied to millimeter-wave (mmWave) communications. In the OFDM-CSIM mmWave system, information is conveyed not only by the classic constellation symbols but also by the on/off status of subcarriers, where the size of constellation symbols and the number of active subcarriers can be beneficially configured for maximizing the system's throughput. We conceive a machine learning (ML) assisted adaptive OFDM-CSIM mmWave system, which simultaneously benefits from the OFDM with index modulation (IM), compressed sensing (CS) and the hybrid beamforming techniques. Specifically, a ML-assisted link adaptation scheme is designed based on the k -nearest neighbors (k -NN) algorithm with the objective to maximize the system's throughput. Our studies show that the proposed ML-assisted link adaptation is capable of providing higher throughput than the conventional threshold-based link adaptation when different antenna structures are considered. Furthermore, the achievable data rates of four types of antenna arrays, including uniform linear array (ULA), uniform rectangular planar array (URPA), uniform circle planar array (UCPA) and uniform cylindrical array (UCYA), are investigated and compared over mmWave channels. The simulation results show that the UCYA achieves the highest data rate among these antenna arrays.","['Array signal processing', 'Modulation', 'Linear antenna arrays', 'Arrays', 'Wireless communication', 'Indexes']","['OFDM', 'mmwave', 'index modulation', 'compressed sensing', 'hybrid beamforming', 'linkadaptation', 'machine learning', 'k-nearest neighbour']"
"Downlink beamforming is a key technology for cellular networks. However, computing beamformers that maximize the weighted sum rate (WSR) subject to a power constraint is an NP-hard problem. The popular weighted minimum mean square error (WMMSE) algorithm converges to a local optimum but still exhibits considerable complexity. In order to address this trade-off between complexity and performance, we propose to apply deep unfolding to the WMMSE algorithm for a MU-MISO downlink channel. The main idea consists of mapping a fixed number of iterations of the WMMSE into trainable neural network layers. However, the formulation of the WMMSE algorithm, as provided in Shi et al. , involves matrix inversions, eigendecompositions, and bisection searches. These operations are hard to implement as standard network layers. Therefore, we present a variant of the WMMSE algorithm i) that circumvents these operations by applying a projected gradient descent and ii) that, as a result, involves only operations that can be efficiently computed in parallel on hardware platforms designed for deep learning. We demonstrate that our variant of the WMMSE algorithm convergences to a stationary point of the WSR maximization problem and we accelerate its convergence by incorporating Nesterov acceleration and a generalization thereof as learnable structures. By means of simulations, we show that the proposed network architecture i) performs on par with the WMMSE algorithm truncated to the same number of iterations, yet at a lower complexity, and ii) generalizes well to changes in the channel distribution.","['Complexity theory', 'Array signal processing', 'Neural networks', 'Downlink', 'Approximation algorithms', 'Network architecture', 'Base stations']","['Deep unfolding', 'downlink beamforming', 'iterative optimization algorithm', 'weighted MMSE algorithm', 'neural network']"
"The fifth-generation wireless system framework provides the option to evaluate the performance of in-band full-duplex (IBFD) operation through flexible duplexing. The resulting self-interference, however, must be mitigated within a fraction of a symbol duration for successful communication. This paper introduces the use of neural network machine learning to accelerate the tuning of multi-tap adaptive RF cancellers. Additionally, the optimal network configurations, input data structures and training dataset densities that optimize the performance of this technique are presented. The tuning results of a prototype system using a two-tap canceller were measured over 20 and 100 MHz bandwidths centered at 2.5 GHz, and demonstrated averages of 40 dB cancellation and 6 tuning iterations. These results are compared to a survey of previously-reported adaptive cancellers, and illustrate that this novel application of machine learning to RF canceller tuning provides the fastest convergence speed to date, which can enable IBFD operation in dynamic interference environments.","['Tuning', 'Radio frequency', 'Neural networks', 'Machine learning', 'Time-frequency analysis', 'Attenuation', '5G mobile communication']","['5G mobile communication', 'in-band full-duplex', 'machine learning', 'RF cancellation', 'self-interference cancellation']"
"Non-orthogonal multiple access (NOMA) was recently regarded as a potential technique for next generation wireless networks. Recent works on relay selection for cooperative NOMA systems have mainly addressed the best relay selection to forward its received signals to terminal nodes. Nonetheless, in practical scenarios, the best relay may be unavailable due to non-ideal conditions such as scheduling and overload constraints or possibly due to channel feedback delay. Therefore, there is compelling need to consider a more practical solution, in which the best available relay is selected. In this article, we examine the error rate performance for simultaneous wireless information and power transfer (SWIPT)-enabled NOMA, while considering the selection of the m th best available relay. In particular, we present an exact pairwise error probability (PEP) expression to obtain a bit error rate (BER) upper bound. The asymptotic PEP is investigated to evaluate the achievable diversity order for NOMA users. Finally, simulation results are provided to verify the accuracy of the derived PEP expressions and to give more insights into the system performance.","['NOMA', 'Upper bound', 'Simulation', 'Bit error rate', 'Receivers', 'Cultural differences', 'Relays']","['Diversity order', 'NOMA', 'pairwise error probability', 'relay selection', 'SWIPT']"
"The majority of stochastic channel models rely on the electromagnetic far-field assumption, which allows to decompose the channel in terms of plane waves. The far-field assumption breaks down in future applications that push towards the electromagnetic near-field region, such as those where the use of electromagnetically large antenna arrays is envisioned. Motivated by this consideration, we show how physical principles can be used to derive a plane-wave scalar channel model that is also valid in the reactive near-field region. Precisely, we show that narrowband wave propagation through a three-dimensional scattered medium can be generally modeled as a linear and space-variant system. We first review the physics principles that lead to a closed-form deterministic plane-wave representation of the channel impulse response. This serves as a basis for deriving a stochastic representation of the channel in terms of statistically independent Gaussian random coefficients for spatially stationary random propagation environments. The very desirable property of spatial stationarity can always be retained in the radiative near-field region by excluding reactive propagation mechanisms confined in close proximity to the source. Remarkably, the provided stochastic representation is directly connected to the Fourier spectral representation of a general stationary spatial random field.","['Electromagnetics', 'Stochastic processes', 'Propagation', 'Wireless communication', 'Antenna arrays', 'Receivers', 'Channel models']","['Physical channel modeling', 'electromagnetic wave propagation', 'stochastic channel modeling', 'Fourier spectral representation', 'Fourier theory', 'electromagnetically large antenna arrays', 'high-frequency communications']"
"Future mobile networks supporting Internet of Things are expected to provide both high throughput and low latency to user-specific services. One way to overcome this challenge is to adopt Network Function Virtualization (NFV) and Multi-access Edge Computing (MEC). Besides latency constraints, these services may have strict function chaining requirements. The distribution of network functions over different hosts and more flexible routing caused by service function chaining raise new challenges for end-to-end performance analysis. In this paper, as a first step, we analyze an end-to-end communication system that consists of both MEC servers and a server at the core network hosting different types of virtual network functions. We develop a queueing model for the performance analysis of the system consisting of both processing and transmission flows. We propose a method in order to derive analytical expressions of the performance metrics of interest, i.e., end-to-end delay, system throughput, task drop rate. Then, we show how to apply the similar method to a larger system and derive a stochastic model for such systems. We observe that the simulation and analytical results are very close. By evaluating the system under different scenarios, we provide insights for the decision making on traffic flow control and its impact on critical performance metrics.","['Servers', 'Task analysis', 'Delays', 'Performance analysis', 'Analytical models', 'Network function virtualization', 'Base stations']","['Applied queueing theory', 'delay analysis', 'end-to-end performance analysis', 'multi-access edge computing', 'network function virtualization', 'throughput analysis']"
"The integration of sensing and communication (ISAC) functionalities have recently gained significant research interest as a hardware-, power-, spectrum- and cost- efficient solution. This experimental work implements a dual-functional sensing and communication framework where a single radiation waveform, either omnidirectional or directional, can realize both sensing and communication functions. We design an orthogonal frequency division multiplexing (OFDM) based multi-user multiple input multiple output (MIMO) software-defined radio (SDR) testbed to validate the dual-functional model. We carry out over-the-air experiments to investigate the optimal trade-off factor to balance the performance for both functions. On the communication side, we obtain bit error rate (BER) results from the testbed to show the communication performance using the dual-functional waveform. On the sensing performance, we measure the output beampatterns of our transmission to examine their similarity to simulation based beampatterns. We also implement a sensing experiment to realize activity detection functions. Our experiment reveals that the dual-functional approach can achieve comparable BER performance with pure communication-based solutions while achieving fine sensing beampatterns and realistic sensing functionality simultaneously.","['Sensors', 'OFDM', 'Radar antennas', 'Radar', 'Symbols', 'MIMO communication', 'Frequency division multiplexing']","['Waveform', 'communications', 'sensing', 'integrated sensing and communications (ISAC)', 'OFDM', 'MIMO', 'software-defined radio (SDR)', 'over-the-air', 'prototyping']"
"Sensing capability is one of the most highlighted new feature of future 6G wireless networks. This paper addresses the sensing potential of Large Intelligent Surfaces (LIS) in an exemplary Industry 4.0 scenario. Besides the attention received by LIS in terms of communication aspects, it can offer a high-resolution rendering of the propagation environment. This is because, in an indoor setting, it can be placed in proximity to the sensed phenomena, while the high resolution is offered by densely spaced tiny antennas deployed over a large area. By treating an LIS as a radio image of the environment relying on the received signal power, we develop techniques to sense the environment, by leveraging the tools of image processing and machine learning. Once a radio image is obtained, a Denoising Autoencoder (DAE) network can be used for constructing a super-resolution image leading to sensing advantages not available in traditional sensing systems. Also, we derive a statistical test based on the Generalized Likelihood Ratio (GLRT) as a benchmark for the machine learning solution. We test these methods for a scenario where we need to detect whether an industrial robot deviates from a predefined route. The results show that the LIS-based sensing offers high precision and has a high application potential in indoor industrial environments.","['Wireless sensor networks', 'Wireless networks', 'Noise reduction', 'Superresolution', 'Machine learning', 'Tools', 'Robot sensing systems']","['Computer vision', 'industry 4.0', 'large intelligent surfaces', 'machine learning', 'sensing']"
"Optical networking is fast evolving towards the applications of the Software-defined Networking (SDN) paradigm down to the (Wavelength-division Multiplexing) WDM transport layer for cost-effective and flexible infrastructure management. Optical SDN requires each network element's software abstraction to enable full control by the centralized network controller. Nowadays, modern network elements, especially photonic switching systems, are developed by exploiting the fast-emerging technology of Photonic Integrated Circuit (PIC) that consists of complex fabrics of elementary units that can be driven individually using a large set of elementary controls. In this work, we focus on modeling the elementary control states of the topological structures behind PIC N ×N switches under a fully blind approach based on Machine Learning (ML) techniques. The ML agent's training and testing datasets are obtained synthetically by software simulation of the photonic switch structure. The proposed technique's scalability and accuracy are validated by considering different dimensions N and applying it to two different switching topologies: the Honey-Comb Rearrangeable Optical Switch and the Beneš network. Excellent results in terms of prediction of the control states are achieved for both of the considered topologies.","['Optical switches', 'Control systems', 'Photonics', 'Topology', 'Network topology', 'Integrated optics', 'Routing']","['Machine learning', 'optical switches', 'photonic integrated circuits', 'silicon photonics', 'microring resonators']"
"A probabilistic protection guarantee enables a cloud provider to improve the availability of their cloud computing system in a cost-efficient manner. A backup resource allocation strategy based on the probabilistic protection guarantee reduces the total amount of required backup computation resources by allowing multiple virtual machines to share the same backup computation resources of a physical machine. There have been no experimental studies that investigate the impact of applying the probabilistic protection guarantee to a cloud computing framework in real use. This paper presents an experiment of failure recovery on the cloud computing system in which the backup computation resources are shared by multiple virtual machines. We implement a prototype cloud system by using the OpenStack framework to demonstrate the failure recovery scenario according to the backup resource allocation strategy. We develop an availability analytical model for the backup resource allocation strategy. Based on the analytical model, we present case studies which derive the availability of cloud system by using the measurement results of the experiment.","['Cloud computing', 'Resource management', 'Prototypes', 'Probabilistic logic', 'Analytical models', 'Virtual machining', 'Informatics']","['Availability', 'cloud computing', 'failure recovery', 'probabilistic protection guarantee shared protection']"
"Explainable Artificial Intelligence (XAI) is transforming the field of Artificial Intelligence (AI) by enhancing the trust of end-users in machines. As the number of connected devices keeps on growing, the Internet of Things (IoT) market needs to be trustworthy for the end-users. However, existing literature still lacks a systematic and comprehensive survey work on the use of XAI for IoT. To bridge this lacking, in this paper, we address the XAI frameworks with a focus on their characteristics and support for IoT. We illustrate the widely-used XAI services for IoT applications, such as security enhancement, Internet of Medical Things (IoMT), Industrial IoT (IIoT), and Internet of City Things (IoCT). We also suggest the implementation choice of XAI models over IoT systems in these applications with appropriate examples and summarize the key inferences for future works. Moreover, we present the cutting-edge development in edge XAI structures and the support of sixth-generation (6G) communication services for IoT applications, along with key inferences. In a nutshell, this paper constitutes the first holistic compilation on the development of XAI-based frameworks tailored for the demands of future IoT use cases.","['Internet of Things', 'Artificial intelligence', 'Data models', 'Medical services', 'Industrial Internet of Things', 'Predictive models', 'Ethics']","['Artificial intelligence', 'deep learning', 'explainability', 'Internet of Things', 'machine learnin']"
"This paper studies optimum detectors and error rate analysis for wireless systems with low-resolution quantizers in the presence of fading and noise. A universal lower bound on the average symbol error probability (SEP), correct for all M-ary modulation schemes, is obtained when the number of quantization bits is not enough to resolve M signal points. In the special case of M-ary phase shift keying (M-PSK), the maximum likelihood detector is derived. Utilizing the structure of the derived detector, a general average SEP expression for M-PSK modulation with n-bit quantization is obtained when the wireless channel is subject to fading with a circularly-symmetric distribution. For the Nakagami-m fading, it is shown that a transceiver architecture with n-bit quantization is asymptotically optimum in terms of communication reliability if n ≥ log 2 M + 1. That is, the decay exponent for the average SEP is the same and equal to m with infinite-bit and n-bit quantizers for n ≥ log 2 M + 1. On the other hand, it is only equal to 1/2 and 0 for n = log 2 M and n <; log 2 M, respectively. An extensive simulation study is performed to illustrate the accuracy of the derived results, energy efficiency gains obtained by means of low-resolution quantizers, performance comparison of phase modulated systems with independent in-phase and quadrature channel quantization and robustness of the derived results under channel estimation errors.","['Quantization (signal)', 'Detectors', 'MIMO communication', 'Fading channels', 'Phase shift keying', 'Wireless communication', 'Error probability']","['Low-resolution ADCs', 'maximum likelihood detectors', 'symbol error probability', 'diversity order']"
"The mapping of a virtual network service onto a physical network infrastructure is a challenging task due to the joint allocation of virtual resources across nodes and links, the diverse technical requirements of end-users, the coordination between multiple host domains, and others. This issue is exacerbated further by the extension of virtualization to the next-generation radio access network (NG-RAN) architecture and the provisioning of radio access network (RAN) slicing. To that end, this article focuses on the mapping problem of the virtual network functions (VNFs), as well as their internal and external virtual links (VLs), of a RAN slice subnet onto intelligent points of presence (I-PoPs) and transport networks in the NG-RAN architecture. In this context, in contrast to the majority of the state-of-the-art proposals, which frequently fail to achieve performance objectives and neglect resource allocation constraints, this article introduces automation and intelligence at an architectural level to map VNFs and VLs onto their corresponding physical nodes and links, with the goal of achieving superior efficiency in virtual resource utilization while granting the performance of a RAN slice subnet. Benefiting from a top-down approach, the key contributions of this article are: (i) to extend the architectural framework of network slicing towards the NG-RAN architecture and provide a comprehensive overview and critical analysis of the components and functionalities of a RAN slice subnet; (ii) to integrate the Experiential Network Intelligence (ENI) framework into a joint architecture of the network functions virtualization–management and orchestration (NFV–MANO), Third Generation Partnership Project-network slicing management system (3GPP-NSMS), and I-PoPs in order to render automation and intelligence to the management and orchestration aspects of a RAN slice subnet in the NG-RAN architecture; and (iii) to propose a learning-assisted architectural solution for mapping the VNFs, as well as their internal and external VLs, of a RAN slice subnet onto the underlying I-PoPs and transport networks.","['Virtualization', 'Network slicing', 'Resource management', 'Radio access networks', '3GPP', 'Computer architecture', 'Next generation networking']","['5G', 'automation', 'beyond 5G', 'intelligence', 'management and orchestration', 'mapping', 'network slicing', 'NG-RAN architecture', 'RAN slicing', 'resource allocation', 'virtual links', 'virtual network functions', 'virtual resources', 'virtualization']"
"It is envisioned that 6G, unlike its predecessor 5G, will depart from connected machines and connected people to connected intelligence. The main goal of 6G networks is to support massive connectivity for time-sensitive and computation-sensitive services in mission-critical applications. The creation of real-time optimization (RTO) enabled by the fast growing data analytic and machine learning will seize the opportunities for 6G wireless networks to support such immersive services such as virtual reality (VR), augmented reality (AR), mixed reality (MR), and tactile Internet. Recently, with the rapid development of quantum computers, quantum-inspired optimization and machine learning algorithms have been exploited as efficient solutions for future wireless networks. In this article, we provide a comprehensive view on the new concept of quantum-inspired RTO and its application to the optimal resource allocation for 6G wireless networks. Our main contributions are to introduce some of the initial research results and introduce the potentiality of quantum-inspired RTO on some 6G emerging technologies. Not only do we review the fundamental principles; we also explore the challenges and opportunities of this exciting research direction.","['Quantum computing', 'Optimization', '6G mobile communication', 'Computers', 'Quantum mechanics', 'Real-time systems', 'Qubit']","['Quantum communications', 'real-time optimization', 'resource allocation', '6G networks']"
"In this article, we study the application of cooperative non-orthogonal multiple access (NOMA) in power line communication (PLC) where two users exchange information using a two-way relay (TWR). Under the assumption of perfect and imperfect successive interference cancellation (SIC), we analyze key performance metrics for PLC log-normal channels impaired by impulsive noise. In particular, for delay-limited (DL) and delay-tolerant (DT) modes, approximate closed-form expressions are derived for the user outage probability and user ergodic rate. The analytic results are verified via Monte Carlo simulations. Compared with the conventional orthogonal multiple access (OMA) scheme, the simulation results show that our proposed NOMA TWR enhances the user outage probability and the user ergodic rate performance in the low signal-to-noise ratio (SNR) region. However, this gain diminishes as the imperfect SIC error increases and is non-existent beyond a certain level of imperfect SIC. Furthermore, the system performance is maximized by optimizing the power allocation coefficients where the system throughput and ergodic sum-rate of the NOMA TWR network in DL and DT operation, respectively, are shown to approach a ceiling in the presence of imperfect SIC. The energy efficiency (EE) of the proposed system is also analyzed for DL and DT modes.","['NOMA', 'Silicon carbide', 'Relays', 'Probability', 'Power system reliability', 'Throughput', 'Resource management']","['Non-orthogonal multiple access (NOMA)', 'two-way relaying', 'power line communication (PLC)', 'successive interference cancellation (SIC)', 'log-normal distribution', 'impulsive noise']"
"The proliferation of ubiquitous computing applications created a multi-dimensional optimization problem that includes several conflicting variables such as spectral efficiency, complexity, power consumption, delay, and error probability. To relax the problem and provide efficient solutions, it was necessary to augment the currently overutilized radio spectrum with new frequency bands such as the optical spectrum, which can be used to off-load some of the traffic of certain applications. Therefore, this paper presents an efficient system design that uses amplitude-coherent (AC) detection to reduce the complexity of optical wireless communication systems (OWC), improve its reliability and spectral efficiency. More specifically, we use amplitude shift keying with orthogonal frequency division multiplexing (OFDM) at the transmitter, and AC detection at the receiver. The complexity reduction is achieved by using a low complexity detector, channel estimator, and peak-to-average power ratio (PAPR) reduction scheme. The spectral efficiency is achieved by using real data symbols with discrete cosine transform (DCT), which requires a subcarrier spacing that is 50% of the discrete Fourier transform (DFT), and does not require Hermitian symmetry to generate real-valued OFDM signals. Moreover, the derived channel estimator is blind, and the PAPR reduction scheme does not require a feedback overhead between the transmitter and receiver.","['Peak to average power ratio', 'Optical transmitters', 'Detectors', 'Complexity theory', 'Modulation', 'Receivers']","['Optical communications', 'visible light communications', 'free space optics', 'OWC', 'VLC', 'FSO', 'DCT-OFDM', 'PAPR', 'amplitude-coherent detection', 'semi-coherent detection']"
"In the study, the coexistence of different waveform structures on the same resource element is studied under the theory of non-orthogonal multiple access (NOMA). This study introduces a paradigm-shift on NOMA towards the application-centric waveform coexistence. Throughout this article, the coexistence of different waveforms is explained with two specific use cases, which are power-balanced NOMA and joint radar-sensing and communication with NOMA. For the first use case, block error rate (BLER) performance in the power-balanced regime for two user is improved compared to conventional NOMA transmission with the same waveform. For the joint radar-sensing and communication aspect, the superiority of proposed NOMA scheme over orthogonal frequency division multiplexing (OFDM) joint radar-communication (JRC) scheme is demonstrated regarding radar ambiguity, channel estimation mean-square error (MSE) and bit-error rate (BER) performances. In addition, some of the previous works in the literature are reviewed regarding waveform coexistence in a non-orthogonal manner. However, the concept is not limited to these use cases. With the rapid development of wireless technology, next-generation wireless systems are proposed to be flexible and hybrid, having different kinds of capabilities such as sensing, security, intelligence, control, and computing. Therefore, the concept of different waveforms' coexistence to meet these concerns are becoming impressive for researchers.","['NOMA', 'OFDM', 'Silicon carbide', 'Downlink', 'Sensors', 'Wireless sensor networks', 'Uplink']","['FMCW', 'joint radar-sensing and communication', 'OFDM', 'OFDM-IM', 'waveform coexistence', 'waveform-domain NOMA']"
"Visible light communication (VLC) systems are promising candidates for future indoor access and peer-to-peer networks. The performance of these systems, however, is vulnerable to line of sight (LOS) link blockage due to objects inside the room. Considering pedestrians as the most common VLC links blocking obstacles, we develop a probabilistic passive pedestrian detection and localization method. Our method takes advantage of the blockage status of VLC LOS links between the user equipment (UE) and transceivers on the ceiling to passively detect a single pedestrian, modeled as a cylinder with a random radius. The VLC network gathers the blockage status and computes the geometry of the LOS link graph through a cooperative scheme between VLC device-equipped users inside the room. We also develop a mathematical framework to obtain an optimum solution for estimating the location and size of the object and conclude with a sub-optimum estimation by simplifying the problem to a quadratic programming approach. Simulation results show that using a5×5grid of transceivers on the ceiling and as few as eight UEs, the root-mean-squared error in estimating the center and radius of the object can be less than 5 cm and 3 cm, respectively.","['Transceivers', 'Shadow mapping', 'Optical sensors', 'Hardware', 'Robot sensing systems', 'Visible light communication']","['Scene-awareness', 'obstacle detection', 'visible light communications', 'shadowing', 'device free localization (DFL)', 'passive visible light positioning (VLP)', 'crowdsourcing']"
"The Release 16 completion unlocks the road to an exciting phase pertain to the sixth generation (6G) era. Meanwhile, to sustain far-reaching applications with unprecedented challenges in terms of latency and reliability, much interest is already getting intensified toward physical layer specifications of 6G. In support of this vision, this work exhibits the forward-looking perception of full-duplex (FD) cooperative relaying in support of upcoming generations and adopts as a mean concern the critical contribution of hybrid automatic repeat request (HARQ) mechanism to ultra-reliable and low-latency communication (URLLC). Indeed, the HARQ roundtrip time (RTT) is known to include basic physical delays that may cause the HARQ abandonment for the 1 ms latency use case of URLLC. Taking up these challenges, this article proposes a hybrid FD amplify-and-forward (AF)-selective decode-and-forward (SDF) relay-based system for URLLC. Over this build system, two HARQ procedures within which the HARQ RTT is shortened, are suggested to face latency and reliability issues, namely, the proposed and the enhanced HARQ procedures. We develop then an analytical framework of this relay based HARQ system within its different procedures. Finally, using Monte-Carlo simulations, we confirm the theoretical results and compare the proposed relay-assisted HARQ procedures to the source-to-destination (S2D) HARQ-based system where no relay assists the communication between the source and the destination.","['Relays', 'Reliability', 'Ultra reliable low latency communication', 'Delays', 'Throughput', '3GPP', 'Probability']","['Cooperative relay communication', 'sixth generation', 'hybrid automatic repeat request', 'roundtrip time', 'low latency communication', 'outage probability']"
"Hybrid satellite-terrestrial networks (HSTNs) are considered to be a promising solution in dealing with coverage and mobility challenges encountered in 5 th generation (5G) networks that employ novel multiple access and connectivity schemes. In this respect, non-orthogonal multiple access (NOMA) as well as network coding (NC) schemes have attracted significant attention due to their performance gains which not only improve the quality of wireless transmission but also effectively exploit the available spectrum. In this paper, a combined NOMA-NC (NNC) scheme is presented and integrated into an HSTN consisting of a low earth orbit (LEO) satellite belonging to an LEO constellation, a terrestrial base station (BS), and multiple terrestrial mobile terminals (MTs). The proposed scheme, termed HST-NNC (Hybrid satellite terrestrial-NNC), allows pairs of users to be simultaneously served through NOMA via the terrestrial BS link and the satellite link. Furthermore, the satellite employs random linear network coding (RLNC), within the general framework of systematic network coding (SNC), to improve the reception of the MTs when errors occur. The proposed HST-NNC, as compared to standalone NOMA, does not require additional channel state information (CSI) overheads because the satellite needs only the indices of user pairs to perform RLNC. Performance comparisons of HST-NNC with conventional orthogonal multiple access (OMA) and NOMA optimal user pairing schemes have shown that significant sum rate and BER gains can be obtained under various operating system parameters, such as varying number of MTs and different channel conditions.","['NOMA', 'Satellites', 'Satellite broadcasting', 'Relays', 'Low earth orbit satellites', 'Artificial neural networks', '5G mobile communication']","['BER', 'hybrid satellite-terrestrial', 'network coding', 'NOMA', 'sum rate']"
"This paper provides a comprehensive study of the harmonics generated by a frequency shifted backscatter communication system. The suppression and the manipulability of different harmonics are of importance to avoid detrimental inter-user interference when a number of backscattering nodes (and perhaps also other active wireless users) operate simultaneously in a network. In this paper the harmonics generated by a widely adopted open-short backscatter tag architecture is firstly presented. Then the ideal backscatter system which generates no unwanted harmonics is discussed, which inspires various harmonic suppression strategies. In particular, practical constraints of the backscatter tag hardware capabilities are applied, e.g., the number of discrete reflection coefficients that can be synthesized, and the dimension of the reflection coefficients (real-valued or complex-valued). Furthermore, the dual-transistor based IQ backscatter modulator is found useful to suppress all mirror harmonics and any specified higher order harmonics. The applicability of these proposed harmonic suppression approaches are demonstrated by an exemplar backscatter network consisting of multiple nodes performing binary frequency shifted keying (2FSK) modulated backscatter communications simultaneously.","['Backscatter', 'Harmonic analysis', 'Frequency shift keying', 'Reflection coefficient', 'Harmonics suppression']","['Backscatter communications', 'harmonic suppression', 'reflection coefficient', 'spectrum sharing']"
"Multi-access Edge Computing (MEC) is a novel edge computing paradigm that moves cloudbased processing and storage capabilities closer to mobile users by implementing server resources in the access nodes. MEC helps fulfill the stringent requirements of 5G and beyond networks to offer anytimeanywhere connectivity for many devices with ultra-low delay and huge bandwidths. Information-Centric Networking (ICN) is another prominent network technology that builds on a content-centric network architecture to overcome host-centric routing/operation shortcomings and to realize efficient pervasive and ubiquitous networking. It is envisaged to be employed in Future Internet including Beyond 5G (B5G) networks. The consolidation of ICN with MEC technology offers new opportunities to realize that vision and serve advanced use cases. However, various integration challenges are yet to be addressed to enable the wide-scale co-deployment of ICN with MEC in future networks. In this paper, we discuss and elaborate on ICN MEC integration to provide a comprehensive survey with a forward-looking perspective for B5G networks. In that regard, we deduce lessons learned from related works (for both 5G and B5G networks). We present ongoing standardization activities to highlight practical implications of such efforts. Moreover, we render key B5G use cases and highlight the role for ICN MEC integration for addressing their requirements. Finally, we layout research challenges and identify potential research directions. For this last contribution, we also provide a mapping of the latter to ICN integration challenges and use cases.","['5G mobile communication', 'Internet of Things', 'Standardization', 'Information-centric networking', 'Computer architecture', 'Cellular networks', 'Cloud computing']","['Multi-access edge computing (MEC)', 'information-centric networking (ICN)', 'Beyond 5G (B5G)/6G networks', 'B5G/6G use cases', 'network softwarization', 'Internet of Things (IoT)', 'standardization', 'edge intelligence', 'blockchain', 'B5G/6G security']"
"The commercial drone market has substantially grown over the past few years. While providing numerous advantages in various fields and applications, drones also provide ample opportunities for misuse by irresponsible hobbyists or malevolent actors. The increasing number of safety/security incidents in which drones are involved has motivated researchers to find new and ingenious ways to detect, locate and counter this type of vehicles. In this paper, we propose a new method to detect frequency hopping spread spectrum - Gaussian frequency-shift keying (FHSS-GFSK) drone communication signals, in a non-cooperative scenario, where no prior information about the signals of interest is available. The system is designed to detect and retrieve data bit sequences through a compressive sampling approach, which includes the extraction of the reduced spectral information and a soft detection algorithm. The performance of the proposed approach is assessed in terms of bit error rate and compared with that of a Viterbi detector and a neural network-based detector. The effectiveness of the method described in the paper highlights the fact that current UAV communications are not infallible and present real security issues.","['Drones', 'Frequency control', 'Security', 'Spread spectrum communication', 'Viterbi algorithm', 'Detectors', 'Modulation']","['Counter-drone measures', 'FHSS-GFSK communication signals', 'non-cooperative data detection', 'compressive sampling', 'security of UAV communications']"
"Full Duplex (FD) radio has emerged as a promising solution to increase the data rates by up to a factor of two via simultaneous transmission and reception in the same frequency band. This paper studies a novel hybrid beamforming (HYBF) design to maximize the weighted sum-rate (WSR) in a single-cell millimeter wave (mmWave) massive multiple-input-multiple-output (mMIMO) FD system. Motivated by practical considerations, we assume that the multi-antenna users and hybrid FD base station (BS) suffer from the limited dynamic range (LDR) noise due to non-ideal hardware and an impairment aware HYBF approach is adopted by integrating the traditional LDR noise model in the mmWave band. In contrast to the conventional HYBF schemes, our design also considers the joint sum-power and the practical per-antenna power constraints. A novel interference, self-interference (SI) and LDR noise aware optimal power allocation scheme for the uplink (UL) users and FD BS is also presented to satisfy the joint constraints. The maximum achievable gain of a multi-user (MU) mmWave FD system over a fully digital half duplex (HD) system with different LDR noise levels and numbers of the radio-frequency (RF) chains is investigated. Simulation results show that our design outperforms the HD system with only a few RF chains at any LDR noise level. The advantage of having amplitude control at the analog stage is also examined, and additional gain for the mmWave FD system becomes evident when the number of RF chains at the hybrid FD BS is small.","['Radio frequency', 'Array signal processing', 'Noise level', 'Transceivers', 'Hardware', 'Dynamic range', 'Uplink']","['Millimeter wave', 'full duplex', 'hybrid beamforming', 'limited dynamic range', 'minorization-maximization']"
"The heterogeneity of optical wireless networks (OWNs) has expanded over decades in terms of services, node densities, mobility requirements, bandwidth needs, responsiveness, and physical device profile. Consequently, incorporating dynamic reconfigurability into future OWNs can satisfy their diverse realm of objectives. Towards this end, we provide our vision for the potential gains of integrating tunable optical elements in OWNs reflected in energy consumption reduction, coverage customization, data transfer rate boosting, highly accommodating multiple access schemes, enhancing physical layer security, supporting simultaneous services, and interference reduction. Finally, we speculate a roadmap for future research directions in tunable optics aided OWNs and the associated challenges with their deployment and operation.","['Optical transmitters', 'Wireless communication', 'Optical sensors', 'Optical receivers', 'Optical refraction', 'Optical fiber communication', 'Optical diffraction']","['Optical wireless communications (OWC)', 'smart optics', 'reconfigurable intelligent surfaces', 'intelligent reflecting surfaces']"
"While the unmanned aerial vehicles (UAVs) swarm travels under a dynamic environment, the cluster head (CH) switching is unavoidable due to the mitigation of mobility, quality of service, and energy consumption. If an attacker becomes the new CH, the entire swarm will be controlled and the sensitive data will be leaked. Unlike the other mobile networks with constant network connectivity, the authentication in the UAV swarm suffers from intermittent connection with the ground station under a hostile environment or spectrum constraint condition. Hence, this paper proposes a novel CH safeguarding mechanism enabled by edge intelligence utilizing a situational-aware authentication scheme. This low-latency mechanism provides extra security at the CH selection and switching without cloud server support. By adopting the unique cross-layer attributes, the system security is significantly improved based on the extracted multi-dimensional information. The Linear Discriminant Analysis (LDA) algorithm fuses the authentication decision accurately by projecting the high dimensional estimations into a low dimensional space for maximum separability by only keeping the necessary attributes. A situation-aware cross-layer attribute selection algorithm is developed to select a minimum number of attributes so that the time required for attribute estimation and computation overhead of authentication can be reduced. The simulation results demonstrate that our scheme performs better under a dynamic environment compared with the physical layer authentication scheme and some existing state-of-the-art authentication techniques.","['Authentication', 'Ad hoc networks', 'Unmanned aerial vehicles', 'Mobile computing', 'Switches', 'Security', 'Physical layer']","['Cross layer authentication', 'edge intelligence', 'linear discriminant analysis', 'situation awareness', 'unmanned aerial vehicles (UAVs)']"
"Due to the greatly increased bandwidth of 5G networks compared with that of 4G networks, the power consumption brought by baseband signal processing of 5G networks is much higher, which inevitably raises the operation expenditures. Cloud Radio Access Network (CRAN) is widely adopted in 5G networks, which splits the traditional base stations into Remote Radio Heads (RRHs) and Baseband Units (BBUs), which are equipped with computing resource for baseband signal processing. The number of required BBUs varies due to the fluctuation of wireless traffic of RRHs. Hence, fixed computing resource allocation might waste power. This paper investigates energy-efficient dynamic computing resource allocation in CRAN by predicting the wireless traffic of RRHs and allocating computing resource based on the prediction results aiming at using fewest BBUs to minimize power consumption. For wireless traffic prediction, a novel method based on two-dimensional CNN LSTM model with temporal aggregation is proposed. By treating the wireless traffic data as images, this model could extract spatial correlation from these data to improve accuracy. Moreover, the problem of dynamic computing resource allocation in CRAN is formulated as an offline four-constraint bin packing problem, considering both uplink and downlink baseband signal processing capacities of BBUs and Common Public Radio Interface (CPRI) bandwidths. For solving this problem, a Multi-start Simulated Annealing (MSA) algorithm is proposed. Simulation results demonstrate that the proposed method for wireless traffic prediction could outperform the state-of-the-art deep learning models. In addition, the proposed MSA algorithm could achieve lower power consumption than the state-of-the-art heuristic algorithms.","['Wireless communication', 'Predictive models', 'Baseband', 'Power demand', 'Deep learning', 'Signal processing', 'Resource management']","['Computing resource allocation', 'wireless traffic prediction', 'CRAN', 'deep learning', 'twodimensional CNN LSTM', 'multi-start simulated annealing']"
"The damage to cellular towers during natural and man-made disasters can disturb the communication services for cellular users. One solution to the problem is using unmanned aerial vehicles to augment the desired communication network. The paper demonstrates the design of a UAV-Assisted Imitation Learning (UnVAIL) communication system that relays the cellular users’ information to a neighbor base station. Since the user equipment (UEs) are equipped with buffers with limited capacity to hold packets, UnVAIL alternates between different UEs to reduce the chance of buffer overflow, positions itself optimally close to the selected UE to reduce service time, and uncovers a network pathway by acting as a relay node. UnVAIL utilizes Imitation Learning (IL) as a data-driven behavioral cloning approach to accomplish an optimal scheduling solution. Results demonstrate that UnVAIL performs similar to a human expert knowledge-based planning in communication timeliness, position accuracy, and energy consumption with an accuracy of 97.52% when evaluated on a developed simulator to train the UAV.","['Optimization', 'Relays', 'Cloning', 'Base stations', 'Task analysis', 'Reinforcement learning', 'Quality of service']","['UAV-assisted communication', 'behavioral cloning', 'disaster communication', 'imitation learning', 'packet delivery']"
"Next-generation wireless communications systems are anticipated to utilize the vast amount of available spectrum in the millimeter-wave and sub-terahertz bands above 100 G Hz to meet the ever-increasing demand for higher data rates. However, the analog-to-digital converter (ADC) power consumption is expected to be a major bottleneck if conventional system designs are employed at these frequencies. Instead, shifting the ADC resolution from the amplitude domain to the time domain by employing 1-bit quantization and temporal oversampling w.r.t. the Nyquist rate is expected to be more energy-efficient. Hence, we consider a system employing 1-bit quantization and temporal oversampling at the receiver, which operates on a wideband line-of-sight channel. We present a practical transceiver design for a zero-crossing modulation waveform, which combines faster-than-Nyquist signaling and runlength-limited (RLL) transmit sequences. To this aim, we derive four fixed-length finite-state machine RLL encoders enabling efficient transmit signal construction and soft-demapping at the receiver. Moreover, we propose a soft-output equalizer, which approximates maximum a posteriori RLL symbol detection. We evaluate the system performance in terms of peak-to-average-power ratio, coded block error rate, and a lower bound on the spectral efficiency (SE) w.r.t. a fractional power containment bandwidth. Our numerical results show that SEs of up to 4 bit/s/Hz are achievable with the presented transceiver design.","['Quantization (signal)', 'Transceivers', 'Receivers', 'Modulation', 'Power demand', 'Equalizers', 'Entropy']","['1-bit', 'quantization', 'oversampling', 'runlength-limited sequences', 'faster-than-Nyquist signaling', 'equalization']"
"In the next generations of communication systems, resources' scarcity devolves to a more critical point as a consequence of the expected massive number of users to be served. Furthermore, the users' contradicting requirements on quality of service forces the network to behave in a dynamic way in terms of the multiple access orchestration and resource assignment between users. Hence, the linear relation between the number of served users and the required (orthogonal or semi-orthogonal) resources in the orthogonal multiple access (OMA) schemes is no longer sufficient. As promising candidates, Non-orthogonal multiple access (NOMA) schemes and index modulation (IM) techniques are emerging to satisfy the ever-increasing connectivity demands and spectral efficiency. In this article, a novel IM-based NOMA downlink scheme, termed as IM-NOMA, is proposed, where the base station (BS) selects a channel or more to serve each user based on the IM concept and the corresponding power level is allocated based on the NOMA concept. At the users' ends, maximum likelihood and successive interference cancellation (SIC) detection methods are considered and their error rate, outage probability and computational complexity are studied. Simulation results confirm the advantage of the proposed IM-NOMA scheme and that it can outperform the conventional NOMA system.","['NOMA', 'Maximum likelihood detection', 'Interference cancellation', 'Modulation', 'Downlink', 'Indexes', 'Computational complexity']","['NOMA', 'index modulation', 'multiple access', 'OFDMA']"
"The cognitive radio (CR) technique has revealed a novel way of utilizing the precious radio spectrum via allowing unlicensed users to opportunistically access unutilized licensed bands. Using such a technique enables agile and flexible access to the radio spectrum and can resolve the spectrum-scarcity problem and maximize spectrum efficiency. However, two major impediments have been limiting the widespread adoption of cognitive radio technology. The software-defined radio technology, which is the enabling technology for the CR technique, is power-hungry and this raises a major concern for battery-constrained devices such as smart phones and laptops. Secondly, the opportunistic and open nature of the CR can lead to major security concerns about the data being sent and how safe it is. In this paper, we introduce an energy-and-security-aware CR-based communication approach that alleviates the power consumption of the CR technique and enhances its security measures according to the confidentiality level of the data being sent. Furthermore, the proposed approach takes into account user-related factors, such as the user's battery level and user's data type, and network-related factors, such as the number of unutilized bands and vulnerability level and then models the research question as a constrained optimization problem. Considering the time complexity of the optimum solution, we also propose a heuristic solution. We examine the proposed solution against existing solutions, and our obtained results show that the proposed approach can save energy consumption up to 18%, increase user throughput up to 20%, and achieve better spectrum utilization, up to 98%. Our proposed admission approach has the potential to open a new research direction towards safer and greener cognitive radio techniques.","['Security', 'Cognitive radio', 'Power demand', 'Batteries', 'Throughput', 'Optimization', 'Quality of experience']","['Cognitive radio', 'next generation wireless networks', 'green communications', 'cognitive radio security issues', 'software defined radio']"
"Full-duplex communications systems that transmit and receive simultaneously suffer self-interference due to the mixing of the transmitted signal and the weaker received signal at the same node. The problem becomes compounded in Multi-Input Multi-Output (MIMO) systems, where considerable overhead is dedicated to training. In this article, we discuss using blind source separation techniques, namely Independent Component Analysis (ICA) to reduce training overhead in MIMO in-band full-duplex wireless communication systems. Practical limitations are discussed and experimental results that compare ICA to traditional Least Square approaches are presented, showing the superiority of ICA, especially in low SNR regimes.","['Interference cancellation', 'MIMO communication', 'Receiving antennas', 'Channel estimation', 'Training', 'Blind source separation', 'Transmitting antennas']","['In-band full-duplex', 'cocktail party problem', 'self-interference', 'suppression', 'independent component analysis', 'blind source separation', 'MIMO']"
"Factory automation is one of the use cases for 5G-and-beyond mobile networks where strict requirements in terms of latency, availability and reliability are required. In this paper, we investigate the potentials of massive MIMO in delivering those promises for industrial automation. Namely, communications between actuators (ACs) and Access Points (APs) inside an industrial scenario is considered and different transmission modes are compared: joint transmission (JT) where the distributed antennas are used to communicate with each AC, cell-free transmission (CFT) where all the ACs are served by all APs, single AP transmission (SAT) where each AC is served by only one AP, and user-centric transmission (UCT) where each AC is served by a subset of APs. A power control strategy, aimed at maximizing the minimum signal-to-interference plus noise ratio (SINR), is also introduced. Numerical results, shown in terms of downlink SINR and achievable rate, evaluated using the final block length capacity formula (FBLC), demonstrate that the use of distributed antenna setting and of power control bring substantial performance improvements in terms of reliability and latency.","['Reliability', 'Production facilities', 'Antennas', 'Antenna arrays', 'Signal to noise ratio', 'Interference', 'Power control']","['Distributed MIMO', 'cell-free massive MIMO', 'user-centric approach', 'microwave', 'signal-to-interference plus noise ratio', 'power control', 'wireless networks', 'factory automation']"
"Integrated Sensing and Communications (ISAC) technology can jointly design radio sensing and communication functionalities, which enable 6G Ere to have the ability to “see” the physical world rather than communication-only. Benefitting from ISAC, vehicular-to-everything (V2X) networks may efficiently complete high-precision traffic environment perception. Furthermore, with the assistance of flexibly deployed Unmanned Aerial Vehicles (UAVs), the V2X networks overcome the limited sensing range of sensors equipped on a vehicle and guarantee safe driving. This paper proposes an energy-efficient computation offloading strategy for multiple sensor data fusion in UAV Aided V2X Network supported by Integrated Sensing and Communication. Firstly, a vehicle-UAV cooperative perception architecture is proposed to perceive a wide range of traffic environments. Secondly, we introduce a computation offloading strategy jointly considering offloading decisions and dynamic computing resource allocation. Finally, a successive convex approximation (SCA) algorithm transforms a non-convex formulation problem into a tractable convex approximation problem. The simulation results show that the strategy proposed in this paper reduces the UAV energy consumption and data fusion task processing delay.","['Task analysis', 'Data integration', 'Intelligent sensors', 'Energy consumption', 'Computational modeling', 'Autonomous aerial vehicles', 'Vehicle-to-everything']","['Unmanned aerial vehicles', 'vehicular-to-everything', 'mobile computing', 'integrated sensing and communication', 'successive convex approximation']"
"Visible Light Communication (VLC) has attracted significant attention over the past decade. Although numerous research studies have been performed to improve the data rate of VLC links, an important fact has been largely neglected: human bodies that host VLC receivers could block their Line-of-Sight (LOS) downlinks , and thus, degrade the system performance greatly. In this paper, we propose a system that can significantly improve the robustness for VLC networks by avoiding performance degradation due to blockage. A novel user-in-the-loop mechanism is designed in which users (including human bodies and VLC receivers) are guided by the network to rotate themselves to improve the system performance and the individual user experience. Both our simulation and experimental results demonstrate that the proposed user-in-the-loop mechanism can improve the system throughput and user fairness on average by 48% and 14%, respectively. For individual users, the average gain in throughput can reach up to 135%. Furthermore, to make the system more practical, two lightweight heuristics are designed and implemented which can achieve similar gains while reducing the computational complexity by 99%.","['Receivers', 'System performance', 'Throughput', 'Light emitting diodes', 'Lighting', 'Azimuth', 'Robustness']","['Blockage', 'robust', 'user-in-the-loop', 'user rotation', 'visible light communication.']"
"End-users and service providers have recently favored lower latency services. Edge computing has improved user quality of service (QoS) guarantees through the reduced geographical distance, decreased use of the network backbone, and flexible placement of the container hosting edge devices. The under-utilized isolated edge computing idling nodes are significant for service providers, especially for Internet of Things (IoT) applications. However, the nodes’ minimal maintenance remains a hindrance due to related increased failures. Orchestrating over the edge alongside core environments allows tolerant services and more demanding ones to coexist without impacting the user experience. Therefore, the orchestrator’s second priority is achieving and maintaining the QoS through optimal recovery method selection by either migrating the live containers or re-instantiating them. This paper proposes an Optimal Container Migration/Re-Instantiation (OC-MRI) model to optimize the orchestration methods focusing on downtime, container dependencies, and latency requirements. Next, we introduce a real-time heuristic-based solution, Edge Computing-enabled Container Migration/Re-Instantiation (EC2-MRI). Both models are bench-marked alongside staple greedy approaches. Simulation results showcase the lowest latencies and downtime with the OC-MRI model. Furthermore, the EC2-MRI model shows comparable results to the optimal model with minimal lag.","['Containers', 'Computational modeling', 'Optimization', 'Edge computing', 'Cloud computing', 'Delays', 'Quality of service']","['hybrid computing', 'integer programming', 'migration/re-instantiation']"
"We investigate the uplink non-orthogonal multiple access (NOMA)-based Internet-of-Things (IoT) networks, where each IoT device (station: STA) equipped with a single antenna exploits the constellation-rotated space-time line code (CR-STLC) to send signals to an access point (AP) equipped with two receive antennas. The AP decodes the signals transmitted from the STAs with the optimal joint maximum-likelihood (JML) detector. As a main result, we mathematically analyze both the bit-error-rate (BER) performance and the spatial diversity order of each STA in the two-user uplink CR-STLC NOMA system. In particular, it is shown that the two-user uplink CR-STLC NOMA system achieves the optimal diversity order regardless of the constellation rotation angle in the high signal-to-noise ratio (SNR) regime. We also consider two different rotation angle optimization techniques (dynamic rotation & fixed rotation) to improve the BER performance in the practical SNR regime, and found that the fixed rotation approach is simple but yields almost the same performance as the dynamic approach. Finally, we show that the uplink CR-STLC NOMA system achieves the spatial diversity order of 1 even when more than two STAs send packets simultaneously. It is worth noting that all STAs obtain an improved spatial diversity gain compared to that without constellation rotation.","['Performance evaluation', 'NOMA', 'Maximum likelihood detection', 'Codes', 'Upper bound', 'Spatial diversity', 'Simulation']","['constellation rotation', 'Internet-of-Things', 'joint maximum-likelihood detection', 'non-orthogonal multiple access', 'space-time line codes', 'spatial diversity', 'uplink networks']"
"In this paper, we present a bit error rate (BER) analysis of a multicarrier spread spectrum (MC-SS) system that accounts for several non-ideal factors that exist in actual systems; specifically, channel estimation error, correlated subcarriers, and frequency selectivity over each subcarrier frequency band. The particular MC-SS system that is addressed makes use of filter banks to keep subcarrier bands non-overlapping. In this analysis, the time varying multipath channel is modeled using the wide-sense stationary uncorrelated scattering (WSSUS) channel model, and the channel is estimated using a pilot symbol assisted modulation (PSAM) method. Numerical results that demonstrate the accuracy of the developed theoretical results when applied to design transceivers for high frequency (HF) skywave communications show the usefulness of the developed theory in a practical case study.","['Channel estimation', 'OFDM', 'Channel models', 'Fading channels', 'Bit error rate', 'Receivers', 'Multipath channels']","['Spread spectrum communication', 'multicarrier', 'bit error rate analysis', 'doubly dispersive channels']"
"In this paper, we examine the error performance of backscatter communication in the presence of ambient interference, where the backscatter device acts as a relay. Specifically, the performance comparison of amplify-and-forward (AF) and decode-and-forward (DF) backscatter relaying is considered for the first time. Considering energy-based detection for on-off keying (OOK) modulation, we derive the statistics of the received signal power, from which the detection thresholds and corresponding bit error rates (BER) are obtained analytically. For the DF scheme, we allow the source node to transmit continuous-wave signals during the relay-to-destination transmission phase to power the backscatter relay. Under a total power budget constraint at the source, we optimize the power allocation for the transmissions in the source-to-relay and relay-to-destination phases. Numerical analysis shows that the DF scheme with optimal power allocation performs similarly compared to the AF scheme, despite the added complexity of the decoding operation. On the other hand, the AF scheme significantly outperforms the DF scheme when the reflection coefficients at the backscatter device do not correspond to perfect OOK. These results provide valuable insights into the design and deployment of backscatter nodes with the goal of improving coverage.","['Backscatter', 'Relays', 'Performance evaluation', 'Interference', 'Bit error rate', 'Complexity theory', 'Decoding']","['Amplify-and-forward', 'backscatter communication', 'bit error rate', 'decode-and-forward', 'energy-based detection', 'outage probability relaying']"
"The exponential growth of Internet connected systems has generated numerous challenges, such as spectrum shortage issues, which require efficient spectrum sharing (SS) solutions. Complicated and dynamic SS systems can be exposed to different potential security and privacy issues, requiring protection mechanisms to be adaptive, reliable, and scalable. Machine learning (ML) based methods have frequently been proposed to address those issues. In this article, we provide a comprehensive survey of the recent development of ML based SS methods, the most critical security issues, and corresponding defense mechanisms. In particular, we elaborate the state-of-the-art methodologies for improving the performance of SS communication systems for various vital aspects, including ML based cognitive radio networks (CRNs), ML based database assisted SS networks, ML based LTE-U networks, ML based ambient backscatter networks, and other ML based SS solutions. We also present security issues from the physical layer and corresponding defending strategies based on ML algorithms, including Primary User Emulation (PUE) attacks, Spectrum Sensing Data Falsification (SSDF) attacks, jamming attacks, eavesdropping attacks, and privacy issues. Finally, extensive discussions on open challenges for ML based SS are also given. This comprehensive review is intended to provide the foundation for and facilitate future studies on exploring the potential of emerging ML for coping with increasingly complex SS and their security problems.","['Security', 'Sensors', 'Interference', 'Synthetic aperture sonar', 'Long Term Evolution', 'Resource management']","['Spectrum sharing', 'machine learning', 'security', 'CRN', 'LTE-U', 'SSDF', 'PUE', 'jamming', 'eavesdropping', 'privacy']"
"A thorough understanding of fundamental limits of wireless-powered unmanned aerial vehicle (UAV) relay networks in millimeter waves is still missing. We narrow this gap by investigating the outage performance of a UAV-assisted wireless network over fluctuating two-ray (FTR) channels. The FTR fading model is particularly appealing since well characterizes the wireless propagation in a wide range of frequencies, including those in millimeter waves. The proposed setup consists of a source-destination pair communicating with the assistance of a UAV, which is a wireless-powered relay station operating in full-duplex mode under the amplify-and-forward protocol. For the wireless energy harvesting at the UAV, wireless power transfer (WPT), simultaneous wireless information and power transfer (SWIPT), and self-recycling energy techniques are employed together. To characterize the system outage probability, we obtain an integral-form expression derived from an approximate analysis and a simple closed-form expression derived from an asymptotic analysis at the high signal-to-noise ratio (SNR) regime. Monte Carlo simulations are provided to validate the correctness of our theoretical results and provide insights on the network performance in terms of key system parameters. Interestingly, obtained results show that the FTR fading parameters corresponding to the first hop and second hop play no role on the system outage performance at high SNR. Instead, it is mainly governed by the effect of the residual self-interference at the UAV, leading to outage floors.","['Wireless communication', 'Protocols', 'Unmanned aerial vehicles', 'Signal to noise ratio', 'Rician channels', 'RF signals', 'Probability']","['Fluctuating two-ray', 'full duplex', 'mmWave communications', 'outage probability', 'simultaneous wireless information and power transfer', 'unmanned aerial vehicle']"
"This paper focuses on providing an analytical framework for the quantification and evaluation of the pointing error for a general case at high-frequency millimeter wave (mmWave) and terahertz (THz) communication links. For this aim, we first derive the probability density function (PDF) and cumulative distribution functions (CDF) of the pointing error between an unstable transmitter (Tx) and receiver (Rx), that have different antenna patterns and for which the vibrations are not similar in the Yaw and Pitch directions. The special case where the Tx and Rx are both equipped with uniform linear array antenna is also investigated. In addition, usingα−μdistribution, which is a valid model for small-scale fading of mmWave/THz links, the end-to-end PDF and CDF of the considered channel is derived for all the considered cases. Finally, by employing Monte-Carlo simulations, the accuracy of the analytical expressions is verified and the performance of the system is studied.","['Millimeter wave communication', 'Directive antennas', 'Vibrations', 'Receiving antennas', 'Antennas', 'Fading channels', 'Wireless communication']","['Antenna pattern', 'antenna misalignment', 'backhaul links', 'pointing errors', 'mmWave', 'THz systems']"
"In this article, we study the performance of an uplink non-orthogonal multiple access (NOMA) network under statistical quality of service (QoS) delay constraints, captured through each user's effective capacity (EC). We first propose novel closed-form expressions for the EC in a two-user NOMA network and show that in the high signal-to-noise ratio (SNR) region, the “strong” NOMA user, referred to as U 2 , has a limited EC, assuming the same delay constraint as the “weak” user, referred to as U1. We demonstrate that for the weak user U1, OMA and NOMA have comparable performance at low transmit SNRs, while NOMA outperforms OMA in terms of EC at high SNRs. On the other hand, for the strong user U 2 , NOMA achieves higher EC than OMA at small SNRs, while OMA becomes more beneficial at high SNRs. Furthermore, we show that at high transmit SNRs, irrespective of whether the application is delay tolerant, or not, the performance gains of NOMA over OMA for U 1 , and OMA over NOMA for U 2 remain unchanged. When the delay QoS of one user is fixed, the performance gap between NOMA and OMA in terms of total EC increases with decreasing statistical delay QoS constraints for the other user. Next, by introducing pairing, we show that NOMA with user-pairing outperforms OMA, in terms of total uplink EC. The best pairing strategies are given in the cases of four and six users NOMA, raising once again the importance of power allocation in the optimization of NOMA's performance.","['NOMA', 'Delays', 'Quality of service', 'Uplink', 'Resource management', 'Signal to noise ratio', 'Downlink']","['Beyond 5G (B5G)', 'effective capacity', 'low latency', 'non-orthogonal multiple access (NOMA)', 'quality of service (QoS)', 'user-pairing']"
"We investigate the problem of sum-rate maximization of a secondary link of full-duplex generalized frequency division multiplexing (GFDM) radios operating over a spectrum hole, which is surrounded by two active primary adjacent channels. Thus, the secondary transmissions must be below an adjacent channel interference (ACI) threshold. In-band distortions and several interference terms are also caused by phase noise, in-phase (I) and quadrature (Q) imbalance, carrier frequency offset (CFO) and the nonlinear power amplifier (PA). Analog domain and digital domain self-interference (SI) cancellation is also considered. We study the two cases of two independent oscillators for local transmitter and receiver and one common shared oscillator. We derive the powers of residual SI, desired signal, interference signal and noise, signal-to-interference-plus noise ratio (SINR) and the power spectral density (PSD) of the transmit signal. By using successive convex approximations, we solve the sum-rate maximization problem. Finally, we show that in full-duplex radios under certain RF impairments, GFDM may double the sum rate compared to that of orthogonal frequency division multiplexing (OFDM).","['Interference', 'OFDM', 'Radio frequency', 'Phase noise', 'Receivers', 'Cognitive radio']","['Full-duplex radios', 'generalized frequency division multiplexing (GFDM)', 'cognitive radio', 'spectrum hole', 'radio frequency (RF) impairments', 'rate optimization']"
"In this article, we propose a hybrid beamforming design for multiuser mmWave massive MIMO systems. We adopt a two-stage approach for designing the analog and digital beamforming separately. The analog beamforming design is based on a constrained low-rank channel decomposition and aims to simultaneously harvest the array gain and reduce the intra- and inter-user interference. The digital beamforming design is conducted by using the regularized channel diagonalization method, which provides a better trade-off between multiuser interference suppression and transmit diversity, thus attaining a better performance in low-SNR scenarios or when communicating to many users or through many data streams. We validate the effectiveness of the proposed design through numerical simulations, which have shown that our design outperforms several other hybrid beamforming designs in the literature.","['Array signal processing', 'Radio frequency', 'Massive MIMO', 'Interference', 'Baseband', 'Channel models', 'Phase shifters']","['Constrained low-rank decomposition', 'hybrid beamforming', 'multiuser MIMO', 'millimeter wave', 'regularized channel diagonalization']"
"In this study, we address the problem of chaotic synchronization over a noisy channel by introducing a novel Deep Chaos Synchronization (DCS) system using a Convolutional Neural Network (CNN). Conventional Deep Learning (DL) based communication strategies are extremely powerful but training on large data sets is usually a difficult and time-consuming procedure. To tackle this challenge, DCS does not require prior information or large data sets. In addition, we provide a novel Recurrent Neural Network (RNN)-based chaotic synchronization system for comparative analysis. The results show that the proposed DCS architecture is competitive with RNN-based synchronization in terms of robustness against noise, convergence, and training. Hence, with these features, the DCS scheme will open the door for a new class of modulator schemes and meet the robustness against noise, convergence, and training requirements of the Ultra Reliable Low Latency Communications (URLLC) and Industrial Internet of Things (IIoT).","['Chaotic communication', 'Synchronization', 'Receivers', 'Recurrent neural networks', 'Mathematical model', 'Electronics packaging']","['synchronization', 'DCS', 'CNN', 'Lorenz system', 'RNN']"
"The ultra-dense deployment of interconnected satellites will characterize future low Earth orbit (LEO) mega-constellations. Exploiting this towards a more efficient satellite network (SatNet), this paper proposes a novel LEO SatNet architecture based on distributed massive multiple-input multipleoutput (DM-MIMO) technology allowing ground user terminals to be connected to a cluster of satellites. To this end, we investigate various aspects of DM-MIMO-based satellite network design, the benefits of using this architecture, the associated challenges, and the potential solutions. In addition, we propose a distributed joint power allocation and handover management (D-JPAHM) technique that jointly optimizes the power allocation and handover management processes in a cross-layer manner. This framework aims to maximize the network throughput and minimize the handover rate while considering the quality-of-service (QoS) demands of user terminals and the power capabilities of the satellites. Moreover, we devise an artificial intelligence (AI)-based solution to efficiently implement the proposed D-JPAHM framework in a manner suitable for real-time operation and the dynamic SatNet environment. To the best of our knowledge, this is the first work to introduce and study DM-MIMO technology in LEO SatNets. Extensive simulation results reveal the superiority of the proposed architecture and solutions compared to conventional approaches in the literature.","['Low earth orbit satellites', 'Satellites', 'Satellite broadcasting', 'Handover', 'Computer architecture', 'Massive MIMO', 'Cross layer design']","['Satellite communication networks', 'LEO constellations', 'cell-free massive MIMO', 'handover management', 'resource allocation']"
"As massive ground users access wireless networks with limited available spectrum, coexisting users that share the same spectrum band severely interfere with each other. Moreover, communication pairs without direct communication links may suffer from severe outage issues. In this paper, unmanned aerial vehicle (UAV) relaying-assisted interference coordination is proposed to enhance data transmission reliability by 1) relaying transmission, and 2) alleviating mutual interference among ground users. To efficiently utilize confined UAV energy such that the UAV relaying service life can be significantly prolonged, the full-duplex (FD) technique is exploited. An exact energy efficiency (EE) maximization problem is formulated, which jointly optimizes throughput and UAV energy consumption. Different from the traditional offline method, online power management and trajectory designs are performed under the information causality and rate constraints. Considering that power management and trajectory are coupled, we design a block coordinate descent method to efficiently solve the optimization problem. Numerical results demonstrate that the proposed scheme outperforms existing schemes, and the EE is about 30% and 46% higher than those of the genetic algorithm and half-duplex scheme, respectively. Moreover, impacts of different positions and the maximum power of ground users on the EE have been demonstrated.","['Trajectory', 'Autonomous aerial vehicles', 'Interference', 'Relays', 'Wireless networks', 'Throughput', 'Optimization']","['Energy efficiency (EE)', 'full-duplex relaying', 'mutual interference coordination', 'unmanned aerial vehicle (UAV) relay']"
"Network slicing is a promising technology for realizing the vision of supporting a wide range of services with diverse and heterogeneous service requirements. With network slicing, the network is partitioned into multiple individual dedicated networks tailored and customized for specific services. However, this causes extra energy consumption to reserve resources for each slice. On the other hand, minimizing the energy consumption in radio access network (RAN) may result in increasing the energy consumption in the cloud and the fronthaul due to higher required processing and data transport. Therefore, the energy should be evaluated from an end-to-end perspective. In this study, we address the problem of minimizing the end-to-end energy consumption of a network with network slicing by jointly reserving communication and computation resources among slices. First, we propose an end-to-end delay and energy model for each slice. We take into account the delays and energy consumption of the radio site, midhaul/fronthaul transport, and the cloud site in an Ethernet-based cloud RAN (C-RAN). Then, we formulate a non-convex optimization problem to minimize the total energy consumption of the network by jointly allocating transmission bandwidth and processing resources in the digital unit pool of the cloud, respectively, to each slice. The constraints of the optimization problem are the total delay requirement of each slice, the maximum allowable bandwidth at each radio unit, the maximum rate limitation of the Ethernet links, and the total processing limit of the cloud. To solve the problem optimally, we transform it into a convex quadratic programming problem. The simulation results show that end-to-end network slicing can decrease the total energy consumption of the network compared to only RAN slicing. We also investigate the impact of the 5G numerology on the allocated resources to each slice and the total energy consumption. We show that using mixed numerology depending on the slice type, we can interplay between delay and energy consumption for each slice.","['Network slicing', 'Cloud computing', 'Energy consumption', 'Computer architecture', 'Quality of service', 'Delays', 'Bandwidth']","['5G', 'C-RAN', 'end-to-end slicing', 'network slicing', 'network architecture', 'resource allocation']"
"Detection of drones carries critical importance for safely and effectively managing unmanned aerial system traffic in the future. Given the ubiquitous presence of the drones across all kinds of environments in the near future, wide area drone detection and surveillance capability are highly desirable, which require careful planning and design of drone sensing networks. In this paper, we seek to meet this need by using the existing terrestrial radio frequency (RF) networks for passive sensing of drones. To this end we develop an analytical framework that provides the fundamental limits on the network-wide drone detection probability. In particular, we characterize the joint impact of the salient features of the terrestrial RF networks, such as the spatial randomness of the node locations, the directional 3D antenna patterns, and the mixed line of sight/non line of sight (LoS/NLoS) propagation characteristics of the air-to-ground (A2G) channels. Since the strength of the drone signal and the aggregate interference in a sensing network are fundamentally limited by the 3D network geometry and the inherent spatial randomness, we use tools from stochastic geometry to derive the closed-form expressions for the probabilities of detection, false alarm and coverage. This, in turn, demonstrates the impact of the sensor density, beam tilt angle, half power beam width (HPBW) and different degrees of LoS dominance, on the projected detection performance. Our analysis reveals optimal beam tilt angles, and sensor density that maximize the network-wide detection of the drones.","['Drones', 'Sensors', 'Radio frequency', 'Sensor phenomena and characterization', 'Interference', 'Three-dimensional displays', 'Geometry']","['A2G channel', 'beam tilt', 'directional antenna pattern', 'drone detection', 'LoS/NLoS', 'stochastic geometry']"
"Physical layer security (PLS) can be adopted for efficient key generation and sharing in secured wireless systems. The inherent random nature of the wireless channel and the associated channel reciprocity (CR) are the main pillars for realizing PLS techniques. However, for applications that involve air-to-air (A2A) transmission, such as unmanned aerial vehicle (UAV) applications, the channel does not generally have sufficient randomness to enable reliable key generation. Therefore, this work proposes a novel system design to mitigate the channel randomness constraint and enable a high-rate secret key generation process. The proposed system integrates physically unclonable functions (PUFs) and CR to generate and exchange secret keys between two nodes securely. Moreover, an adaptive and controllable artificial fading (AF) level with interleaving is used to mitigate the impact of low randomness variations in the wireless channel. Moreover, we propose a novel bit extraction scheme to reduce the number of overhead bits required to share the intermediate keys. The obtained Monte Carlo simulation results show that the proposed system can operate efficiently even when the channel is nearly flat or time-invariant. Consequently, the time required for generating and sharing a key is significantly shorter than conventional techniques. Furthermore, the results show that a key agreement can be reached at the first trial for moderate and high signal-to-noise ratios (SNRs) substantially faster than other PLS techniques. Adopting the AF into static channels managed to reduce the mismatch ratio between the generated secret sequences and degrade the eavesdropper’s capability to predict the secret keys.","['Wireless communication', 'Security', 'Communication system security', 'Scattering', 'Time-frequency analysis', 'Autonomous aerial vehicles', 'Protocols']","['Physical layer security (PLS)', 'channel reciprocity (CR)', 'physically unclonable function (PUF)', 'secret key generation (SKG)', 'static environments', 'artificial fading (AF)', 'bit extraction (BE)', 'received signal strength (RSS)']"
"Intrusion detection plays a critical role in cyber-security domain since malicious attacks cause irreparable damages to cyber-systems. In this work, we propose the I2SP prototype, which is a novel Information Sharing Platform, able to gather, pre-process, model, and distribute network-traffic information. Within the I2SP prototype we build several challenging deep feature learning models for network-traffic intrusion detection. The learnt representations will be utilized for classifying each new network measurement into its corresponding threat level. We evaluate our prototype's performance by conducting case studies using cyber-security data extracted from the Malware Information Sharing Platform (MISP)-API. To the best of our knowledge, we are the first that combine the MISP-API in order to construct an information sharing mechanism that supports multiple novel deep feature learning architectures for intrusion detection. Experimental results justify that the proposed deep feature learning techniques are able to predict accurately MISP threat-levels.","['Anomaly detection', 'Malware', 'Prototypes', 'Intrusion detection', 'Computer architecture', 'Feature extraction']","['Malware information sharing platform', 'network intrusion detection anomaly detection', 'deep feature learning', 'convolutional neural networks', 'long-short memory neural networks', 'stacked-sparse autoencoders']"
"Ultra-reliable and low latency communications (URLLC) will be the backbone of the upcoming sixth-generation (6G) systems and will facilitate mission-critical scenarios. A design accounting for stringent reliability and latency requirements for URLLC systems poses a challenge for both industry and academia. Recently, unmanned aerial vehicles (UAV) have emerged as a potential candidate to support communications in futuristic wireless systems due to providing favourable channel gains thanks to Line-of-Sight (LoS) communications. However, usage of UAV in cellular infrastructure increases interference in aerial and terrestrial user equipment (UE) limiting the performance gain of UAV-assisted cellular systems. To resolve these issues, we propose low-complexity algorithms for intercell interference coordination (ICIC) using cognitive radio when single and multi-UAVs are deployed in a cellular environment to facilitate URLLC services. Moreover, we model BS-to-UAV (B2U) interference in downlink communication, whereas in uplink we model UAV-to-BS (U2B), UAV-to-UAV (U2U), and UE-to-UAV (UE2U) interference under perfect/imperfect channel state information (CSI). Results demonstrate that the proposed perfect ICIC accounts for fairness among UAV especially in downlink communications compared to conventional ICIC algorithms. Furthermore, in general, the proposed UAV-sensing assisted ICIC and perfect ICIC algorithms yield better performance when compared to conventional ICIC for both uplink and downlink for the single and multi-UAV frameworks.","['Autonomous aerial vehicles', 'Interference', 'Downlink', 'Ultra reliable low latency communication', 'Uplink', 'Intercell interference', 'Delays']","['URLLC', 'multi-UAV', 'cognitive radio', 'intercell interference coordination (ICIC)']"
"Non-orthogonal multiple access (NOMA)-based diamond relaying (NDR) is an efficient approach for combining NOMA and relaying techniques in such a way as to enhance the achievable rate from the source to the destination in the network. This paper examines the problem of joint power allocation among all the transmission phases during the operation of such networks. Based on the Karush-Kuhn-Tucker (KKT) condition and the second-order sufficient condition (SOSC), at least the local optimal solution is derived and analyzed. In addition, a new protocol for NDR networks based on cooperative communications is introduced and the associated joint power-allocation problem is examined. It is shown that this cooperative NDR (C-NDR) network further improves the achievable rate in some typical placements of the relays. Simulation results verify the correctness of the deviation and confirm the effectiveness of the proposed joint power-allocation method for both NDR and C-NDR networks.","['NOMA', 'Resource management', 'Downlink', 'Diamond', 'Relay networks (telecommunications)', 'Protocols']","['Non-orthogonal multiple access (NOMA)', 'relay networks', 'NOMA-based diamond relaying (NDR)', 'cooperative communications', 'Karush-Kuhn-Tucker (KKT) condition', 'second-order sufficient condition (SOSC)', 'joint power allocation', 'achievable rate']"
"In this work, we introduce a machine-learning (ML) based detection attack, where an eavesdropper (Eve) is able to learn the symbol detection function based on precoded pilots. With this ability, an Eve can correctly detect symbols with a high probability. To counteract this attack, we propose a novel symbol-level precoding (SLP) scheme that enhances physical-layer security (PLS) while guaranteeing a constructive interference effect at the intended users. Contrary to conventional SLP schemes, the proposed scheme is robust to the ML-based attack. In particular, the proposed scheme enhances security by designing Eve’s received signal to lie at the boundaries of the detection regions. This distinct design causes Eve’s detection decisions to be based almost purely on noise. The proposed countermeasure is then extended to account for multi-antennas at the Eve and also for multi-level modulation schemes. In the numerical results, we validate both the detection attack and the countermeasures and show that this gain in security can be achieved at the expense of only a small additional power consumption at the transmitter, and more importantly, these benefits are obtained without affecting the performance at the intended user.","['Precoding', 'Security', 'Interference', 'Receivers', 'Machine learning', 'MISO communication', 'Downlink']","['Multi-user interference', 'constructive interference', 'symbol-level precoding', 'physical-layer security', 'machine learning', 'convex optimization', 'MISO', 'bit-error rate']"
"Future integrated terrestrial, aerial, and space networks will involve thousands of low-Earth-orbit (LEO) satellites, which will form a network of mega-constellations. These mega-constellations will play a significant role in providing communication and Internet services anywhere, at any time, and for everything. Due to the large scale and highly dynamic nature of future LEO satellite networks (SatNets), their management will be a complicated process, especially the aspect of mobility management and its two components: location management and handover management. In this article, we present a comprehensive and critical review of the state-of-the-art research in location management for LEO SatNets. First, we give an overview of the Internet Engineering Task Force (IETF) mobility management standards (e.g., Mobile IPv6 and Proxy Mobile IPv6) and discuss the limitations of their location management techniques for future LEO SatNets. We highlight the mobility characteristics of future LEO SatNets and their challenging features, and we describe two unprecedented future location management scenarios. A taxonomy of existing location management solutions for LEO SatNets is also presented with solutions classified according to three approaches. The “Issues to consider” section draws attention to critical points related to each of the reviewed approaches that should be considered in future LEO SatNets location management. To identify the research gaps, the current state of LEO SatNets location management is summarized. Noteworthy future research directions are recommended. The article provides a road map for researchers and industry to shape the future of location management for LEO SatNets.","['Satellites', 'Low earth orbit satellites', 'Handover', 'Routing', 'Manganese', 'Satellite broadcasting', 'IP networks']","['Satellite networks', 'mega-constellation', 'LEO', 'mobility management', 'location management']"
"In-band full-duplex (IBFD) is an attractive solution for increasing the throughput of Power Line Communication (PLC) systems. In IBFD, each network node is allowed to transmit and receive simultaneously in the same frequency band. This paper discusses the importance of defining figures of merit to analyze IBFD performance, which is often forgotten by the literature that addresses IBFD from a pure system level and signal processing perspective. This is because in IBFD hardware-related aspects are of great importance. The focus is then given to the first self-interference (SI) cancellation stage, namely the analog coupling and self-interference cancellation stage (ASICS). Two architectures are considered. A detailed analysis is offered by taking into account circuit-level aspects. A broadband multiple conductor PLC scenario is assumed to enable multiple-input multiple-output (MIMO) IBFD communication. The first considered architecture performs SI subtraction exploiting operational amplifiers. The second proposed architecture exploits the characteristics of a three ports magnetic circuit together with a signal generator to remove the SI. Design, analysis, and hardware realization of these circuits have been done to compare and show the practical feasibility of the ASICS for PLC, well known to be challenged by its line impedance matching problems and severe frequency selective channel characteristics. This paper is the first documented contribution of the IBFD ASICS for 2×2 MIMO broadband PLC physically realized and tested in the field.","['Ports (computers)', 'Operational amplifiers', 'Interference cancellation', 'Transmitters', 'Broadband amplifiers', 'Receivers', 'Hardware']","['In-band-full-duplex', 'MIMO', 'power line communications', 'self-interference cancellation', 'transceiver design']"
"Non-orthogonal multiple access (NOMA) is being widely considered as a potential candidate to enhance the spectrum utilization in beyond fifth-generation (B5G) communications. In this article, we derive closed-form expressions for the ergodic rate and outage probability of a multiple-antenna-assisted NOMA-based cooperative relaying system (CRS-NOMA). We present the performance analysis of the system for two different receive diversity schemes- selection combining (SC) and maximal-ratio combining (MRC), in Nakagami-mfading. We also evaluate the asymptotic behavior of the CRS-NOMA to determine the slope of the ergodic rate and diversity order. Our results show that in contrast to the existing CRS-NOMA systems, the CRS-NOMA with receive diversity outperforms its orthogonal multiple access (OMA) based counterpart even in the low-SNR regime, by achieving higher ergodic rate. Diversity analysis confirms that the CRS-NOMA achieves full diversity order using both SC and MRC schemes, and this diversity order depends on both the shape parameter mand the number of receive antennas. We also discuss the problem of optimal power allocation for the minimization of the outage probability of the system, and subsequently use this optimal value to obtain the ergodic rate. An excellent match is observed between the numerical and the analytical results, confirming the correctness of the derived analytical expressions.","['Diversity reception', 'Receiving antennas', 'Relays', 'Probability', 'Power system reliability', 'NOMA', 'Transmitting antennas']","['Non-orthogonal multiple access', 'cooperative communications', 'relaying', 'Nakagami-m fading', 'ergodic rate', 'outage probability', 'diversity order']"
"Wireless full-duplex (FD) transceivers have become more commonplace in the recent years, enabled by improvements in hardware as well as in signal processing algorithms dedicated to self-interference (SI) mitigation. A key design target for any FD radio is to limit the analog-to-digital converter (ADC) quantization noise affecting the signal of interest (SoI), which results from the difference in dynamic range between the SI and the SoI at the input of the ADC. Indeed, when using conventional ADCs, since the SI power is much larger than that of the SoI, the SI spans most of the ADC's dynamic range and the SoI becomes distorted by a large quantization noise. Reducing the power of the SI before the ADC, for example via analog domain SI cancellation, is so far the only means to mitigate this undesired effect. In this paper we consider the use of so-called modulo ADCs as a new tool to reduce the quantization noise that affects the SoI in the presence of SI. We demonstrate theoretically and numerically that by substituting the conventional ADCs with modulo-ADCs, which fold the analog input signal before analog-to-digital conversion, and by using appropriately designed analog gain control and digital-domain SI cancellation, one can reduce the quantization noise that affects the SoI. As such, the work in this paper provides a new technique for counteracting the detrimental effect of SI in analog-to-digital conversion and, thus, provides a new route to be explored in order to enhance the performance of FD radios.","['Interference cancellation', 'Transceivers', 'Quantization (signal)', 'Dynamic range', 'Receivers', 'Wireless communication', 'Radio transmitters']","['Full-duplex', 'analog-to-digital conversion', 'modulo-ADC', 'unlimited sampling', 'wireless communication']"
"Recently, tag-to-tag (T2T) backscattering technique in a passive RFID system has received broad attention due to its superiority for large-scale network applications. If used to implement a Network of Tags, use of T2T communication allows inherent communication parallelism, thus supporting orders of magnitude larger capacity than centralized RFID reader-based systems. To unleash the potential of T2T communication, turbo backscattering operation enables the implementation of a multi-hop network of tags, which supports larger network coverage of a Network of Tags. However, due to asymmetric communication links and interferences among tags' transmissions in such a T2T backscattering based network, the routing protocol design has become one of the main technical challenges, especially for large-scale networks. Furthermore, the computation time of T2T routing protocols increases exponentially with the number of tags, greatly limiting the practicability of such large-scale backscattering networks. In this paper, we present the design of a Network of Tags model to address these challenges, and we propose novel routing protocols for three distinct types of tags with different hardware capabilities. To address the issue of computational processing time of the routing protocol for large-scale T2T networks, we propose a new scheme with linear time complexity. We evaluate and compare the performance of the proposed protocols, as well as investigate the impact of network parameters on the performance.","['Routing protocols', 'Radiofrequency identification', 'Backscatter', 'Monitoring', 'Spread spectrum communication', 'Time complexity']","['Backscattering communications', 'Internet of Things', 'large-scale networks', 'multi-hop routing protocol', 'network of tags', 'RFID passive tag', 'tag-to-tag communication']"
"Ambient backscatter communication is an emerging and promising low-energy technology for the Internet of Things. In such a system, a tag sends a binary message to a reader by backscattering a radio frequency signal generated by an ambient source. The tag can operate without battery and without generating additional radio waves. However, the tag-to-reader link suffers from the source-to-reader interference. In this paper, we propose a polarization-based reconfigurable antenna in order to improve the robustness of the tag-to-reader link against the source-to-reader direct interference. More precisely, we compare different types of tags' antennas, different tags' encoding schemes, and different detectors at the reader. By using analysis, numerical simulations, and experiments, we show that a polarization-based reconfigurable tag with four polarization directions significantly outperforms a non-reconfigurable tag, and provides almost the same performance as an ideal reconfigurable tag with a large number of reconfigurable polarization patterns.","['Backscatter', 'Dipole antennas', 'Encoding', 'Antenna radiation patterns', 'Interference', 'Detectors']","['Internet of Things', 'ambient backscatter communication', 'compact reconfigurable antennas']"
"This article develops a quantum bacterial foraging optimization (QBFO) algorithm, a quantum intelligence algorithm based on quantum computing and bacterial foraging optimization (BFO), with application in MIMO system optimization designs. In QBFO, a multiqubit is used to represent a bacterium, and a quantum rotation gate is used to mimic chemotaxis. Because the quantum bacterium with multiqubit has the advantage that it can represent a linear superposition of states (binary solutions) in search space probabilistically, the proposed QBFO algorithms shows better performance on solving combinatorial optimization problems than its classical counterpart BFO and Quantum Genetic Algorithm (QGA), especially for parallel non-gradient optimization. A sparse channel estimation scheme based on QBFO with adaptive phase rotation (AQBFO) in 3D MIMO system is proposed, and simulation results show that AQBFO achieved a better performance than existing algorithms including least squares (LS), iteratively reweighted least squares (IRLS), matching pursuit (MP), and orthogonal matching pursuit (OMP). We further improve some critical aspects such as reproduction and dispersal processes of AQBFO, propose an improved IQBFO algorithm, and apply it for interference coordination in 3D multi-cell multi-user MIMO systems, aiming to maximize the spectral efficiency. It considers user fairness and jointly optimizes cell-center and cell-edge user specific antenna downtilts and power to maximize each user’s sum rate. This problem is a combinatorial non-convex optimization problem that cannot be solved by the traditional Karush-Kuhn-Tucker Lagrangian algorithm whereas the IQBFO algorithm solves it effectively.","['Microorganisms', 'Optimization', 'MIMO communication', 'Logic gates', 'Matching pursuit algorithms', 'Three-dimensional displays', 'Quantum computing']","['Quantum bacterial foraging optimization', '0-1 knapsack problem', '3D MIMO', 'sparse channel estimation', 'interference coordination']"
"Laser inter-satellite links (LISLs) between satellites in a free-space optical satellite network (FSOSN) can be divided into two classes: permanent LISLs (PLs) and temporary LISLs (TLs). TLs are not desirable in next-generation FSOSNs (NG-FSOSNs) due to high LISL setup time, but they may become feasible in next-next-generation FSOSNs (NNG-FSOSNs). Using the satellite constellation for Phase I of Starlink, we study the impact of TLs on network latency in an NG-FSOSN (which has only PLs) versus an NNG-FSOSN (which has PLs and TLs) under different long-distance inter-continental data communications scenarios, including Sydney–Sao Paulo, Toronto–Istanbul, Madrid–Tokyo, and New York–Jakarta, and different LISL ranges for satellites, including 659.5 km, 1,319 km, 1,500 km, 1,700 km, 2,500 km, 3,500 km, and 5,016 km. It is observed from the results that TLs provide higher satellite connectivity and thereby higher network connectivity, and they lead to lower average network latency for the NNG-FSOSN compared to the NG-FSOSN in all scenarios at all LISL ranges. In comparison with the NG-FSOSN, the improvement in latency with the NNG-FSOSN is significant at LISL ranges of 1,500 km, 1,700 km, and 2,500 km, where the improvement is 16.83 ms, 23.43 ms, and 18.20 ms, respectively, for the Sydney–Sao Paulo inter-continental connection. For the Toronto–Istanbul, Madrid–Tokyo, and New York–Jakarta inter-continental connections, the improvement is 14.58 ms, 23.35 ms, and 23.52 ms, respectively, at the 1,700 km LISL range.","['Satellites', 'Orbits', 'Artificial neural networks', 'Low earth orbit satellites', 'Optical fiber networks', 'Lasers', 'Satellite constellations']","['Network latency', 'next-generation free-space optical satellite networks', 'next-next-generation free-space optical satellite networks', 'Starlink', 'temporary laser inter-satellite links']"
"Recent advancements in vehicle-to-everything (V2X) communication have notably improved existing transport systems by enabling increased connectivity and driving autonomy levels. The remarkable benefits of V2X connectivity come inadvertently with challenges which involve security vulnerabilities and breaches. Addressing security concerns is essential for seamless and safe operation of mission-critical V2X use cases. This paper surveys current literature on V2X security and provides a systematic and comprehensive review of the most relevant security enhancements to date. An in-depth classification of V2X attacks is first performed according to key security and privacy requirements. Our methodology resumes with a taxonomy of security mechanisms based on their proactive/reactive defensive approach, which helps identify strengths and limitations of state-of-the-art countermeasures for V2X attacks. In addition, this paper delves into the potential of emerging security approaches leveraging artificial intelligence tools to meet security objectives. Promising data-driven solutions tailored to tackle security, privacy and trust issues are thoroughly discussed along with new threat vectors introduced inevitably by these enablers. The lessons learned from the detailed review of existing works are also compiled and highlighted. We conclude this survey with a structured synthesis of open challenges and future research directions to foster contributions in this prominent field.","['Security', 'Vehicle-to-everything', 'Vehicular ad hoc networks', 'Privacy', 'Cryptography', 'Taxonomy', 'Authentication']","['Artificial intelligence', 'attack classification', 'cybersecurity solutions', 'machine learning', 'misbehavior detection', 'privacy preservation', 'proactive/reactive security', 'security threats', 'trust management', 'V2X communication']"
"Wireless IoT networks have seen an unprecedented rise in number of devices, heterogeneity and emerging use cases which led to diverse throughput, reliability and latency (Quality of Service) requirements. Fulfilling these diverse requirements in a rapidly changing and dynamic wireless environment is a complex and challenging task. On top of including new technologies and wireless standards, one solution is to deploy cross-layer Design (CLD) and multiple Radio Access Technologies (Multi-RAT) to develop scalable QoS-aware IoT networks. However, the complexity of such solutions is high as it involves complex inter-layer interactions and dependencies and inter-application dependencies in multi-RAT networks. Moreover, the wireless environment is very dynamic, so having an optimal constellation of parameters is a challenging task. In this paper, we address the possibilities of using Artificial Intelligence (AI) and Machine Learning (ML) to address these high dimensional and dynamic problems. Based on our findings, we have proposed a distributed network management framework employing AI & ML for studying inter-layer dependencies and developing cross-layer design, traffic classification and traffic prediction at the edge devices for reliable QoS in multi-RAT IoT networks. A thorough discussion on future directions and emerging challenges related to our proposed framework has also been provided for further research in this field.","['Quality of service', 'Internet of Things', 'Radio access technologies', 'Cross layer design', 'Reliability', 'Optimization', 'Wireless sensor networks']","['QoS in IoT networks', 'AI & ML for cross-layer design', 'cross-layer optimization', 'reliable QoS', 'multi-RAT networks', 'edge intelligence']"
"Non-orthogonal multiple access (NOMA) technique and unmanned aerial vehicles (UAVs) have been recognized as promising technologies for enabling the stringent requirements of the different network infrastructures expected for the next generation of wireless networks. In parallel, intelligent reconfigurable surfaces (IRSs) have been widely pointed out as an auspicious solution to further improve spectral efficiency, coverage range, and connectivity. By integrating IRS with UAV and NOMA schemes with multiple-input multiple-output (MIMO) it is possible to smartly improve the overall network performance. In order to explore some of these potentials, this paper provides a comprehensive discussion about the interplay of aerial IRS in MIMO-NOMA (AIRS-NOMA) networks, as well its architecture, functionality principles, and performance gains. In particular, attractive gains related to the data rate maximization, user fairness, energy efficiency, and coverage range are highlighted. Simulation results are provided to support our insightful discussions, in which it is revealed that the performance gains of AIRSNOMA networks are superior when compared to terrestrial deployment. In addition, to guide new studies perspectives, it is addressed some issues and research opportunities associated with this potential integration.","['NOMA', 'Autonomous aerial vehicles', 'Array signal processing', 'Resource management', 'Trajectory', 'MIMO communication', 'Wireless networks']","['Intelligent reflecting surface (IRS)', 'multiple-input multiple-output (MIMO)', 'nonorthogonal multiple access (NOMA)', 'unmanned aerial vehicle (UAV)']"
"Ambient backscatter communication (AmBC) is an emerging technology that has the potential to offer spectral- and energy-efficient solutions for the next generation wireless communications networks, especially for the Internet of Things (IoT). Intelligent reflecting surfaces (IRSs) are also perceived to be an integral part of the beyond 5G systems to complement the traditional relaying scheme. To this end, this paper proposes a novel system design that enables the co-existence of a backscattering secondary system with the legacy primary system. This co-existence is primarily driven by leveraging the AmBC technique in IRS-assisted unmanned aerial vehicle (UAV) networks. More specifically, an aerial-IRS mounted on a UAV is considered to be employed for cooperatively relaying the transmitted signal from a terrestrial primary source node to a user equipment on the ground. Meanwhile, capitalizing on the AmBC technology, a backscatter capable terrestrial secondary node transmits its information to a terrestrial secondary receiver by modulating and backscattering the ambient relayed radio frequency signals from the UAV-IRS. We comprehensively analyze the performance of the proposed design framework with co-existing systems by deriving the outage probability and ergodic spectral efficiency expressions. Moreover, we also investigate the asymptotic behavior of outage performance in high transmit power regimes for both primary and secondary systems. Importantly, we analyze the performance of the primary system by considering two different scenarios, i.e., optimal phase shifts design and random phase shifting at IRS. Finally, based on the analytical performance assessment, we present numerical results to provide various useful insights and also provide simulation results to corroborate the derived theoretical results.","['Backscatter', 'Autonomous aerial vehicles', 'Internet of Things', 'Wireless networks', 'Spectral efficiency', 'Radio frequency', 'RF signals']","['Ambient backscatter communication (AmBC)', 'intelligent reflecting surface (IRS)', 'unmanned aerial vehicle (UAV)', 'performance analysis', 'relaying']"
"Full-duplex (FD) transmissions have emerged as a spectrally efficient technology with the evolution of enhanced self-interference (SI) suppression techniques. However, the FD transmission mode (TM) may not consistently outperform the half-duplex (HD) TM due to the residual SI. In contrast, an adaptive FD/HD transmission scheme utilizes the advantages of both FD and HD TM. This paper proposes an adaptive FD/HD transmission scheme for a cooperative device-to-device (C-D2D) communications system wherein cellular uplink data is relayed through a D2D transmitter (DT). Additionally, a joint DT and TM selection algorithm is introduced to identify the best relay and favorable TM for the multiuser C-D2D system. Further, a probabilistic mathematical framework is developed to evaluate the proposed adaptive FD/HD transmission scheme. The analytical expressions of D2D and cellular outage probability for each TM (FD/HD) have been derived. Results show that FD TM performs better than HD TM when a maximum of two D2D users mapped to a cellular user. However, a trade-off between FD and HD TM is observed for a large number of D2D users. Specifically, when the cellular outage constraint is low, HD TM performs better than FD TM, whereas FD TM outperforms HD TM for high cellular outage constraint.","['Adaptive systems', 'Interference cancellation', 'Transmitters', 'Probability', 'Probabilistic logic', 'Device-to-device communication', 'Power system reliability']","['C-D2D communications system', 'OFDMA', 'full-duplex radio', 'outage probability']"
"This study analyzes the physical layer security (PLS) performance of a differential chaos shift keying (DCSK) modulation-based Power Line Communication (PLC) system by exploiting the novel Farlie-Gumbel-Morgenstern (FGM) Copula approach. A power line Wyner’s wiretap channel model is investigated, where the main channel and the wiretap channel are assumed to be correlated and Log-normally distributed. The Gamma approximation to the Log-normal distribution is employed to simplify the computation. Concurrently, the PLC channel noise is modeled as a Bernoulli-Gaussian random process. Utilizing a Copula based approach to model the dependence among the correlated PLC channels, the PLS performance of the PLC system is evaluated in terms of the secure outage probability (SOP) and the strictly positive secrecy capacity (SPSC). It is revealed through the asymptotic SOP analysis that the secrecy diversity order depends on the shaping parameter(mγM)of the main channel. We also propose an algorithm to maximize the secrecy throughput under SOP constraints. Based on the insights from this analysis, it has been seen that the SOP performance degrades when the value of the dependence parameter(θ)increases. Also, the secrecy throughput performance improves with a lower optimal threshold value of the signal-to-noise ratio (SNR),γth. Furthermore, some other insightful observations are presented by studying the impact of different parameters such as spreading factor(β), impulsive noise occurrence probability(p), transmitted power(PT), and impulsive noise index(K).","['Modulation', 'Internet of Things', 'Security', 'Fourth Industrial Revolution', 'Fading channels', 'Computational modeling', 'Mathematical models']","['Power line communication', 'physical layer security', 'secure outage probability', 'strictly positive secrecy capacity', 'log-normal distribution', 'Bernoulli-Gaussian random process', 'secrecy throughput']"
"Massive multiple-input multiple-output (MaMIMO) and cognitive radio networks (CRNs) are two promising technologies for improving spectral efficiency of next-generation wireless communication networks. In this paper, we investigate the problem of physical layer security in the networks that jointly use both technologies, named MaMIMO-CRN. Specifically, to investigate the vulnerability of this network, we design an optimized attacking scenario to MaMIMO-CRNs by a jammer. For having the most adversary effect on the uplink transmission of the legitimate MaMIMO-CRN, we propose an efficient method for power allocation of the jammer. The legitimate network consists of a training and a data transmission phase, and both of these phases are attacked by the jammer using an optimized power split between them. The resulting power allocation problem is non-convex. We thus propose three different efficient methods for solving this problem, and we show that under some assumptions, a closed-form solution can also be obtained. Our results show the vulnerability of the MaMIMO-CRN to an optimized jammer. It is also shown that increasing the number of antennas at the legitimate network does not improve the security of the network.","['Jamming', 'Training', 'Resource management', 'Antennas', 'Uplink', 'Physical layer security', 'Channel estimation']","['Massive MIMO', 'cognitive radio network', 'physical layer security', 'jamming', 'optimization', 'power allocation', 'spectral efficiency']"
"Cellular-assisted vehicular-to-everything (C-V2X) communication is a promising technology to enhance the road safety and support various infortainment services in future vehicular networks. However, the integration of vehicular communication with the conventional cellular network faces many new design challenges, as vehicles typically use different spectrum band (5.9 GHz) from the cellular systems, and they usually follow different distributions and have more stringent reliability and latency requirements, as compared with the conventional cellular users. In this paper, we study C-V2X communication where the V2X resource pool is managed by the eNodeBs and reused in different cells, with the cells divided according to the Voronoi tessellation of the eNodeBs. We investigate three connection modes for a vehicle to receive safety/entertainment information, namely, from its associated eNodeB, from a neighboring vehicle on the same road, or from a neighboring vehicle on a different road. We propose a weighted-power-based mode selection scheme which offers great flexibility to balance the load of the cellular network and improve the coverage probability. By using stochastic geometry, we derive the analytical expressions for the mode selection and coverage probabilities, which are validated by extensive simulations. Furthermore, by optimizing the weighting factors, the proposed mode selection scheme leads to significantly better performance than the conventional distance-based and maximum-power-based mode selection schemes.","['Roads', 'Vehicle-to-everything', 'Cellular networks', 'Analytical models', 'Stochastic processes', 'Interference', 'Resource management']","['Vehicular communication', 'C-V2X', 'mode selection', 'stochastic geometry', 'coverage probability', 'load balance']"
"Optical beam center position on an array of detectors is an important parameter that is essential for estimating the angle-of-arrival of the incoming signal beam. In this paper, we have examined the beam position estimation problem for photon-counting detector arrays, and to this end, we have derived and analyzed the Cramér-Rao lower bounds on the mean-square error of unbiased estimators of beam position. Furthermore, we have also derived the Cramér-Rao lower bounds of other system parameters such as signal peak intensity, and dark current noise power, on the array. In this sense, we have considered robust estimation of beam position in which none of the parameters are assumed to be known beforehand. Additionally, we have derived the Cramér-Rao lower bounds of beam and noise parameters for observations based on both pilot and data symbols of a pulse position modulation (PPM) scheme. Finally, we have considered a two-step estimation problem in which the signal peak and dark current noise intensities are estimated using a method of moments estimator, and the beam center position is estimated with the help of a maximum likelihood estimator.","['Detectors', 'Optical beams', 'Photonics', 'Optical arrays', 'Optical receivers', 'Optical detectors', 'Adaptive optics']","['Angle-of-arrival', 'beam center position', 'Cramèr-Rao lower bound', 'dark current', 'maximum likelihood estimator', 'method of moments estimator', 'photon-counting detector arrays', 'pulse position modulation']"
"Social networks are playing an increasingly important role in information dissemination in wireless ad hoc networks with the wide adoption of the mobile Internet. In particular, in social application scenarios such as Facebook and Twitter, the questions of how to fully consider the social relationships between users to ensure the quality of service (QoS) urgently needs to be explored. This paper investigates an optimal social relay selection scheme with high intimacy requirements to maximize the system throughput for social-physical ad hoc networks with device-to-device (D2D) communication. Different from previous studies, we jointly consider feasible relays and multiuser cooperative mobility with satisfactory link reliability for throughput maximization. On the one hand, we formulate a nonlinear and nonconvex problem, which is typically NP-hard, for throughput optimization. Specifically, this optimization problem is divided into two subproblems: i) selecting the optimal relays and ii) determining the best mobility strategy in terms of throughput. On the other hand, we first convert the relay selection subproblem into an optimal stopping phase problem, and then propose a definition of a graph representing the degrees of interference between physical links, transforming the original optimization problem into a convex problem that is solvable using the Lagrange multiplier method. Based on the above, we propose the Relay selection and Link Interference Degree Graph (RS-LIDG) algorithm to solve the two subproblems. Numerical simulations verify that the proposed RS-LIDG method improves the throughput gain by 26.29%, 123.43%, and 236.47% compared to the intuitive method (IM), the social-trust-based random mobility selection method (STS-RM), and the physical-based random mobility selection method (PHS-RM), respectively.","['Relays', 'Throughput', 'Quality of service', 'Optimization', 'Interference', 'Device-to-device communication', 'Collaboration']","['Social network', 'wireless ad hoc networks', 'throughput', 'relay selection', 'multiuser cooperative mobility']"
"A polar projection-based algorithm is proposed to reduce the computational complexity associated with dimension reduction in unsupervised learning. This algorithm employs K-means clustering. A new distance metric is developed to account for peak consumption in cluster consumer load profiles. It is used to cluster the load profiles according to both total and peak consumption. To accelerate the clustering process, a stochastic-based approach is developed to reduce the search space to find the cluster centers. Numerical results are presented which show a significant reduction in computational complexity using both polar-based and stochastic-based clustering compared to conventional approaches. Further, the estimation error is low.","['Measurement', 'Dimensionality reduction', 'Estimation error', 'Clustering algorithms', 'Smart meters', 'Computational complexity', 'Unsupervised learning']","['Clustering', 'computational complexity', 'dimension reduction', 'smart grid']"
"High-speed rail (HSR) communication has always been an attractive research topic with the continuous progress of transportation systems and communication technologies. Recently, Hyperloop has emerged as a candidate for very high-speed transportation systems. Because of its outstanding potential, Hyperloop can usher in a new transportation era with several attractive features. Developing suitable communication system solutions is crucial to bring this promising technology closer to reality. In addition, the Hyperloop communication system is essential to support monitoring-and-controlling services and deliver communication services inside its capsules/pods. Throughout this article, we overview Hyperloop technology and discuss its general characterization to recognize and estimate its relative position among the current HSR communication systems. Then, we investigate different attributes of the communication system, including network architecture, quality-of-service (QoS) requirements, and critical challenges to implement a reliable communication system. With the high speed of the capsule/pod and the system’s unique structure, severe Doppler effect and frequent handover may considerably affect the operation of the communication system. Because Hyperloop is a recent development, it is necessary to study the existing HSR communication technologies to determine whether they can establish robust communication links for the Hyperloop system and deliver data with the required QoS. Furthermore, we provide technical details of the current HSR technologies and related research. Subsequently, we present the recent advances in the Hyperloop communication system with classification depending on whether it is a radio-, network-, antenna-, or software-based solution. Finally, we propose future research directions that can promote an improved communication performance.","['Communication systems', 'Rail transportation', 'Transportation', 'Reliability', 'Europe', 'Electron tubes', 'GSM-R']","['Hyperloop communications', 'high-speed rail communications', 'vacuum tube communications', 'vehicular communications']"
"Caching popular videos at the edge has been confirmed as a promising way to support low-latency video transmission and alleviate the backhaul traffic burden. Meanwhile, mobile edge computing (MEC) has also been regarded as an effective solution to meet the 5G low-latency service requirements. In this article, we propose to fully utilize both the storage and computing resources at edge servers to support multiple bitrate video streaming. We design the video caching, processing, and user association models that aim to minimize the average retrieval latency of all users. This problem is modeled as a mixed-integer bilinear problem, which is NP-hard . We show that under practical constraints on storage, bandwidth, and processing capacity, the problem does not exhibit sub-modular property and the performance of a greedy algorithm may not be strictly guaranteed. To deal with this challenging problem, we decompose the original problem into a cache placement problem and a user-BS association problem, while still preserving the interplay between the two sub-problems. A linearization and rounding algorithm, including: (i) a greedy rounding proactively caching scheme and (ii) a random-rounding user-BS association scheme, is then proposed, with performance bounds derived. Extensive simulation results show that the proposed scheme can achieve a near-optimal performance under various storage, computing capacity, and downlink bandwidth settings.","['Streaming media', 'Bit rate', 'Servers', 'Wireless communication', 'Quality of experience', 'Bandwidth', 'Transcoding']","['Video caching', 'video transcoding', 'multiple bitrate video', 'mobile edge computing (MEC)', 'submodularity']"
"Due to limited interference range, the advantage of visible light communication (VLC) over wireless fidelity (WiFi) lies more in the unit-area transmission rate rather than the single-link transmission rate. To characterize the achievable transmission rate per unit area, we consider an indoor downlink VLC network with dense attocell configuration for the transmitters with single-color light-emitting diode (LED) and multi-color LEDs, assuming binomial distributed users. We divide each attocell into central region and boundary region, and propose the transmission protocols based on such attocell division. We optimize the LED half-power angle to maximize the mean achievable transmission rate per unit area. More specifically, we investigate the rates of cell-center and cell-boundary users under fairness consideration for the single LED transmitter system. We characterize the mean achievable transmission rate per unit area under the white light constraints in the multiple LEDs transmitter system. The performance of the proposed transmission protocols is evaluated by numerical results.","['Light emitting diodes', 'Visible light communication', 'Optical transmitters', 'Interference', 'Protocols', 'Optical receivers']","['Dense coverage', 'multi-color LED', 'unit-area achievable transmission rate', 'VLC']"
"Control and performance optimization of wireless networks of Unmanned Aerial Vehicles (UAVs) require scalable approaches that go beyond architectures based on centralized network controllers. At the same time, the performance of model-based optimization approaches is often limited by the accuracy of the approximations and relaxations necessary to solve the UAV network control problem through convex optimization or similar techniques, and by the accuracy of the channel network models used. To address these challenges, this article introduces a new architectural framework to control and optimize UAV networks based on Deep Reinforcement Learning (DRL). Furthermore, it proposes a virtualized, `ready-to-fly' emulation environment to generate the extensive wireless data traces necessary to train DRL algorithms, which are notoriously hard to generate and collect on battery-powered UAV networks. The training environment integrates previously developed wireless protocol stacks for UAVs into the CORE/EMANE emulation tool. Our `ready-to-fly' virtual environment guarantees scalable collection of high-fidelity wireless traces that can be used to train DRL agents. The proposed DRL architecture enables distributed data-driven optimization (with up to 3.7 × throughput improvement and 0.2 × latency reduction in reported experiments), facilitates network reconfiguration, and provides a scalable solution for large UAV networks.","['Optimization', 'Ad hoc networks', 'Protocols', 'Drones', 'Emulation', 'Wireless sensor networks', 'Virtual environments']","['UAV networks', 'non-terrestrial netoworks', 'deep reinforcement learning', 'AI for wireless networks', '6G']"
"Benefiting from the high resolution in beamspace, millimeter wave (mmwave) communication has been regarded as a high-accuracy localization solution, where the location information is embedded in the channel via angle and time delay, for example. In this paper, to locate a user equipment (UE) and scatterers, we present the localization model in mmwave communications as a compressed sensing assisted channel estimation problem, which is solved using a proposed two-stage channel estimation based localization scheme. During the first stage, a sparse Bayesian learning (SBL) algorithm is operated to attain a coarse estimation. Then during the second stage, a multi-stage grid refinement assisted fine estimation is achieved by a distributed compressed sensing simultaneous orthogonal matching pursuit (DCS-SOMP) algorithm. Moreover, in our approach, the few-bit analog to digital converters (ADCs) are utilized by the receiver of UE so as to attain a good trade-off among performance, complexity and energy-efficiency. Finally, the performance of channel estimation and positioning is comprehensively investigated and compared. It can be shown that our proposed two-stage approach is capable of achieving centimeter-level accuracy with the required number of quantization bits of ADCs less than four.","['Channel estimation', 'Location awareness', 'Estimation', 'Signal processing algorithms', 'Matching pursuit algorithms', 'Receivers', 'Millimeter wave communication']","['Compressive sensing', 'mmwave', 'sparse sensing matrix', 'massive MIMO', 'few-bit ADC', 'localization', 'positioning', 'channel estimation', 'beamforming']"
"We consider a two-way half-duplex decode-and-forward (DF) relaying system with multiple pairs of single-antenna users assisted by a cell-free (CF) massive multiple-input multiple-output (mMIMO) architecture with multiple-antenna access points (APs). Under the practical constraint of imperfect channel state information (CSI), we derive the achievable sum spectral efficiency (SE) for a finite number of APs with maximum ratio (MR) linear processing for both reception and transmission in closed-form. Notably, the proposed CF mMIMO relaying architecture, exploiting the spatial diversity, and providing better coverage, outperforms the conventional collocated mMIMO deployment. Moreover, we shed light on the power-scaling laws maintaining a specific SE as the number of APs grows. A thorough examination of the interplay between the transmit powers per pilot symbol and user/APs takes place, and useful conclusions are extracted. Finally, differently to the common approach for power control in CF mMIMO systems, we design a power allocation scheme maximizing the sum SE.","['Spectral efficiency', 'Spatial diversity', 'Power control', 'Layout', 'Process control', 'Massive MIMO', 'MIMO']","['decode-and-forward', 'power-scaling law', 'beyond 5G MIMO']"
"This paper investigates multiple parallel federated learning in cellular networks, where a base station schedules several FL tasks in parallel and each task has a group of devices involved. To reduce the communication overhead, over-the-air computation is introduced by utilizing the superposition property of multiple access channels (MAC) to accomplish the aggregation step. Since all devices use the same radio resource to transfer their local updates to the BS, in order to separate the received signals of different tasks, we use the zero-forcing receiver combiner to mitigate the mutual interference across different groups. Besides, we analyze the impact of receiver combiner and device selection on the convergence of our multiple parallel FL framework. Also, we formulate an optimization problem that jointly considers receiver combiner vector design and device selection for improving FL performance. We address the problem by decoupling it into two sub-problems and solve them alternatively, adopting successive convex approximation (SCA) to derive the receiver combiner vector, and then solve the device scheduling problem with a greedy algorithm. Simulation results demonstrate that the proposed framework can effectively solve the straggler issue in FL and achieve a near-optimal performance on all tasks.","['Receivers', 'Computational modeling', 'Performance evaluation', 'Task analysis', 'Servers', 'Convergence', 'Collaborative work']","['Device selection', 'federated learning', 'multiple access channel', 'over-the-air computation', 'receiver combiner']"
"In this article, we evaluate the overall outage probability (OOP) of pairwise Non-orthogonal Multiple Access (NOMA) for both uplink and downlink. We also propose a dynamic decoding order (DDO) together with a fixed pairwise power allocation (FPPA) scheme, in which the optimal decoding order is decided based on the instantaneous channel gains, and thereafter, a pair of power levels is assigned in accordance with the selected decoding order. Exact closed-form expressions of the OOPs for both uplink and downlink pairwise NOMA considering all proposed decoding orders over Nakagami-m fading are derived. Further, we find the optimal fixed power levels for different power allocation strategies so that the OOPs are minimized. Moreover, we investigate the influence of the distances between the source nodes and the access point (AP), the target transfer rates and the path-loss exponents on the OOPs for all cases of decoding orders. In addition, we benchmark our proposed DDO against other decoding orders in terms of the OOP. The results show that assigning optimal fixed power levels which takes the instantaneous decoding order into account not only improves the communication reliability, but also reduces the complexity and computational load at the AP.","['NOMA', 'Decoding', 'Resource management', 'Downlink', 'Uplink', 'Complexity theory', 'Computer crime']","['Overall outage probability', 'pairwise NOMA', 'dynamic decoding order', 'optimal power allocation', 'uplink/downlink NOMA']"
"Deploying access and backhaul as wireless links, a.k.a. integrated access and backhaul (IAB), is envisioned as a viable approach to enable flexible and dense networks. Even further, mobile IAB (mIAB) is a candidate solution to enhance the connectivity of multiple user equipment (UE) moving together. In this context, different of other works from the literature, the present work overviews the basis for the deployment of mIAB by presenting: 1) the current status of IAB standardization in the fifth generation (5G) new radio (NR); 2) a new taxonomy for state-of-the-art works regarding fixed IAB and mIAB; 3) an extensive performance analysis of mIAB based on simulation results; and 4) open challenges and potential future prospects of mIAB. Specifically, the proposed taxonomy classifies IAB works according to different perspectives and categorizes mIAB works according to the type of mobile node. For each type of mobile node, the main studied topics are presented. Regarding the performance evaluation, we consider an urban macro scenario where mIAB nodes are deployed in buses in order to improve the passengers’ connection. The results show that, compared to other network architectures, the deployment of mIAB nodes remarkably improves the passengers’ throughput and latency in both downlink and uplink.","['Backhaul networks', 'Wireless communication', '5G mobile communication', '3GPP', 'Protocols', 'Microprocessors', 'Computer architecture']","['5G standardization', '6G', 'backhaul', 'integrated access and backhaul (IAB)', 'mobile IAB', 'mobility', 'moving cell']"
"In this paper, we propose a relay-assisted device-to-device (D2D) communication as an underlay in a cellular network, where two sequential best relay nodes based on different selection criteria are chosen. Novel analytical expressions of the secrecy outage probability (SOP) and the probability of nonzero secrecy capacity (PNSC) are derived to evaluate the secrecy performance of the cellular network. In addition, we derive a simple and explicit approximation of the SOP in the high signal-to-noise ratio (SNR) to gain some insights on how various system parameters affect the aforementioned secrecy performance. Also, for the sake of comparison, the analytical SOP expression when only the cellular network exists, i.e., without the interference of the D2D communication, is obtained. Furthermore, to assess the usefulness of the cooperation between the D2D communication and the cellular network, we derive an analytical expression for the mutual outage probability (MOP) which represents the true outage probability across both the cellular and D2D networks. An asymptotic analysis of the MOP in the high SNR regime is obtained and the diversity order is derived. Finally, our proposed mathematical framework is compared with Monte-Carlo simulations to verify the accuracy of the derivations.","['Device-to-device communication', 'Cellular networks', 'Relays', 'Security', 'Interference', 'Fading channels', 'Wireless communication']","['Device-to-device communications', 'mutual outage probability', 'η-μ fading', 'physical layer security', 'security provisioning']"
"Rate-Splitting (RS) has been shown recently to be a powerful approach for the design of non-orthogonal transmission, multiple access, and interference management strategies in multi-user multi-antenna systems. RS, through the split of messages into common and private parts, relies on the transmission of common streams decoded by all users, and private streams decoded only by its intended user. This enables RS to bridge the extreme of fully decode interference and fully treat interference as noise. In this paper, we depart from Gaussian signaling and study RS under finite input alphabet for multi-user multi-antenna system and propose a constructive interference (CI) exploitation approach to further enhance the sum-rate achieved by RS. To that end, new analytical expressions for the ergodic sum-rate are derived for two precoding techniques of the private messages, namely, 1) a traditional interference suppression zero-forcing (ZF) precoding approach, 2) a closed-form CI precoding approach. Our analysis is presented for perfect channel state information at the transmitter (CSIT), and is extended to imperfect CSIT knowledge. A novel power allocation strategy, specifically suited for the finite alphabet setup, is derived and shown to lead to superior performance for RS over conventional linear precoding not relying on RS (NoRS). The results in this work validate the significant sum-rate gain of RS with CI over the conventional RS with ZF and NoRS.","['Interference suppression', 'Bridges', 'Transmitters', 'Precoding', 'Resource management', 'Channel state information', 'Antennas']","['Rate splitting', 'zero forcing', 'constructive interference', 'phase-shift keying signaling']"
"We a framework for the design of low-complexity and high-performance receivers for multidimensional overloaded non-orthogonal multiple access (NOMA) systems. The framework is built upon a novel compressed sensing (CS) regularized maximum likelihood (ML) formulation of the discrete-input detection problem, in which the l 0 -norm is introduced to enforce adherence of the solution to the prescribed discrete symbol constellation. Unlike much of preceding literature,.g., (Assaf et al., 2020, Yeom et al., 2019, Nagahara, 2015, Naderpour and Bizaki, 2020, Hayakawa and Hayashi, 2017, Hayakawa and Hayashi, 2018, and Zeng et al., 2020), the method is not relaxed into the l 1 -norm, but rather approximated with a continuous and asymptotically exact expression without resorting to parallel interference cancellation (PIC). The objective function of the resulting formulation is thus a sum of concave-over-convex ratios, which is then tightly convexified via the quadratic transform (QT), such that its solution can be obtained via the iteration of a simple closed-form expression that closely resembles that of the classic zero-forcing (ZF) receiver, making the method particularly suitable to large-scale set-ups. By further transforming the aforementioned problem into a quadratically constrained quadratic program with one convex constraint (QCQP-1), the optimal regularization parameter to be used at each step of the iterative algorithm is then shown to be the largest generalized eigenvalue of a pair of matrices which are given in closed-form. The method so obtained, referred to as the Iterative Discrete Least Square (IDLS), is then extended to address several factors of practical relevance, such as noisy conditions, imperfect channel state information (CSI), and hardware impairments, thus yielding the Robust IDLS algorithm. Simulation results show that the proposed art significantly outperforms both classic receivers, such as the linear minimum mean square error (LMMSE), and recent CS-based state-of-the-art (SotA) alternatives, such as the sum-of-absolute-values (SOAV) and the sum of complex sparse regularizers (SCSR) detectors. It is also shown via simulations that the technique can be integrated with existing iterative detection-and-decoding (IDD) methods, resulting in accelerated convergence.","['NOMA', 'Correlation', 'Simulation', 'Detectors', 'Receivers', 'Transforms', 'Hardware']","['Non-orthgonal multi-access (NOMA)', 'detection', 'overloaded systems', 'fractional programming', 'iterative least square']"
"The emerge of Internet of Things (IoT) brings up revolutionary changes to wireless communications. Cognitive radio (CR) can be seen as one of the prominent solutions to spectrum scarcity in IoT, where multi-band cooperative spectrum sensing (CSS) is the key. However, lack of centralized control and increase in number of devices place a room for many challenges. One of the main challenges is secondary users' (SUs') scheduling to sense a subset of channels in heterogeneous distributed CR networks (CRNs). To overcome the aforementioned challenge, in this paper, we propose a novel heterogeneous multi-band multi-user CSS (HM2CSS) scheme. The proposed scheme allows heterogeneous SUs to sense multiple channels and consists of two stages. We formulate a mathematical model to optimize leader-selection for each channel in the first stage. We then formulate another optimization problem to determine corresponding cooperative SUs to sense these channels in the second stage. After that, diffusion learning is used to decide on the availability of channels. Simulations illustrate that the proposed scheme improves detection performance and CRN throughput, is scalable in terms of detection performance, and provides fair energy consumption for CSS on all channels compared to existing multi-band CSS schemes.","['Cascading style sheets', 'Detectors', 'System performance', 'Internet of Things', 'Optimization', 'Energy consumption']","['Cognitive radio', 'cooperative communications and distributed processing', 'heterogeneous networks', 'Internet of Things']"
"Artificial Intelligence (AI) is changing every technology we are used to deal with. Autonomy has long been a sought-after goal in vehicles, and now more than ever we are very close to that goal. Big auto manufacturers as well are investing billions of dollars to produce Autonomous Vehicles (AVs). This new technology has the potential to provide more safety for passengers, less crowded roads, congestion alleviation, optimized traffic, fuel-saving, less pollution as well as enhanced travel experience among other benefits. But this new paradigm shift comes with newly introduced privacy issues and security concerns. Vehicles before were dumb mechanical devices, now they are becoming smart, computerized, and connected. They collect huge troves of information, which needs to be protected from breaches. In this work, we investigate security challenges and privacy concerns in AVs. We examine different attacks launched in a layer-based approach. We conceptualize the architecture of AVs in a four-layered model. Then, we survey security and privacy attacks and some of the most promising countermeasures to tackle them. Our goal is to shed light on the open research challenges in the area of AVs as well as offer directions for future research.","['Security', 'Roads', 'Privacy', 'Automobiles', 'Accidents', 'Navigation', 'Monitoring']","['Autonomous vehicles', 'communication system security', 'information system security', 'data privacy']"
"Around 40% of the world’s population is currently without access to the Internet. The digital divide is due to the high cost of provisioning these services and the low return on investment for network operators. We propose using 5G network slicing with multi-tenancy (also known as neutral host networks (NHN)) for macro-cells and small cells in rural areas to reduce the costs. This paper investigates the techno-economic feasibility of using rural 5G NHN to minimise the digital divide. A generic model is developed to analyse the techno-economic analysis of 5G NHN deployment around the world, with a special focus on rural areas where no MNO is interested in providing services. To understand the application, it is applied to the Indian scenario. First, a discussion on existing infrastructure, competition and statistics for Indian telecommunications is presented. Next, the technical requirements are analysed using the key performance indicators (KPI) required for the rural 5G NHN for the Indian scenario. The study also analyses the relationship between coverage, investment in the network, the number of subscribers, investment time, demand, the investment per user and sensitivity analysis to understand the feasibility of the proposed solution for Indian villages with different input conditions. Later, a case study is carried out based on the proposed approach, along with coverage modelling for a few Indian villages having different topologies. The results show that 5G NHN using network slicing can significantly reduce the total investment required for providing 5G services in rural areas. Furthermore, the study shows that rural 5G NHN is a viable investment and a key enabler for Internet connectivity for villages with 10-year investment, having a subscribers’ base as low as 100 with a customer growth rate of 7%.","['5G mobile communication', 'Network slicing', 'Costs', 'Indium phosphide', 'III-V semiconductor materials', 'Investment', 'Statistics']","['5G mobile communication', 'multi-tenancy', 'network slicing', 'rural connectivity', 'techno-economic analysis']"
"Uncrewed aerial vehicle-mounted base stations (UAV-BSs), also know as drone base stations, are considered to have promising potential to tackle the limitations of ground base stations. They can provide cost-effective Internet connection to users that are out of infrastructure. They can also take over quickly as service providers when ground base stations fail in an unanticipated manner. UAV-BSs benefit from their mobile nature that enables them to change their 3D locations if the demand profile changes rapidly. In order to effectively leverage the mobility of UAV-BSs so as to maximize the performance of the network, 3D location of UAV-BSs requires continuous optimization. However, solving the optimization problem of UAV-BSs is NP-hard with no deterministic solution in polynomial time. In this paper, we propose a continuous actor-critic deep reinforcement learning solution in order to solve the location optimization problem of UAV-BSs in the presence of mobile endpoints. The simulation results show that the proposed model significantly improves the network performance compared to Q-learning, deep Q-learning and conventional algorithms. While the Q-learning and deep Q-learning-based baselines reach the sum data rate of 35 Mbps and 42 Mbps respectively, our proposed ACDQL-based strategy maximizes the sum data rate of endpoints to 45 Mbps. Furthermore, the proposed ACDQL-based methodology reduces the convergence time of the UAV-BS placement optimization by 85 percent compared to the Q-learning and deep Q-learning baselines.","['Optimization', 'Q-learning', 'Base stations', '6G mobile communication', 'Three-dimensional displays', 'Internet', 'Delays']","['Aerial base stations', 'artificial intelligence', '5G', '6G', 'reinforcement learning', 'deep reinforcement learning', 'actor-critic deep reinforcement learning', 'Q-learning', 'deep Q-learning']"
"Full-duplex (FD) wireless - simultaneous transmission and reception on the same frequency - is a promising technique that can significantly improve spectrum efficiency and reduce communication latency in future wireless networks. To enable FD operation, the powerful self-interference (SI) signal leaking from the transmitter (TX) into the receiver (RX) needs to be suppressed and canceled to an extreme degree at the antenna interface as well as in the RF/analog and digital domains. Various approaches to achieve TX-RX isolation at the antenna interface and SI cancellation (SIC) in the RF/analog and digital domains have been demonstrated, with a focus on using bench-top off-the-shelf components. Recent advances in integrated circuit (IC) implementations of various types of shared antenna interfaces and cancellers in the RF and/or analog baseband (BB) domains have further pushed the frontier of realizing FD wireless in small-form-factor/hand-held devices. Moreover, it is still important to fundamentally study the SIC performance that can be achieved by different types of cancellers. In this paper, we present a first-of-its-kind comprehensive overview on the implementation and comparison of various state-of-the-art integrated shared antenna interfaces and SI cancellers in both the RF and analog BB domains. We define two figures of merit (FOM) for the IC-based shared antenna interfaces and SI cancellers, which capture various design considerations and performance tradeoffs including the achievable isolation/SIC, bandwidth, noise figure degradation, TX/SI power handling, and power consumption. In particular, we focus on two types of integrated shared antenna interfaces including electrical-balance duplexers and circulators. We also discuss two types of SI cancellers based on the time-domain and frequency-domain approaches, which respectively employ parallel delay lines and bandpass filters to emulate the SI channel. Based on realistic measurements, we perform extensive numerical evaluations to illustrate and compare the achievable RF SIC performance in different scenarios, as well as discuss various design tradeoffs. Finally, we provide an overview of the IC-based implementations and performance evaluations of both types of cancellers based on our recent work.","['Antennas', 'Radio frequency', 'Circulators', 'Integrated circuits', 'Interference cancellation', 'Wireless communication', 'Impedance']","['Full-duplex wireless', 'integrated circuit (IC)', 'antenna interface', 'self-interference cancellation', 'systems and testbeds']"
"Recently, we have witnessed an expeditious growth in the intelligence and processing ability of user equipment accompanied by the explosive increase of wireless data and traffic. With such a huge demand, and a shortage of resources to fulfill that need, device-to-device (D2D) communication has surfaced as a propitious solution to reduce costs associated with backhaul links by collaboratively caching files. This paper explores the existing content caching frameworks in D2D communication environments. Further, it proposes two novel frameworks - one leveraging a combination of recurrent and deep neural networks and another based on the latest deep language models that use attention mechanisms known as transformers. The developed frameworks require minimum apriori knowledge about the environment and utilize the growth of user data to achieve performance enhancement. Our experiments show that the proposed frameworks are adaptive in nature and learn from the historical data of the environment. Further, we achieve an overall 25% increase in the D2D cache hit rate over a recently proposed framework that uses neural networks for collaborative filtering (NCF) to make caching decisions.","['Device-to-device communication', 'Costs', 'Predictive models', 'Transformers', 'Deep learning', 'Complexity theory', 'Stochastic processes']","['Traffic offloading', 'D2D caching', 'machine learning', 'deep learning', 'edge intelligence', 'time-series']"
"In this paper, we propose a differential multiple-input multiple-output (MIMO) scheme based on the novel concept of chaos-based time-varying unitary matrices to demonstrate—for the first time in the literature—the ability of differential encoding in achieving practical physical layer security even without the need for using channel estimation. In the proposed scheme, an erroneous secret key, which is extracted from the wireless nature, is used to initialize a chaos sequence that is responsible for generating artificially time-varying unitary matrices capable of obfuscating the transmitted data symbols from illegitimate eavesdroppers. Contrary to conventional studies, the key agreement ratio in this study is assumed to be imperfect, which is often true and very realistic in high-mobility scenarios. Following this, we conceive a new calibration algorithm for reconciling the chaotic sequence generated at the legitimate parties, thus making this calibration algorithm a unique, novel solution to the key sharing problem of conventional chaos-based communication techniques, which has been overlooked over the past few decades. It is found out that differential encoding obviates additional complexity and insecurity in dealing with channel estimation, whereas an eavesdropper must tackle the complicated differentially encoded patterns, which have an exponentially increasing complexity order. In addition, the obtained simulation results demonstrate that the proposed scheme can outperform conventional chaos-based MIMO schemes that assume perfect channel knowledge.","['MIMO communication', 'Security', 'Wireless communication', 'Encoding', 'Costs', 'Chaotic communication', 'Channel estimation']","['MIMO', 'differential modulation', 'differential space-time block codes', 'physical layer security', 'physical layer encryption', 'chaos theory', 'phase ambiguity', 'constrained capacity', 'secrecy rate', 'security gap']"
"Internet of Health Things (IoHT) involves intelligent, low-powered, and miniaturized sensors nodes that measure physiological signals and report them to sink nodes over wireless links. IoHTs have a myriad of applications in e-health and personal health monitoring. Because of the data's sensitivity measured by the nodes and power-constraints of the sensor nodes, reliability and energy-efficiency play a critical role in communication in IoHT. Reliability is degraded by the increase in packets' loss due to inefficient MAC, routing protocols, environmental interference, and body shadowing. Simultaneously, inefficient node selection for routing may cause the depletion of critical nodes' energy resources. Recent advancements in cross-layer protocol optimizations have proven their efficiency for packet-based Internet. In this article, we propose a MAC/Routing-based Cross-layer protocol for reliable communication while preserving the sensor nodes' energy resource in IoHT. The proposed mechanism employs a timer-based strategy for relay node selection. The timer-based approach incorporates the metrics for residual energy and received signal strength indicator to preserve the vital underlying resources of critical sensors in IoHT. The proposed approach is also extended for multiple sensor networks, where sensor in vicinity are coordinating and cooperating for data forwarding. The performance of the proposed technique is evaluated for metrics like Packet Loss Probability, End-To-End delay, and energy used per data packet. Extensive simulation results show that the proposed technique improves the reliability and energy-efficiency compared to the Simple Opportunistic Routing protocol.","['Sensors', 'Wireless communication', 'Relays', 'Reliability', 'Body area networks', 'Wireless sensor networks', 'Routing']","['Internet of Health Things (IoHT)', 'wireless communication', 'cross-layer protocols', 'wireless body area networks (WBAN)', 'opportunistic routing', 'MAC protocols', 'energy efficiency', 'reliability', 'relay-selection']"
"Using space as an additional degree of freedom is one possible solution to cope with future bandwidth issues. In free-space optics (FSO), spatial multiplexing of structured light modes is limited by the impact of the atmospheric turbulence. Therefore, beating the effects of turbulence is a major problem for structured light-based FSO communication. Here, we model the propagation of Laguerre-Gaussian beams using a modified von Kármán turbulence model with time-dependent turbulence phase screens. We equally investigate the performance of a zero-forcing pre-coding technique to mitigate the effects of turbulence in a full-duplex Laguerre-Gaussian mode based FSO. In the modeling, we account for various limiting factors, including phase estimation errors, noise, imperfect, and outdated channel-state information on the pre-coding approach.","['Atmospheric modeling', 'Channel estimation', 'Crosstalk', 'Phase estimation', 'Optical distortion', 'Transceivers', 'System performance']","['Atmospheric turbulence', 'free space optics', 'spatial mode multiplexing', 'structured light modes', 'zero-forcing']"
"To solve the complex beam alignment issue in non-line-of-sight (NLOS) millimeter wave communications, this paper presents a deep neural network (DNN) based procedure to predict the angle of arrival (AOA) and angle of departure (AOD) both in terms of azimuth and elevation, i.e., AAOA/AAOD and EAOA/EAOD. In order to evaluate the performance of the proposed procedure under practical assumptions, we employ a trajectory prediction method by considering dynamic window approach (DWA) to estimate the location information of the user equipment (UE), which is utilized as the input parameter of the trained DNN to generate the prediction of AAOA/AAOD and EAOA/EAOD. The robustness of the prediction procedure is analyzed in the presence of prediction errors, which proves that the proposed DNN is a promising tool to predict AOA and AOD in NLOS scenarios based on the estimated UE location. Simulation results shows that the prediction errors of the AOA and AOD can be maintained within an acceptable range of ±2°.","['Trajectory', 'Neural networks', 'Training', 'Predictive models', 'Millimeter wave communication', 'Mathematical model', 'Array signal processing']","['Deep learning', 'mmWave', 'NLOS', 'trajectory prediction', 'estimation']"
"In this paper, an uplink pairwise Non-Orthogonal Multiple Access (NOMA) scenario using a mobile access point (AP) or an unmanned aerial vehicle in the presence of a jamming attack is considered. To mitigate the influence of the jamming attack, a joint power allocation and AP placement design is proposed. Accordingly, closed-form expressions of the overall outage probability (OOP) and the individual outage probability (IOP) considering imperfect channel state information for each of the source nodes the AP serves, are derived over Nakagami- m fading channels using dynamic decoding order and fixed pairwise power allocation. We conduct an investigation of the effect of different parameters such as power allocation, source node placements, AP placement, target rates, and jammer location on the OOP and the IOP performance. By adapting the power allocation and the AP placement to the jamming attack, the communication reliability can be increased significantly compared to neglecting the presence of the jammer or treating the jammer as noise. Since the malicious jammer and the AP have conflicting interests in terms of communication reliability, we formulate a non-cooperative game for the two players considering their positions and the power allocation of the NOMA nodes as their strategies and the OOP as utility function. We propose using hybrid simulated annealing - greedy algorithms to address the joint power allocation and AP placement problem for the cases of both a fixed and a mobile jammer. Finally, the Nash equilibrium points are obtained and then the UAV goes directly to this position and keeps staying there to save power consumption.","['Jamming', 'NOMA', 'Resource management', 'Uplink', 'Unmanned aerial vehicles', 'Decoding', 'Probability']","['Dynamic decoding order', 'imperfect CSI', 'outage performance', 'UAV placement', 'pairwise NOMA', 'game theory', 'metaheuristic optimization']"
"The rolling-out of 5G networks is recently including 5G Base Stations (BSs) operating on millimeter-Wave (mm-Wave) frequencies. The goal of this work is to shed light on the exposure assessment from commercial 5G mm-Wave 5G BSs, by focusing on the impact of downlink traffic on the exposure levels. To this aim, we adopt an innovative measurement framework, based on hardware and software components, able to satisfy the challenging measurement requirements of mm-Wave frequencies. In addition, we design a completely softwarized algorithm, called M-WAVE, in order to measure the mm-Wave exposure with a programmable spectrum analyzer. Results, obtained from a commercial 5G scenario, reveal that the exposure from the mm-Wave BS is directly proportional to the amount of traffic injected on the wireless link. However, the electric field is always lower than 0.08 V/m, while the downlink traffic is even larger than 800 Mbps.","['5G mobile communication', 'Frequency measurement', 'Smart phones', 'Antenna measurements', 'Power measurement', 'Position measurement', 'Wireless communication']","['5G networks', 'millimeter-wave frequencies', 'EMF measurements', 'traffic measurements']"
"This work proposes novel synchronous, asynchronous, and session-based designs for energy-efficient massive multiple-input multiple-output networks to support federated learning (FL). The synchronous design relies on strict synchronization among users when executing each FL communication round, while the asynchronous design allows more flexibility for users to save energy by using lower computing frequencies. The session-based design splits the downlink and uplink phases in each FL communication round into separate sessions. In this design, we assign users such that one of the participating users in each session finishes its transmission and does not join the next session. As such, more power and degrees of freedom will be allocated to unfinished users, resulting in higher rates, lower transmission times, and hence, higher energy efficiency. In all three designs, we use zero-forcing processing for both uplink and downlink, and develop algorithms that optimize user assignment, time allocation, power, and computing frequencies to minimize the energy consumption at the base station and users, while guaranteeing a predefined maximum execution time of each FL communication round.","['Energy efficiency', 'Massive MIMO', 'Energy consumption', 'Servers', 'Resource management', 'Radio spectrum management', 'Federated learning']","['Asynchronous transmission', 'energy efficiency', 'federated learning', 'massive MIMO', 'session-based transmission', 'synchronous transmission', 'resource allocation']"
"IoT networks are getting overcrowded following the vast increase in number of Internet-of-Things (IoT) devices and connections. Networks can be extended with more gateways, increasing the number of supported devices. However, as investigated in this work, massive MIMO has the potential to increase the number of simultaneous connections and moreover lower the energy expenditure of these devices. We present a study of the channel characteristics of massive MIMO in the narrowband unlicensed sub-GHz band. The goal is to support IoT applications with strict requirements in terms of number of devices, power consumption, and reliability. The assessment is based on experimental measurements using both a uniform linear and a rectangular array. Our study demonstrates and validates the advantages of deploying massive MIMO gateways to serve IoT nodes. While the results are general, here we specifically focus on static nodes. The array gain and channel hardening effect yield opportunities to lower the transmit power of IoT nodes while also increasing reliability. The exploration confirms that exploiting large arrays brings great opportunities to connect a massive number of IoT devices by separating the nodes in the spatial domain. In addition, we give an outlook on how static IoT nodes could be scheduled based on partial channel state information.","['Antenna arrays', 'Internet of Things', 'Channel estimation', 'Massive MIMO', 'Correlation', 'Antenna measurements', 'Base stations']","['Channel measurements', 'low-power wide-area networks', 'massive MIMO', 'Internet-of-Things', 'sub-GHz', 'test-bed and trials']"
"Multiple-input multiple-output (MIMO) and multiple-input single-output (MISO) schemes have yielded promising results in free space optical (FSO) communications by providing diversity against fading of the received signal intensity. In this article, we have analyzed the probability of error performance of a muliple-input single-output (MISO) free-space optical channel that employs array(s) of detectors at the receiver. In this regard, we have considered the maximal ratio combiner (MRC) and equal gain combiner (EGC) fusion algorithms for the array of detectors, and we have examined the performance of these algorithms subject to phase and pointing errors for strong atmospheric turbulence conditions. It is concluded that when the variance of the phase and pointing errors are below certain thresholds, signal combining with a single array of detectors yields significantly better performance than a multiple arrays receiver. In the final part of the paper, we examine the probability of error of the single detector array receiver as a function of the beam radius, and the probability of error is minimized by (numerically) optimizing the beam radius of the received signal beams.","['MIMO', 'Optical fiber communication', 'Error analysis']","['Array of detectors', 'beam radius', 'equal gain combiner', 'maximal ratio combiner', 'multiple-input single-output', 'pointing error', 'strong atmospheric turbulence']"
"Full-duplex amplify-and-forward multiple-input multiple-output relaying has been the focus of several recent studies, due to the potential for achieving a higher spectral efficiency and lower latency, together with inherent processing simplicity. However, when the impact of hardware distortions are considered, such relays suffer from a distortion-amplification loop, due to the inter-dependent nature of the relay transmit signal covariance and the residual self-interference covariance. The aforementioned behavior leads to a significant performance degradation for a system with a low or medium hardware accuracy. In this work, we analyze the relay transfer function as well as the mean squared-error performance of a full-duplex amplify-and-forward multiple-input multiple-output relaying communication, under the impact of collective sources of additive and multiplicative transmit and receive impairments. Building on the performed analysis, an optimization problem is devised to minimize the communication Mean Squared-Error and solved by employing the recently proposed penalty dual-decomposition framework. The proposed solution converges to a stationary point of the original problem via a sequence of convex quadratic programs, thereby enjoying an acceptable arithmetic complexity as the problem dimensions grow large. Numerical simulations verify the significance of the proposed distortion-aware design and analysis, compared to the common simplified approaches, as the hardware accuracy degrades.","['Interference cancellation', 'Transfer functions', 'Numerical simulation', 'Distortion', 'Hardware', 'MIMO', 'Complexity theory']","['Full-duplex', 'amplify-and-forward', 'multiple-input multiple-output relay', 'hardware impairments', 'penalty-dual-decomposition']"
"In this article, a new modulation scheme named Direct Current biased Optical Generalised Frequency Division Multiplexing (DCO-GFDM) for Visible Light Communications (VLC) is explored. The DCO-GFDM enhances spectral efficiency by slicing both grids on the time-frequency plane and uses a circularly rotating filter for pulse shaping. In the Circular Pulse shaped DCO-GFDM (CP-DCO-GFDM), Root Raised Cosine (RRC) pulse is widely employed. This destroys the orthogonality among sub-carriers and degrades the Bit Error Rate (BER) performance. Therefore, in this work, a Linearly Pulse shaped DCO-GFDM (LP-DCO-GFDM) is proposed to retain the orthogonality among the sub-carriers and simultaneously improve BER and Peak-to-Average Power Ratio (PAPR) performance. The performance of CP/LP-DCO-GFDM is studied under double sided clipping distortion that occurs at the Light Emitting Diode (LED). Presented simulation results agree well with the theoretical results. Additionally, we present an experimental validation of the LP-DCO-GFDM on a Universal Software Radio Peripherals (USRP) based VLC test bed. The experimental results, in terms of Error Vector Magnitudes (EVM)s, the received constellations, and the received spectrum in comparison with DCO-OFDM shows improved performance.","['Optical transmitters', 'Visible light communication', 'Pulse shaping methods', 'Light emitting diodes', 'Modulation', 'Frequency division multiplexing', 'Optical distortion']","['DC biased optical generalised frequency division multiplexing (DCO-GFDM)', 'error vector magnitudes (EVM)', 'linearly pulse shaped DCO-GFDM (LP-DCO-GFDM)', 'universal software radio peripherals (USRP)', 'visible light communications (VLC)']"
"Buffer-aided (BA) relaying improves the diversity of cooperative networks often at the cost of increasing end-to-end packet delays. This characteristic renders BA relaying unsuitable for delay-sensitive applications. However, the increased diversity makes BA relaying appealing for ultra-reliable communications. Towards enabling ultra-reliable low-latency communication (URLLC), we aim at enhancing BA relaying for supporting delay-sensitive applications. In this paper, reliable full-duplex (FD) network operation is targeted and for this purpose, hybrid relay selection algorithms are formulated, combining BA successive relaying (SuR) and delay- and diversity-aware (DDA) half-duplex (HD) algorithms. In this context, a hybrid FD DDA algorithm is presented, namely LoLa4SOR, switching between SuR and HD operation. Additionally, a low-complexity distributed version is given, namely d-LoLa4SOR, providing a trade-off among channel state information requirements and performance. The theoretical analysis shows that the diversity of LoLa4SOR equals to two times the number of available relays K, i.e., 2K, when the buffer size L is greater than or equal to 3. Comparisons with other HD, SuR and hybrid algorithms reveal that LoLa4SOR offers superior outage and throughput performance while, the average delay is reduced due to SuR-based FD operation and the consideration of buffer state information for relay-pair selection. d-LoLa4SOR, as one of the few distributed algorithms in the literature, has a reasonable performance that makes it a more practical approach.","['Relays', 'Delays', 'Throughput', 'Diversity reception', 'Rough surfaces', 'Diversity methods', 'Reliability']","['Buffer-aided relaying', 'diversity', 'full-duplex communication', 'low-latency', 'relay selection', 'successive relaying']"
"Nowadays, mobile communication radio frequency transceivers for LTE-A/5G usually operate in frequency-division duplex mode and support carrier aggregation. The resulting complexity requires compromises in the analog front-end. One consequence is a limited stop-band attenuation of the duplex filters that separate the transmit and the receive paths. The resulting transmit leakage signal combined with other non-idealities, like power amplifier (PA) harmonics, can create self-interference problems that desensitize the receivers. In this work, we propose an optimized digital self-interference cancellation scheme to mitigate the unacceptable performance degradation caused by PA induced harmonics. We derive an appropriate interference model and quantify the interference power levels for a specific PA. Based on these results, our low-complex cancellation approach solely targets the dominating components. Our algorithm relies on the auto-covariance matrix for the second and third power of LTE signals, which we derive for the first time in an analytic form. This statistical information can be combined with the transform-domain concept to noticeably improve the performance of a least-mean squares based cancellation algorithm. The capabilities of the approach are demonstrated by means of simulations and are validated on measurement data.","['Radio frequency', 'Harmonic analysis', 'Interference cancellation', 'Transceivers', 'Distortion', 'Power system harmonics', 'Steady-state']","['Adaptive filters', 'interference cancellation', 'LTE', 'PA nonlinearity', 'RF transceivers']"
"Intelligent reflecting surface (IRS) is a revolutionary and low-cost technology for boosting the spectrum and energy efficiencies in future wireless communication network. In order to create controllable multipath transmission in the conventional line-of-sight (LOS) wireless communication environment, an IRS-aided directional modulation (DM) network is considered. In this paper, to improve the transmission security of the system and maximize the receive power sum (Max-RPS), two alternately optimizing schemes of jointly designing receive beamforming (RBF) vectors and IRS phase shift matrix (PSM) are proposed: Max-RPS using general alternating optimization (Max-RPS-GAO) algorithm and Max-RPS using zero-forcing (Max-RPS-ZF) algorithm. Simulation results show that, compared with the no-IRSassisted scheme and no-PSM optimization scheme, the proposed IRS-assisted Max-RPS-GAO method and Max-RPS-ZF method can significantly improve the secrecy rate (SR) performance of the DM system. Moreover, compared with the Max-RPS-GAO method, the proposed Max-RPS-ZF method has a faster convergence speed and a certain lower computational complexity.","['Artificial intelligence', 'Array signal processing', 'Wireless communication', 'Signal to noise ratio', 'Iterative methods', 'Transmitters', 'Security']","['Intelligent reflecting surface', 'directional modulation', 'secrecy rate', 'receive beamforming', 'receive power sum']"
"Programmable network hardware is emerging as a viable option for offloading and thus accelerating network functions. However, the heterogeneous resources available in the network calls for a disaggregated deployment approach. Programmable switches, programmable Network Interface Cards (NICs), and in-network compute nodes exposes different peculiar resources and capabilities that can be maximally exploited only if the network functions are decomposed into multiple smaller network functions. This work presents a framework for the automatic deployment of disaggregated and decomposed network functions. The framework comprises an orchestrator capable of deploying the decomposed network functions on programmable network hardware and software switches running in containers. The orchestrator exploits an optimization model for choosing the best decomposition according to the traffic demands, the network topology, and other constraints. An improved seamless deployment model and heuristic also accounts for the necessity of rerouting traffic when hardware nodes need to be re-programmed. Furthermore, the framework provides a tool to combine multiple functions into a single P4 program via a template pipeline that can be deployed to a programmable switch. Numerical results highlight the advantages of offloading decomposed network functions to programmable network hardware. Furthermore, we show how the seamless deployment model and heuristic have negligible effects on the allocation time and accepted traffic requests while guaranteeing the rerouting of traffic when switches are put in maintenance mode.","['Hardware', 'Optimization', 'Software', 'Pipelines', 'Numerical models', 'Heterogeneous networks', 'Computational modeling']","['Network function virtualization', 'software defined networking', 'programmable networks']"
"We consider a method for developing a radio-wave propagation prediction model in a mountainous forested area. A new path loss development approach uses a free-space path loss (FSPL) model and an empirical path loss model. To improve the prediction accuracy, the transmission path distance, free space area, and forest area were calculated separately. We obtained the transmission path distance for free space and forest areas from the digital surface model (DSM), which represents surface elevation information, including vegetation and object height. In this study, the results showed that by combining the empirical model with FSPL for free space area, the accuracy for all the empirical models was improved. We confirmed that the transmission distance calculation of the free space area and forest area with a combination of the empirical models showed a better performance than the model with physical distance. The predicted model results were validated using the actual radio wave propagation in the 920 MHz band measurement data. The overall path loss prediction accuracy was improved for the empirical models average of 8.05 dB on the experimental data.","['Forestry', 'Predictive models', 'Data models', 'Propagation losses', 'Transmitting antennas', 'Radio transmitters', 'Mathematical models']","['Digital surface model', 'LoRa', 'path loss prediction', 'modified empirical model', 'drone mapper']"
"This paper proposes a buffer-aided relay selection that is capable of exploiting both half-duplex and virtual full-duplex (VFD) transmissions in a hybrid manner. In the proposed scheme, we consider a two-hop cooperative network where each node is equipped with a single antenna, and no direct link between the source to the destination exists. The proposed scheme consists of five relay selection modes: two unicast modes, a broadcast mode, a cooperative beamforming mode, and a VFD mode. Owing to the broadcast mode, multiple relay nodes can seamlessly have a common packet; this mode is capable of solving the inter-relay interference problem imposed on the VFD mode. The proposed scheme's theoretical performance bounds on the throughput, the outage probability, and the delay are derived. It is demonstrated in our simulations that the proposed scheme attains higher performance in terms of a throughput and an outage probability than the conventional schemes while maintaining an acceptable average end-to-end delay profile.","['Relays', 'Unicast', 'Delays', 'Probability', 'Power system reliability', 'Array signal processing', 'Rough surfaces']","['Buffer', 'half-duplex', 'outage probability', 'relay selection', 'theoretical analysis', 'virtual full-duplex']"
"Algorithms are provided to build a large set of unique complex-valued orthogonal space-time block codes (STBCs) from a known or standard STBC. A physical layer security (PLS) scheme is proposed to take advantage of this set by alternating the STBC in use over a multiple-input single-output (MISO) or multiple-input multiple-output (MIMO) communications link between base station (BS) and user equipment (UE). A practical procedure is proposed and demonstrated to build individual STBCs from the set without use of a lookup table. The sufficient statistic is given and proven to allow for maximal ratio combining (MRC) by the intended receiver for all STBCs in the set. An algorithm is offered for the UE to update the MRC matrix in use as the STBC alternates. Definitions are provided for cryptograms, key residues (KRs), key residue classes (KRCs), and message and cryptogram residue classes pertaining to STBC PLS schemes. These definitions are used to present analysis of the information-theoretic security of the proposed PLS scheme to include message and key equivocation. Theoretical expected bit error rate (BER) for a passive eavesdropper is proven and plotted along with Monte Carlo simulations for confirmation. Discussion of different attack models is provided. Cost and attack complexity are compared between the proposed scheme and two related techniques from the literature.","['MIMO communication', 'Bit error rate', 'Buildings', 'Network security', 'Physical layer', 'Block codes']","['Cryptogram residue class', 'key equivocation', 'key residue class', 'message equivocation', 'physical layer security', 'space-time block code']"
"In this paper, we propose novel reduced-complexity fast Fourier transform (FFT)-spread multicarrier faster-than-Nyquist (MFTN) signaling with power allocation for a frequency-selective fading channel. The information rate of the proposed MFTN signaling is derived by relying on the circulant approximation of the FTN-specific intersymbol interference matrix and noise covariance matrix. This allows us to constitute efficient calculations of precoding and weighting matrices. The power allocation coefficients are optimized such that the approximated information rate is maximized. Our simulation results demonstrate that the proposed scheme with power allocation achieves the bit error ratio (BER) performance close to the conventional eigenvalue-decomposition (EVD)-precoded FTN signaling counterpart that is optimal in terms of an achievable information rate while significantly reducing the computational complexity as low as the order ofO(NlogN). While the proposed scheme exhibits a high peak-to-average-power ratio similar to the conventional EVD-precoded counterpart due to the effects of FFT-based precoding, it achieves better BER performance than the conventional open-loop single-carrier FTN signaling scheme and the Nyquist signaling scheme, employing the same root-raised cosine shaping filter. It is also confirmed that the proposed MFTN signaling scheme does not suffer from any significant bandwidth broadening.","['Resource management', 'Frequency-selective fading channels', 'Information rates', 'Covariance matrices', 'Receivers', 'Precoding', 'Bandwidth']","['Cyclic prefix', 'eigenvalue decomposition', 'faster-than-Nyquist signaling', 'fast Fourier transform', 'frequency-selective channel', 'low-complexity detection', 'Nyquist criterion', 'power allocation']"
"This paper investigates the communication reliability for single-input-multiple-output wireless systems with low-resolution phase quantizers. First, the maximum-likelihood detector withn-bit phase quantization is derived when there areNantennas at the receiver. Then, three low-complexity antenna selection strategies for data detection are proposed and their symbol error probability performance is characterized. It is shown that having 3 or more bits is sufficient to attain the full diversity orderN, achievable with infinite-bit quantizers, for quadrature phase shift keying modulation under Rayleigh fading. In particular, it is established that the proposed low-complexity max-distance and max-norm antenna selection strategies perform the same as the maximum-likelihood detector in terms of the asymptotic system reliability forn≥3. On the other hand, the diversity order decreases dramatically fromNtoN2whennis equal to 2, as illustrated by our numerical results and proven for the case ofN=2. An extensive numerical and simulation study is performed to illustrate the accuracy of the derived results and asymptotic system reliability performance as well as verifying our hypotheses in the high signal-to-noise ratio regime.","['Detectors', 'Quantization (signal)', 'Signal to noise ratio', 'Reliability', 'Receiving antennas', 'Wireless communication', 'Phase shift keying']","['Low-resolution quantization', 'ML detectors', 'selection combining', 'symbol error probability', 'diversity order']"
"Optical Wireless Communication (OWC) has attracted significant attention from academia and industry as a promising solution to complement traditional radio frequency communication. In Light-Fidelity (LiFi) networks, to enable soft handover and provide robustness against blockage of the line-of-sight links, LiFi receivers are designed to see multiple LiFi transmitters within their field-of-view. To trade-off spatial multiplexing with harmful interference, a proper user scheduling technique is required. In this work, we first derive practical and exact expressions for determining which simultaneous transmissions will create harmful interference and should be scheduled in multiple time-slots. Afterwards, relying on this expression, we propose our semi-distributed Spatially Extended TDMA algorithm, which is based on a time division contention–free access scheme. In contrast to more complex algorithms in the state-of-the-art, our scheduling protocol can reduce the computation time while also achieving a superior minimal user throughput. For a scenario with 9 transmitters and up to 15 receivers, our protocol is at least a factor 120 faster and can improve the minimal user throughput up to 91%, making it suitable for practical roll-out as a promising solution for very dense LiFi networks.","['Light emitting diodes', 'Light fidelity', 'Optical transmitters', 'Interference', 'Receivers', 'Protocols', 'Time division multiple access']","['Access protocols', 'optical wireless communication', 'scheduling algorithms', 'time division multiple access']"
"The development of next generation wireless communication systems focuses on the expansion of existing technologies, while ensuring an accord between various devices within a system. In this article, we target the aspect of precoder design for simultaneous wireless information and power transmission (SWIPT) in a multi-group (MG) multicasting (MC) framework capable of handling heterogeneous types of users, viz., information decoding (ID) specific, energy harvesting (EH) explicit, and/or both ID and EH operations concurrently. Precoding is a technique well-known for handling the inter-user interference in multi-user systems, however, the joint design with SWIPT is not yet fully exploited. Herein, we investigate the potential benefits of having a dedicated precoder for the set of users with EH demands, in addition to the MC precoding. We study the system performance of the aforementioned system from the perspectives of weighted sum of signal-to-interference-plus-noise-ratio (SINR) and fairness. In this regard, we formulate the precoder design problems for (i) maximizing the weighted sum of SINRs at the intended users and (ii) maximizing the minimum of SINRs at the intended users; both subject to the constraints on minimum (non-linear) harvested energy, an upper limit on the total transmit power and a minimum SINR required to close the link. We solve the above-mentioned problems using distinct iterative algorithms with the help of semi-definite relaxation (SDR) and slack-variable replacement (SVR) techniques, following suitable transformations pertaining the problem convexification. The main novelty of the proposed approach lies in the ability to jointly design the MC and EH precoders for serving the heterogeneously classified ID and EH users present in distinct groups, respectively. We illustrate the comparison between the proposed weighted sum-SINR and fairness models via simulation results, carried out under various parameter values and operating conditions.","['Interference', 'Signal to noise ratio', 'Optimization', 'Precoding', 'Receivers', 'Wireless communication', 'Wireless sensor networks']","['Multi-group (MG) multicast (MC) precoding', 'simultaneous wireless information and power transfer (SWIPT)', 'system fairness', 'weighted sum-SINR optimization']"
"Visible light communication (VLC) is considered a new technology for interconnecting devices in 5G and beyond. VLC can provide additional bandwidth for communicating devices which can help to reduce the radio frequency bandwidth congestion. In this work, we aim to maximize the bandwidth usages for VLC. The proposed modulation technique can impose more bits due to the combined utilization of both pulse height and width changes. Moreover, we subdivide the single bit duration to impose more bits for data transmission. The proposed modulation is capable of transmitting more bits by utilizing fewer optical pulses than other traditional modulation techniques such as multilevel modulation, on-off keying, pulse width modulation, pulse position modulation, pulse amplitude modulation, and multiple pulse position modulation. We have evaluated the theoretical bit error rate (BER) expression in terms of average signal to noise ratio and compared the experimental BER performance for the proposed system. The experimental results demonstrate that the proposed modulation technique can achieve a communication distance of 3 m in an indoor environment.","['Modulation', 'Pulse width modulation', 'Visible light communication', 'Bandwidth', 'Bit error rate', 'Cameras', 'Brightness']","['Modulation', 'visible light communication', 'optical communication system', 'indoor VLC']"
"Spatial-multiplexing multiple-input multiple-output (MIMO) systems have been developed and enhanced over the past two decades. In particular, a great amount of effort has gone towards development of capacity achieving detectors with affordable computational complexity. The developed detectors may be broadly divided into two classes: (i) deterministic sampling, such as list sphere decoding detector; and (ii) stocastic sampling, such as those based on Markov chain Monte Carlo (MCMC) search schemes. This paper proposes a novel detection scheme that is based on stochastic sampling, but is fundamentally different from the MCMC detectors. While MCMC follows a set of sequential sampling steps, hence, the sample sets obtained are highly correlated, the method proposed in this paper takes stochastic samples that are completely independent. This new approach of stochastic sampling leads to a detector with significantly reduced complexity. It also allows reduction in the detector latency.","['Detectors', 'MIMO communication', 'Quadrature amplitude modulation', 'Maximum likelihood decoding', 'Markov processes', 'Signal to noise ratio', 'Sampling methods']","['MIMO communications', 'soft detector', 'stochastic detector']"
"Recent advancements in the domain of Network Function Virtualization (NFV), and rollout of next-generation networks have necessitated the requirement for the upkeep of latency-critical application architectures in future networks and communications. While Cloud service providers recognize the evolving mission-critical requirements in latency sensitive verticals such as autonomous driving, multimedia, gaming, telecommunications, and virtual reality, there is a wide gap to bridge the Quality of Service (QoS) constraints for the end-user experience. Most latency-critical services are over-provisioned on all fronts to offer reliability, which is inefficient towards scalability in the long run. To address this, we propose a strategy to model frequent violations on the application level as a multi-output target to enable more complex decision-making in the management of virtualised communication networks. In this work, we utilize data from a real-world deployment to configure and draft a realistic set of Service Level Objectives (SLOs) for a voice based NFV application, and develop a deep neural network based multi-label classification methodology to identify and predict multiple categories of SLO breaches associated with an application state. With this, we aim to gain granular SLA and SLO violation insights, enabling us to study and mitigate their impact and inform precision in drafting proactive scaling policies. We further compare the performance against a set of multi-label compatible machine learning classifiers, and address class imbalance in a multi-label setup. We perform a comprehensive evaluation to assess the performance on example-based, label-based and ranking-based measures, and demonstrate the suitability of deep learning in such a use-case.","['Quality of service', 'Predictive models', 'Deep learning', 'Service level agreements', 'Measurement', 'Machine learning', 'Cloud computing']","['Network function virtualization', 'machine learning', 'deep learning', 'neural networks', 'classification algorithms', 'multi-label classification', 'prediction methods', 'quality of service', 'service level agreements', 'quality of experience', 'supervised learning', 'artificial neural networks', 'multi-layer neural network', 'naive Bayes methods', 'random forests', 'decision trees', 'boosting', 'support vector machines', 'imbalanced classification', 'probabilistic classification']"
"In distributed device-to-device (D2D) communications, no common reference time is available and the devices must employ distributed synchronization techniques. In this context, pulse-based synchronization, which can be implemented by distributed phase-locked loops is preferred due to its scalability. Several factors degrade the performance of pulse-based synchronization, such as duplexing scheme, clock skew and propagation delays. Furthermore, in distributed networks, devices should be aware of the synchronization status of others in order to initiate data communications. To address these prevailing issues, we first introduce a half-duplex timing-advance synchronization algorithm wherein each device alternates between being a transmitter and receiver in their exchange of synchronization pulses at each clock period. Based on this algorithm, we propose a novel fully-distributed pulse-based synchronization protocol for half-duplex D2D communications in 5G wireless networks. The protocol allows participating devices to become aware of the global synchronization status, so that they can complete the synchronization process ideally at the same time and proceed to data communication. In simulation experiments over multi-path frequency selective channels, the proposed synchronization protocol is shown to outperform a benchmark approach from the recent literature over a wide range of conditions, e.g., clock skew, number of devices, and network topology.","['Synchronization', 'Clocks', 'Device-to-device communication', 'Propagation delay', 'Protocols', 'Data communication', '5G mobile communication']","['Distributed synchronization', 'phase locked-loops', 'timing-advance', 'device-to-device communication', 'half-duplex', '5G']"
"This article focuses on computing resource allocation in Cloud Radio Access Networks. A game-based optimization algorithm was developed to distribute the computing resources among BaseBand Units (BBUs) in a BBU-pool whereby resources utilization is maximized. The model allocates computing resources on-demand, based on the instantaneous requests of BBUs, using a game-theory bargaining approach. In the case that the available resources are not sufficient to fulfil all instantiation requests, BBUs are prioritized to ensure the adequate Quality of Service, low-priority ones being always guaranteed a minimum computing resource to avoid them to crash. The performance of the proposed model is observed over time, concerning resource usage, BBU fulfilment level and efficiency. Simulations in a group of cells with a mixture of heterogeneous services in tidal traffic conditions show that resources allocated to BBUs are consistent with the priority of ongoing services and in line with real-time demand. Results also show that improving the average fulfilment level from 98% to 100% requires doubling the available resources at the cost of the average resources usage being cut in half.","['Resource management', 'Computational modeling', 'Servers', 'Quality of service', 'Real-time systems', 'Load modeling', '5G mobile communication']","['Cloud-RAN', 'computing resource utilization', 'resource allocation efficiency', 'wireless communications']"
"Some of the most serious security threats facing computer networks involve malware. To prevent malware-related damage, administrators must swiftly identify and remove the infected machines that may reside in their networks. However, many malware families have domain generation algorithms (DGAs) to avoid detection. A DGA is a technique in which the domain name is changed frequently to hide the callback communication from the infected machine to the command-and-control server. In this article, we propose an approach for estimating the randomness of domain names by superficially analyzing their character strings. This approach is based on the following observations: human-generated benign domain names tend to reflect the intent of their domain registrants, such as an organization, product, or content. In contrast, dynamically generated malicious domain names consist of meaningless character strings because conflicts with already registered domain names must be avoided; hence, there are discernible differences in the strings of dynamically generated and human-generated domain names. Notably, our approach does not require any prior knowledge about DGAs. Our evaluation indicates that the proposed approach is capable of achieving recall and precision as high as 0.9960 and 0.9029, respectively, when used with labeled datasets. Additionally, this approach has proven to be highly effective for datasets collected via a campus network. Thus, these results suggest that malware-infected machines can be swiftly identified and removed from networks using DNS queries for detected malicious domains as triggers.","['Organizations', 'Malware', 'Computer networks', 'Security', 'Servers', 'Communication networks']","['Domain generation algorithm', 'domain name system', 'malware', 'network security']"
"While deep learning (DL) technologies are now pervasive in state-of-the-art Computer Vision (CV) and Natural Language Processing (NLP) applications, only in recent years have these technologies started to sufficiently mature in applications related to wireless communications, a field loosely termed Radio Frequency Machine Learning (RFML). In particular, recent research has shown DL to be an enabling technology for Cognitive Radio (CR) applications as well as a useful tool for supplementing expertly defined algorithms for spectrum awareness applications such as signal detection, estimation, and classification. A major driver for the usage of RFML is that little, to no, a priori knowledge of the intended spectral environment is required, given that there is an abundance of representative raw Radio Frequency (RF) data to facilitate training and evaluation. However, in addition to this fundamental need for sufficient data, there are other key considerations, such as trust, security, and hardware requirements, that must be taken into account before deploying RFML systems in real-world wireless communication applications that largely go unaddressed in the current literature. This paper examines the prior works related to these major research considerations, with focus on the dependencies between them and factors unique to the RFML space.","['Maximum likelihood estimation', 'Radio frequency', 'Cognitive radio', 'Security', 'Wireless sensor networks', 'Routing', 'Modulation']","['Survey', 'deep learning', 'neural networks', 'radio frequency machine learning', 'spectrum awareness', 'dynamic spectrum access', 'cognitive radio', 'automatic modulation classification', 'specific emitter identification', 'signal detection']"
"The free-space optical (FSO) communication represents a promising futuristic technology due to several benefits, which has attracted significant attention recently for various applications. FSO communication is susceptible to fading due to atmospheric turbulence. FSO channel, being a slow fading channel, inherently does not allow the efficient use of error-correcting codes (ECCs). ECCs have shown to have good performance in a fast fading channel. A way to convert the slow fading channel into a fast fading channel is by using techniques like diversity. Diversity makes the bits pass through many channels simultaneously, and thus the fast fading channel effect is observed at the receiver. In this paper, we first derive the capacity and bit error rate performance of the low-density parity-check (LDPC) codes in a fast fading channel. We then study the performance of LDPC codes in receiver diversity systems. Finally, we see the impact of using LDPC coded multiple-input multiple-output (MIMO) systems in the FSO channel. We use regular (3,6) LDPC code and irregular LDPC codes of rate 0.5, where the desired degree distribution is optimized for the channel condition.","['Fading channels', 'Analytical models', 'Visualization', 'Parity check codes', 'Optical receivers', 'Adaptive optics', 'Space division multiplexing']","['LDPC code', 'fast fading channel', 'free-space optical communication', 'spatial multiplexing']"
"This article presents a novel application of the t-distributed Stochastic Neighbor Embedding (t-SNE) clustering algorithm to the telecommunication field. t-SNE is a dimensionality reduction algorithm that allows the visualization of large dataset into a 2D plot. We present the applicability of this algorithm in a communication channel dataset formed by several scenarios (anechoic, reverberation, indoor and outdoor), and by using six channel features. Applying this artificial intelligence (AI) technique, we are able to separate different environments into several clusters allowing a clear visualization of the scenarios. Throughout the article, it is proved that t-SNE has the ability to cluster into several subclasses, obtaining internal classifications within the scenarios themselves. t-SNE comparison with different dimensionality reduction techniques (PCA, Isomap) is also provided throughout the paper. Furthermore, post-processing techniques are used to modify communication scenarios, recreating a real communication scenario from measurements acquired in an anechoic chamber. The dimensionality reduction and classification by using t-SNE and Variational AutoEncoders show good performance distinguishing between the recreation and the real communication scenario. The combination of these two techniques opens up the possibility for new scenario recreations for future mobile communications. This work shows the potential of AI as a powerful tool for clustering, classification and generation of new 5G propagation scenarios.","['Antenna measurements', 'Reverberation', 'Artificial intelligence', 'Communication channels', 'Clustering algorithms', 'Dimensionality reduction', 'Wireless communication']","['reduction', 'propagation', 't-SNE', 'unsupervised learning', 'wireless communications']"
"In this paper, we consider a cooperative non-orthogonal multiple access (NOMA) aided underwater optical wireless system in which the source transmits to two users where the near user serves as a relay node to the far user. Our proposed system consists of multiple narrow-angle light-emitting diode (LED)/photodiode (PD) elements at the source, near user, and far user. In order to achieve communication, our system selects a single LED/PD at each node. We propose several low complexity LED/PD selection schemes that aim to maximize the link throughput and in addition consider optimal and random LED/PD selection for benchmarking. In order to characterize the performance of each scheme, bounds and closed-form tight approximations on the average achievable sum rates are presented. The use of multi element nodes and NOMA increase the average sum rate significantly over conventional orthogonal access. Moreover, near-optimal throughput can be achieved using channel gain based and line-of-sight based LED/PD selection schemes in the medium-to-high transmit power regimes. The derived expressions are also useful to investigate the impact of key system and channel parameters such as the source transmit power, power allocation factor, node placement, and the number of elements at each node.","['NOMA', 'Optical transmitters', 'Optical receivers', 'Wireless communication', 'Optical sensors', 'Light emitting diodes', 'Relays']","['Underwater optical wireless communication', 'cooperative non-orthogonal multiple access (NOMA)', 'performance analysis', 'lower bound', 'achievable sum rate', 'LED/PD selection']"
"Enabling accurate and automated identification of wireless devices is critical for allowing network access monitoring and ensuring data authentication for large-scale IoT networks. RF fingerprinting has emerged as a solution for device identification by leveraging the transmitters’ inevitable hardware impairments that occur during manufacturing. Although deep learning is proven efficient in classifying devices based on hardware impairments, the performance of deep learning models suffers greatly from variations of the wireless channel conditions, across time and space. To the best of our knowledge, we are the first to propose leveraging MIMO capabilities to mitigate the channel effect and provide a channel-resilient device classification framework. We begin by showing that for AWGN channels, combining multiple received signals improves the testing accuracy by up to 30%. We then show that for more realistic Rayleigh channels, blind channel estimation enabled by MIMO increases the testing accuracy by up to 50% when the models are trained and tested over the same channel, and by up to 69% when the models are tested on a channel that is different from that used for training.","['Fingerprint recognition', 'MIMO communication', 'Signal to noise ratio', 'Diversity reception', 'Training', 'Radio frequency', 'Wireless communication']","['Automated network access', 'deep learning', 'IoT device fingerprinting', 'multiple-input multiple-out (MIMO)']"
"Noncoherent demodulation is an attractive choice for many wireless communication systems. It requires minimal protocol overhead for carrier synchronization, and it is robust to radio impairments commonly found in low-cost transceivers. Machine learning techniques, such as neural networks and deep learning, offer additional benefits for these systems. Practical communication systems often include nonlinearities, non-stationarity, and non-Gaussian noise, which complicate mathematical derivation of optimum demodulators. Learning approaches can optimize demodulator performance directly from simulated or measured radio data, which is often plentiful in the design and verification of today's integrated transceivers. This paper examines several candidate neural network topologies for use in noncoherent demodulation and provides a mathematical framework for their comparison. Each is based on a complex-valued feature detection layer, which may be characterized as coherent or noncoherent, followed by one or more real-valued classification layers. Backpropagation equations for the noncoherent feature layer include a synchronization term that facilitates training with noncoherent input data. The coherent layer does not synchronize training data, however a noncoherent demodulator can still be constructed by increasing the coherent layer capacity and adding a max pooling layer to marginalize the unknown signal phase. A frequency classification example highlights the differences between the topologies and confirms that optimum noncoherent demodulation can be learned in the presence of AWGN and random phase offsets. The topologies considered here are suitable for noncoherent demodulation of power-efficient modulations such as FSK and ASK, which are typical in today's short-range wireless communication systems. It is hoped that such topologies will lead to a future common architecture that can support the wide range of modulation formats in this space.","['Demodulation', 'Neural networks', 'Topology', 'Mathematical model', 'Training', 'Protocols']","['Neural networks', 'complex-valued', 'demodulation', 'communications', 'signal processing']"
"Covert Timing Channels(CTCs) is a technique to leak information. CTCs only modify inter-arrival time sequence(IATs) between packets, consequently, traditional network security mechanisms, such as firewalls and proxies, can not effectively detect CTCs. If CTCs are maliciously utilized by criminals, will pose a great threat to network security. Classic CTCs detection methods, such as KS-test, Entropy-test, etc, not only have less universality and robustness, but also require more sampled IATs to detect CTCs, therefore, how to improve performance of detection methods against CTCs, has became a popular research in recent years. In this paper, a new CTCs detection method based on time series symbolization is proposed. It firstly converts the sampled IATs to symbolic time series, and regards each discrete value as a status. Then counts the times of transition for each status to status, and calculates the status transition probability matrix(STPM). Finally, it differentiates the label(overt or covert) of sampled IATs, by calculating similarity score. Experimental results about detection accuracy show that, in an ideal network environment, compared with classic methods, our method has better performance, with average accuracy of about 96%. Besides, our method has better performance as well, with the existence of network interference.","['Time series analysis', 'Entropy', 'Standards', 'Network security', 'Clustering algorithms', 'Receivers', 'Delays']","['Covert timing channel', 'detection method', 'network security', 'symbolization']"
"Internet of Things (IoT) is envisioned to expand Internet connectivity of the physical world, and the mobile edge cloud can be leveraged to enhance the resource-constrained IoT devices. The performance of the cloud-enhanced IoT applications depends on various system-wide information, such as the wireless channel states between IoT devices and their corresponding serving edge cloud nodes. However, with the semi-trusted edge resources and the public nature of wireless channels, public sharing of system information should be avoided to better balance the tradeoff between performance and security. In this paper, the benefits of local information exchange is investigated, where the privately-owned physical layer channel information is leveraged to extract lightweight keys. For the point-to-point wireless communications links with multiple passive eavesdroppers, the security metric in terms of conditional min-entropy is evaluated via the proposed Dynamic Bayesian Model. The proposed model can flexibly incorporate various dynamic information flows in the system and quantify the information leakage caused by wireless broadcasting. The rigorously defined and derived security metrics for such a key generation pipeline has been verified via the real-world collected time-varying wireless channel data. The designed model can achieve previously inconceivable security properties.","['Security', 'Wireless communication', 'Communication system security', 'Pipelines', 'Internet of Things', 'Measurement', 'Hidden Markov models']","['Conditional min-entropy', 'dynamic Bayesian model', 'key extraction', 'physical layer security']"
"First person view (FPV) technology for unmanned aerial vehicles (UAVs) provides an immersive experience for pilots and enables various personal and commercial applications such as aerial photography, drone racing, search and rescue operations, agricultural surveillance, and structural inspection. While real time video streaming from a UAV and vision-based collision avoidance strategies have been studied in literature as separate topics, in this paper we tackle collision avoidance in FPV scenarios, taking into account network delays and real time video parameters. We present a theoretical model for obstacle collisions that considers the current communication channel conditions, the real time video parameters, and the UAV's position relative to the closest obstacle. A video adaptation algorithm is then designed, using this metric, to tune the FPV video resolution, number of re-transmission attempts, and the modulation scheme to maximize the probability of avoiding collisions. This algorithm also takes into account specific latency constraints of the application. This video algorithm was evaluated in various scenarios and its ability to respond to both distances to the obstacle as well as the communication channel conditions was demonstrated. It was found that, for the considered scenarios, the performance of the proposed adaptive algorithm was, on an average, 58.63% higher than the closest non-adaptive one in terms of maximizing the probability of avoiding collision. Such collision avoidance strategies could be used to make UAV FPV applications safer and more reliable.","['Streaming media', 'Unmanned aerial vehicles', 'Collision avoidance', 'Optical sensors', 'Feeds', 'Real-time systems', 'Measurement']","['Collision avoidance', 'FPV', 'safety', 'UAV', 'video adaptation']"
"Numerous applications and devices use Global Navigation Satellite System (GNSS)-provided position, velocity and time (PVT) information. However, unintentional interference and malicious attacks render GNSS-provided information unreliable. Receiver Autonomous Integrity Monitoring (RAIM) is considered an effective and lightweight protection method when a subset of the available satellite measurements is affected. However, conventional RAIM Fault Detection and Exclusion (FDE) can be computationally expensive, due to iterative search to exclude faulty signals, in case of many faults and more so for multi-constellation GNSS receivers. Therefore, we propose a fast multiple fault detection and exclusion (FM-FDE) algorithm, to detect and exclude multiple faults for both single and multi-constellation receivers. The novelty is that FM-FDE can effectively exclude faults without an iterative search for faulty signals. FM-FDE calculates position distances of any subset pairs with max{3 + P, 2P} measurements, where P is the number of constellations. Then, the algorithm utilizes statistical testing to examine the distances and identify faulty measurements to exclude from the computation of the resultant PVT solution. We evaluate FM-FDE with synthesized faulty measurements in a collected data set; it shows that FM-FDE is practically equally effective as the conventional Solution Separation (SS) FDE in a single constellation receiver. The computational advantage of FM-FDE is more pronounced in a multi-constellation setting, e.g., being more efficient for GPS-Galileo receivers facing more than 2 faults across both constellations. The trade-off is that FM-FDE slightly degrades performance in terms of detection and false alarm probabilities with small errors, compared to the conventional SS FDE.","['Satellites', 'Receivers', 'Satellite broadcasting', 'Global navigation satellite system', 'Fault detection', 'Global Positioning System', 'Monitoring']","['Hypothesis testing', 'chi-square test', 'solution separation', 'FDE', 'multi-constellation']"
"The emergence of visible light communication (VLC) as a subset of optical wireless communication (OWC) in the early 2000s has turned any light-emitting diode (LED) source into a potential data transmitter. The design process for any VLC or OWC system typically involves a link budget analysis performed by studying the signal-to-noise ratio (SNR) at the receiver. Since this SNR strongly depends on the radiant flux collected by the receiver, an accurate model for this parameter is required. The point-source model has been widely used since 1979 and generally provides a good approximation of the received radiant flux. However, it might be less accurate for typical extended lighting sources like LED panels or large-area organic LEDs. In this paper, the radiant flux distribution of flat Lambertian rectangular or circular sources is derived from a vector analysis of their irradiance. It is then validated through actual measurements in the case of a circular source. The resulting extended-source models thus better capture the light beam pattern of such transmitters to enable a more accurate link budget. It provides at the same time almost identical results to the point-source model for small sources and can, therefore, be seen as a natural extension of this already widely used model.","['Optical transmitters', 'Light sources', 'Light emitting diodes', 'Signal to noise ratio', 'Optical receivers', 'Visible light communication']","['LED pattern', 'irradiance pattern', 'light power pattern', 'non-imaging optics', 'link budget', 'optical wireless communication', 'visible light communication']"
"The ever-increasing demands for higher data rates in telecommunications networks, along with the significant benefits offered by wireless communications, have pushed the technological developments to new frontiers. Free-space optical (FSO) communications are among the technologies that can address these emerging demands, offering large and unregulated bandwidth. In addition, recent advances in radio frequency (RF) systems have enabled the development of in-band full-duplex (FD) radios with the employment of advanced self-interference cancelation techniques. Therefore, in this paper we study a robust FD relaying system comprised of parallel hybrid FSO/RF communication links, where the coordination of transmissions between the FSO/RF subsystems is carried out by the hard-switching protocol. The operation of the RF links is impaired by Nakagami-mfading, the residual self-interference due to the FD relaying operation, and the in-phase and quadrature-phase imbalance effect due to imperfect RF front-ends. As far as the FSO links are concerned, we consider the influence of the joint effects of atmospheric turbulence, beam wander and pointing errors. We first derive analytical closed-form expressions for the outage probability of both subsystems as well as for the overall FD relaying hybrid system. Then, asymptotic expressions are extracted for the outage performance in the high signal-to-noise ratio regime. The achievable outage diversity orders of both FSO and RF subsystems are determined, which provide significant insight into the design of such hybrid systems in a wide variety of operating conditions. Finally, the numerical results are presented using the derived outcomes, and significant conclusions are drawn about FD relaying hybrid systems.","['Radio frequency', 'Relays', 'Fading channels', 'Signal to noise ratio', 'Probability', 'Power system reliability', 'Diversity reception']","['Hybrid RF/FSO systems', 'full-duplex relaying', 'Nakagami-m fading', 'self-interference', 'IQ imbalance', 'atmospheric turbulence', 'beam wander', 'pointing errors', 'outage probability']"
"The rolling-out of 5G antennas over the territory is a fundamental step to provide 5G connectivity. However, little efforts have been done so far on the exposure assessment from 5G cellular towers over young people and “sensitive” buildings, like schools and medical centers. To face such issues, we provide a sound methodology for the numerical evaluation of 5G (and pre-5G) downlink exposure over children, teenagers, schools and medical centers. We then apply the proposed methodology over two real scenarios. Results reveal that the exposure from 5G cellular towers will increase in the forthcoming years, in parallel with the growth of the 5G adoption levels. However, the exposure levels are well below the maximum ones defined by international regulations. Moreover, the exposure over children and teenagers is similar to the one of the whole population, while the exposure over schools and medical centers can be lower than the one of the whole set of buildings. Finally, the exposure from 5G is strongly lower than the pre-5G one when the building attenuation is introduced and a maturity adoption level for 5G is assumed.","['5G mobile communication', 'Buildings', 'Hospitals', 'Poles and towers', 'Pediatrics', 'Statistics', 'Sociology']","['5G networks', 'EMF analysis', 'population analysis']"
"With the rapid adoption of the Internet of Things, it is necessary to go beyond fifth-generation applications and apply stringent high reliability and low latency requirements, closely related to strict delay demands. These requirements support massive network connectivity for multiple Internet of Things devices. Hence, in this paper, we optimize energy efficiency and achieve quality-of-service requirements by mitigating co-channel interference, performing efficient power control of transmitters, and harvesting energy using time-slot exchanges. Due to a nonconvex optimization problem, we propose an iterative algorithm for power allocation and time slot interchange to reduce the computational complexity. To achieve a high degree of ultra-reliability and low latency with quality-of-service-aware instantaneous reward under massive connectivity, we efficiently employ multiagent reinforcement learning by addressing the intelligent resource management problem via a novel Double Deep Q Network. The network prioritizes experience replay to exploit the best policy and maximize accumulative rewards. It also learns the optimal policy and enhances learning efficiency by maximizing its reward function to make decisions with high intelligence and guarantee strict ultra-reliability and low latency. The simulation result shows that the Double Deep Q Network with prioritized experience replay can guarantee stringent ultra-reliability and low latency. As a result, the co-channel interference between transmission links and the high-power consumption density associated with the massive connectivity of the Internet of Things devices are mitigated.","['Internet of Things', 'Reliability', 'Ultra reliable low latency communication', 'Resource management', 'Quality of service', 'Performance evaluation', 'Interference']","['Internet of things', 'beyond fifth-generation', 'energy efficiency', 'massive connectivity']"
"Magnetic induction communication (MIC) demonstrates high penetration efficiency and low propagation loss in extreme environments. It is used in underground and underwater environments to enable critical applications that cannot be achieved by using terrestrial wireless techniques. This article studies the channel and antenna modeling for MIC in inhomogeneous environments, where a transmitter and a receiver are in two different media. This problem finds a large number of practical applications such as communicating with wireless sensors in the soil, water, walls, and the human body using a wireless device in the air. This article develops a joint channel and antenna model considering the inhomogeneity of the surrounding environments. An exact full-wave model and a near field approximation model are developed and evaluated to show the characteristics of this novel wireless technique as well as the limitations of existing solutions. Using the communication with underground sensors as an example, the results show that the path loss of MIC strongly depends on the propagation media which is drastically different from electromagnetic wave-based wireless communications in terrestrial environments. Also, we find that the MIC channel in inhomogeneous media exhibits non-reciprocity due to different transmission coefficients through the boundary. Moreover, the boundary can be neglected when the carrier frequency is sufficiently low. The developed model provides a tool to design wireless sensor networks in inhomogeneous extreme environments.","['Microwave integrated circuits', 'Nonhomogeneous media', 'Sensors', 'Wireless communication', 'Soil', 'Transmitters', 'Coils']","['Channel model', 'inhomogeneous media', 'inter-media communication', 'magnetic induction communication', 'path loss']"
"Sparse code multiple access (SCMA), as a code-domain non-orthogonal multiple access (NOMA) scheme, has received considerable research attention for enabling massive connectivity in future wireless communication systems. In this paper, we present a novel codebook (CB) design for SCMA based visible light communication (VLC) system, which suffers from shot noise. In particular, we introduce an iterative algorithm for designing and optimizing CB by considering the impact of shot noise at the VLC receiver. Based on the proposed CB, we derive and analyze the theoretical bit error rate (BER) expression for the resultant SCMA-VLC system. The simulation results show that our proposed CBs outperform CBs in the existing literature for different loading factors with much less complexity. Further, the derived analytical BER expression well aligns with simulated results, especially in high signal power regions.","['Visible light communication', 'Optical transmitters', 'Optical receivers', 'Optical noise', 'Thermal noise', 'NOMA', 'Bit error rate']","['sparse code multiple access (SCMA)', 'visible light communication (VLC)']"
"Vehicular communications are the key enabler of traffic reduction and road safety improvement referred to as cellular vehicle-to-everything (C-V2X) communications. Considering the numerous transmitting entities in next generation cellular networks, most existing resource allocation algorithms are impractical or non-effective to ensure reliable C-V2X communications which lead to safe intelligent transportation systems. We study a centralized framework to develop a low-complexity, scalable, and practical resource allocation scheme for dense C-V2X communications. The NP-hard sum-rate maximization resource allocation problem is formulated as a mixed-integer non-linear non-convex optimization problem considering both cellular vehicular links (CVLs) and non-cellular VLs (NCVLs) quality-of-service (QoS) constraints. By assuming that multiple NCVLs can simultaneously reuse a single cellular link (CL), we propose two low-complexity sub-optimal matching-based algorithms in four steps. The first two steps provide a channel-gain-based CVL priority and CL assignment followed by an innovative scalable min-max channel-gain-based CVL-NCVL matching. We propose an analytically proven closed-form fast feasibility check theorem as the third step. The objective function is transformed into a difference of convex (DC) form and the power allocation problem is solved optimally using majorization-minimization (MaMi) method and interior point methods as the last step. Numerical results verify that our schemes are scalable and effective for dense C-V2X communications. The low-complexity and practicality of the proposed schemes for dense cellular networks is also shown. Furthermore, it is shown that the proposed schemes outperform other methods up to 6% in terms of overall sum-rate in dense scenarios and have a near optimal performance.","['Resource management', 'Device-to-device communication', 'Wireless networks', 'Cellular networks', 'Quality of service', 'Autonomous vehicles', 'Safety']","['Cellular vehicle-to-everything (C-V2X)', 'dense cellular networks', 'next generation cellular networks', 'resource allocation', 'sidelink enhancement', 'spectral efficiency']"
"Sparse signals, encountered in many wireless and signal acquisition applications, can be acquired via compressed sensing (CS) to reduce computations and transmissions, crucial for resource-limited devices, e.g., wireless sensors. Since the information signals are often continuous-valued, digital communication of compressive measurements requires quantization. In such a quantized compressed sensing (QCS) context, we address remote acquisition of a sparse source through vector quantized noisy compressive measurements. We propose a deep encoder-decoder architecture, consisting of an encoder deep neural network (DNN), a quantizer, and a decoder DNN, that realizes low-complexity vector quantization aiming at minimizing the mean-square error of the signal reconstruction for a given quantization rate. We devise a supervised learning method using stochastic gradient descent and backpropagation to train the system blocks. Strategies to overcome the vanishing gradient problem are proposed. Simulation results show that the proposed non-iterative DNN-based QCS method achieves higher rate-distortion performance with lower algorithm complexity as compared to standard QCS methods, conducive to delay-sensitive applications with large-scale signals.","['Quantization (signal)', 'Decoding', 'Rate-distortion', 'Wireless communication', 'Complexity theory', 'Wireless sensor networks', 'Signal reconstruction']","['Compressed sensing', 'data compression', 'feedforward neural networks', 'supervised learning', 'vector quantization']"
"The millimeter wave (mmWave) bands have attracted considerable attention for high precision localization applications due to the ability to capture high angular and temporal resolution measurements. This paper explores mmWave-based positioning for a target localization problem where a fixed target broadcasts mmWave signals and a mobile robotic agent attempts to capture the signals to locate and navigate to the target. A three-stage procedure is proposed: First, the mobile agent uses tensor decomposition methods to detect the multipath channel components and estimate their parameters. Second, a machine-learning trained classifier is then used to predict the link state , meaning if the strongest path is line-of-sight (LOS) or non-LOS (NLOS). For the NLOS case, the link state predictor also determines if the strongest path arrived via one or more reflections. Third, based on the link state, the agent either follows the estimated angles or uses computer vision or other sensor to explore and map the environment. The method is demonstrated on a large dataset of indoor environments supplemented with ray tracing to simulate the wireless propagation. The path estimation and link state classification are also integrated into a state-of-the-art neural simultaneous localization and mapping (SLAM) module to augment camera and LIDAR-based navigation. It is shown that the link state classifier can successfully generalize to completely new environments outside the training set. In addition, the neural-SLAM module with the wireless path estimation and link state classifier provides rapid navigation to the target, close to a baseline that knows the target location.","['Navigation', 'Wireless communication', 'Location awareness', 'Estimation', 'Wireless sensor networks', 'Robots', 'Simultaneous localization and mapping']","['Millimeter wave', 'positioning', 'SLAM', 'robotics', 'navigation', '5G']"
"This work investigates the performance of a laser-powered unmanned aerial vehicle (UAV) based hybrid wireless network consisting of a UAV-mounted base station (UAV-BS), cellular user, a low-power IoT user, and a secondary IoT network consisting of a group of multiple IoT devices. Communication among these devices takes place in two different phases. In the first phase of transmission, UAV-BS fulfills its power requirements by harvesting power from a distributed laser charging-based laser power transfer and uses non-orthogonal multiple access (NOMA) signaling to transmit the information to both the users. Further, in the second phase of transmission, the IoT user harvests power using power-splitting (PS) protocol and uses it to communicate with the selected IoT device, selected using a signal to interference-plus-noise ratio based selection strategy from the secondary IoT network, while the cellular user transmits uplink information to UAV-BS. We discuss the effect of non-linear energy harvesting on the performance of secondary IoT network and UAV-BS. Next, we analyze the performance of the proposed system by deriving the closed-form expressions of the outage probabilities, throughput, and ergodic capacity of both the users, the secondary IoT network, and UAV-BS. Furthermore, we find the optimal values of power coefficient and target rates that maximize the throughput of the IoT user while attaining a desired throughput for the cellular user. We also demonstrate that a judicious choice of the power allocation coefficient is essential in order to maximize the sum throughput of the system and hence the energy efficiency. Simulation results verify the accuracy of derived expressions and validate the effectiveness of proposed algorithms.","['Internet of Things', 'NOMA', 'Throughput', 'Power lasers', 'Performance evaluation', 'Autonomous aerial vehicles', 'Wireless networks']","['Internet of Things (IoT)', 'laser power transfer', 'non-linear energy harvesting', 'nonorthogonal multiple access (NOMA)', 'unmanned aerial vehicle (UAV)']"
"The ever-increasing demand for applications with stringent constraints in device density, latency, user mobility, or peak data rate has led to the appearance of the last generation of mobile networks (i.e., 5G). However, there is still room for improvement in the network spectral efficiency, not only at the waveform level but also at the Radio Resource Management (RRM). Up to now, solutions based on multicast transmissions have presented considerable efficiency increments by successfully implementing subgrouping strategies. These techniques enable more efficient exploitation of channel time and frequency resources by splitting users into subgroups and applying independent and adaptive modulation and coding schemes. However, at the RRM, traditional multiplexing techniques pose a hard limit in exploiting the available resources, especially when users’ QoS requests are unbalanced. Under these circumstances, this paper proposes jointly applying the subgrouping and Non-Orthogonal Multiple Access (NOMA) techniques in 5G to increase the network data rate. This study shows that NOMA is highly spectrum-efficient and could improve the system throughput performance in certain conditions. In the first part of this paper, an in-depth analysis of the implications of introducing NOMA techniques in 5G subgrouping at RRM is carried out. Afterward, the validation is accomplished by applying the proposed approach to different 5G use cases based on vehicular communications. After a comprehensive analysis of the results, a theoretical approach combining NOMA and time division is presented, which improves considerably the data rate offered in each use case.","['NOMA', '5G mobile communication', 'Resource management', 'Cost function', 'Throughput', 'Quality of service', 'Time-frequency analysis']","['RRM', 'subgrouping', 'wireless communications']"
"In reconfigurable intelligent surface (RIS)-assisted communications, the signal-to-noise ratio (SNR) is improved by optimizing the reflecting elements’ phases to make the reflected signals add coherently at the receiver. Nevertheless, the RIS cannot control the phase of the direct link if it exists. Although the RIS phase can still be controlled such that the direct and reflected signals add coherently, the SNR gain might be hindered. Therefore, this paper considers the performance analysis of such scenarios where a novel analytical framework is developed to evaluate the SNR, outage probability, and bit error rate (BER). To capture a broad range of fading conditions, the channels are modeled as independent but not identically distributed generalized \kappa- \mushadowed fading channels. The Laplace transform is used to derive an accurate approximation of the probability density function (PDF) and cumulative distribution function (CDF) of the instantaneous fading, which are used to derive the PDF and CDF of the instantaneous SNR. The paper also considers deriving the asymptotic PDF, CDF, moment generating function (MGF) of the SNR, as well as the outage probability and BER. The obtained results show that a strong direct link may limit the gain obtained using the RIS.","['Channel models', 'Signal to noise ratio', 'Rician channels', 'Rayleigh channels', 'Gamma distribution', 'Probability density function', 'Internet of Things']","['κ-μ shadowed fading', 'RIS', 'intelligent surfaces', 'diversity order', 'outage probability', 'MGF', 'BER', 'sixth generation (6G)']"
"Content caching at the edge of network is a promising technique to alleviate the burden of backhaul networks. In this paper, we consider content caching along time in a base station with limited cache capacity. As the popularity of contents may vary over time, the contents of cache need to be updated accordingly. In addition, a requested content may have a delivery deadline within which the content needs to be obtained. Motivated by these, we address optimal scheduling of content caching in a time-slotted system under delivery deadline and cache capacity constraints. The objective is to minimize a cost function that captures the load of backhaul links. For our optimization problem, we prove its NP-hardness via a reduction from the Partition problem. For problem solving, via a mathematical reformulation, we develop a solution approach based on repeatedly applying a column generation algorithm and a problem-tailored rounding algorithm. In addition, two greedy algorithms are developed based on existing algorithms from the literature. Finally, we present extensive simulations that verify the effectiveness of our solution approach in obtaining near-to-optimal solutions in comparison to the greedy algorithms. The solutions obtained from our solution approach are within 1.6% from global optimality.","['Greedy algorithms', 'Servers', 'Optimal scheduling', 'Partitioning algorithms', 'Base stations', '5G mobile communication']","['Base station', 'content caching', 'deadline', 'scheduling', 'time-varying popularity']"
"In next-generation wireless networks, relay-based packet forwarding, emerged as an appealing technique to extend network coverage while maintaining the required service quality. The incorporation of multiple frequency bands, ranging from MHz/GHz to THz frequencies, and their opportunistic and/or simultaneous exploitation by relay nodes can significantly improve system capacity, however at the risk of increased packet latency. Since a relay node can use different bands to send and receive packets, there is a pressing need to design an efficient channel allocation algorithm without a central oracle. While existing greedy heuristics and game-theoretic techniques, which were developed for multi-band channel assignment to relay nodes, achieve minimum packet latency, their performance drops significantly when network dynamism (i.e., user mobility, non-quasi-static channel conditions) is introduced. Since this problem involves multiple relay nodes, we model it as a Markov Decision Process (MDP) involving various stages, which essentially means that achieving an optimal and stable solution is a computationally hard problem. Since solving the MDP, traditionally, consumes a great deal of time and is intractable for relay nodes, we explore how to approximate the optimal solution in a distributed manner by reformulating a reinforcement learning-based, smart channel adaptation problem in the considered multi-band relay network. By customizing a Q-Learning algorithm that adopts an epsilon-greedy policy, we can solve this re-formulated reinforcement learning problem. Extensive computer-based simulation results demonstrate that the proposed reinforcement learning algorithm outperforms the existing methods in terms of transmission time, buffer overflow, and effective throughput. We also provide the convergence analysis of the proposed model by systematically finding and setting the appropriate parameters.","['Channel allocation', 'Relay networks (telecommunication)', 'Topology', 'Optimization', 'Network topology', 'Interference', 'Signal to noise ratio']","['Machine learning', 'reinforcement learning', 'Markov decision process', 'Q-learning', 'multi-band communication', 'relay network']"
"This work investigates the performance of a downlink non-orthogonal multiple access (NOMA) based coordinated low-earth orbit (LEO) satellite system. Two LEO satellites are assumed to coordinate their transmissions to serve three users simultaneously using NOMA protocol, where only one user lies in the intersection of the footprints of the two satellites. We investigate the reliability of the proposed architecture, which is expressed in terms of the outage probability (OP). We derive closed-form expressions of the users’ OPs taking into considerations different realistic losses effects on the link budget including receiver antenna gain, satellite antenna gain, antennas pointing errors, shadowing, small-scale fading, and large-scale free space path loss. The channels between satellites and the three users are assumed to follow shadowed-Rician (SR) fading. The mathematical analysis is verified by the extensive representative Monto-Carlo simulations. Finally, we demonstrate the impact of the system parameters on the considered system as well as the superiority of the NOMA scheme over conventional OMA system.","['Satellites', 'NOMA', 'Low earth orbit satellites', 'Fading channels', 'Satellite broadcasting', 'Signal to noise ratio', 'Probability']","['Coordinated transmission', 'low-earth orbit (LEO)', 'non-orthogonal multiple access (NOMA)', 'outage probability (OP)', 'satellite communication', 'shadowed-Rician fading']"
"In this article, we explore the performance of optical wireless technology for ensuring audio communications inside an aircraft cockpit. One advantage is that, unlike radio frequencies, opaque objects block optical signals and, therefore, signals cannot pass through walls. This can reduce security risks against eavesdropping and hacking of the physical layer, which is one of the main concerns in the aviation environment. However, optical wireless technology faces some issues, including range limitation and sensitivity to blockages. To study the achievable performances, we propose a modeling of the channels for the uplink and the downlink between the headsets of the four pilots of an Airbus A350 and the access point at the cockpit ceiling. A ray-tracing approach associated with a Monte-Carlo method takes into account the 3D geometric model of the cockpit, the presence of the pilots and their movements. We show that using spatial diversity for headset transceivers can improve performance. Using IEEE 802.11 medium access control mechanism to ensure multi-user communication, the approach highlights the trade-offs between power and delay for a successful communication, linked to the maximum achievable data rate for a given performance level.","['Headphones', 'Optical sensors', 'Wireless communication', 'Transceivers', 'Aircraft', 'Biomedical optical imaging', 'Optical reflection']","['Optical wireless communication', 'channel modeling', 'channel access control', 'in-flight communications']"
"Ultraviolet communication (UVC) is emerging as an attractive alternative to the existing optical wireless communication (OWC) technologies. UVC experiences negligible noise on the earth's surface, and also has the ability to operate in non-line-of-sight (NLOS) mode, thereby making it a perfect choice for outdoor communication. However, due to strong interaction of ultraviolet waves with atmospheric particles, it suffers from a very high path loss and turbulence-induced fading, which limits UVC system's performance. We consider a decode-and-forward based cooperative relaying technique to improve the performance of NLOS UVC system, and to extend its communication range. We consider the practical case of imperfect channel state information at the receiver and derive outage probability of the system. We also consider impact of elevation angles, receiver field-of-view (FOV), and turbulence strength on the system performance. We compute the relative diversity order of the system and demonstrate its convergence through asymptotic analysis. Next, we obtain the novel expression of probability density function of the end-to-end instantaneous signal-to-noise-ratio. We use single subcarrier intensity modulation employing quadrature amplitude modulation (QAM) and derive the novel generalized analytical expressions for rectangular QAM, cross QAM, and futuristic hexagonal QAM schemes. We carry out a detailed performance study considering different system configurations and several interesting insights are highlighted, which reinforces UVC as a futuristic OWC technology. Correctness of the derived analytical expressions is confirmed using Monte-Carlo simulations.","['Nonlinear optics', 'Quadrature amplitude modulation', 'Modulation', 'Radio frequency', 'Relays', 'Probability', 'Power system reliability']","['NLOS UVC', 'decode-and-forward (DF)', 'outage probability', 'average symbol error rate (ASER)', 'quadrature amplitude modulation (QAM)', 'cross QAM (XQAM)', 'hexagonal QAM (HQAM)']"
"Vehicular networks allow billions of vehicular users to be connected to report and exchange real-time data for offering various services, such as navigation, ride-hailing, smart parking, traffic monitoring, and vehicular digital forensics. Fifth generation (5G) is a new radio access technology with greater coverage, accessibility, and higher network density. 5G-supported Vehicular Networks (5GVNs) have attracted plenty of attention from both academia and industry. Geared with new features, they are expected to revolutionize the mobility ecosystem to empower a portfolio of new services. Meanwhile, the development of such communication capabilities, along with the development of sensory devices and the enhancement of local computing powers, have lead to an inevitable reality of massive data (e.g., identity, location, and trajectory) collection from vehicular users. Unfortunately, 5GVN are still confronted with a variety of privacy threats. Such threats are targeted at users' data, identity, location, and trajectory. If not properly handled, such threats will cause unimaginable consequences to users. In this survey, we first review the state-of-the-art of survey papers. Next, we introduce the architecture, features, and services of 5GVN, followed by the privacy objectives of 5GVN and privacy threats to 5GVN. Further, we present existing privacy-preserving solutions and analyze them in-depth. Finally, we define some future research directions to draw more attention and down-to-earth efforts into this new architecture and its privacy issues.","['Privacy', 'Security', '5G mobile communication', 'Computer architecture', 'Roads', 'Data privacy', 'Monitoring']","['Vehicular networks', '5G', 'privacy', 'privacy-preserving solutions']"
"This article presents the research carried out in developing and targeting a novel real-time Dynamic Spectrum Access (DSA) Frequency Spread Filter Bank Multicarrier (FS-FBMC) transmitter prototype to programmable ‘ZynqSDR’ Software Defined Radio (SDR) hardware, and introduces a series of experiments used to validate the design’s ‘cognitive’ DSA capabilities. This transmitter is a proof of concept, that uses DSA techniques to enable Secondary Users (SUs) to access the band traditionally used for FM Radio broadcasting (88-108 MHz), and establish data communication channels in vacant parts of the FM Radio Primary User (PU) spectrum using a multicarrier modulation scheme with a Non Contiguous (NC) channel mask. Once implemented on the hardware, the transmitter is subjected to various FM Radio environments sampled from around Central Scotland, and it is demonstrated that it can dynamically adapt its NC transmitter mask in real time to protect the FM Radio signals it detects. A video is presented of this dynamic on-hardware spectral reconfiguration, and the reader is encouraged to view the video to appreciate the responsiveness of the design. An investigation into potential FBMC guardband sizes is carried out, with initial findings indicating a guardband of 200 kHz (either side of an FM Radio station) is required in order to prevent interference with the PUs. This article also demonstrates the capabilities of the MATLAB and Simulink Model Based Design Zynq-Based Radio workflow, and provides a case study and reference design that we feel other researchers working in this field can benefit from.","['Frequency modulation', 'Radio transmitters', '5G mobile communication', 'Radio frequency', 'Real-time systems', 'Internet']","['access', 'filter bank multicarrier', 'FBMC/OQAM', 'FS-FBMC', 'FM radio', 'PHYDYAS', 'non contiguous', 'shared spectrum', 'software defined radio', 'USRP', 'ZynqSDR']"
"This paper addresses the optimization of distributed compression in a sensor network. A direct communication among the sensors is not possible so that noisy measurements of a single relevant signal have to be locally compressed in order to meet the rate constraints of the communication links to a common receiver. This scenario is widely known as the Chief Executive Officer (CEO) problem and represents a long-standing problem in information theory. In recent years significant progress has been achieved and the rate region has been completely characterized for specific distributions of involved processes and distortion measures. While algorithmic solutions of the CEO problem are principally known, their practical implementation quickly becomes challenging due to complexity reasons. In this contribution, an efficient greedy algorithm to determine feasible solutions of the CEO problem is derived using the information bottleneck (IB) approach. Following the Wyner-Ziv coding principle, the quantizers are successively designed using already optimized quantizer mappings as side-information. However, processing this side-information in the optimization algorithm becomes a major bottleneck because the memory complexity grows exponentially with number of sensors. Therefore, a sequential compression scheme leading to a compact representation of the side-information and ensuring moderate memory requirements even for larger networks is introduced. This internal compression is optimized again by means of the IB method. Numerical results demonstrate that the overall loss in terms of relevant mutual information can be made sufficiently small even with a significant compression of the side-information. The performance is compared to separately optimized quantizers and a centralized quantization. Moreover, the influence of the optimization order for asymmetric scenarios is discussed.","['Distortion measurement', 'Distortion', 'Optimization', 'Loss measurement', 'Noise measurement', 'Source coding', 'Receivers']","['Chief executive officer', 'distributed compression', 'distributed source coding', 'information bottleneck']"
"The fifth generation (5G) cellular network provides users with high-quality services due to its high transmission rate and low latency. It will support Internet of Things (IoT) devices and enable new applications in health, banking, education, etc. Security is essential in this network because vulnerabilities may be exploited to disrupt these applications which may directly impact our life. Authentication and key agreement (AKA) and handover (HO) are usually the target of cyberattacks in any cellular network. On the other hand, blockchain is a peer-to-peer network that aims to maintain an immutable and secure ledger. This new technology will be used widely to secure many applications. This paper aims to develop an efficient and secure AKA scheme and uniform handover protocol for 5G network using blockchain. The home network (HN) is not involved in the AKA scheme and HO protocol to protect the HN from attacks, such as denial of service (DoS) attacks, and also lower the communication and computation overhead. Moreover, our HO protocol is uniform in the sense that it can be used for all HO scenarios. The protocol is also efficient because it requires exchanging a few amount of data. It can also achieve forward/backward secrecy. Furthermore, the blockchain is used to verify the public keys of the network nodes which is necessary to secure our AKA scheme and HO protocol. It also records the locations of the users which is necessary for the functionality of the network. Our evaluations demonstrate that the proposed HO protocol is secure, uniform, and can achieve the forward/backward secrecy. Furthermore, our AKA scheme requires lower computation and computation overhead comparing to the existing schemes, and preserves the energy of the limited-energy mobile devices.","['Protocols', 'Handover', '5G mobile communication', 'Blockchains', 'Public key', 'Security', '3GPP']","['Security', 'authentication and key agreement', 'uniform and secure handover', 'blockchain and 5G']"
"The prospects of utilizing single-carrier (SC) and multi-carrier (MC) waveforms in future terahertz (THz)-band communication systems remain unresolved. On the one hand, the limited multi-path components at high frequencies result in frequency-flat channels that favor low-complexity wideband SC systems. On the other hand, frequency-dependent molecular absorption and transceiver characteristics and the existence of multi-path components in indoor sub-THz systems can still result in frequency-selective channels, favoring off-the-shelf MC schemes such as orthogonal frequency-division multiplexing (OFDM). Variations of SC/MC designs result in different THz spectrum utilization, but spectral efficiency is not the primary concern with substantial available bandwidths; baseband complexity, power efficiency, and hardware impairment constraints are predominant. This paper presents a comprehensive study of SC/MC waveforms for THz communications, utilizing an accurate wideband THz channel model and highlighting the various performance and complexity trade-offs of the candidate schemes. Simulations demonstrate that discrete-Fourier-transform spread orthogonal time-frequency space (DFT-s-OTFS) achieves a lower peakto- average power ratio (PAPR) than OFDM and OTFS and enhances immunity to THz impairments and Doppler spreads, but at an increased complexity cost. Moreover, DFT-s-OFDM is a promising candidate that increases robustness to THz impairments and phase noise (PHN) at a low PAPR and overall complexity.","['Peak to average power ratio', 'Radio frequency', 'Complexity theory', 'Frequency modulation', 'Baseband', 'Millimeter wave communication', 'Frequency division multiplexing']","['THz communications', 'CP-OFDM', 'SC-FDE', 'DFT-s-OFDM', 'OQAM/FBMC', 'OTFS', 'DFTs-OTFS']"
"For the demonstration of ultra-wideband bandwidth and pencil-beamforming, the terahertz (THz)-band has been envisioned as one of the key enabling technologies for the sixth generation networks. However, the acquisition of the THz channel entails several unique challenges such as severe path loss and beam-split. Prior works usually employ ultra-massive arrays and additional hardware components comprised of time-delayers to compensate for these loses. In order to provide a cost-effective solution, this paper introduces a sparse-Bayesian-learning (SBL) technique for joint channel and beam-split estimation. Specifically, we first model the beam-split as an array perturbation inspired from array signal processing. Next, a low-complexity approach is developed by exploiting the line-of-sight-dominant feature of THz channel to reduce the computational complexity involved in the proposed SBL technique for channel estimation (SBCE). Additionally, based on federated-learning, we implement a model-free technique to the proposed model-based SBCE solution. Further to that, we examine the near-field considerations of THz channel, and introduce the range-dependent near-field beam-split. The theoretical performance bounds, i.e., Cramér-Rao lower bounds, are derived both for near- and far-field parameters, e.g., user directions, beam-split and ranges. Numerical simulations demonstrate that SBCE outperforms the existing approaches and exhibits lower hardware cost.","['Channel estimation', 'Millimeter wave communication', 'Estimation', 'Array signal processing', 'Hardware', 'Bandwidth', 'Perturbation methods']","['Terahertz', 'channel estimation', 'beam split', 'sparse Bayesian learning', 'near-field', 'federated learning']"
"While 5G networks are driving a growing number of use cases in the fields of iot and industrial applications, the vision of the next generation of mobile communications systems already includes concepts massively transforming the way people will interact with the digital world through the network, as humans are shifting into the center of diverse network driven applications. Envisaged use cases and possibilities to provide services and resources in a distributed manner render an architectural solution for trust establishment a critical component of 6G networks. This survey provides an overview of terms and visions related to the topic of trust in general and in mobile communications systems. Requirements for an end-to-end trust building framework are derived, in order to give a starting point for the design process of a trust anchor service as a component of 6G networks.","['6G mobile communication', 'Security', 'Peer-to-peer computing', 'Buildings', 'Quality of service', 'Distributed ledger', 'Biological system modeling']","['6G', 'AI', 'DLT', 'trust anchors', 'trust as a service']"
"Cloud Radio Access Networks (Cloud-RANs) have recently emerged as a promising architecture to meet the increasing demands and expectations of future wireless networks. Such an architecture can enable dynamic and flexible network operations to address significant challenges, such as higher mobile traffic volumes and increasing network operation costs. However, the implementation of compute-intensive signal processing Network Functions (NFs) on the General Purpose Processors (General Purpose Processor) that are typically found in data centers could lead to performance complications, such as in the case of overloaded servers. There is therefore a need for methods that ensure the availability and continuity of critical wireless network functionality in such circumstances. Motivated by the goal of providing highly available and fault-tolerant functionality in Cloud-RAN-based networks, this paper proposes the design, specification, and implementation of live migration of containerized Baseband Units (BBUs) in two wireless network settings, namely Long Range Wide Area Network (LoRaWAN) and Long Term Evolution (LTE) networks. Driven by the requirements and critical challenges of live migration, the approach shows that in the case of LoRaWAN networks, the migration of BBUs is currently possible with relatively low downtimes to support network continuity. The analysis and comparison of the performance of functional splits and cell configurations in both networks were performed in terms of fronthaul throughput requirements. The results obtained from such an analysis can be used by both service providers and network operators in the deployment and optimization of Cloud-RANs services, in order to ensure network reliability and continuity in cloud environments.","['Long Term Evolution', 'Baseband', 'Wireless networks', 'Computer architecture', 'Wide area networks', 'Cloud computing', 'Time-frequency analysis']","['Cloud-RAN', 'live migration', 'LoRaWAN', 'LTE', 'network function virtualization (NFV)']"
"We consider a cell-free massive MIMO (CF-mMIMO) system in which multiple access points (APs), connected to a common central processing unit (CPU) through unbounded fronthaul, collaboratively serve multiple users in a heterogeneous scenario in which each user equipment (UE) has a different number of antennas, and therefore is capable of communicating via distinct numbers of digital streams. For such a user-heterogeneous system, new joint transmit (TX)/receive (RX) beamforming (BF) algorithms are then proposed, both for downlink and uplink modes and integrated with two alternative transmit (TX) power and spatial resource allocation strategies, which enable interference-free communications. To that end, a novel tensor decomposition scheme is presented, based on an orthogonality-enforcing modification of the recently-proposed multilinear generalized singular value decomposition (ML-GSVD). Simulation results show both that the new orthogonality-enforcing ML-GSVD (OEML-GSVD) achieves greater accuracy than the previous multilinear generalized singular value decomposition (ML-GSVD) without sacrificing convergence speed, and that the corresponding OEML-GSVD-based proposed beamformers outperform state-of-the-art (SotA) techniques, as well as an equivalent beamformer based on the previous ML-GSVD alternative.","['Tensors', 'Linear antenna arrays', 'Downlink', 'Matrix decomposition', 'Array signal processing', 'Uplink', 'Singular value decomposition']","['Cell-free massive MIMO (CF-mMIMO)', 'beamforming design', 'tensor factorization', 'multilinear generalized singular value decomposition (ML-GSVD)']"
"Autonomous navigation of mobile robots in complex environments is challenging. Solving the problems of inaccuracy localization and frequent tracking losses of mobile robots in challenging scenes is beyond the power of point-based visual simultaneous localization and mapping (vSLAM). This paper proposes a real-time and robust point-line based monocular visual inertial SLAM (VINS) system for mobile robots of smart cities towards 6G. To extract robust line features for tracking in challenging scenes, EDLines with adaptive gamma correction is adopted to fast extract a larger ratio of long line features among all extracted line features. A real-time line feature matching approach is proposed to track the extracted line features between adjacent frames without the need of computing descriptors. Compared with LSD and KNN matching method based on LBD descriptors, the proposed method runs three times faster. Furthermore, a tightly coupled sensor fusion optimization framework is constructed for accurate state estimation, which contains point-line feature reprojection errors and IMU residuals. By evaluating on public benchmark datasets, our VINS system has high localization accuracy, real-time performance and robustness compared with other advanced SLAM systems. Our VINS system enables mobile robots to locate accurately in smart cities with complex environments.","['Feature extraction', 'Visualization', 'Mobile robots', 'Simultaneous localization and mapping', 'Real-time systems', 'Optimization', 'Smart cities']","['SLAM', 'smart cities', 'mobile robots', 'sensor fusion', '6G']"
"First-order Marcum Q -function is observed in various problem formulations. However, it is not an easy-to-handle function. For this reason, in this article, we first present a semi-linear approximation of the Marcum Q -function. Our proposed approximation is useful because it simplifies, e.g., various integral calculations including Marcum Q -function as well as different operations such as parameter optimization. Then, as an example of interest, we apply our proposed approximation approach to the performance analysis of predictor antenna (PA) systems. Here, the PA system is referred to as a system with two sets of antennas on the roof of a vehicle. Then, the PA positioned in the front of the vehicle can be used to improve the channel state estimation for data transmission of the receive antenna that is aligned behind the PA. Considering spatial mismatch due to the mobility, we derive closed-form expressions for the instantaneous and average throughput as well as the throughput-optimized rate allocation. As we show, our proposed approximation scheme enables us to analyze PA systems with high accuracy. Moreover, our results show that rate adaptation can improve the performance of PA systems with different levels of spatial mismatch.","['Throughput', 'Optimization', 'Antennas', 'Resource management', 'Closed-form solutions', 'Channel estimation', 'System performance']","['Backhaul', 'channel state information (CSI)', 'integrated access and backhaul (IAB)', 'linear approximation', 'Marcum Q-function', 'mobility', 'mobile relay', 'outage probability', 'predictor antenna', 'rate adaptation', 'spatial correlation', 'throughput', 'vehicle communication', 'V2X', 'V2I']"
"Multiband spectrum access plays an essential role in cognitive radio systems so as to increase the network's throughput through wideband spectrum sensing. It includes identifying the number of subbands comprising a wide spectrum by edge detection, and also examining their occupancy through primary user detection techniques. Despite the offered accuracy of the wavelet-based approaches, their complexity becomes a drawback. Remarkably, the features revealing property of cepstral analysis and its implementation simplicity make it a suitable candidate for signal detection. Motivated by these reasons, this paper presents a wideband spectrum sensing approach based on cepstral analysis. First, we propose the differential log spectral density algorithm for the edge detection phase in order to detect the spectral boundaries within the wideband of interest. Also, we present a mathematical framework of the proposed algorithm and an expression for the detection threshold of the proposed detector is derived. The simulation results have showed a superior performance of the edge detection algorithm to different wavelet-based techniques at low-to-medium noise power. Used in conjunction with denoising, the proposed edge detector shows good detection results at low signal-to-noise ratio. For the primary user detection phase, we introduce the improved passband autocepstrum detector to tackle the misdetection problem of noise-like signals and it outperforms different state-of-the-art techniques. Finally, the uncertainty problem of the subbands center frequencies is addressed and the baseband autocepstrum detector is introduced as a potential solution to improve signal detection in frequency selective fading.","['Detectors', 'Image edge detection', 'Wideband', 'Cepstral analysis', 'Signal to noise ratio', 'Wavelet transforms']","['Baseband autocepstrum detector', 'cognitive radio', 'differential log spectral density', 'wideband spectrum sensing']"
"We address the problem of access point (AP) placement in small-cell networks with partial infrastructure flexibility, i.e., a novel class of problem in Beyond 5G, resultant from the utilization of unmanned aerial vehicles (UAVs) with AP functionalities (UAV-APs), to aid fixed wireless networks in coping with momentary peak-capacity requirements. We use the signal-to-generated-interference-plus-noise ratio (SGINR) metric as an alternative to the traditional signal-to-interference-plus-noise ratio (SINR) to quantify the effects of inter-cell interference (ICI) on the per-user capacity. From average SGINR, we derive the ICI-aware distortion measure leading to the Inter-AP Lloyd algorithm to obtain throughput-optimal AP placement for a fully flexible infrastructure. We then impose a hybridity constraint to the AP placement problem which turns a fraction of the network into a fixed infrastructure composed of terrestrial APs (T-APs) while the remainder is constituted by UAV-APs with flexibility in position. This newly formulated AP placement problem is solved by the proposed Lloyd-type algorithm called Hybrid AP Placement Algorithm (HAPPA). Furthermore, we present an initialization method for the Lloyd and Lloyd-type algorithms for Gaussian mixture models (GMMs) that offers an AP allocation leading to a higher rate compared to the k-means++ initialization. Finally, computer simulations show that the Inter-AP Lloyd algorithm can improve the performance of the worst users by up to 42.75% in achievable rate, assuming a fully flexible network. By using HAPPA on hybrid networks, we achieve improvements of up to 71.92% in sum rate over the fixed network and close the performance gap with fully flexible networks down to 2.02%, when an equal number of UAV-APs and T-APs is used. Further, our proposed initialization scheme always results in a balanced AP allocation, which means a more even distribution of users per AP, whereas the k-means++ scheme results in unbalanced allocations at least 30% of the time, resulting in a worse minimum rate.","['Interference', 'Throughput', 'Signal to noise ratio', 'Resource management', 'Unmanned aerial vehicles', 'Distortion', 'MIMO communication']","['Beyond 5G', 'hybrid network', 'inter-cell interference (ICI)', 'Lloyd algorithm', 'unmanned aerial vehicle (UAV)']"
"Ubiquitous connection to modern vehicles is mandatory to support diverse intelligent functions, including autonomous driving, telediagnosis, infotainment services, and others. Since the deployment of the cellular access points is still scarce in rural areas, the low earth orbit (LEO) constellation-based communication is believed to provide a realistic alternative. However, due to the long propagation delay and limited satellite on-board processing ability, the design of security protocols in LEO constellations remain challenging. Aiming at these challenges, we propose a secure user access and inter-satellite handover mechanism, which achieves the control- and user-plane key separation. Specifically, our proposed scheme exploits an identity-based encryption scheme with proxy re-encryption to achieve the key establishments with high efficiency, and it also achieves the highly efficient secure batch handover with the assistance of a stack. Detailed analysis is performed to demonstrate the security properties of our proposed scheme, in terms of confidentiality, authentication, and forward/backward key separation. Furthermore, simulation results illustrate that our proposed scheme achieves high computational complexity and communication overheads in comparison with a traditional scheme.","['Satellites', 'Handover', 'Low earth orbit satellites', '5G mobile communication', 'Security', 'Authentication', 'Laser beams']","['LEO constellation', 'user access', 'handover', 'security']"
"In the context of a massive MIMO system, the number of RF chains increases prohibitively. To solve the problem, hybrid beamforming (BF) has been proposed. Nevertheless, the amplitude constraint of the analog part in hybrid BF limits achieving the optimal performance given by the optimal digital BF. Some studies have been conducted recently to relax the amplitude constraint by deploying multiple phase shifters and switches (that select phase shifters) in the hybrid beamforming structures. The tremendous number of switches in the structure makes the problem of switch state selection very complex. In this paper, we propose an efficient low-complexity algorithm to obtain the optimal switch states. In the proposed algorithm, we decompose the main problem into sub-ones to reduce the complexity and solve the problem using a finite search space. The results of our study suggest that the optimal solution will be attainable by searching through less than 1% of the search space, if the number of phase levels exceeds 14. Moreover, the proposed algorithm has the potential to perfectly implement linear beamformers, designed for fully digital architectures, in a hybrid structure, while applying the minimum number of RF chains. Finally, the simulation results prove the efficiency of the algorithm and confirm the sufficiency of the analog part of the BF to achieve the optimal digital BF performance just by applying a few number of fixed phase shifters.","['Radio frequency', 'Array signal processing', 'Phase shifters', 'Antennas', 'Computer architecture', 'Transmission line matrix methods', 'Transmitters']","['Millimeter wave communications', 'massive MIMO', 'hybrid beamforming', 'fixed phase shifter']"
"The capability to provide guarantees for network metrics, such as latency, data rate, and reliability will be an important factor for widespread adoption of next generation mobile networks and hence, such metrics play a central role in standards for new wireless communication technologies. However, due to the inherently stochastic nature of mobile communications, any guarantees can only be of statistical nature and are highly dependent on the actual physical environment. To analyze the stochastic behavior, this paper presents a tool chain for measurement, collection, evaluation, and prediction of controlled mobile communications drive test data. We also publish the underlying data set of measurements covering two years’ worth of highway traffic on a 25 km long section comprising 267 198 data points. We statistically evaluate the data set and validate it with a corresponding data set from another source. Applying machine learning to the data set illustrates possible use cases: Feed-forward neural networks to predict the data rate in five application scenarios, LIME to explain the behavior of the model, and an autoencoder to describe the interaction of five signal strength parameters. The data set and the tool chain show how machine learning can be applied to wireless networks and provide fellow researchers with the means to make further experiments.","['Machine learning', 'Monitoring', 'Data models', 'Current measurement', 'Behavioral sciences', 'Throughput', 'Standards']","['learning', 'prediction']"
"The objective of this paper is to articulate the problem of attribution in cyber warfare incidents, including, surveillance, data theft, espionage, and misinformation campaigns. As the stakes increase, concerted efforts are being made by intelligence and law enforcement agencies to identify the perpetrators with much painstaking effort. Attribution tools and techniques for malicious activities on the Internet are still nascent, relying mainly on technical measurements, the provenance of malicious code, and non-technical assessments of attack and attacker characteristics to link attack activities to individuals or groups. Attribution of attacks is typically done through a burdensome manual process that relies on both technical analysis and ground intelligence. As a result, this cumbersome and laborious process of attribution is primarily reserved for the most egregious cyber attack cases and those conducted against well resourced organizations. Over time, our attribution abilities have improved, however, this improvement is a two-edged sword: as attribution capabilities improve, Internet privacy is increasingly diluted. This paper discusses attribution for two vastly different types of attacks that are central to cyber conflict today: network intrusions and social bot-led misinformation campaigns. The paper discusses the state of the art regarding attribution abilities across both types of attack, provides recommendations for improved attribution, and lays out future research directions.",[],[]
"Full-duplex relaying is an enabling technique of sixth generation (6G) mobile networks, promising tremendous rate and spectral efficiency gains. In order to improve the performance of full-duplex communications, power control is a viable way of avoiding excessive loop interference at the relay. Unfortunately, power control requires channel state information of source-relay, relay-destination and loop interference channels, thus resulting in increased overheads. Aiming to offer a low-complexity alternative for power control in such networks, we adopt reward-based learning in the sense of multi-armed bandits. More specifically, we present bandit-based power control, relying on acknowledgements/negative-acknowledgements observations by the relay. Our distributed algorithms avoid channel state information acquisition and exchange, and can alleviate the impact of outdated channel state information. Two cases are examined regarding the channel statistics of the wireless network, namely, strict-sense stationary and non-stationary channels. For the latter, a sliding window approach is adopted to further improve the performance. Performance evaluation highlights a performance-complexity trade-off, compared to optimal power control with full channel knowledge and significant gains over cases considering channel estimation and feedback overheads, outdated channel knowledge, no power control and random power level selection. Finally, it is shown that the sliding-window bandit-based algorithm provides improved performance in non-stationary settings by efficiently adapting to abrupt changes of the wireless channels.","['Wireless communication', 'Power control', 'Channel estimation', 'Resource management', 'Relay networks (telecommunication)', 'Encoding', 'Wireless sensor networks']","['Full-duplex relaying', 'power control', 'reinforcement learning', 'multi-armed bandits', 'non-stationary wireless channels', 'outdated CSI', 'sliding-window', 'upper confidence bound policies']"
"Code-based cryptosystems are promising candidates for post-quantum cryptography. Recently, generalized concatenated codes over Gaussian and Eisenstein integers were proposed for those systems. For a channel model with errors of restricted weight, those q-ary codes lead to high error correction capabilities. Hence, these codes achieve high work factors for information set decoding attacks. In this work, we adapt this concept to codes for the weight-one error channel, i.e., a binary channel model where at most one bit-error occurs in each block ofmbits. We also propose a low complexity decoding algorithm for the proposed codes. Compared to codes over Gaussian and Eisenstein integers, these codes achieve higher minimum Hamming distances for the dual codes of the inner component codes. This property increases the work factor for a structural attack on concatenated codes leading to higher overall security. For comparable security, the key size for the proposed code construction is significantly smaller than for the classic McEliece scheme based on Goppa codes.","['Codes', 'Decoding', 'Generators', 'Concatenated codes', 'Complexity theory', 'Security', 'Channel models']","['Code-based cryptography', 'generalized concatenated codes', 'McEliece cryptosystem', 'public-key cryptography', 'restricted error values']"
"Grant-free access is an attractive approach to enable spectrum-efficient low-latency access for systems with massive number of users. Pilot design plays a crucial role for grant-free access as it needs to provide a large number of access codes with fast collision detection capability and good channel estimation performance. Recently, pilot designs with fast collision detection capability have been proposed for compressed sensing (CS) based channel estimation. But the existing designs are not optimized for the estimation of highly sparse and block-sparse channels. In this paper, we present several propositions related to the performances of the CS based sparse and block-sparse channel estimation. Utilizing these propositions, we develop a novel non-orthogonal pilot design with fast collision detection capability for grant-free access in block-sparse channels. We also propose two methods to optimize the Peak-to-Average Power Ratio (PAPR) of the proposed non-orthogonal pilot codes. The simulation results illustrate that the proposed design provides similar or better channel estimation and collision detection performances with much better pilot resource efficiency when compared to the existing designs. Finally, we investigate the trade-offs among different design parameters and the channel estimation performances to facilitate better design choices.","['Channel estimation', 'Collision avoidance', 'MIMO communication', 'Uplink', 'Resource management', 'Peak to average power ratio', 'Receivers']","['Compressed sensing', 'grant-free access', 'MIMO', 'non-orthogonal pilot', 'sparse channel']"
"Unmanned Aerial Vehicle (UAV) clustering is promising for performing large-scale missions because of high mobility and easy deployment. However, the connectivity problems caused by the high mobility of UAVs and the interferences are mostly ignored in existing UAV mission executions. Rapid restoration of UAV network connectivity is essential for preventing communication disruption and improving the overall network performance. Thus, a graph coalition formation game that integrates the UAV time-varying topology graph with the coalition formation game is proposed to rapidly restore the connectivity of the UAV network in need. The method improves the utility of UAV clustering and ensures the network connectivity. Then, a graph coalition formation game algorithm based on the shortest path tree (SPT-GCF) rapidly forms an approximately optimal coalition structure. The simulation results show that the proposed approach increases the average utility of UAV clustering by 6.5% and 14.5% respectively when compared with the existing non-overlapping coalition formation game (NOCFG) and the coalition formation game without considering the cluster connectivity.","['Games', 'Autonomous aerial vehicles', 'Network topology', 'Interference', 'Heuristic algorithms', 'Vehicle dynamics', 'Task analysis']","['UAV cluster', 'connectivity', 'overlapping coalition formation game', 'graph coalition formation game']"
"The cloud-based solutions are becoming inefficient due to considerably large time delays, high power consumption, and security and privacy concerns caused by billions of connected wireless devices and typically zillions of bytes of data they produce at the network edge. A blend of edge computing and Artificial Intelligence (AI) techniques could optimally shift the resourceful computation servers closer to the network edge, which provides the support for advanced AI applications (e.g., video/audio surveillance and personal recommendation system) by enabling intelligent decision making on computing at the point of data generation as and when it is needed, and distributed Machine Learning (ML) with its potential to avoid the transmission of the large dataset and possible compromise of privacy that may exist in cloud-based centralized learning. Besides, the deployment of AI techniques to redesign end-to-end communication is attracting attention to improve communication performance. Therefore, the interaction of AI and wireless communications generates a new concept, named native AI wireless networks. In this paper, we conduct a comprehensive overview of recent advances in distributed intelligence in wireless networks under the umbrella of native AI wireless networks, with a focus on the design of distributed learning architectures for heterogeneous networks, on AI-enabled edge computing, on the communication-efficient technologies to support distributed learning, and on the AI-empowered end-to-end communications. We highlight the advantages of hybrid distributed learning architectures compared to state-of-the-art distributed learning techniques. We summarize the challenges of existing research contributions in distributed intelligence in wireless networks and identify potential future opportunities.","['Artificial intelligence', 'Wireless networks', 'Data models', 'Computational modeling', 'Servers', 'Training', 'Distance learning']","['Distributed intelligence', 'distributed machine learning', 'edge computing', 'end-to-end communications', 'federated learning', 'split learning']"
"This paper studies the problem of energy efficiency (EE) maximization via user association, power, and backhaul (BH) flow control in the downlink of millimeter wave BH heterogeneous networks. This problem is mathematically formulated as a mixed-integer non-linear program, which is non-convex. To get a tractable solution, the initial problem is separated into two sub-problems and optimized sequentially. The first is a joint user association and power control sub-problem for the access network (AN) (AN sub-problem). The second is a joint flow and power control sub-problem for the BH network (BH sub-problem). While the BH sub-problem is a convex optimization problem and hence can be efficiently solved, the AN sub-problem assumes the form of a generalized assignment problem, which is known to be NP-hard. To that end, we utilize Lagrangian decomposition to propose two polynomial time solution techniques that obtain a high-quality solution for the AN sub-problem. The first, referred to as Technique A, uses dynamic programming, the subgradient method, and a heuristic. The second, named Technique B, uses the multiplier adjustment method, the sorting algorithm, and a heuristic. Simulation results are used to demonstrate the effectiveness of the proposed energy efficient user association, power, and BH flow control algorithms as compared with benchmark user association schemes that incorporate the BH sub-problem algorithm, in terms of the total AN power, BH power, and overall network (AN plus BH) EE. The computational complexity and practical implementation of the proposed algorithms are discussed.","['Resource management', 'Wireless communication', 'Power control', 'Power demand', 'Throughput', 'Energy efficiency', 'Base stations']","['Flow control', 'generalized assignment problem', 'Lagrangian multipliers', 'multiplier adjustment method', 'power control', 'subgradient', 'user association']"
"This article analyzes the performance of a distributed joint power/packet diversity random access scheme in the presence of energy requirements. In particular, the two main approaches separately developed in the recent years for exploiting the Interference Cancellation (IC) capabilities of modern receivers, that is, Non-Orthogonal Multiple Access (NOMA) and Packet Repetition (PR), are firstly compared and then combined by imposing a constraint on the total available power. This constraint, which is alternative to the commonly adopted one based on the maximum power allowed for a transmission, results much more practical, since it enables to better infer the energy efficiency of a scheme, be it a pure NOMA, PR or combined NOMA/PR one. Both theoretical derivations and numerical simulations are carried out to evaluate the success probability and the throughput of the considered schemes by accounting for evolved reception criteria and different fading scenarios. Furthermore, the influence of several nonidealities, including imperfect IC and packet overhead, is discussed together with the impact of the system parameters, such as the user rate, the average signal to noise ratio, and the number of slots that compose a random access frame.","['Integrated circuits', 'NOMA', 'Diversity reception', 'Receivers', 'Throughput', 'Reliability', 'Next generation networking']","['Random access', 'slotted Aloha', 'NOMA', 'packet diversity', 'energy constraints']"
"LoRaWAN is a low-power wireless technology that provides long-range connectivity to battery-powered Internet of Things (IoT) devices. To minimize the energy consumption of the IoT nodes, LoRaWAN networks use for the uplink a pure non-slotted ALOHA multiple access scheme. Since the devices are not synchronized in time, collisions between uplink packets are the main source of errors when the number of nodes becomes important. To improve the reliability of dense LoRaWAN networks, we propose in this paper a successive interference cancellation LoRa receiver capable of decoding frames from two colliding users. The proposed two-user detector leverages the bit-interleaved coded modulation scheme of LoRa to improve the detection and cancellation of the strongest interfering user. We show that in the presence of two interfering users, the usage of a low coding-rate and iterative soft-detection are essential to attain error rates sufficiently close to the single-user scenario. Using network-level simulations, we subsequently evaluate the gains of our proposed two-user receiver in a realistic LoRaWAN network. To this end, we build advanced models of the studied receivers using Monte-Carlo simulations at the physical layer. For an overall packet error rate of 1%, simulation results indicate that a LoRaWAN network employing our two-user detector may serve 4.7 times more devices than a network with only a single-user receiver at the gateway.","['Receivers', 'Logic gates', 'Throughput', 'Synchronization', 'Silicon carbide', 'Reliability', 'Iterative decoding']","['ALOHA', 'bit-interleaved coded modulation (BICM)', 'Internet of Things (IoT)', 'iterative soft-demodulation', 'LoRa', 'multi-user receiver', 'network simulation', 'successive interference cancellation (SIC)']"
"This work investigates the theoretical bounds of the joint localization and synchronization processes in a reconfigurable intelligent surface (RIS)-assisted system. We address the case of millimeter-wave ( mm-Wave) multiple-input single-output (MISO) orthogonal frequency-division multiplexing (OFDM) with non-ideal transceivers. Considering a single antenna mobile station (MS) aims to estimate the parameters of the downlinks from the base station (BS) and the RIS by observing a known sequence received by the MS directly from the BS and indirectly through the RIS. The theoretical bounds of the estimation process are assessed by using the Fisher information matrix (FIM). A transformation matrix is then used to convert the FIM of the downlink channel parameters to the FIM of the MS joint localization and synchronization parameters. Specifically, the transformation matrix is derived based on the geometric relationships that convert the estimated downlink channels’ parameters to the position coordinates and clock offset. Next, the Cramer-Rao lower bound (CRLB) matrix of the joint localization and synchronization process is obtained by using the pseudo-inverse of the FIM. Thus, the position error bound (PEB), as well as the synchronization error bound (SEB), are calculated. Computer simulation results are provided to illustrate the adverse effects of the hardware impairments (HWIs) on the accuracy of localization and synchronization. These results are given in proportion to the effective signal-to-noise ratio (SNR), the number of pilot transmissions, and the number of the RIS elements.","['Location awareness', 'Synchronization', 'Estimation', 'Downlink', '5G mobile communication', 'Hardware', 'Uplink']","['OFDM', 'mm-wave', 'localization', 'synchronization', 'fisher information matrix', 'hardware impairments', 'RIS']"
"In this work, we optimize the 3D trajectory of an unmanned aerial vehicle (UAV)-based portable access point (PAP) that provides wireless services to a set of ground nodes (GNs). Moreover, as per the Peukert effect, we consider pragmatic non-linear battery discharge for UAV’s battery. Thus, we formulate the problem in a novel manner that represents the maximization of a fairness-based energy efficiency metric and is named fair energy efficiency (FEE). The FEE metric defines a system that lays importance on both the per-user service fairness and the PAP’s energy efficiency. The formulated problem takes the form of a non-convex problem with non-tractable constraints. To obtain a solution we represent the problem as a Markov Decision Process (MDP) with continuous state and action spaces. Considering the complexity of the solution space, we use the twin delayed deep deterministic policy gradient (TD3) actor-critic deep reinforcement learning (DRL) framework to learn a policy that maximizes the FEE of the system. We perform two types of RL training to exhibit the effectiveness of our approach: the first (offline) approach keeps the positions of the GNs the same throughout the training phase; the second approach generalizes the learned policy to any arrangement of GNs by changing the positions of GNs after each training episode. Numerical evaluations show that neglecting the Peukert effect overestimates the air-time of the PAP and can be addressed by optimally selecting the PAP’s flying speed. Moreover, the user fairness, energy efficiency, and hence the FEE value of the system can be improved by efficiently moving the PAP above the GNs. As such, we notice massive FEE improvements over baseline scenarios of up to 88.31%, 272.34%, and 318.13% for suburban, urban, and dense urban environments, respectively.","['Trajectory', 'Three-dimensional displays', 'Autonomous aerial vehicles', 'Power demand', 'Throughput', 'Lead acid batteries', 'Measurement']","['UAV communication', 'energy-efficiency', 'TD3', '3D trajectory optimization', 'reinforcement learning', 'user fairness']"
"The integrated use of non-terrestrial network (NTN) entities such as the high-altitude platform station (HAPS) and low-altitude platform station (LAPS) has become essential elements in the space-air-ground integrated networks (SAGINs). However, the complexity, mobility, and heterogeneity of NTN entities and resources present various challenges from system design to deployment. This paper proposes a novel approach to designing a heterogeneous network consisting of HAPSs and unmanned aerial vehicles (UAVs) being LAPS entities. Our approach involves jointly optimizing the three-dimensional trajectory and channel allocation for aerial base stations, with a focus on ensuring fairness and the provision of quality of service (QoS) to ground users. Furthermore, we consider the load on base stations and incorporate this information into the optimization problem. The proposed approach utilizes a combination of deep reinforcement learning and fixed-point iteration techniques to determine the UAV locations and channel allocation strategies. Simulation results reveal that our proposed deep learning-based approach significantly outperforms learning-based and conventional benchmark models.","['Autonomous aerial vehicles', 'Trajectory', 'Heuristic algorithms', 'Resource management', 'Quality of service', 'Uplink', 'Deep learning']","['platform station', 'resource allocation', 'fairness', 'unmanned aerial vehicles', 'non-terrestrial networks']"
"Cell-free massive multiple-input multiple-output (CF-mMIMO) systems are expected to provide faster and more robust connections to user equipments by cooperation of a massive number of distributed access points (APs), and to be one of the key technologies for beyond 5G (B5G). CF-mMIMO systems with multiple-antenna APs have been investigated from various viewpoints recently. However, no comprehensive analysis of the impact of antenna distribution on CF-mMIMO system performance has been done so far, which is important for practical deployment. Besides spectral efficiency, in B5G, energy efficiency of user equipments is one of the key indicators because various kinds of battery-limited devices connect to the network. Thus, this paper provides a comprehensive performance analysis of the impact of antenna distribution on the performance indicators, while considering several combining/precoding schemes and transmit power control algorithms. For uplink maximal-ratio combining, the concentrated deployment has the best performance thanks to the channel hardening and favorable propagation phenomena. On the other hand, the concentrated deployment prominently suffers from shadowing effects. For uplink/downlink minimum mean-square error combining with transmit power control, the semi-distributed deployments show the best performance, and it implies that we can reduce the number of APs to 1/4 for uplink and to 1/2 for downlink while keeping the same performance as the fully-distributed deployment.","['Antennas', 'Power control', 'Uplink', 'Signal processing algorithms', 'Transmitting antennas', 'Shadow mapping', 'Minimax techniques']","['Antenna distribution', 'battery lifetime prolongation', 'cell-free massive MIMO', 'energy efficiency', 'spectral efficiency', 'transmit power control']"
"The recent surge in human-controlled robotics and haptic devices research is expediting a paradigm shift in today’s communication networks towards human-to-robot (H2R) centric technologies that support Industrial Internet of Things (IIoT) and Industry 5.0. In both IIoT and Industry 5.0, human skills are extended through collaboration with robots that are geographically separated from the human. Depending on the dynamicity of the actual use case, human-to-robot communications necessitate low-latency networking. While Long Term Evolution (LTE) cellular technology has been successful in fulfilling the bandwidth demands of massively-connected sensors and devices of Industry 4.0, it is insufficient to meet the low latency demands of the future Industry 5.0 where dynamic interactions between humans and robots are paramount. In reducing the latency caused by radio resource contention in wireless H2R communications, in this work, we propose a novel approach that exploits an Attention-based Recurrent Neural Network (Att-RNN) to improve the Semi-Persistent Scheduling (SPS) resource allocation scheme adopted by LTE and new radio (NR) standards developed for the fifth generation (5G) mobile networks. We conduct a series of real haptic experiments to collect H2R traffic traces to train, test and evaluate the accuracy of Att-RNN in predicting H2R traffic. Then, with extensive simulations based on the empirical H2R traffic traces, we show that our proposed Att-RNN SPS scheme outperforms classic SPS and other existing resource allocation schemes in terms of reduced latency and improved resource allocation efficiency, thus making Att-RNN SPS a suitable candidate in future Industry 5.0 deployments.","['Uplink', 'Long Term Evolution', 'Predictive models', 'Industries', 'Resource management', 'Collaboration', 'Haptic interfaces']","['Haptic communications', 'remote human-to-robot collaboration', 'low-latency', 'long term evolution', 'prediction methods', 'machine learning', '5G']"
"Federated learning (FL) is a well-regarded distributed machine learning technology that leverages local computing resources while protecting privacy. The over-the-air (OTA) computation has been adopted for FL to prevent excessive consumption of communication resources by employing the superposition nature of wireless waveform. Meanwhile, energy harvesting technology can relieve the energy constraint of clients and enable durable computation for FL. However, few of the existing works on OTA FL have considered jointly performing client selection and receive beamforming optimization with energy harvesting clients. The objective of this work is to address this issue to improve the learning performance of OTA FL. Specifically, we first derive the expression of the optimality gap regarding client selection and receive beamforming design. Then, to minimize the optimality gap, a mixed-integer nonlinear programming (MINLP) problem is formulated and decomposed into two sub-problems. Next, the semidefinite relaxation method and the channel-energy-data (CED)-based method are developed to optimize the receive beamforming sub-problem and client selection sub-problem iteratively. One alternative optimization method is proposed to deal with the decoupled sub-problems for obtaining the solutions to the original MINLP problem. Our simulation results demonstrate that the proposed solution is superior to the other comparison schemes in various parameter settings.","['Training', 'Energy harvesting', 'Convergence', 'Array signal processing', 'Energy consumption', 'Power control', 'Optimization']","['Federated learning', 'over-the-air computation', 'client selection', 'receive beamforming', 'energy harvesting']"
"We consider a multicarrier chirp-based waveform for joint radar and communication (JRC) systems and derive its time discrete periodic ambiguity function (AF). A set of waveform parameters (e.g., chirp rate) can together with the transmit sequence be selected to shape the AF to be thumbtack-like, or to be ridge-like, either along the delay axis or the Doppler axis. We demonstrate how these shapes are applicable for different use cases, e.g., radar target detection or time- and frequency synchronization. The results show that better signal detection performance than OFDM and DFT-s-OFDM can be achieved on channels with large Doppler shift. Furthermore, it is shown how transmit sequences can be selected in order to achieve low peak-to-average-power-ratio (PAPR) of the waveform.","['Radar', 'Chirp', 'Doppler radar', 'Time-frequency analysis', 'Symbols', 'Peak to average power ratio', 'Synchronization']","['Ambiguity function (AF)', 'chirp', 'joint radar and communication (JRC)', 'peak-to-average-power ratio (PAPR)', 'radar', 'sequence', 'synchronization']"
"Network traffic matrix (TM) is a critical input for capacity planning, anomaly detection and many other network management related tasks. The TMs are often computed from link load measurements. The TM estimation problem is the determination of the TM from link load measurements. The relationship between the link loads and the TM that generated the link loads can be modeled as an under-determined linear system and has multiple feasible solutions. Therefore, prior knowledge of the traffic demand pattern has to be used in order to find a potentially feasible TM. In this paper, we consider the TM estimation problem with limited prior information. Unlike previous methods that require past measurements of complete TMs, which are hard to obtain or protected by regulations, our method works even if only the distribution of TMs is known. We develop an iterative projection based algorithm to solve this problem. If large number of past TMs can be measured, we propose a Generative Adversarial Network (GAN) based approach for solving the problem. We compare the strengths of the two approaches and evaluate their performance for several networks using varying amounts of past data.","['Estimation', 'Generative adversarial networks', 'Gallium nitride', 'Current measurement', 'Regulation', 'Correlation', 'Privacy']","['Traffic matrix', 'estimation', 'machine learning', 'generative adversarial networks']"
"Metaverse is envisioned as the next-generation paradigm that links a virtual world to the physical world, providing immersive experience for human activities in the hypothetical environment with augmented reality (AR) and virtual reality (VR) technology. Recent development of wireless multiaccess edge computing (MEC) has accelerated the realization of this vision. In this paper, we consider location-dependent AR services in MEC-enabled Metaverse systems. In such a system, each user performs simultaneous localization and communication first to estimate its position and request AR contents from the MEC server. Due to limited computation and communication resources, the MEC server adaptively adjusts the resolution of AR contents. However, two challenges arise in such a system. First, the localization error will degrade the user’s quality-of-experience (QoE); and second, the coupling of communication and computation complicates the resource management. To address these issues, we first model the QoE taking the localization errors into consideration, and then propose a minimum QoE maximization scheme to achieve fairness among users by jointly optimizing the waveform at users, the transmit power at the base station (BS), and the resolution of transmitted videos. Simulation results verify the effectiveness of the proposed scheme in comparison with benchmark schemes.","['Metaverse', 'Quality of experience', 'Wireless communication', 'Uplink', 'Location awareness', 'Downlink', 'Estimation error']","['Metaverse', 'location-dependent augmented reality', 'multi-access edge computing', 'resource management']"
"In order to address the shortcomings of orthogonal frequency division multiplexing (OFDM) and extend the lifetime of energy-constrained Internet-of-Things (IoT) devices, the combination of filter bank multi-carrier (FBMC) and simultaneous wireless information and power transfer (SWIPT) is investigated in this paper. Specifically, a multi-user FBMC-based SWIPT system is proposed in which user nodes (UNs) have the capability for both energy harvesting (EH) and information decoding (ID) with the aid of separate antennas. A practical non-linear EH model, which considers the saturation effects of the EH circuit, is considered. The information receiver at both the UNs and base station (BS) adopts an iterative interference cancellation (IIC) receiver to cancel the intrinsic interference in the demodulated FBMC signal. A sum-rate maximization problem is solved to jointly optimize parameters such as time, power, and weight allocations. Sub-optimal schemes are proposed for comparison. Numerical results show that the optimal solution significantly outperforms the sub-optimal methods in terms of achievable sum-rate and amount of harvested energy. Moreover, the results show that the proposed algorithm converges within a few iterations.","['OFDM', 'Interference', 'Resource management', 'Wireless communication', 'Internet of Things', 'Receiving antennas', 'Time division multiple access']","['Filter bank multi-carrier (FBMC)', 'simultaneous wireless information and power transfer (SWIPT)', 'time division multiple access (TDMA)', 'sum-rate maximization', 'Internet-of-Things (IoT) networks']"
"In this paper, we investigate the performance of a non-orthogonal multiple access (NOMA) wireless system, in which signal transmission of a far user is assisted by a near user. In particular, we examine the outage performance of the underlying scenario with two cooperative forwarding protocols incorporated, namely, the hybrid decode-amplify-forward (HDAF) protocol and the incremental hybrid decode-amplify-forward (IHDAF) protocol. HDAF combines the advantages of decode-and-forward (DF) and amplify-and-forward (AF) and overcomes their limitations. On the other hand, the IHDAF protocol gives priority to successful decoding from the direct link before enabling HDAF. Specifically, we first derive outage probability expressions for the two users under the HDAF protocol. Furthermore, a closed-form outage expression of the weak user is derived considering the IHDAF protocol, which is envisioned to further boost the outage performance. Also, we present a comprehensive mathematical framework in order to evaluate the system throughput performance of the underlying system model, for the HDAF and IHDAF protocols. Analytical and simulation results reveal that the outage performance of the weak user under IHDAF outperforms that of HDAF.","['Protocols', 'NOMA', 'Signal to noise ratio', 'Interference', 'Decoding', 'Relays', 'Throughput']","['Hybrid decode-amplify-forward (HDAF)', 'incremental HDAF (IHDAF)', 'non-orthogonal multiple access (NOMA)', 'outage performance', 'user cooperation']"
"In this paper, we analyze the outage performance of a rate-splitting multiple access (RSMA)-aided semi-grant-free (SGF) transmission system, in which a grant-based user (GBU) and multiple grant-free users (GFUs) access the base station by sharing the same resource blocks. In the RSMA-aided SGF (RSMA-SGF) transmission system, the GBU and admitted GFU are respectively treated as the primary and secondary users by using the cognitive radio principle. With the aid of RSMA, the admitted GFU’s transmit power allocation, target rate allocation, and successive interference cancellation decoding order are jointly optimized to attain the maximum achievable rate for the admitted GFU, without deteriorating the GBU’s outage performance compared to orthogonal multiple access. Taking into account the extended non-outage zone achieved by rate-splitting, a closed-form expression is derived for the outage probability of the admitted GFU in the considered RSMA-SGF system. Asymptotic analysis for the admitted GFU’s outage probability is also provided. The superior outage performance and full multiuser diversity gain achieved by the RSMA-SGF transmission system are verified by the analytical and simulation results.","['Uplink', 'Resource management', 'NOMA', 'Decoding', 'Probability', 'Power system reliability', 'Downlink']","['Grant-free transmissions', 'multiple access', 'outage probability', 'rate-splitting']"
"In this paper, we analyze the performance of a wireless-powered communication network that consists of a multiple-antenna hybrid access point (AP) and a single-antenna user. The AP transmits radio frequency (RF) power in the downlink (DL) using maximal ratio transmission (MRT) type energy beamforming. The user harvests it and transmits data to the AP in the uplink (UL), which is received with maximal ratio combining (MRC). Previous analyses have assumed the availability of perfect channel estimation. In contrast, we eliminate this unrealistic assumption and consider the effect of imperfect channel estimates. We first derive the distributions of the received AP signal-to-noise ratio (SNR). We then analyze the average throughput performance of delay-limited and delay-tolerant modes by evaluating the outage probability (OP) and ergodic capacity (EC). We also derive the exact bit error rates (BERs) and symbol error rates (SERs) of several digital modulations. Asymptotic performance expressions in the high SNR regime and the large antenna regime are also developed. Finally, analytical and asymptotic results are validated by Monte-Carlo simulations. The impacts of the transmit power, the energy harvesting (EH) time, the number of antennas and the efficiency of EH are investigated.","['Signal to noise ratio', 'Throughput', 'Wireless communication', 'Radio frequency', 'Antennas', 'Wireless sensor networks', 'Array signal processing']","['Channel estimation errors', 'energy harvesting', 'average throughput', 'outage probability', 'ergodic capacity', 'symbol error rate']"
"Reconfigurable Intelligent Surface (RIS) can intelligently control the wireless propagation environment by adjusting the signal phase and amplitude in real time, which is considered as one of the key technologies of 6G. Although RIS-assisted wireless communication systems greatly improve transmission efficiency under ideal conditions, there are various uncertainties in actual communication systems. In this paper, RIS-assisted communication system is investigated where the RIS cannot completely eliminate the phase error and the user location is randomly distributed with uncertainty. In addition, there are some interference sources around the user, which are simplified as co-channel interference (CCI). Then we study the performance of RIS-assisted communication system, and derive the closed-form expressions for the outage probability (OP) and channel capacity of the system. Furthermore, we analyze the effects of various parameters on the OP and channel capacity. Finally Monte Carlo simulation is carried out which verify the accuracy of the derivation.","['Wireless communication', 'Communication systems', 'Channel capacity', 'Uncertainty', 'System performance', 'Signal to noise ratio', 'Performance evaluation']","['interference', 'outage probability', 'channel capacity']"
"The evolving nature of network traffic challenges existing learning-based models, as frequently re-training the hyper-parameters is required to adaptively learn and predict its behavior. Benefiting from discovering the sparsity and self-similarity of network traffic, it becomes possible to develop compact algorithms with high accuracy and low computational complexity. However, how to properly make use of these properties still remains to be explored. In this article, we establish a light-weight learning framework for network traffic prediction based on sparse representation, and try to take the full advantage of these properties to enhance the capability of tracking its highly evolving characteristics. Specifically, 1). The strict causality constraint makes it difficult to equip the conventional sparse representation with predictive capability. To solve this issue, we make use of the self-similarity and train the representative/predictive dictionaries in a joint manner, such that the query point is embedded in terms of a sparse combination of dictionary atoms, and jointly coded with its T + 1 time slot behind counterpart, which proves to be optimal in the concatenated representative/predictive feature space. Then, the query point can be estimated through iterative projection method. 2). The consideration of the sparsity constraint loosens the upper bound of the time averaged prediction error. To address this problem, we slightly modify the sparse representation-based prediction by adopting Lyapunov optimization, and try to minimize the time averaged prediction error. Finally, the simulation results verify the performance improvements.","['Time series analysis', 'Dictionaries', 'Computational modeling', 'Prediction algorithms', 'Predictive models', 'Training', 'Computational complexity']","['Communication system traffic', 'predictive models', 'sparse coding', 'Lyapunov optimization']"
"In this contribution, a thorough investigation of the performance of rate splitting is conducted in terms of outage and secrecy outage for the simultaneous service to a near user and far user, where the latter attempts to overhear the message of the former. The source transmits a linear combination of the users’ common stream and private streams. Once the common stream is retrieved, two decoding strategies can be adopted by each user. In the first strategy, the nodes (near or far) treat the far user’s private stream as noise to retrieve the private stream of the near user, then the far user decodes its own stream. In the second strategy, the nodes decode the far user’s private stream by treating the one of the near user as noise, then the near user retrieves its private stream while the far user decodes the stream of the near user in its attempt to overhear it. Considering the four decoding combinations, we obtain exact closed-form expressions for the outage probability, and provide tight approximations for the secrecy outage probability. Comparative results are also provided. In particular, it is shown that to achieve better outage probability, with no concern about secrecy, once the decoding of the common stream is completed, each user should first retrieve the private stream with lower target data rate by treating the other private stream as noise. To improve the secrecy outage probability, once the common stream is decoded, the near user must first decode the far user’s private stream, and the far user should first retrieve the private stream with lower target data rate.","['Decoding', 'Power system reliability', 'Resource management', 'NOMA', 'Electronic mail', 'Eavesdropping', 'Throughput']","['Rate splitting', 'physical-layer security', 'outage probability', 'secrecy outage probability']"
"While sixth-generation (6G) wireless systems are expected to bring about an explosion of accessible user information and novel technologies, along with new threats to terrestrial and non-terrestrial networks, major concerns associated with the development of 6G networks are privacy and security. Reconfigurable intelligent surfaces (RISs), which have recently emerged as promising candidates to support 6G physical platforms, have proved to be capable of boosting security of next-generation wireless systems. However, due to their easy reconfigurability and low cost, RISs are vulnerable to several security threats, and this vulnerability has not yet been thoroughly addressed in previous research. To fill this gap in the literature, in this review, we aim to thoroughly analyze the security challenges affecting RIS-empowered 6G wireless networks. To this end, we review the attributes of RISs that distinguish them from other relevant technologies, such as multiple-input-multiple-output (MIMO), conventional relaying, backscatter communication (BackCom), as well as outline security and privacy attacks in RIS-assisted 6G applications. Our specific focus is on security and privacy threats associated with the use of RISs with different vital 6G technologies, including millimeter wave (mmWave), terahertz (THz), device-to-device (D2D) communication, Internet of Things (IoT) networks, multi-access edge computing (MEC), integrated sensing and communication (ISAC), simultaneous wireless information and power transfer (SWIPT), and non-terrestrial network. The review concludes with an outline of open research challenges and promising future directions to further increase secrecy of RIS-assisted 6G applications. The results of this review contribute to previous research on 6G network security, in general, and RIS-based 6G network security, in particular.","['6G mobile communication', 'Security', 'Communication system security', 'Wireless networks', 'Millimeter wave communication', 'MIMO communication', 'Wireless sensor networks']","['Reconfigurable intelligent surfaces', '6G', 'physical layer security', 'privacy', 'wireless communication']"
"Massive machine type communications (mMTC) is one of the three major scenarios in 5G wireless networks. mMTC is an uplink-dominant scenario that supports massive connectivity of small-size packets and sporadic traffic. To meet the requirement of mMTC, non-orthogonal multiple access (NOMA) with grant-free transmission is considered a promising technology because of its higher spectrum efficiency and lower signaling overhead. Request-and-grant-based scheduling process is eliminated in grant-free transmission and thus the base stations need to identify active users first before data detection. Previously, several user activity detection (UAD) algorithms were proposed and they can obtain good performance in flat-fading channels. However, the performance of these works degrades significantly in doubly selective fading channels. In this work, we propose an effective solution for uplink grant-free NOMA with a new user sequence scheme and a novel UAD algorithm suitable for practical channel scenarios. Simulation results validated that the proposed sequences can mitigate the inter-user interference at the base stations, and the proposed UAD algorithm can achieve better detection accuracy. Moreover, detailed complexity analysis showed that the proposed UAD algorithm has relatively low complexity and good scalability.","['NOMA', 'Uplink', 'Computational complexity', 'Scalability', 'OFDM', 'Fading channels', 'Channel models']","['Grant-free transmission', 'non-orthogonal multiple access (NOMA)', 'user activity detection (UAD)', 'sequence design', 'expectation maximization (EM)']"
"The upcoming sixth generation (6G) mobile communication system is expected to operate across a wide range of spectrum that includes not only the bands used by previous generations but also higher frequency bands such as millimeter wave (mmWave), which are currently assigned to fifth generation (5G) networks, terahertz (THz), and optical spectrum. By utilizing a broader range of frequencies, it will be possible to support 6G applications with faster data rates, higher capacity, and lower latency. However, the higher frequency bands pose unique challenges such as higher path loss, absorption loss, and engineering difficulties for antennas and radio frequency (RF) circuitry design, which require advanced technologies and innovative solutions. Given that the spectrum is a scarce resource, efficient management is crucial to ensure the most effective exploitation of frequency bands. The spectrum management has evolved over the years, with different approaches being used to assign and utilize frequency bands. In this paper, we provide a review of spectrum management approaches, including their use in awarding 5G spectrum, and explore their expected use in 6G. We then offer a brief overview of spectrum sharing and its role in enabling the efficient use of spectrum resources. The regulations, standardization, features, limitations, and potential use cases of higher frequency bands such as, mmWave, THz, and visible light (VL) are analyzed to provide a comprehensive understanding of the spectrum options available for the upcoming 6G technology.","['6G mobile communication', 'Radio spectrum management', '5G mobile communication', 'Millimeter wave communication', 'Standardization', 'Surveys', 'Artificial intelligence']","['6G communications', 'millimeter wave', 'THz band', 'optical wireless communication', 'spectrum management approaches', 'spectrum sharing']"
"Recent works have demonstrated Filter Bank Multicarrier Spread Spectrum (FBMC-SS) to be a robust communication scheme in the presence of high-power interferers. Existing FBMC-SS symbol detector designs based on analysis filter banks (AFB) suggest using an optimal combining scheme to suppress the interferers, necessitating some noise/interference power estimation method. In this paper, we introduce a symbol detector with blind interference suppression by extending a recently developed packet detection method. We then provide an analysis to show that the existing AFB-based symbol detector and the one proposed in this paper are equivalent in typical usage scenarios. A fully-fledged receiver design is proposed utilizing this symbol detector, with specifics presented for estimation of the channel impulse response and carrier frequency offset (CFO). We also outline a method of iterating upon the channel and CFO estimations to improve the quality of both parameters. Moreover, a modification to allow improved performance of the symbol detector at high SNR is provided. Finally, simulated performance results are presented to corroborate these findings and demonstrate the efficiency of this receiver design.","['Receivers', 'Detectors', 'Channel estimation', 'Estimation', 'OFDM', 'Interference suppression']","['Filter bank multicarrier', 'interference suppression', 'receivers', 'spread spectrum communication']"
"Shortening the packet length has been a consensus in wireless network design for supporting the ultra-low latency Internet of Things (IoT) applications. Yet, with short-packet transmission, the rate loss would occur, which further depends on the blocklength, making the network optimization notoriously difficult, especially for random access networks. This paper focuses on the representative random access network, i.e., Aloha, with short packet transmission, namely, short-packet Aloha. Specifically, we aim to optimize the sum rate and access delay of short-packet Aloha. By deriving the probability of successful transmissions of packets, both the network sum rate and the probability generating function of access delay are obtained as explicit functions of key system parameters. The maximum sum rate and the minimum mean access delay are further derived by jointly tuning the packet transmission probability and the blocklength of packets. The effect of system parameters on the optimal sum rate and access delay performance is investigated. It is shown that the maximum sum rate is insensitive to the retry limitM, while deteriorates as the information bits per packetkdecreases. In contrast, the optimal delay performance can be improved with a smallMork. The reliability performance is also evaluated and shown to be enhanced with a large retry limitM. The analysis sheds important light on the access design of practical short-packet Aloha networks. By taking LTE-M as an example, it is found that to improve access delay performance, the information bits per packetkshould not exceed an upperbound, which polynomially decreases as the network size increases.","['Delays', 'Encoding', 'Reliability', 'Optimization', 'Tuning', 'Receivers', 'Decoding']","['Aloha', 'finite block length region', 'low latency', 'maximum sum rate', 'short-packet']"
"Even at the current age of sophisticated communication systems, remote rural communities with low user density throughout the world lack broadband Internet service of good quality. Traditional ways fall short to meet the demand of those regions due to technological or geographical challenges as well as less economic viability. Since the unlicensed operation of spare TV (Television) spectra was approved by the authorities in many countries of the world, various applications of those spectra, known as TV White Space (TVWS), have been researched. In many literature and implementations, TVWS has been found to be feasible in providing quality broadband Internet service to deprived rural communities. Penetration through obstacles, good non-line-of-sight connectivity, long-distance transmission, wider horizontal beam-width, and abundance of unused TV channels in remote rural regions are some of the promising features of TVWS which can be leveraged to connect rural communities to broadband Internet and related digital services. We made a comprehensive effort to evaluate TVWS based broadband Internet connectivity through laboratory testing and outdoor testing with a measurement campaign at Stillwater, Oklahoma and by providing live Internet service to a client in the rural area of Tillman County, Oklahoma, USA. Radio frequency (RF) coverage heatmaps have been created over the geographical map using the received signal data measured. Also, profiles of received power, signal to noise ratio (SNR) and throughput have been analyzed against antenna directionality. The purpose of this study is to provide the details of our TVWS trial and lessons learned from it, followed by performance analysis, comparison of results with theory and future directions.","['Throughput', 'TV', 'Broadband communication', 'White spaces', 'Signal to noise ratio', 'Antenna measurements', 'Testing']","['Television White Space (TVWS)', 'dynamic spectrum access', 'cognitive radio', 'broadband Internet', 'digital divide']"
"Terahertz (THz) communications systems have been considered as one of the enabling technologies for beyond 5G mobile networks, with the ability to offer higher capacity and lower latency compared to the current systems. In this paper, we develop a new THz channel model that captures rough surface scattering while considering attenuation due to molecular absorption of the atmosphere. The main advantage of the proposed model is that it conserves the energy of the channel impulse response, regardless of the number of launched rays in the ray-tracing simulator, or the size of the tiles for the scattering surface. Using the proposed model, a simple propagation scenario for the indoor environment has been analyzed, and the improvement over other models discussed in detail.","['Scattering', 'Surface roughness', 'Rough surfaces', 'Surface waves', 'Frequency measurement', 'Transmitters', 'Reflection']","['Terahertz band', 'rough surface scattering', 'channel impulse response', 'ray-tracing', 'multipath channel modeling']"
"In this paper, the joint access point (AP) association and transmission time allocation problem in densely deployed multirate WLANs based on time sharing medium access control (MAC) is considered. In our study, the centralized coordinated joint AP association and transmission time allocation problem with heterogeneous station (STA) throughput demands is first formulated as an NP-hard single non-zero programming (SNZP) optimization problem with proportionally fair (PF) throughput based objective function. To solve the NP-hard SNZP problem, two novel algorithms, the SNZP relaxation (SNZPR) algorithm and the iterative SNZPR (iSNZPR) algorithm, are proposed. The property and complexity of the proposed algorithms are analyzed. In addition, for dynamic network scenarios where there are new STAs arriving at and trying to join the network, a distributed joint admission, AP association and transmission time allocation (DAAA) algorithm is proposed to determine whether to admit a new STA arriving at the network or not. The performance of the proposed algorithms is then investigated and compared with that of some existing algorithms based on numerical results.","['Throughput', 'Resource management', 'Heuristic algorithms', 'Wireless LAN', 'Quality of service', 'Interference', 'Optimization']","['Access point association', 'time sharing', 'IEEE 802.11ax', 'wireless local area networks', 'cross-layer design and optimization']"
"This paper provides a review of user selection algorithms for massive multiple-input multiple-output (MIMO) systems under the line-of-sight (LoS) propagation model. Although the LoS propagation is extremely important to some promising technologies, like in millimeter-wave communications, massive MIMO systems are rarely studied under this propagation model. This paper fills this gap by providing a comprehensive study encompassing several user selection algorithms, different linear precoders and simulation setups, and also considers the effect of partial channel state information (CSI). One important result is the existence of practical cases in which the LoS propagation model may lead to significant levels of interference among users within a cell; these cases are not satisfactorily addressed by the existing user selection algorithms. Motivated by this issue, a new user selection algorithm based on inter-channel interference (ICI) called ICI-based selection (ICIBS) is proposed. Unlike other techniques, the ICIBS accounts for the ICI in a global manner, thus yielding better results, especially in cases where there are many users interfering with each other. In such scenarios, simulation results show that when compared to the competing algorithms, the proposed approach provided an improvement of at least 10.9% in the maximum throughput and 7.7% in the 95%-probability throughput when half of the users were selected.","['Massive MIMO', 'Interference', 'Antennas', 'Clustering algorithms', 'Signal to noise ratio', 'Machine learning algorithms', 'Throughput']","['Massive MIMO', 'favorable propagation', 'user selection', 'line-of-sight channel', 'inter-channel interference']"
"Fibbing is a network technology that can provide flexible routing in IP networks. In a Fibbing network, the network controller cleverly broadcasts link state advertisements (LSAs) of the open shortest path first (OSPF) protocol to generate fake nodes. These fake nodes enlarge the physical topology and turn the network into a virtual network. Although the routers still follow the shortest path routing in the virtual network, the flows are steered along the desired paths in the physical network. Even though a Fibbing network is capable of flexible routing, methods to achieve fast failure recovery have not been well studied. Conventionally, an IP network running the OSPF protocol takes a long time to converge after a failure occurs. The IETF proposed loop-free alternate (LFA) IP fast reroute technology to speed up failovers. However, we discovered that LFA technology is incompatible with Fibbing. The direct application of LFA technology in a Fibbing network will generate traffic loops. In this work, we propose a novel monitoring-cycle-based approach for fast failure recovery. By examining the liveness of the monitoring cycles, the system controller can promptly identify a failed link or a failed node and then perform traffic rerouting. Due to the properties of IP packet forwarding, a monitoring cycle in an IP network must be a simple cycle. We prove that any network that is both 2-vertex- and 3-edge-connected (2V3E) possesses a set of monitoring cycles that can be used to detect and identify any single link failure. In conjunction with the status pattern provided by the monitoring cycles, any single node failure can be detected by introducing an additional probe message. We propose an algorithm to obtain these monitoring cycles and design and implement a Fibbing controller for the network. To evaluate the performance of the proposed approach, we construct a physical testbed and an emulation network. The experimental results show that the proposed system works stably and can achieve a fast failover within a short period of time.","['Contracts', 'Task analysis', 'Companies', 'Planning', 'Employment', 'Mixed integer linear programming', 'Regulation']","['IP fast failure recovery', 'link failure detection', 'node failure detection', 'monitoring cycles', 'fibbing controller design']"
"High performance computing clusters are increasingly operating under a shared/buy-in paradigm. Under this paradigm, users choose between two tiers of services: shared services and buy-in services. Shared services provide users with access to shared resources for free, while buy-in services allow users to purchase additional buy-in resources in order to shorten job completion time. An important feature of shared/buy-in computing systems consists of making unused buy-in resources available to all other users of the system. Such a feature has been shown to enhance the utilization of resources. Alongside, it creates strategic interactions among users, hence giving rise to a non-cooperative game at the system level. Specifically, each user is faced with the questions of whether to purchase buy-in resources, and if so, how much to pay for them. Under quite general conditions, we establish that a shared/buy-in computing game yields a unique Nash equilibrium, which can be computed in polynomial time. We provide an algorithm for this purpose, which can be implemented in a distributed manner. Moreover, by establishing a connection to the theory of aggregative games, we prove that the game converges to the Nash equilibrium through best response dynamics from any initial state. We justify the underlying game-theoretic assumptions of our model using real data from a computing cluster, and conduct numerical simulations to further explore convergence properties and the influence of system parameters on the Nash equilibrium. In particular, we point out potential unfairness and abuse issues and discuss solution venues.","['Games', 'Nash equilibrium', 'Computational modeling', 'Pricing', 'Convergence', 'Numerical models', 'Data models']","['Computing clusters', 'dynamics', 'equilibrium analysis', 'pricing']"
"Long Range (LoRa) has become a key enabler technology for low power wide area networks. However, due to its ALOHA-based medium access scheme, LoRa has to cope with collisions that limit the capacity and network scalability. Collisions between randomly overlapped signals modulated with different spreading factors (SFs) result in inter-SF interference, which increases the packet loss likelihood when signal-to-interference ratio (SIR) is low. This issue cannot be resolved by channel coding since the probability of error distance is not concentrated around the adjacent symbol. In this paper, we analytically model this interference, and propose an interference cancellation method based on the idea of segmentation of the received signal. This scheme has three steps. First, the SF of the interference signal is identified, then the equivalent data symbol and complex amplitude of the interference are estimated. Finally, the estimated interference signal is subtracted from the received signal before demodulation. Unlike conventional serial interference cancellation (SIC), this scheme can directly estimate and reconstruct the non-aligned inter-SF interference without synchronization. Simulation results show that the proposed method can significantly reduce the symbol error rate (SER) under low SIR compared with the conventional demodulation. Moreover, it also shows high robustness to fractional sample timing offset (STO) and carrier frequency offset (CFO) of interference. The presented results clearly show the effectiveness of the proposed method in terms of the SER performance.","['Symbols', 'Interference cancellation', 'Synchronization', 'Receivers', 'Logic gates', 'Estimation', 'Discrete Fourier transforms']","['IoT', 'Interference cancellation', 'LoRa', 'LPWAN', 'signal reconstruction']"
"Increasing the availability of Unmanned Aerial Vehicles (UAV's) platforms leads to a variety of applications for aerial exploration, surveillance, and transport. Many of these applications rely on the communication between the UAV and the ground receiver which is subjected to high mobility that may lead to restrictions on link connectivity and throughput. In order to design high throughput and efficient communication schemes for these scenarios, a deep understanding of the communication channel behavior is required, especially taking into account measurement data from flight experiments. Channel propagation in urban environments involves diffraction effects which modify the Line-of-Sight (LoS) contribution of the total received signal, especially when the receiver is located on the ground. This process leads to scenarios where Multiple-Input Multiple-Output (MIMO) signal processing can take advantage from this situation. In this context, the goal of this paper is to study the diffraction effects of the LoS component through spatial correlation metrics of the signal. To accomplish this, we propose the use of a geometric stochastic technique to model the channel behavior which lies between High Altitude Platforms (HAP) and terrestrial link communications.","['Correlation', 'Diffraction', 'Urban areas', 'Receiving antennas', 'Throughput', 'Unmanned aerial vehicles', 'Space division multiplexing']","['MIMO', 'unmanned aerial vehicles', 'channel model', 'air-to-ground']"
"In this paper, we study the uplink transmission in an intelligent reflecting surface (IRS) assisted cell-free multiple-input multiple-output (MIMO) system where the central processing unit (CPU) only has statistical channel state information (CSI) to detect symbols, and to design the receiver filter coefficients, the power allocations, and the IRS phase shifts. The access points (APs) estimate only their local end-to-end channels with the users using minimum mean squared error (MMSE) estimation to implement matched filtering, thereby avoiding the large overhead associated with estimating individual IRS-assisted channels. Under this framework, we derive a closed-form expression for the achievable uplink net rate that only depends on the channel statistics. Using this expression, we formulate the problem of maximizing the minimum (max-min) signal-to-interference plus noise ratio (SINR) to design the receiver filter coefficients at the CPU, the power allocations at the users, and the phase shifts at the IRS, subject to per user power constraints as well as IRS phase shift resolution constraints. The resulting problem is jointly non-convex in the three design variables and is solved using an alternating optimization algorithm. In particular, the receiver filter design is formulated as a generalized eigenvalue problem leading to a closedform solution, the power allocation problem is solved using a geometric programming (GP) approach, and the IRS phase shifts are designed using an alternating maximization algorithm. For comparison, we also formulate and solve the max-min SINR problem for the scenario where the instantaneous imperfect CSI of all individual direct and IRS-assisted channels is available at the CPU. Numerical results show that the scheme designed using statistical CSI has the potential to outperform the scheme based on instantaneous CSI for moderate to large number of IRS elements, due to savings in the channel estimation overhead.","['Channel estimation', 'Massive MIMO', 'Antenna arrays', 'Uplink', 'Training', 'Protocols', 'Interference']","['Intelligent reflecting surface', 'channel state information (CSI)', 'statistical CSI', 'instantaneous CSI', 'achievable rates', 'alternating optimization']"
"In order to reduce the hardware complexity and cost of mmWave transceivers, hybrid beamforming techniques have been developed, which rely on the channel state information (CSI) available to the receiver and/or transmitter. In mmWave channel estimation, the compressed sensing (CS)-based algorithms like orthogonal matching pursuit (OMP) have been widely studied to take the advantages of the sparse characteristics of mmWave channels. Specifically, the OMP-assisted adaptive codebook channel estimation has the merit of reduced implementation complexity, but it performs undesirably in low signal to noise ratio (SNR) scenarios. To circumvent this problem, in this paper, we develop an improved adaptive codebook channel estimation algorithm for orthogonal frequency division multiplexing (OFDM) mmWave systems, which enhances the estimation performance by exploiting the multi-carrier signals for joint decision making. Our studies show that the proposed channel estimation is capable of significantly improving the estimation accuracy at low SNR, while enjoying a low complexity for implementation.","['Channel estimation', 'Millimeter wave communication', 'Radio frequency', 'Array signal processing', 'OFDM', 'Matching pursuit algorithms', 'Adaptive systems']","['mmWave', 'channel estimation', 'adaptive codebook', 'OFDM', 'hybrid beamforming']"
"In this article, several extensions to an alternating space-time (ST) code (STC) physical layer security (PLS) scheme from the literature are proposed. These contributions include alternative orthogonal STCs as well as non-orthogonal and spatially-multiplexed (SM) STCs for improved bandwidth efficiency. Phase rotation (PR) algorithms are provided to build very large sets of unique alternative STCs for use with this scheme. Decoding methods are discussed for alternating non-orthogonal and SM STCs. Monte Carlo simulations are provided to compare bit error rate (BER) performance between different decoding methods. Secrecy system nomenclature is adapted to the alternative STCs and proposed algorithms. Information-theoretic security analysis including message and key equivocation is provided along with expected eavesdropper BER based on an assumed attack methodology. A comparison of security offered by the alternating STC PLS scheme with and without incorporation of proposed PR algorithms is performed. Substantially greater exhaustive key search attack complexity is achieved by using the PR algorithms proposed in this article.","['Buildings', 'Security', 'Three-dimensional displays', 'Bit error rate', 'Signal processing algorithms', 'Physical layer security', 'Decoding']","['Key equivocation', 'key residue class', 'message equivocation', 'phase rotation', 'physical layer security', 'space-time coding']"
"The undeniable potential of computation offloading has been attracting attention from researchers for more than a decade. With advances in multi-access edge computing (MEC), computation offloading has become a more critical issue because of the heterogeneity in the computational power of edge devices and the elevated importance of extending their lifespan. Due to the apparent advantages, the use of MEC in 6G networks, where a vertical heterogeneous network composed of space, air, and ground networks is only natural. The non-terrestrial networking elements constitute effective computational resources. However, recent research investigating the potential of computational offloading in 6G networks has involved models that do not adequately reflect the complexity of the underlying processes. In this study, we propose a realistic computation model for 6G networks that considers crucial properties of the offloaded job, including the inter-dependency of the job tasks and the decomposability of the job. Our model is based on the mature application domain of MEC, where proven solutions are already studied. We also investigate the potential of a high altitude platform station (HAPS)-aided MEC platform using this model. The proposed model allows us to design offloading strategies to enable adaptive computational offloading. Through numerical analyses, we show that the proposed model provides sufficient insight to reduce the total processing time significantly.","['Task analysis', 'Computational modeling', '6G mobile communication', 'Adaptation models', 'Numerical models', 'Atmospheric modeling', 'Satellites']","['Computation offloading', 'high altitude platform station', 'multi-access edge computing']"
"The global averaged civilian positioning accuracy is still at meter level for all existing Global Navigation Satellite Systems (GNSSs), and the performance is even worse in urban areas. At lower altitudes than satellites, high altitude platform stations (HAPS) offer several benefits, such as lower latency, less pathloss, and likely smaller overall estimation error for the parameters associated in the pseudorange equation. HAPS can support GNSSs in many ways, and in this paper we treat the HAPS as another type of ranging source. In so doing, we examine the positioning performance of a HAPS-aided GPS system in an urban area using both a simulation and physical experiment. The HAPS measurements are unavailable today; therefore, they are modeled in a rather simple but logical manner in both the simulation and physical experiment. We show that the HAPS can improve the horizontal dilution of precision (HDOP), the vertical dilution of precision (VDOP), and the 3D positioning accuracy of GPS in both suburban and dense urban areas. We also demonstrate the applicability of a RAIM algorithm for the HAPS-aided GPS system, especially in the dense urban area.","['Satellites', 'Global Positioning System', 'Global navigation satellite system', 'Urban areas', 'Receivers', 'Low earth orbit satellites', 'Extraterrestrial measurements']","['High altitude platform station (HAPS)', 'horizontal dilution of precision (HDOP)', 'pseudorange', 'receiver autonomous integrity monitoring (RAIM)', 'vertical dilution of precision (VDOP)']"
"The millimeter-wave (mmWave) large-scale antenna arrays (LSAAs) systems play a vital role in increasing the beamforming (BF) gain and acquiring highly directional propagation. Recently, non-orthogonal multiple access (NOMA) has been integrated into these systems to manage massive connectivity and achieve spectral-efficient communications. This paper focuses on angle-domain (AD) hybrid beamforming (BF) for mmWave LSAAs and NOMA systems, thanks to the low complexity, power consumption, and channel estimation overhead. However, with limited radio-frequency chains, the hybrid BF-based single-beam (SB)-NOMA scheme generating a single beam to serve the NOMA users fails to exploit the multi-user diversity due to narrow beams with LSAAs. To tackle this limitation, we design schemes offering additional degrees of freedom. More importantly, they require only the knowledge of angular information and are suitable for either linear or rectangular antenna arrays, unlike those proposed in the literature. The first scheme exploits the time-domain resources to schedule groups having high spatial interference within distinct time slots. To minimize the need for fast and precise synchronization when applying time division multiple access (TDMA) with mmWave NOMA, we leverage the multi-beam (MB)-NOMA framework. And we propose a joint SB- and MB-NOMA scheme to benefit from NOMA multi-user diversity, whatever the cell load and the users’ positions. Using the New York University channel simulator (NYUSIM), we further validate the performance of the proposed schemes compared to the solution proposed in the literature and others using fully digital BF. Specifically, the proposed TDMA-based scheme achieves a sum-rate gain of up to 83% over the TDMA-based one existing in the literature. Moreover, we verify the superiority of applying both SB- and MB-NOMA instead of only MB-NOMA.","['Millimeter wave communication', 'NOMA', 'Radio frequency', 'Time division multiple access', 'Resource management', 'Interference', 'Complexity theory']","['Millimeter-wave communications', 'massive multiple-input multiple-output', 'hybrid beamforming', 'non-orthogonal multiple access', 'angle-domain information']"
"Network slicing has been a significant technological advance in the 5G mobile network allowing delivery of diverse and demanding requirements. The slicing grants the ability to create customized virtual networks from the underlying physical network, while each virtual network can serve a different purpose. One of the main challenges yet is the allocation of resources to different slices, both to best serve different services and to use the resources in the most optimal way. In this paper, we study the radio resource slicing problem for Ultra-Reliable Low Latency Communications (URLLC) and enhanced Mobile Broadband (eMBB) as two prominent use cases. The URLLC and eMBB traffic is multiplexed over multiple numerologies in 5G New Radio, depending on their distinct service requirements. Therein, we present our optimization algorithm, Mixed-numerology Mini-slot based Resource Allocation (MiMRA), to minimize the impact on eMBB data rate due to puncturing by different URLLC traffic classes. Our strategy controls such impact by introducing a puncturing rate threshold. Further, we propose a scheduling mechanism that maximizes the sum rate of all eMBB users while maintaining the minimum data rate requirement of each eMBB user. The results obtained by simulation confirm the applicability of our proposed resource allocation algorithm.","['Ultra reliable low latency communication', '5G mobile communication', 'Resource management', 'Optimization', 'Throughput', 'Reliability', 'Network slicing']","['B5G', 'eMBB', 'numerology', 'puncturing', 'resource allocation', 'URLLC']"
"Data communication has seen exponential growth recently, and it currently dominates wireless communication. As a result, proactive caching was developed to minimize peak traffic rates by storing content, in advance, at different nodes in the network. We consider proactive caching for a broadcast wireless network with one central hub such as a satellite (ST) andKassociated mobile units (MUs) such as mobile mini-ground stations or end users. The ST has a library of files, and the MUs demands are assumed to be limited to this library, while the popularity of the library files changes over time. We assume that the MUs demands arrive at different times, and hence, asynchronous file delivery is necessary. We propose a new scheme that minimizes the files delivery sum rate and show that we can use the file delivery messages to proactively and constantly update the MU finite caches. We show that this mechanism reduces the downloaded traffic of the network. The proposed scheme uses index coding to jointly encode the delivery of different demanded files with the cache updates to other MUs to follow the changes in the files popularities. An offline optimization of the delivery sum rate of the scheme is proposed, where it requires knowledge of the files popularities across the whole transmission period. In particular, the problem is formulated as a linear program and the optimal caching is obtained numerically. Moreover closed form solutions to two special cases are derived and a lowerbound to the achievable delivery sum rate is developed. Numerical results show the benefits of the proposed scheme over conventional caching schemes, in terms of reducing the delivery sum rate.","['Indexes', 'Encoding', 'Receivers', 'Satellite broadcasting', 'Libraries', 'Wireless networks', 'Transmitters']","['Coded caching', 'data communications', 'index coding', 'proactive caching', 'satellite broadcast']"
"Signal classification is a universal problem in adversarial wireless scenarios, especially when an eavesdropping radio receiver attempts to glean information about a target transmitter’s patterns, attributes, and contents over a wireless channel. In recent years, research surrounding the idea of Machine Learning (ML)-based signal classification has focused on modulation classification, with the downstream objective of demodulation. However, while the computer vision data domain has made significant progress in ensuring robust classification of images despite crafted perturbations, this success has not been translated to secure modulation classification. In this work, we perform the first-ever physical test of an eavesdropping ML-based modulation classifier radio, which we trained offline using a ensemble of i.i.d. models. Each model is trained with a weighted mixture of data perturbed by iterative, “least likely” white box attacks and non-attacked data. We then tested the ensemble online using coaxial-connected Software Defined Radios (SDRs). We conducted a case study comparing our results to the state-of-the-art computer vision approaches to investigate the presence of “label leaking”, model capacity sensitivity, understand the viability of parallel and sequential variations on perturbation training, and assess the effectiveness of iterative attack training. Our results show that perturbations can result in guessing-level classification performance from eavesdroppers, and that varying levels of robustness can be achieved against all presented attacks. These findings confirm that any receiver presents a new attack vector by utilizing ML techniques for classification tasks, and can be vulnerable to evasion attacks at little-to-no cost to transmitters. Consequently, we argue for the use of our training scheme in all ML-based classifying radios where security is a concern.","['Perturbation methods', 'Training', 'Wireless communication', 'Modulation', 'Radio transmitters', 'Computational modeling', 'Communication system security']","['Adversarial perturbations', 'adversarial training', 'modulation classification', 'supervised learning', 'software defined radio']"
"In 5G-and-beyond networks, the concept of Network Slicing is used to support multiple independent and co-existing logical networks on a physical network infrastructure. The infrastructure provider (InP) owns the set of virtual and physical resources that are used to support the tenant slice requests. Each slice request specifies a service level agreement (SLA) that contains the required slice-level resources (computation and communication) and the revenue provided by the tenant. Due to limited resources, the InP cannot accommodate all requests made by the tenants. In general, it has been found that tenants tend to overestimate their resource demands (e.g., for 5G Core computation) to reduce possible SLA violations. In this paper, we consider two major slice types: Elastic (low priority and low revenue) and Inelastic (high priority and high revenue). We apply the concept of overbooking, where the InP accepts more slices while considering slice priorities in order to maximize the overall revenue and utilization. We consider a multi-tenant environment and propose a slice admission system named PRLOV, which is based on Reinforcement Learning (RL) and prediction methods. The system predicts the future resource demands of Elastic slices and applies an opportunistic overbooking technique to overbook InP for accepting more slices. In addition, the admission decision is formulated as an Markov Decision Process (MDP) problem and solved using standard RL techniques (Policy Iteration, Q-Learning, DQN). The performance of our proposed work is compared against three other heuristics (Basic, Prediction, PRL) that do not use overbooking. Data traces from the Materna data center networks were used for prediction purposes. The important performance metrics measures include InP total revenue, the acceptance rate of respective slices and overall resource utilization for different slices. The results show that the proposed work significantly outperforms the other mechanisms in terms of revenue gain and resource utilization. Simulation results show that PRLOV provides a revenue gain of 6%, 26%, and 102% compared to the PRL, Prediction and Basic scheme.","['Indium phosphide', 'III-V semiconductor materials', '5G mobile communication', 'Service level agreements', 'Resource management', 'Network slicing', 'Q-learning']","['5G networks', 'network slicing', 'infrastructure provider', 'overbooking', 'reinforcement learning', 'prediction', 'slice priority']"
"An array of detectors is widely used in deep space optical communications for both fine beam tracking and symbol detection. In this paper, we have analyzed and compared the performance of different combining schemes—such as equal gain combiner (EGC), selection combiner (SC) and the maximal ratio combiner (MRC)—that are used to fuse the outputs from the elements of such arrays. The three combining schemes are compared in terms of error rate performance and computational complexity. Additionally, for the limiting case of continuous arrays of circular configuration, the optimization of two important parameters—the spot size and the array size—is also discussed in order to achieve the best error rate performance.","['Detectors', 'Diversity reception', 'Symbols', 'Optical arrays', 'Arrays', 'Adaptive optics', 'Optical receivers']","['Array of detectors', 'equal gain combiner', 'free-space optics', 'maximal ratio combiner', 'probability of error', 'selection combiner']"
"Visible light communication (VLC) has been introduced as a key enabler for high-data rate wireless services in future wireless communication networks. In addition to this, it was also demonstrated recently that non-orthogonal multiple access (NOMA) can further improve the spectral efficiency of multi-user VLC systems. In this context and owing to the significantly promising potential of artificial intelligence in wireless communications, the present contribution proposes a deep Q-learning (DQL) framework that aims to optimize the performance of an indoor NOMA-VLC downlink network. In particular, we formulate a joint power allocation and LED transmission angle tuning optimization problem, in order to maximize the average sum rate and the average energy efficiency. The obtained results demonstrate that our algorithm offers a noticeable performance enhancement into the NOMA-VLC systems in terms of average sum rate and average energy efficiency, while maintaining the minimum convergence time, particularly for higher number of users. Furthermore, considering a realistic downlink VLC network setup, the simulation results have shown that our algorithm outperforms the genetic algorithm (GA) and the differential evolution (DE) algorithm in terms of average sum rate, and offers considerably less run-time complexity.","['Resource management', 'NOMA', 'Visible light communication', 'Light emitting diodes', 'Q-learning', 'Optimization', 'Energy efficiency']","['Deep reinforcement learning', 'multiple access', 'resource allocation', 'sum-rate', 'visible light communications']"
"The scarce spectrum and power resources, the inter-beam interference, together with the high traffic demand, pose new major challenges for the next generation of Very High Throughput Satellite (VHTS) systems. Accordingly, future satellites are expected to employ advanced resource/interference management techniques to achieve high system spectrum efficiency and low power consumption while ensuring user demand satisfaction. This paper proposes a novel demand and interference aware adaptive resource management for geostationary (GEO) VHTS systems. For this, we formulate a multi-objective optimization problem to minimize the total transmit power consumption and system bandwidth usage while matching the offered capacity with the demand per beam. In this context, we consider resource management for a system with full-precoding, i.e., all beams are precoded; without precoding, i.e., no precoding is applied to any beam; and with partial precoding, i.e., only some beams are precoded. The nature of the problem is non-convex and we solve it by jointly using the Dinkelbach and Successive Convex Approximation (SCA) methods. The simulation results show that the proposed method outperforms the benchmark schemes. Specifically, we show that the proposed method requires low resource consumption, low computational time, and simultaneously achieves a high demand satisfaction.","['Precoding', 'Interference', 'Optimization', 'Bandwidth', 'Satellites', 'Satellite broadcasting', 'Resource management']","['Demand satisfaction', 'Dinkelbach method', 'high throughput GEO Satellite', 'precoding', 'radio resource management technique', 'successive convex approximation']"
"Next generation cellular networks are expected to connect billions of devices through its massive machine-type communication (mMTC) use case. To achieve this, waveforms with improved spectral confinement and high spectral efficiency compared to cyclic-prefix orthogonal division multiplexing (CP-OFDM) are required. In this paper, filter-bank muilticarrier with quadrature amplitude modulation (FBMC-QAM) is studied as an alternative to CP-OFDM for such applications. However, FBMC-QAM presents the challenge of high intrinsic interference due to the loss of complex orthogonality in time-frequency domain. Bit-interleaved coded modulation with iterative decoding (BICM-ID) has been shown to exhibit capacity-approaching decoding performance. Therefore, in this paper, an iterative interference cancellation (IIC) based BICM-ID receiver is designed to cancel the intrinsic interference in FBMC-QAM systems. The proposed receiver consist of an inner decoder, which combines iterative demodulation and interference cancellation, and an outer decoder which is a low density parity check (LDPC) channel decoder. To evaluate the convergence behaviour of the proposed receiver, extrinsic information transfer (EXIT) chart analysis is employed, where EXIT curves for the components of the iterative decoder are derived. Numerical results show that the intrinsic interference in FBMC-QAM systems can be removed by adopting the proposed IIC-based BICM-ID receiver. With a receiver that is able to successfully cancel intrinsic interference, FBMC-QAM becomes an interesting alternative waveform for asynchronous mMTC applications due to its superior frequency localization compared to CP-OFDM. Furthermore, it has been shown that FBMC-QAM with the IIC-based receiver can achieve similar bit-error-rate (BER) performance as a CP-OFDM benchmark under different fading channels. Finally, the complexity of the proposed receiver for FBMC-QAM is analysed and compared to the complexity of the CP-OFDM benchmark.","['Receivers', 'Interference', 'Decoding', 'Iterative decoding', 'Convergence', 'Modulation']","['Non-orthogonal waveforms', 'OFDM', 'FBMC-QAM', 'iterative decoding', 'BICM-ID', 'LDPC', 'EXIT charts', 'interference cancellation', 'mMTC']"
"Driven by the extended applications and scarce spectrum resources, integrating the radio sensing functions into the future mobile system has been a consensus between the stakeholders. This paper demonstrates the feasibility of joint localization and imaging functions by exploiting the reference signal in the radio frame defined in the mobile system. Essentially, the subspace-based algorithms are adopted to jointly estimate the angle and distance and to enable the localization function for uplink and downlink signals and the derivation of theoretical performance boundaries. The vector antenna and virtual array concepts are introduced in the downlink scenario to enhance the angle estimation resolution. The uplink sounding reference signal is exploited to enable the imaging function as analog to synthetic-aperture radar (SAR). The joint localization and imaging performances are verified with a realistic ray-tracing channel based on the 3D ground and buildings model of Muret airport in France. The simulation results show that the reference signal can provide acceptable localization accuracy and target-distinguishing capability by adopting a virtual array and joint time-spatial smoothing in downlink and uplink, respectively. The joint localization and imaging results prove that the future mobile network is potentially a viable infrastructure to provide economic surveillance solutions for airports, particularly for secondary airports that are not well equipped with dedicated surveillance systems.","['Location awareness', 'Airports', '5G mobile communication', 'Imaging', 'Sensors', 'Surveillance', 'Estimation']","['5G', '6G', 'airport surveillance', 'localization', 'vector antenna', 'multiple signal classification (MUSIC)', 'angle and delay estimation', 'virtual array', 'sounding reference signal', 'positioning reference signal', 'Integrated sensing and communications (ISAC)']"
"In this paper, an ensemble learning-driven edge caching (ELDEC) strategy and a meta-based ensemble learning-driven edge caching (MELDEC) strategy are proposed for content popularity prediction and cache content placement in Internet-of-Vehicles (IoV) networks. Specifically, the proposed MELDEC and ELDEC strategies incorporate meta learning and ensemble learning for enhanced content popularity prediction in IoV networks. Closed-form outage probability and finite signal-to-noise ratio (SNR) diversity gain expressions are also derived to establish the relationship between the proposed edge caching strategies and the wireless performance of IoV networks. When compared against benchmark schemes, the proposed MELDEC and ELDEC strategies achieve near-optimal cache hit rates, outage probability, and finite SNR diversity gain under imperfect channel state information (CSI) estimation. We also show that the outage probability decay rate in the IoV network depends on the number of base stations and roadside units, and it is independent of the content popularity prediction of the MELDEC strategy, ELDEC strategy, and benchmark schemes. The performance analysis demonstrates that the proposed MELDEC and ELDEC strategies are promising solutions towards achieving reliable content access in IoV networks.","['Signal to noise ratio', 'Probability', 'Power system reliability', 'Diversity methods', 'Benchmark testing', 'Deep learning', 'Backhaul networks']","['Deep learning', 'ensemble learning', 'meta learning', 'multi-access edge computing', 'edge caching', 'Internet-of-Vehicles', 'vehicular networks', 'outage probability', 'finite signal-to-noise ratio (SNR)', 'diversity']"
"Non-orthogonal multiple access (NOMA) continues to receive enormous attention as a potential technique for improving the spectral efficiency (SE) of wireless networks. Although for several years most research efforts on the performance of NOMA systems focused on the ergodic sum-rate and outage probability, recent works have shifted towards error rate analysis of various NOMA configurations and designs. While the influx of publications on this topic is rich in lessons and innovations, the sheer volume of it makes it easy to get caught up in the details, so much so that one often loses sight of the overall picture. This paper serves as a survey on NOMA error rate analysis, painted in the large with bold and immediately recognizable strokes of insights to facilitate for the reader to understand and follow the up-to-date progress in this area. In addition to summarizing the principles of NOMA error rate analysis, this work aims to minimize redundancy and overlaps, identify research gaps, and outline future research directions.","['NOMA', 'Surveys', 'MIMO communication', 'Internet of Things', 'Bit error rate', 'Wireless networks', 'Resource management']","['Non-orthogonal multiple access (NOMA)', 'bit error rate (BER)', 'power assignment (PA)', 'enhanced performance']"
"In smart city, traffic congestion and parking problems will be solved assisted by precise vehicle location. To address the vehicle localization problem in smart city, a novel extrinsic information aided fingerprint localization algorithm is proposed in this paper. Firstly, on the basis of massive multiple-input multiple-output (MIMO) and orthogonal frequency division multiplexing (OFDM), the angle-delay domain channel matrix (ADDCM) is extracted. Then, an amplitude ratio based non-line of sight (NLoS) identification method is provided to estimate link states. For the case of LoS existence, in order to save storage space, a tuple fingerprint is proposed to record angle of LoS path. In other case, when all APs service in NLoS scenarios, to improve the localization robustness in dynamic environments, the correlated ADDCM (CADDCM) is taken as location fingerprint which can extract the constant information related to fixed scattering clusters. A greedy-based fingerprint matching scheme is used to search the nearest reference point (RP). Furthermore, the extrinsic information provided by neighbor vehicles, such as estimated location and measured distance, is utilized to improve the localization stability. Simulation results show that the extrinsic information aided algorithm could improves the localization accuracy and robustness in dynamic environments.","['Location awareness', 'Global Positioning System', 'Robustness', 'Scattering', 'Vehicle dynamics', 'Heuristic algorithms', 'Fingerprint recognition']","['Cell-free massive MIMO-OFDM', 'extrinsic information', 'fingerprint localization', 'scattering environment', 'vehicle localization']"
"Millimeter-wave (mmWave) communication, the main success behind the fifth generation of mobile communication networks, will increase the ultra-dense small cell deployment under its limited coverage characteristics. Therefore, providing a seamless connection to its users, to whom transitioning between indoor and outdoor in a heterogeneous network environment particularly is a significant issue that needs to be addressed. In this paper, we present a two-fold contribution with a comprehensive study on mm-wave handovers. A user-based indoor mobility prediction via Markov chain with an initial transition matrix is proposed in the first step. Based on this acquired knowledge of the user’s movement pattern in the indoor environment, we present a pre-emptive handover algorithm in the second step. This algorithm aims to keep the QoS high for indoor users when transitioning between indoor and outdoor in a heterogeneous network environment. The proposed algorithm shows a reduction in the handover signalling cost by more than 50%, outperforming conventional handover algorithms.","['Handover', 'Quality of service', 'Markov processes', '5G mobile communication', 'Bandwidth', 'Trajectory', 'Predictive models']","['mmWave 5G', 'Markov Chain', 'online learning', 'user trajectory', 'indoor mobility', 'pre-emptive handover', 'femtocells', 'heterogeneous network indoor-to-outdoor handover']"
"The multipath is unavoidable in radio frequency (RF) wireless communication, and affects almost every element of the communication systems. The impact of multipath on the received signal depends on whether the delay spread (i.e., spread of time delays associated with different multipath components) is large or small relative to the symbol period of the wireless communication system. In narrowband channels, the symbol period is set such that the delay spread is about one tenth (or less) of it. In broadband channels, it is set such that the delay spread is many times greater than the symbol period. In between these two extremes, there appears to exist an important, yet overlooked, class of channels whose delay spread is neither small nor large enough for them to fall into these two basic channel classes. In this paper, we study the effect of multipath on channels that fall in the transitional region between narrowband and broadband referred henceforth as “ mediumband ”. This paper shows that mediumband channels possess a distinct channel model, and pose both challenges and opportunities for reliable wireless communication. For instance, mediumband channels enable signalling at a significantly higher rate than that of narrowband channels, but on the flip side, as the degree of mediumband-ness increases, the quality of the channel deteriorates rapidly due to the excessive inter-symbol-interference (ISI). However, mediumband channels have an inherent ability to avoid deep fading, and if designed properly, mediumband wireless communication, which refers to wireless communication through mediumband channels, could be made to be significantly more reliable too.","['Wireless communication', 'Delays', 'Narrowband', 'Channel models', 'Fading channels', 'Broadband communication', 'Symbols']","['Communication theory', 'multipath', 'delay spread', 'wireless channel models']"
"While encryption is powerful at protecting information, it critically relies upon the mystery/private cryptographic key's security. Poor key management would compromise any robust encryption algorithm. In this way, securing information is reduced to the issue of securing such keys from unauthorized access. In this work, KeyShield is proposed, a scalable and quantum-safe key management scheme. KeyShield provides the highest security level as it relies on the impossibility of finding a unique solution to an underdetermined linear system of equations. KeyShield achieves the rekeying using a single broadcast message, called a secure lock, in an open channel rather than pairwise secure channels. Security analyses for a list of attacks are provided, along with a detailed discussion on the quantum-safe feature. KeyShield outperforms state-of-the-art schemes in several aspects, including quantum-resistance, computation cost, message overhead, storage cost, and rekeying delay.","['Cryptography', 'Quantum computing', 'Encryption', 'Servers', 'Linear systems', 'Unicast', 'Resistance']","['Key management', 'key distribution', 'quantum resistance', 'quantum-safe', 'linear system of equations', 'security', 'group security']"
"An integrated non-geostationary orbit (NGSO) satellite communication and radio astronomy system (SCRAS), was recently proposed in [1] as a new coexistence paradigm. In SCRAS, the transportation of the radio astronomical observation (RAO) data from the space to the ground stations is an important problem, for which [1] proposed a linear programming optimization based algorithm. However, the computational complexity of this algorithm is quite high which is exacerbated by the need to re-compute the algorithm frequently due to the time-varying characteristics of the observing satellites, the Earth stations, and the RAO region. In this paper, to address this high complexity issue, we develop a low-complexity RAO data transport algorithm. In addition to the static resource constraint scenario considered in the existing work, we introduce a dynamic resource constraint scenario to exploit the knowledge of the satellite communication system (SCS) traffic statistics. We also present a modified version of the algorithm in [1] for the dynamic resource constraint scenario. In addition to the number of inter-satellite link (ISL) hops as the RAO data transport cost metric, we also evaluate the metric of the sum of the squared ISL hop distances which reflects the transmission energy cost. Furthermore, computational complexity analysis and data transport costs of the algorithms are presented. Our results show that the proposed algorithm yields several orders of computational complexity saving over the existing method while incurring a modest increase in the data transport cost. Our proposed dynamic resource constraint scenario provides plausible reduction of the RAO data transport cost.","['Satellites', 'Satellite broadcasting', 'Low earth orbit satellites', 'Orbits', 'Radio astronomy', 'Wireless communication', 'Telescopes']","['Clustering', 'low-complexity', 'non-geostationary orbit', 'radio astronomy', 'satellite communication']"
"We study the problem of fair and efficient mechanism design for allocating multiple resources in multiple servers among a set of users with Leontief utilities. This problem is motivated by a mobile edge computing environment where each mobile user cannot establish a wireless connection simultaneously to multiple edge servers. Each user is a selfish utility maximizing agent that chooses a single server for its job execution. When a server is shared by multiple users, a resource allocation rule decides the utility that each user must receive. Our goal is to design a mechanism that always admits a Nash Equilibrium (NE), i.e., a state where no user has incentive to change its server, that (1) can be reached in polynomial time and (2) provides fair and efficient resource allocation. We propose the Multi-resource Allocation Game Induced by Kalai-Smorodinsky bargaining solution (MAGIKS) and prove that under discrete resource demands it finds an NE in \mathcal {O}({\textrm {poly}(n)})moves for any fixed server configuration, where nis the number of users. Furthermore, MAGIKS satisfies envy-freeness, sharing incentive, and Pareto optimality on each server. Regarding fairness among users in different servers, we prove that MAGIKS satisfies 2-approximate envy-freeness and maximin share guarantee. Moreover, we show that 2-approximate envy-freeness is the best that any mechanism that satisfies local Pareto optimality can achieve at its NEs.","['Servers', 'Resource management', 'Games', 'Task analysis', 'NIST', 'Wireless communication', 'Nash equilibrium']","['Envy-freeness', 'fair allocation', 'maximin share guarantee', 'Nash equilibrium']"
"Knowledge of channel state information (CSI) is fundamental to many functionalities for mobile communication systems. With the advance of machine learning (ML) and digital maps, i.e., digital twins, we have a big opportunity to learn the propagation environment and design novel methods to derive and report CSI. In this work, we propose to combine untrained neural networks (UNNs) and conditional generative adversarial networks (cGANs) for MIMO channel recreation based on prior knowledge. The UNNs learn the prior-CSI for some locations which are used to build the input to a cGAN. Based on the prior-CSI estimates, their locations and the location of the desired channel, the cGAN is trained to output the channel expected at the desired location. This combined approach can be used for low overhead CSI reporting as, after training, we only need to report the desired location. Our results show that our CSI recreation method is successful in modelling the wireless channel under different configurations of prior-CSI spatial sampling. In addition, the results consider a real world measurement campaign for indoor line of sight and non-line of sight channels. The signal to noise ratio (SNR) achieved by our CSI recreation is better than the SNR reported by the measured campaign providers. Moreover, our CSI recreation provides means for low overhead CSI reporting as the UNN structure is underparameterized compared to the full explicit CSI, and only the desired location is needed for the cGAN to recreate the desired CSI.","['Channel estimation', 'Wireless communication', 'Digital twins', 'MIMO communication', 'Interpolation', 'Data models', 'Generative adversarial networks']","['Channel estimation', 'channel interpolation', 'UNN', 'cGAN', 'digital twin']"
"Grant-free non-orthogonal multiple access (GF-NOMA) has emerged as a promising access technology for the fifth generation and beyond wireless networks that enable ultra-reliable and low-latency communications (URLLC) to ensure low access latency and high connectivity density. Furthermore, designing energy-efficient (EE) resource allocation strategies is a crucial aspect of future cellular system development. Taking these goals into account, this paper proposes an EE sub-channel and power allocation strategy for URLLC-enabled GF-NOMA (URLLC-GF-NOMA) systems based on multi-agent (MA) deep reinforcement learning (MADRL). In particular, the URLLC-GF-NOMA methods using MA dueling double deep Q network (MA3DQN), MA double deep Q network (MA2DQN), and MA deep Q network (MADQN) techniques are designed to enable users to select the most appropriate sub-channel and transmission power for their communications. The aim is to build an efficient MADRL-based solution, ensuring rapid convergence with small signaling overhead, to maximize the network EE while fulfilling the URLLC requirements of all users. Simulation results show that the MADQN and MA2DQN methods, which have lower complexity than MA3DQN, are more appropriate for the URLLC-GF-NOMA systems under consideration. Moreover, our proposed methods exhibit superior convergence characteristics, a reduction in signaling overhead, and enhanced EE performance compared to other benchmark strategies.","['Ultra reliable low latency communication', 'NOMA', 'Resource management', 'Reliability', 'System performance', 'Energy efficiency', 'Convergence']","['Energy efficiency', 'grant-free NOMA', 'multi-agent deep reinforcement learning', 'URLLC']"
"In this paper, we study a multi-pair two-way large-scale multiple-input multiple-output (MIMO) decode-and-forward relay system. Multiple single-antenna user pairs exchange information via a shared relay working at half-duplex. The proposed scenario considers a practical case where an increasing number of antennas is deployed in a fixed physical space, giving rise to a trade-off between antenna gain and spatial correlation. The channel is assumed imperfectly known, and the relay employs linear processing methods. We study the large-scale approximations of the sum spectral efficiency (SE) and investigate the energy efficiency (EE) with a practical power consumption model when the number of relay antennas becomes large. We demonstrate the impact of the relay antenna number and spatial correlation with reducing inter-antenna distance on the EE performance. We exploit the increasing spatial correlation to allow an incomplete channel state information (CSI) acquisition where explicit CSI is acquired only for a subset of antennas. Our analytical derivations and numerical results show that applying the incomplete CSI strategy in the proposed system can improve the EE against complete CSI systems while maintaining the average SE performance.","['Relays', 'Antennas', 'Correlation', 'Massive MIMO', 'Power demand', 'System performance', 'Uplink']","['Decode-and-forward relaying', 'incomplete CSI', 'massive MIMO', 'spatial correlation', 'spectral efficiency and energy efficiency']"
"Two key challenges in underlay dynamic spectrum access (DSA) are how to establish an interference limit from the primary network (PN) and how cognitive radios (CRs) in the secondary network (SN) become aware of the interference they create on the PN, especially when there is no exchange of information between the two networks. These challenges are addressed in this paper by presenting a fully autonomous and distributed underlay DSA scheme where each CR operates based on predicting its transmission effect on the PN. The scheme is based on a cognitive engine with an artificial neural network that predicts, without exchanging information between the networks, the full adaptive modulation and channel coding configuration for the primary link that is received with highest power by a transmitting CR. By managing the effect of the SN on the PN, the presented technique maintains the relative average throughput change in the PN within a prescribed maximum value, while also finding transmit settings for the CRs that result in throughput as large as allowed by the PN interference limit. Simulation results show that the ability of the cognitive engine in estimating the effect of a CR transmission on the full adaptive modulation and coding (AMC) mode leads to a very fine resolution underlay transmit power control. This ability also provides higher transmission opportunities for the CRs, compared to a scheme that can only estimate the modulation scheme used at the PN link.","['Modulation', 'Interference', 'Throughput', 'Radio transmitters', 'Receivers', 'Engines', 'Channel coding']","['Cognitive radio', 'underlay dynamic spectrum access', 'NARX neural network', 'adaptive modulation and coding']"
"Distributed Artificial Intelligence (DAI) is one of the most promising techniques to provide intelligent services under strict privacy protection regulations for multiple clients. By applying DAI, training on raw data is carried out locally. At the same time, the trained outputs, e.g., model parameters from multiple local clients, are sent back to a central server for aggregation. DAI is recently studied in conjunction with wireless communication networks to achieve better practicality, incorporating various random effects brought by wireless channels. However, because of wireless channels’ complex and case-dependent nature, a generic simulator for applying DAI in wireless communication networks is still lacking. To accelerate the development of DAI in wireless communication networks, we propose a generic system design in this paper and an associated simulator that can be set according to wireless channels and system-level configurations. Details of the system design and analysis of the impacts of wireless environments are provided to facilitate further implementations and updates. We employ a series of experiments to verify the effectiveness and efficiency of the proposed system design and reveal its superior scalability.","['Wireless communication', 'Computational modeling', 'Servers', 'Training', 'Computer architecture', 'Artificial intelligence', 'Data models']","['Distributed deep learning (DDL)', 'federated learning (FL)', 'system design', 'simulator design', 'wireless environment', 'convergence analysis']"
Performance of wireless powered wireless systems is analyzed. The wireless devices in such systems scavenge energy from sources in downlinks and use the energy to communicate in uplinks. We introduce two new models for these energy harvesters to consider their nonlinear circuitry and their functioning over multiple line-of-sight and non-line-of-sight channels. The newly proposed Beaulieu-Xie fading model is used to characterize this manifold of channels. Performance analyses of average harvested energy and transmission outage probability show good fit between the proposed models and measured data.,"['Wireless communication', 'Integrated circuit modeling', 'Rayleigh channels', 'Protocols', 'Analytical models', 'Data models', 'Receivers']","['Energy harvesting', 'communication systems', 'communication networks', 'radio frequency', 'performance analysis', 'channel modeling']"
"In this paper, with the help of an intelligent reflecting surface (IRS), the source (S) and destination (D) exchange information through the two-way decode-and-forward relay (TW-DFR). We focus on the phase optimization of IRS to improve the system sum rate performance. Firstly, a maximizing receive power sum (Max-RPS) method is proposed via eigenvalue decomposition (EVD) with an appreciable system sum rate enhancement, which is called Max-RPS-EVD. To further achieve a higher system sum rate, a method of maximizing minimum rate (Max-Min-R) is proposed with high complexity. To reduce its complexity, a low-complexity method of maximizing the sum rate (Max-SR) via general power iterative (GPI) is proposed, which is called Max-SR-GPI. Simulation results show that the proposed three methods outperform the random phase method, especially the proposed Max-SR-GPI method is the best one achieving at least 20% system sum rate gain over random phase. Additionally, it is also proved the optimal system sum rate can be achieved when TW-DFR and IRS are located in the middle of S and D.","['Signal to noise ratio', 'Relay networks (telecommunication)', 'Resource management', 'Optimization', 'Communication system security', 'Transceivers', 'Symbols']","['Intelligent reflecting surface', 'phase optimization', 'two-way decode-and-forward relay', 'general power iterative', 'rate']"
"The vertical heterogeneous network (VHetNets) architecture aims to provide global connectivity for a variety of services by combining terrestrial, aerial, and space networks. Satellites complement cellular networks to overcome coverage and reliability limitations. However, the services of low-earth orbit (LEO) satellites are vulnerable to spoofing attacks. Physical layer authentication (PLA) can provide robust satellite authentication using machine learning (ML) with physical attributes. In this paper, an adaptive PLA scheme is proposed using Doppler frequency shift (DS) and received power (RP) features with a one-class classification support vector machine (OCC-SVM). One class-classification is a ML technique for outlier and anomaly detection which uses only legitimate satellite training data. This scheme is evaluated for fixed satellite services (FSS) and mobile satellite services (MSS) at different altitudes. Results are presented which show that the proposed scheme provides a higher authentication rate (AR) using DS and RP features simultaneously compared to other approaches in the literature.","['Satellites', 'Authentication', 'Frequency modulation', 'Satellite broadcasting', 'Low earth orbit satellites', 'Programmable logic arrays', 'Support vector machines']","['Doppler frequency shift', 'received power', 'physical layer authentication', 'one-class classification', 'machine learning', 'support vector machine', 'vertical heterogeneous network']"
"The sparsity of millimeter wave (mmWave) channels in angular and temporal domains has been exploited for channel estimation, while the associated channel parameters can be utilized for localization. However, line-of-sight (LoS) blockage makes localization highly challenging, which may lead to big positioning inaccuracy. One promising solution is to employ reconfigurable intelligent surfaces (RIS) to generate virtual line-of-sight (VLoS) paths. Hence, it is essential to investigate the wireless positioning in RIS-aided mmWave systems. In this paper, an adaptive joint LoS and VLoS localization scheme is proposed, where the VLoS is constructed by a beamforming protocol operated between RIS and mobile station (MS). More specifically, to sense the location and orientation of a MS, a novel interlaced scanning beam sweeping algorithm is proposed to acquire the optimal beams. In this algorithm, LoS and VLoS paths are separately estimated and the optimal beams are selected according to the received signal strength so as to mitigate the LoS blockage problem. Then, based on the selected beams and received signal strength, angle of arrival (AoA), angle of departure (AoD), angle of reflection (AoR) and time of arrival (ToA) are estimated. Finally, with the aid of these estimated parameters, the location and orientation of the MS are estimated. We derive the Cramer-Rao lower bounds (CRLBs) for both location estimation and orientation estimation, and compare them with the corresponding results obtained from simulations. We compare the performance of our proposed scheme with that attained by three legacy schemes, when various aspects are considered. The performance results show the superiority of our proposed beam training algorithm, which is capable of achieving a localization error within 15 cm and an orientation error within 0.003 rads. Furthermore, the training overhead is 100 times less than that of the conventional exhaustive search algorithm, while obtaining a 13 dBm power gain when compared with the hierarchical codebook based search algorithm.","['Location awareness', 'Millimeter wave communication', 'Estimation', 'Array signal processing', 'Channel estimation', 'Training', 'Line-of-sight propagation']","['mmWave', 'localization', 'positioning', 'reconfigurable intelligent surfaces', 'beamforming']"
"Owing to the breakthrough of artificial meta-materials, the emergence and evolution of reconfigurable intelligent surface (RIS) have drawn a novel blueprint for the development of the sixth-generation (6G) wireless networks, through the creation of smart radio environment to meet the requirement of ubiquitous connectivity. Meanwhile, the vision for the 6G illuminates the integration of sensing and communication, bringing out a demand for both high-quality communication and high-precision localization. Therefore, regarding the future wireless localization, the potential of RIS to tag the signals provides additional degrees of freedom. To this end, to fulfil the increasing demands for positioning, RIS has drawn a growing amount of research both from academia and industry. In this paper, we first introduce the emerging investigations and development of RIS, and we give a concise summary of the localization principles. Then we explain the potential of RIS in localization, and provide a comprehensive survey of the current state of research on RIS-assisted localization. Finally, we discuss the most significant research challenges to tackle for RIS-aided localization in the future.","['Location awareness', 'Metamaterials', 'Electromagnetics', 'Hardware', 'Antenna arrays', '6G mobile communication', 'Wireless networks']","['environment', 'localization', 'positioning', 'sixth-generation (6G)']"
"In the context of digital subscriber line (DSL) systems, where the long-reach (LR) extension of G.fast has recently been proposed, interest in techniques dealing with long channel impulse responses (CIRs) without increasing the cyclic prefix (CP) length of the discrete multi-tone (DMT) modulation has recently resurfaced. The technique under consideration in this article is referred to as channel shortening, and applies FIR filters to the received signal to shorten the apparent CIR. Time-domain equalization (TEQ) as well optimal per-tone equalization (PTEQ) FIR filters will be considered. When channel shortening techniques are analyzed, it is often overlooked that the received signals after DMT demodulation are generally improper when the CP is too short. Hence, the state-of-the-art signal-to-interference-plus-noise ratio (SINR) and bit loading expressions employed to assess system performance, which implicitly assume the received signals to be proper, misrepresent the true achievable performance. New expressions for the SINR and bit loading are therefore presented that explicitly take signal impropriety into account. Based on these expressions, it is then observed that - if the received signals are improper - the PTEQ FIR filters are no longer optimal, and that the achievable bit loading depends on a particular phase shift experienced by the transmitted signals. This article therefore introduces a novel widely linear PTEQ - which is again optimal when the received signals are improper - and additionally proposes to optimally rotate signals in the complex plane prior to transmission. Finally, this article assesses the performance increase obtainable by explicitly accounting for signal impropriety.","['Loading', 'Signal to noise ratio', 'Demodulation', 'DSL', 'Interference', 'Finite impulse response filters', 'Mathematical model']","['Augmented complex', 'channel shortening', 'composite real', 'discrete multi-tone (DMT)', 'digital subscriber lines (DSL)', 'improper signals', 'proper signals', 'PTEQ', 'TEQ', 'widely linear filters']"
"As the services and requirements of next-generation wireless networks become increasingly diversified, it is estimated that the current frequency bands of mobile network operators (MNOs) will be unable to cope with the immensity of anticipated demands. Due to spectrum scarcity, there has been a growing trend among stakeholders toward identifying practical solutions to make the most productive use of the exclusively allocated bands on a shared basis through spectrum sharing mechanisms. However, due to the technical complexities of these mechanisms, their design presents challenges, as it requires coordination among multiple entities. To address this challenge, in this paper, we begin with a detailed review of the recent literature on spectrum sharing methods, classifying them on the basis of their operational frequency regime—that is, whether they are implemented to operate in licensed bands (e.g., licensed shared access (LSA), spectrum access system (SAS), and dynamic spectrum sharing (DSS)) or unlicensed bands (e.g., LTE-unlicensed (LTE-U), licensed assisted access (LAA), MulteFire, and new radio-unlicensed (NR-U)). Then, in order to narrow the gap between the standardization and vendor-specific implementations, we provide a detailed review of the potential implementation scenarios and necessary amendments to legacy cellular networks from the perspective of telecommunication vendors and regulatory bodies. Next, we analyze applications of artificial intelligence (AI) and machine learning (ML) techniques for facilitating spectrum sharing mechanisms and leveraging the full potential of autonomous sharing scenarios. Finally, we conclude the paper by presenting open research challenges, which aim to provide insights into prospective research endeavors.","['5G mobile communication', 'Synthetic aperture sonar', 'Ultra reliable low latency communication', 'Industrial Internet of Things', 'Economics', 'Broadband communication', 'Wireless networks']","['Spectrum sharing', 'MNO', 'LSA', 'SAS', 'LTE-U', 'LAA', 'MulteFire', 'NR-U']"
"Various applications for inter-machine communications are on the rise. Whether it is for autonomous driving vehicles or the Internet of everything, machines are more connected than ever to improve their performance in fulfilling a given task. While in traditional communications the goal has often been to reconstruct the underlying message, under the emerging task-oriented paradigm, the goal of communication is to enable the receiving end to make more informed decisions or more precise estimates/computations. Motivated by these recent developments, in this paper, we perform an indirect design of the communications in a multi-agent system (MAS) in which agents cooperate to maximize the averaged sum of discounted one-stage rewards of a collaborative task. Due to the bit-budgeted communications between the agents, each agent should efficiently represent its local observation and communicate an abstracted version of the observations to improve the collaborative task performance. We first show that this problem can be approximated as a form of data-quantization problem which we call task-oriented data compression (TODC). We then introduce the state-aggregation for information compression algorithm (SAIC) to solve the formulated TODC problem. It is shown that SAIC is able to achieve near-optimal performance in terms of the achieved sum of discounted rewards. The proposed algorithm is applied to a geometric consensus problem and its performance is compared with several benchmarks. Numerical experiments confirm the promise of this indirect design approach for task-oriented multi-agent communications.","['Task analysis', 'Data compression', 'Quantization (signal)', 'Communication channels', 'Multi-agent systems', 'Semantics', 'Collaboration']","['Task-oriented communications', 'semantic communications', 'data quantization', 'machine learning for communications', 'communications for machine learning']"
"Federated learning is one of the most appealing alternatives to the standard centralized learning paradigm, allowing a heterogeneous set of devices to train a machine learning model without sharing their raw data. However, it requires a central server to coordinate the learning process, thus introducing potential scalability and security issues. In the literature, server-less federated learning approaches like gossip federated learning and blockchain-enabled federated learning have been proposed to mitigate these issues. In this work, we propose a complete overview of these three techniques, proposing a comparison according to an integral set of performance indicators, including model accuracy, time complexity, communication overhead, convergence time, and energy consumption. An extensive simulation campaign permits to draw a quantitative analysis considering both feedforward and convolutional neural network models. Results show that gossip federated learning and standard federated solution are able to reach a similar level of accuracy, and their energy consumption is influenced by the machine learning model adopted, the software library, and the hardware used. Differently, blockchain-enabled federated learning represents a viable solution for implementing decentralized learning with a higher level of security, at the cost of an extra energy usage and data sharing. Finally, we identify open issues on the two decentralized federated learning implementations and provide insights on potential extensions and possible research directions on this new research field.","['Servers', 'Data models', 'Training', 'Federated learning', 'Security', 'Energy consumption', 'Blockchains']","['Blockchain', 'decentralized learning', 'edge computing', 'energy efficiency', 'federated learning', 'machine learning']"
"Mobile network traffic is increasing and so is the energy consumption. The Radio Access Network (RAN) part is responsible for the largest share of the mobile network energy consumption, and thus; an important consideration when expanding mobile networks to meet traffic demands. This work analyses how the energy consumption of future mobile networks can be minimised by using the right RAN architecture, share the network with other operators and implementing the most efficient energy minimising technologies in the RAN. It is explored how the different approaches can be realised in real life networks as well as the research state of the art is highlighted. Furthermore, this work provides an overview of future research directions for 6G energy saving potentials. Different energy saving contributions are evaluated by a common methodology for more realistic comparison, based on the potential energy saving of the overall mobile network consumption. Results show that implementing selected technologies and architectures, the mobile network overall energy consumption can be reduced by approximately 30%, corresponding to almost half of the RAN energy consumption. Following this, a set of guidelines towards an energy optimised mobile network is provided, proposing changes to be made initially and in the longer run for brownfield network operators as well as a target network for greenfield network operators.","['Energy consumption', '5G mobile communication', 'Energy efficiency', 'Radio access networks', 'Computer architecture', '6G mobile communication', 'Optimization']","['Green RAN', 'C-RAN', 'vRAN', 'O-RAN', 'MOCN', 'MORAN', 'network slicing', '5G', '6G', 'AI']"
"While mobile networks are evolving rapidly, the battle between ever-growing traffic demands and out-paced network capacities will continue and require more efficient solutions. Emerging techniques such as mobile edge computing and device-to-device (D2D) communications can help relieve traffic at the mobile edge and accommodate surging traffic demands from various content-centric services. In this work, we focus on exploiting device caching and user collaboration to offload content distribution traffic. Specifically, we investigate the request offloading problem, which aims to appropriately select caching devices and maximize the content requests that can be fulfilled through D2D communications. Given the constraints of individual transmission and caching capacities, the number of available D2D channels, and information privacy with social-awareness, we can decouple the request offloading problem into two subproblems, i.e., the device caching and matching problem, and the D2D channel allocation problem. As we prove that both problems are NP-hard, we propose efficient algorithms that iteratively make a best local decision in each step. Simulation results show that the proposed algorithms perform fairly closely to optimal solutions in small-scale instances and outperform the reference schemes under various situations.","['Device-to-device communication', 'Channel allocation', 'Collaboration', 'Interference', 'Social networking (online)', 'Complexity theory', 'Servers']","['Mobile edge computing', 'D2D communications', 'collaborative content distribution', 'device caching', 'device matching', 'channel allocation']"
"In this paper, a general transformation of binary linear block codes (BLBCs) to (possibly, multi-kernel) polar codes with dynamic frozen bits is proposed. Through a simple matrix permutation operation, a one-to-one connection between the codewords of a BLBC and its transformed polar code can be established. This transformation allows the usage of any decoding algorithm of polar codes for efficient soft decoding of BLBCs, including the powerful successive cancellation list (SCL) decoding algorithm. Simulations show that the soft SCL polar decoding of BLBCs can achieve a comparative performance to the order statistic decoding (OSD), as well as the maximum-likelihood decoding (MLD) in certain cases, with a much lower computational complexity.","['Polar codes', 'Maximum likelihood decoding', 'Generators', 'Heuristic algorithms']","['Channel coding', 'Polar codes', 'binary linear block codes and soft decoding']"
"In the presence of mobile ground users, it is imperative to optimize the placement of unmanned aerial vehicle-base stations (UAV-BSs) in a UAV-assisted communication network. Moreover, the placement update interval is also a crucial parameter that impacts the network performance; hence, it needs to be optimized. Our work aims to jointly optimize the mobile users’ association with the UAV-BSs, resource allocation, UAV-BS placement, and the update interval. We propose to divide the above optimization into two phases: phase 1 and phase 2. In phase 1, the user association, resource allocation, and UAV-BS placement are optimized, whereas the update interval is optimized in phase 2. Two different frameworks, namely, max sum rate and max min rate, are utilized for the joint optimization. Specifically, in phase 1 of the max sum rate framework, the objective is to maximize the sum rate of the users, whereas, in phase 1 of the max min rate framework, the worst-off user rate is maximized. In phase 2, the update interval is optimized by minimizing the total UAV-BS flight time and maximizing the user coverage probability. Further, the analytical expression for the coverage probability of the mobile users is also derived. A sequential approach is proposed to solve phase 1 and phase 2 jointly. We prove that the convergence of the sequential approach is guaranteed. It is observed that, in the max sum rate framework, the average update interval is independent of the number of UAV-BSs. However, in the max min rate framework, the average update interval is dependent on the number of UAV-BSs. The proposed work is also compared with a benchmark approach wherein the update interval is not optimized. It has been shown that, unlike the proposed work, the benchmark approach cannot adapt to the desired priority of service time and coverage probability.","['Resource management', 'Optimization', 'Trajectory', 'Communication networks', 'Autonomous aerial vehicles', 'Wireless sensor networks', 'Quality of service']","['Multiple UAV base stations', 'user mobility awareness', 'UAV base station placement', 'resource allocation', 'optimal update interval']"
"As a promising candidate for the future generations of mobile communication networks, the cell-free (CF) massive multiple-input multiple-output (mMIMO) networks have been shown to provide high spectral efficiency (SE) and more uniform signal coverage than cellular mMIMO networks, enabling smart cities more diverse application services. The fronthaul link in such networks, defined as the transmission link between the central processing unit and the access point, requires a high capacity, but is often constrained. Hence, the optimization of the user rate under the limited capacity of fronthaul is the key problem to be solved in a CF networks. In this paper, we consider the downlink transmission of CF mMIMO orthogonal frequency division multiplexing (OFDM) systems with constrained transmit power and fronthaul capacity. Based on the min-max and sum-max principles, quadratic transform-min max (QT-MM) and quadratic transform-sum max (QT-SM) optimization algorithms are proposed, respectively. Simulation results show that compared with the weighted minimum mean square error (WMMSE) algorithm, both of the proposed algorithms can effectively prevent very low user rates when the fronthaul capacity is limited. When the fronthaul link capacity constraint is high, the QT-SM algorithm has better rate performance than the distributed WMMSE-d algorithm and the centralized WMMSE-c algorithm.","['Optimization', 'Downlink', 'Channel estimation', 'Transforms', 'OFDM', 'Central Processing Unit', 'Antenna arrays']","['Cell-free massive MIMO', 'fronthaul link capacity constraint', 'quadratic transform', 'user rate optimization']"
"The ever advances in wireless communication and mobile networks have brought novel workflow-formed applications, such as virtual reality and live-streaming, to our daily life. Arousing a growing need for workflow execution efficiency. An edge network is widely considered a promising way of bridging the gap between intensive resource demand and the limited computation capabilities of mobile terminals. However, when an edge network is partially connected, ordinary workflow scheduling algorithms suffer degradations as the data transmission time is prolonged. In this paper, we address the challenge of workflow makespan minimization in a partially connected edge network. Contrary to the general assumptions of a fully connected edge network, the edge servers under discussion are partly interconnected but can be reached within limited hops by using different paths. Here, the placement of interdependent tasks and selection of routing paths are two major factors that influence the makespan. We first propose a critical path analysis based dynamic task sorting algorithm to determine the scheduling order of tasks. Then the path quality is introduced as a reflection of path availability and is employed as the major indicator in selecting disjoint subpaths. We further model the workflow scheduling process into a Markov decision process and propose a reinforcement learning–based workflow embedding (RLWE) scheme to minimize the makespan of the workflow. With the fine-trained agent, the proposed scheme can coordinate the demand of computing resources and routing paths of interdependent tasks and provide a near-optimal makespan of the workflow. Numerical results validate the feasibility of our proposed scheme as its performance exceeds existing baselines with an improved quality of service in terms of makespan.","['Task analysis', 'Routing', 'Cloud computing', 'Servers', 'Heuristic algorithms', 'Edge computing', 'Dynamic scheduling']","['scheduling', 'edge computing', 'multipath routing', 'deep reinforcement learning']"
"An interacting multiple model based on unscented Kalman filter (IMM-UKF) is widely applied to positioning and tracking targets in various tracking scenarios. At the same time, visible light positioning (VLP) is developing rapidly due to the low cost and accuracy. Therefore, indoor positioning and tracking based on VLP combined with the IMM-UKF has attracted considerable interest. However, existing algorithms work on the assumption that the light-emitting diodes used for tracking are all point light sources, which ignores the geometry of these transmitters and results in low tracking accuracy. To overcome this problem, this paper proposes an innovative tracking algorithm based on VLP. This algorithm considers the shapes of the lights in combination with existing tracking algorithms, such as IMM-UKF. Simulation results show that in a standard Gaussian noise environment, the larger the transmitter is, the more meaningful the proposed algorithm is.","['Transmitters', 'Receivers', 'Shape', 'Target tracking', 'Kalman filters', 'Light emitting diodes', 'Position measurement']","['Target positioning and tracking', 'visible light positioning (VLP)', 'interacting multiple model (IMM)', 'unscented Kalman filter (UKF)', 'shapes of transmitters']"
"In millimeter wave (mmWave) communications, fast and reliable beam alignment is crucial to the achievable data rate. The problem is even more challenging when either the transmitter or the receiver, or both move that need timely update of the best beam pair. Some existing work tracks the channel changes by Kalman filter (KF). Alternatively, compressed sensing (CS) approaches are useful to reconstruct the channel based on the sparsity of mmWave channels. However, the aforementioned methods either need a full scan over all possible beam pairs or require frequent beam training, leading to high overhead. In this paper, a novel beam alignment scheme based on an integrated KF and CS framework is proposed for the single-user mmWave channel. Since the CS performance is heavily dependent on the sparsity level, the proposed method increases the signal sparsity level by applying adaptive CS on the observation residual computed from the previous estimate of the support to predict the angles of the dominant paths, while the corresponding path gains are tracked by the reduced-order KF. To further exploit the spatial correlation of sparse signals, our approach considers the adaptation of the sensing matrix based on the previous estimate to enhance the estimation accuracy, and the weighted CS is adopted for shrinking the possible range of solutions. To balance the estimation accuracy and alignment overhead, a switching mechanism is proposed that determines the timing for switching the estimation policy between beam training and beam tracking. Simulations are performed to evaluate the performance of the proposed method subject to numerous important factors, such as signal-to-noise ratio (SNR), number of available beams, overhead for beam alignment, beam variation rate, and the dominance of line-of-sight path.","['Millimeter wave communication', 'Training', 'Channel estimation', 'Estimation', 'Switches', 'Compressed sensing', 'Tracking']","['Beam alignment', 'beam tracking', 'compressed sensing', 'Kalman filter', 'millimeter wave', 'mobile communications']"
"This paper proposes a parametric network for the joint compensation of multiple linear impairments in coherent optical communication systems. The considered linear impairments include both static and time-variant effects such as in-phase/quadrature (IQ) imbalance, laser phase noise (PN), chromatic dispersion (CD), polarization mode dispersion (PMD), and carrier frequency offset (CFO). To jointly compensate for these considered impairments, the proposed network is composed of parametric layers that exploit the particular signal model of each impairment. The layers’ parameters are jointly learned during a training stage. This stage uses a supervised step that exploits the knowledge of some transmitted data (preamble and/or pilots) and a self-labeling step that uses the knowledge of the symbol constellation. In addition, a new validation technique that does not require a different dataset is developed to avoid overfitting. The parametric network performance is compared to classical digital signal processing (DSP), and Deep Learning (DL) approaches using simulated data. Simulation results show that the proposed network outperforms the competing approaches in terms of Bit Error Rate (BER) while maintaining a relatively reduced computational complexity. In the scenarios considered, compared to the parametric network, the DSP approach introduces an OSNR penalty between 0.2 dB and 1.7 dB at a BER of4×10−3. Furthermore, simulation results demonstrate that the proposed network is way more flexible than other approaches since it can easily be adapted to a different scenario and coupled with other techniques.","['Optical transmitters', 'Optical polarization', 'Optical receivers', 'Laser modes', 'Optical filters', 'Optical fiber networks', 'Mathematical models']","['Parametric network', 'linear imperfections', 'machine learning', 'digital signal processing', 'coherent optical communications']"
"In this work, we provide a unified framework for full-duplex (FD) massive multiple-input multiple-output (MIMO) cellular networks with low-resolution analog-to-digital and digital-to-analog converters (ADCs and DACs). An objective of this work is to derive an accurate model to account for a wide variety of network irregularities and imperfections, including loopback self-interference (SI) that arises in full-duplex systems and quantization error from low-resolution data converters. Our contributions for forward and reverse links include (1) deriving signal-to-quantization-plus-interference-plus-noise ratio (SQINR) under pilot contamination, linear minimum mean square error (LMMSE) channel estimation and channel hardening; (2) deriving closed-form and approximate analytical expressions of spectral efficiency; (3) deriving asymptotic results and power scaling laws with respect to the number of quantization bits, base station antennas, and users, as well as base station and user equipment power budgets; and (4) analyzing outage probability and spectral efficiency vs. cell shape, shadowing, noise, cellular interference, pilot contamination, pilot overhead, and frequency reuse. Cell shapes include hexagonal, square, and Poisson Point Process (PPP) tessellations. In simulation, we quantify spectral and energy efficiency as well as the impact of SI power, inter-user interference, and cell shape on outage probability. We carry out the analysis for sub-7 GHz long term evolution (LTE) bands and then extend the framework to support millimeter wave (mmWave) bands.","['Spectral efficiency', 'Massive MIMO', 'Channel estimation', 'Millimeter wave communication', 'Transmitting antennas', 'Receiving antennas', 'Quantization (signal)']","['Full-duplex', 'massive MIMO', 'low resolution ADC/DAC', 'cellular networks', 'interference', 'networks lattice/tessellation', 'LTE', 'mmWave']"
"We evaluate the outage probability (OP) for L-branch equal gain combining (EGC) receivers operating over fading channels, i.e., equivalently the cumulative distribution function (CDF) of the sum of the L channel envelopes. In general, closed form expressions of OP values are out of reach. The use of Monte Carlo (MC) simulations is not a good alternative as it requires a large number of samples for small values of OP. In this paper, we use the concept of importance sampling (IS), being known to yield accurate estimates using fewer simulation runs. Our proposed IS scheme is based on sample rejection where the IS density is the truncation of the underlying density over the L dimensional sphere. It assumes the knowledge of the CDF of the sum of the L channel gains in closed-form. Such an assumption is not restrictive since it holds for various challenging fading models. We apply our approach to the case of independent Rayleigh, correlated Rayleigh, and independent and identically distributed Rice fading models. Next, we extend our approach to the interesting scenario of generalised selection combining receivers combined with EGC under the independent Rayleigh environment. For each case, we prove the desired bounded relative error property. Finally, we validate these theoretical results through some selected experiments.","['Receivers', 'Rayleigh channels', 'Diversity reception', 'Power system reliability', 'Probability', 'Closed-form solutions']","['Outage probability', 'equal gain combining', 'importance sampling', 'sample rejection', 'generalised selection combining', 'bounded relative error']"
"Optimum controller placement in the presence of several conflicting objectives has received significant attention in the Software-Defined Wide Area Network (SD-WAN) deployment. Multi-objective evolutionary algorithms, like Non-dominated Sorting Genetic Algorithm II (NSGA-II) and Multi-objective Particle Swamp Optimization (MOPSO), have proved helpful in solving Controller Placement Problem (CPP) in SD-WAN. However, these algorithms were associated with the challenge of scalability (when there are more than three objectives) for optimization in the SD-WAN. Hence, this study proposed an adapted NSGA-III (A-NSGA-III) to resolve the scalability challenges associated with NSGA-II and MOPSO algorithms in the presence of more than three objectives. This study developed and introduced a repair-based operator into the existing Mechanical Engineering based NSGA-III to propose the A-NSGA-III for optimal controller placement in the SD-WAN. The proposed A-NSGA-III, the NSGA-II and MOPSO algorithms were subjected to evaluation using datasets from Internet2 OS3E WAN topology with six objective functions. The Hypervolume indicator, Percentage Coefficient of Variation (PCV), the percentage difference and the Parallel Coordinate Plots (PCP) confirmed that the proposed A-NSGA-III exhibited high convergence and diversification than the NSGA-II and MOPSO algorithms in the presence of scalability challenge (when the number of objective function exceeded three). The result confirmed that the proposed A-NSGA-III solved the scalability challenges associated with the optimal Controller Placement in the SD-WAN. Hence, A-NSGA-III was recommended over NSGA-II and MOPSO algorithms, subject to the confirmation usage conditions.","['Optimization', 'Scalability', 'Measurement', 'Sorting', 'Heuristic algorithms', 'Wide area networks', 'Maintenance engineering']","['Controller placement', 'adapted NSGA-III', 'repair operator-based mechanism', 'Pareto-frontier', 'SD-WAN']"
"Free-space optical (FSO) communication is crucial for the next-generation 5G+ wireless networks. The FSO links suffer from atmospheric-turbulence-induced bit errors. For the increasing link's performance, low-density parity-check (LDPC) codes, complemented by the belief-propagation (BP) algorithm, are an excellent option. The bit error rate (BER) of the LDPC code is characterized by a parameter called the threshold. The threshold is the signal-to-noise ratio (SNR), after which the BER falls arbitrarily and becomes close to zero. We derive the threshold for the LDPC codes under the BP algorithm for an uncorrelated flat FSO channel. The determination of the FSO channel threshold is a tedious task as the density of the log-likelihood ratio from the FSO channel cannot be assumed as Gaussian and is available only in a numerical form. It, thus, requires testing different values of SNR as a possible threshold systematically. Therefore, we propose the divide and conquer algorithm. The threshold depends on the degree distributions, channel state information (CSI), and the turbulence level. When CSI is known, we obtain the threshold at an SNR of 8.10 dB in high turbulence for a regular (3, 6) LDPC code. This threshold steps up to 12.48 dB when the CSI is unknown at the receiver. We evaluate the threshold values for various degree distributions (regular and irregular LDPC codes) under high, moderate, and low turbulence levels for both channel models (CSI known and unknown at the receiver). We also confirm the derived threshold values with MATLAB simulations.","['Parity check codes', 'Signal to noise ratio', 'Probability density function', 'Bit error rate', 'Optical receivers', 'Decoding', 'Thermal noise']","['Channel state information', 'FSO communication', 'LDPC code', 'threshold']"
"This paper considers the problem of symbol detection in massive multiple-input multipleoutput (MIMO) wireless communication systems. We consider hard-thresholding preceded by two variants of the regularized least squares (RLS) decoder; namely the unconstrained RLS and the RLS with a box constraint, which is called Box-RLS. For all schemes, we focus on the evaluation of the mean squared error (MSE) and the symbol error probability (SEP) forM-ary pulse amplitude modulation (M-PAM) symbols transmitted over a massive MIMO system when the channel is estimated using linear minimum mean squared error (LMMSE) estimator. Under such circumstances, the channel estimation error is Gaussian which allows for the use of the convex Gaussian min-max theorem (CGMT) to derive asymptotic approximations for the MSE and SEP when the system dimensions and the coherence duration grow large with the same pace. The obtained expressions are then leveraged to derive the optimal power distribution between pilot and data under a total transmit energy constraint. In addition, we derive an asymptotic approximation of the goodput for all schemes which is then used to jointly optimize the number of training symbols and their associated power. Numerical results are presented to support the accuracy of the theoretical results.","['Symbols', 'Channel estimation', 'Decoding', 'Resource management', 'Receiving antennas', 'Transmitting antennas', 'Training']","['Resource allocation', 'channel estimation', 'mean squared error', 'symbol error probability', 'goodput', 'constrained least squares']"
"This paper considers a multi-user multiple-input multiple-output (MU-MIMO) system with independent phase noise (PN) at user terminals (UTs) due to non-synchronous noisy local oscillators. The conventional factor graph of MU-MIMO involves an observation factor with multiple high-dimensional variables in the form of multiplication and summation. In this work, the MU-MIMO factor graph is represented by introducing auxiliary variables, which enables the decomposition of the observation factor into several simpler sub-factors and then use different message passing techniques to obtain approximate marginals. Specifically, belief propagation and expectation propagation (BP-EP) are used for the linear factors for random walk process, modulation and coding, while mean field (MF) is used for non-linear factor in exponential form. To reduce the computational complexity, the non-Gaussian MF messages are approximated to be Gaussian by using the second order Taylor expansion at its belief obtained in the previous iteration. The proposed receiver has a quadratic computational complexity per symbol per iteration. Simulation results show that the performance of the proposed receiver is better than existing methods and is close to the matched filter bound (MFB) in terms of the bit error rate (BER).","['Symbols', 'Channel estimation', 'Receivers', 'Message passing', 'Decoding', 'Computational modeling', 'Manganese']","['MU-MIMO', 'phase noise', 'hybrid message passing', 'joint channel estimation and detection', 'iterative receiver']"
"Significant efforts are being made to integrate satellite and terrestrial networks into a unified wireless network. One major aspect of such an integration is the use of unified user terminals (UTs), which work for both networks and can switch seamlessly between them. However, supporting broadband connectivity for handheld UTs directly from low Earth orbit (LEO) satellite networks is very challenging due to link budget reasons. This paper proposes using distributed massive multiple-input multiple-output (DM-MIMO) techniques to improve the data rates of handheld devices with a view to supporting their broadband connectivity by exploiting the ultra-dense deployment of LEO satellites and high-speed inter-satellite links. In this regard, we discuss DM-MIMO-based satellite networks from different perspectives, including the channel model, network management, and architecture. In addition, we evaluate the performance of such networks theoretically by deriving closed-form expressions for spectral efficiency and using extensive simulations based on actual data from a Starlink constellation. The performance is compared with that of collocated massive MIMO connectivity (CMMC) and single-satellite connectivity (SSC) scenarios. The simulation results validate the analytical results and show the superior performance of DM-MIMO-based techniques compared to CMMC and SSC modes for improving the data rates of individual users.","['Satellite broadcasting', 'Low earth orbit satellites', 'Satellites', 'MIMO communication', 'Satellite antennas', 'Massive MIMO', 'Broadband antennas']","['Satellite communication networks', 'LEO constellations', 'distributed massive MIMO', 'direct satellite connectivity']"
"Millimeter wave (mmWave) communications are widely preferred due to the rich bandwidth and potentially huge spectrum resources. Nowadays, mixed-ADC architecture combined with mmWave massive MIMO has become a communication mainstream, which can effectively solve the issue of high total power consumption and cost of base station (BS) circuits. However, the channel estimation problem for mmWave massive MIMO systems with mixed-ADC architecture has not been studied yet. In this paper, we develop the sparse channel estimation method on this framework. Specifically, by exploiting the sparsity of mmWave channels, the beamspace channel estimation problem can be transformed into a sparse matrix recovery problem, the channel parameters are recovered using compressive sensing (CS) techniques. Simulation results show that the algorithms quantized by the mixed-ADC outperforms the low-resolution ADC, and the best performance can be achieved when the low-resolution ADC in the mixed-ADC architecture reaches five-bit.","['Channel estimation', 'Millimeter wave communication', 'Radio frequency', 'Quantization (signal)', 'Array signal processing', 'Matching pursuit algorithms', 'Computer architecture']","['Mixed-ADC', 'channel estimation', 'narrowband', 'hybrid beamforming', 'millimeter-wave', 'compressed sensing']"
"This paper focuses on the combined radar and communications problem and conducts a thorough analytical investigation on the effect of phase and frequency change on the communication and sensing functionality. First, we consider the classical stepped frequency radar waveform and modulate data usingM-ary phase shift keying (MPSK). Two important analytical tools in radar waveform design, namely the ambiguity function (AF) and the Fisher information matrix (FIM) are derived, based on which, we make the important conclusion that MPSK modulation has a negligible effect on radar local accuracy. Next, we extend the analysis to incorporate frequency permutations and propose a new signalling scheme in which the mapping between incoming data and waveforms is performed based on an efficient combinatorial transform called the Lehmer code. We also provide an efficient communications receiver based on a modified Hungarian algorithm. From the communications perspective, we consider the optimal maximum likelihood (ML) detector and derive the union bound and nearest neighbour approximation on the block error probability. From the radar sensing perspective, we discuss the broader structure of the waveform based on the AF derivation and quantify the radar local accuracy based on the FIM. Extensive numerical examples are provided to illustrate the accuracy of our results.","['Radar', 'Frequency modulation', 'OFDM', 'Sensors', 'Phase modulation', 'Codes', 'Receivers']","['Joint communications and radar', 'maximum likelihood', 'ambiguity function', 'Fisher information matrix']"
"Massive machine-type communications to support a large number of low-cost, low-power, and/or low-rate radio-frequency transceivers such as Internet-of-Things (IoT) devices is a major usage scenario of the 5G mobile communication system. To save cost and power, highly nonlinear power amplifiers (PAs) are expected to be used in the devices. However, the envelope fluctuation of the input of a PA may make the output suffer from severe nonlinear distortion. In this paper, a continuous-phase modulation is proposed for discrete-Fourier transform (DFT)-spread localized orthogonal frequency-division multiplexing (OFDM) of π/2-BPSK symbols. In particular, the combination of a spectrum shaping vector and an extra rotation angle is proposed, which enables the use of a receiver that is fully compatible with the current standard. It is shown that the proposed design significantly outperforms the conventional designs in terms of effective signal-to-distortion ratio and error vector magnitude at the cost of a negligible increase in out-of-band emission.","['Standards', 'Modulation', 'Peak to average power ratio', 'Discrete Fourier transforms', 'Binary phase shift keying', 'Phase modulation', 'Uplink']","['Continuous-phase modulation', 'constellation rotation', 'DFT-spread OFDM', 'nonlinear power amplifier', 'minimum-shift keying', 'pulse shaping']"
"Efficient spectrum utilization is always the fundamental challenge of mobile communication technology toward 6G. Instead of conventional spectral efficiency in bps/Hz, geographical region shall be brought into consideration in bps/Hz/unit-area, which suggests spatial domain technology as a generalization of conventional MIMO to elevate next generation mobile communication technology. Therefore, this paper introduces the formation of smart radio environment considering efficient utilization of radio spectrum in any given geographical area, which can be also viewed as a generalization of cognitive radio technology. By smart management of reconfigurable intelligent surface (RIS) and ambient backscatter communication (ABC) technologies, smart radio environment can be formed in any given area to achieve spectral-spatial efficiency. In this paper, the control of electromagnetic spatial radiation over a region is explored by utilizing repositionable dynamic RIS and harmony of multiple ABC nodes assisted with machine learning (ML) based control mechanism to form spectrum map. Such smart and reconfigurable radio technology demonstrates the superiority of mutual usage of RIS and ABC in terms of shaping the electromagnetic energy in coexisting radio systems. First, the use of multiple RISs is studied to enhance the capacity of secondary use inside the specific area by shaping the electromagnetic energy in the spatial domain. Secondly, a new degree of freedom with repositionable dynamic RIS is introduced. Controlling the time varied shadowing effects by re-positioning the RIS, it is shown that 15% more capacity can be achieved. Finally, the use of multiple coordinated ABCs to protect the region of primary use against the radiation due the secondary uses is investigated. Having coordinated network with repositionable dynamic RIS and multiple ABCs allow us to shape the electromagnetic wave in spatial domain by creating radiation rejection and coverage extension zones. A centralized or distributed mechanism to construct the spectrum map based on ABC sensor is also introduced to instruct existing repositionable dynamic RISs for better coverage. Merging these promising technologies will pave the way for the smart radio environment creating high spectral efficient wireless systems.","['Interference', 'Wireless sensor networks', 'Cognitive radio', '6G mobile communication', 'Wireless networks', 'Next generation networking', 'Robot sensing systems']","['Coexistence', 'cognitive radio', 'spatial-spectral efficiency', 'reconfigurable intelligent surface', 'ambient backscatter communications', 'interference management', 'smart radio environment']"
"The potential of Reconfigurable Intelligent Surfaces (RISs) for energy-efficient and performance-boosted wireless communications is recently gaining remarkable research attention, motivating their consideration for various 5-th Generation (5G) Advanced and beyond applications. In this paper, we consider a Multiple-Input Multiple-Output (MIMO) Physical Layer Security (PLS) system with multiple data streams including one legitimate passive RIS and one malicious passive RIS, with the former being transparent to the multi-antenna eavesdropper and the latter’s presence being unknown at the legitimate multi-antenna transceivers. We first present a novel threat model for the RIS-boosted eavesdropping system and design a joint optimization framework for the eavesdropper’s receive combining matrix and the reflection coefficients of the malicious RIS. Focusing next on the secrecy rate maximization problem, we present an RIS-empowered PLS scheme that jointly designs the legitimate precoding matrix and number of data streams, the Artificial Noise (AN) covariance matrix, the receive combining matrix, and the reflection coefficients of the legitimate RIS. The proposed optimization algorithms, whose convergence to at least local optimum points is proved, are based on alternating maximization, minorization-maximization, and manifold optimization, including semi-closed form expressions for the optimization variables. Our extensive simulation results for two representative system setups reveal that, in the absence of a legitimate RIS, transceiver spatial filtering and AN are incapable of offering non-zero secrecy rates, even for malicious RISs with small numbers of elements. However, when an L-element legitimate RIS is deployed, confidential communication can be safeguarded against eavesdropping systems possessing even more than a 5L-element malicious RIS.","['Eavesdropping', 'Optimization', 'MIMO communication', 'Covariance matrices', 'Threat modeling', 'Precoding', 'Reflection coefficient']","['Artificial noise', 'area of influence', 'MIMO', 'manifold optimization', 'physical layer security', 'reconfigurable intelligent surface', 'secrecy rate', 'threat modeling']"
"We study the problem of symbol detection in downlink coded multiple-input multiple-output (MIMO) systems with precoding and without the explicit knowledge of the channel-state information (CSI) at the receiver. In this context, we investigate the impact of imperfect CSI at the transmitter (CSIT) on the detection performance. We first model the CSIT degradation based on channel estimation errors to investigate its impact on the detection performance at the receiver. To mitigate the effect of CSIT deterioration at the latter, we propose learning-based techniques for hard and soft detection that use downlink precoded pilot symbols as training data. We note that these pilots are originally intended for signal-to-interference-plus-noise ratio (SINR) estimation. We validate the approach by proposing a lightweight implementation that is suitable for online training using several state-of-the-art classifiers. We compare the bit-error rate (BER) and the runtime complexity of the proposed approaches where we achieve superior detection performance in harsh channel conditions while maintaining low computational requirements. Specifically, numerical results show that severe CSIT degradation impedes the correct detection when a conventional detector is used. However, the proposed learning-based detectors can achieve good detection performance even under severe CSIT deterioration, and can yield 4–8 dB power gain for BER values lower than 10 −4 when compared to the classic linear minimum mean square error (MMSE) detector.","['Degradation', 'Training', 'Precoding', 'Channel estimation', 'Training data', 'Detectors', 'Receivers']","['MIMO detection', 'precoding', 'machine learning', 'channel coding', '18 imperfect CSIT']"
"Internet of Drones (IoD) is one of the promising technologies to enhance the performance of wireless networks. Deploying IoD to assist wireless networks, however, needs to address various design issues. Due to the highly dynamic nature of IoD networks, conventional methods are expected to encounter inadequacies that can be resolved using emerging deep reinforcement learning (DRL) techniques. In this paper, we discuss the application of DRL for addressing various issues in IoD networks. We first overview the main features, types, applications, and services of IoD networks. Then, we briefly discuss some DRL algorithms used to address the issues and challenges of IoD networks. After that, we explain the most crucial issues in IoD networks and discuss some papers that show how DRL can address them. Finally, we provide insights into some promising research directions in the context of using DRL in IoD networks.","['Drones', 'Wireless networks', 'Surveillance', 'Task analysis', 'Reinforcement learning', 'Deep learning', 'Traffic control']","['Deep reinforcement learning', 'Internet of drones networks', 'issues']"
"With real-time communication being a key part of the fourth industrial revolution, the need for Quality of Service (QoS) in industrial networks is gaining increasing importance. Time-Sensitive Networking (TSN) faces this need, for example, by introducing new scheduling mechanisms. The Credit-Based Shaper (CBS) has been introduced to TSN to offer low delays for multiple traffic classes by applying rate limitations. Currently, flows are reserved decentrally in CBS networks using a Stream Reservation Protocol (SRP). In contrast, the new TSN standard IEEE 802.1Qcc allows for a centralized architecture to favor short reconfiguration latencies. However, no online admission control scheme which offers safe delay bounds has been proposed for this central architecture. To close this gap, we propose two models for admission control in TSN networks using CBS. Both models offer deadline-guaranteeing flow allocation, including routing and prioritization of flows, and configure forwarding devices while eliminating packet loss. Our models utilize the mathematical framework of Network Calculus to calculate worst-case flow delays and buffer sizes. We show how our models allow for more reservations than the decentralized standard approach by improved resource utilization. We validate our models both in synthetic and industrial network scenarios. Additionally, we compare the effects and parameters of our two models, providing guidance on when to choose them.","['Delays', 'Standards', 'Admission control', 'Mathematical models', 'Logic gates', 'Routing', 'Resource management']","['Admission control', 'computer network management', 'credit-based shaper', 'network calculus', 'quality of service', 'routing', 'time-sensitive networking']"
"This paper proposes novel designs of line-of-sight multiple-input multiple-output (LOS-MIMO) and a unique sample-level coding scheme for wireless fronthaul (WFH) deployment in Open Radio Access Network (O-RAN) and beyond the fifth-generation (5G) systems. The objective of these designs is to achieve high capacity, robustness, and ultra-reliability in WFH, providing performance comparable to fiber-optic fronthaul solutions. Firstly, by optimizing array configuration parameters, such as array aperture size, antenna separation, and the proposed analog multipath beamforming scheme, LOS-MIMO operating at millimeter-wave (mmWave) bands emerges as a critical enabler for WFH, delivering the required high data rates to support lower-level split options as defined by the 3rd Generation Partnership Project for fronthaul networks. Furthermore, the use of a non-uniform array design effectively enhances the robustness of WFH by addressing the performance degradation caused by spatial aliasing when LOS-MIMO is employed with uniformly spaced linear array at shorter distances. Additionally, the proposed augmented sample-level coding not only seamlessly integrates with the fronthaul design but also provides benefits for ultra-reliable applications carried over the fronthaul. Through extensive simulations at both system and link levels, it is demonstrated that employing high-rank LOS-MIMO at mmWave bands for WFH ensures high capacity, robustness, and reliable performance. Overall, the analysis presented in this paper establishes that WFH can be effectively designed to meet the stringent requirements of fronthaul networks and represents an attractive option for future disaggregated RAN deployments.","['Wireless communication', 'Millimeter wave communication', 'Antenna arrays', 'Optical fiber networks', 'Arrays', '5G mobile communication', 'Optical fiber cables']","['Wireless fronthaul', 'LOS-MIMO', 'sample-level coding', 'mmWave', 'open RAN', 'disaggregated RAN', 'Beyond 5G']"
"Skip graph is a distributed data structure that provides a scalable structured overlay network by routing in logarithmic time for resource location and dynamic node addition/deletion. However, most of the routing paths are quite longer than the shortest paths because each node in the network knows only its neighbors, rather than the global topology. In general, long routing paths lead to the high latency and the low fault tolerance. Herein, we propose Detouring Skip Graph, which performs more efficient routing through the use of detour routes. It does not require construction of extra links or modification of its topology; thereby, it shortens the paths without additional costs while maintaining the advantages of Skip Graph. Our evaluation experiments show that the proposed method tends to shorten the paths considerably, and in particular, that the average path length is approximately 20%-30% shorter than that of Skip Graph.","['Routing', 'Topology', 'Fault tolerance', 'Fault tolerant systems', 'Data structures', 'Overlay networks', 'Electronic mail']","['Detour route', 'routing algorithm', 'skip Graph', 'structured overlay']"
"Digital twin-aided (DT) edge computing is investigated, where users utilize grant-free random access with adaptive rate to offload their tasks to the edge server. A novel, with lower implementation complexity, probabilistic partial offloading scheme is introduced, while each device is assumed to have an infinite buffer to store its tasks. The aim of the proposed work is to minimize the average delay of the partial offloading. To that end, the average delay of waiting in the queue, the delay of offloading, and the local computation delay are extracted by using queuing theory tools. Then, the non-convex problem of minimizing the average delay of all clients is formulated, while taking into account DT imperfections. Successive convex approximation (SCA), alternating optimization (AO), and various algebraic manipulations are utilized to transform the problem into an equivalent convex problem with tractable solution. Finally, simulation results showcase the value of the proposed analysis and offer important insights for the proposed DT-aided edge network. Specifically, the proposed partial offloading scheme is shown to be more delay efficient compared to both local computing and full offloading, particularly, for greater task generation rates at the users. Also, the impact of the DT imperfections at the average delay is shown to be more notable as the number of users, or the tasks’ size, increases.","['Delays', 'Task analysis', 'Servers', 'Internet of Things', 'Edge computing', 'Ultra reliable low latency communication', 'Queueing analysis']","['Grant-free random access', '6G', 'mobile edge computing', 'digital twin']"
"In this paper, we investigate a resource allocation problem for a Cellular Vehicle to Everything (C-V2X) network to improve energy efficiency of the system. To address this problem, self-organizing mechanisms are proposed for joint and disjoint subcarrier and power allocation procedures which are performed in a fully distributed manner. A multi-agent Q-learning algorithm is proposed for the joint power and subcarrier allocation. In addition, for the sake of simplicity, it is decoupled into two sub-problems: a subcarrier allocation sub-problem and a power allocation sub-problem. First, to allocate the subcarrier among users, a distributed Q-learning method is proposed. Then, given the optimal subcarriers, a dynamic power allocation mechanism is proposed where the problem is modeled as a non-cooperative game. To solve the problem, a no-regret learning algorithm is utilized. To evaluate the performance of the proposed approaches, other learning mechanisms are used which are presented in Fig. 8 . Simulation results show the multi-agent joint Q-learning algorithm yields significant performance gains of up to about 11% and 18% in terms of energy efficiency compared to proposed disjoint mechanism and the third disjoint Q-learning mechanism for allocating the power and subcarrier to each user; however, the multi-agent joint Q-learning algorithm uses more memory than disjoint methods.","['Resource management', 'Device-to-device communication', 'Q-learning', 'Games', 'Interference', 'Learning systems', 'Uplink']","['Cellular vehicle-to-everything (C-V2X) communication', 'PD-NOMA', 'resource allocation', 'learning algorithm']"
"Non-orthogonal multiple access (NOMA) is considered by the 3GPP as a potential technology for beyond 5G wireless networks to support massive connectivity with robust reliability. However, the massive increase in connected users critically leads to data security problems owing to the high possibility of eavesdropping. In this paper, uplink secure NOMA (sNOMA) schemes based on chaotic physical layer security (PLS) are proposed to achieve two-fold secrecy of integrated channel coding and secret key approaches. Different code-domain and power-domain techniques are employed for sNOMA designs over realistic fading channel environments. Equal power allocation strategy is used for the transmitted chaotic signals in code-domain sNOMA (CD-sNOMA), whereas dynamic power control is employed in power-domain sNOMA (PD-sNOMA) and the hybrid code-power-domain sNOMA (CPD-sNOMA) approaches. The former design adopts joint maximum Likelihood (ML) signal detection while the later scenarios utilize integrated receiver designs based on successive interference cancellation (SIC) and ML techniques. For the proposed sNOMA schemes, power control algorithms are presented to optimize the system performance over constrained total received power and target error rate. Numerical results validate the effectiveness of sNOMA designs compared with the benchmark systems under the worst-case secrecy of unauthorized receiver with complete knowledge of the transmission scheme and associated channel. Valuable tradeoffs are demonstrated between the achieved error rate, connectivity, security gap, and complexity. Moreover, the utilized chaotic signals offer robust and cost-effective PLS solutions with a huge key-space to combat the most powerful brute-force eavesdropping attacks.","['NOMA', 'Security', 'Chaotic communication', 'Receivers', 'Interference cancellation', 'Uplink', 'Multiaccess communication']","['Chaotic signals', 'NOMA schemes', 'physical layer security', 'Rician channels', 'wireless communication networks']"
"Recent advances in 5G and beyond have further expanded the potential of IoT applications, bringing unprecedented levels of connectivity, speed, and low latency. However, these advances come with significant security threats that can cause widespread damage. An effective approach to addressing these issues involves the integration of cutting-edge technologies like machine learning (ML), particularly deep reinforcement learning (DRL). DRL is a specialized area of ML that integrates the concepts of deep learning and reinforcement learning to create effective solutions for various tasks. In particular, DRL can facilitate the creation of intelligent security systems that can adapt to dynamic and intricate IoT applications connected to 5G and beyond networks. However, effectively implementing DRL-based intrusion detection frameworks in IoT applications connected to 5G networks poses significant challenges due to bandwidth utilization and device behavior. The data generated by IoT devices is often limited, and malicious behavior may be infrequent, making it difficult to accurately identify and train the algorithm to detect such behavior. Moreover, DRL algorithms pose a significant challenge for IoT devices constrained by limited bandwidth, as communicating large amounts of data required by DRL algorithms can cause network congestion and delay critical communications. In this article, we introduce a novel approach to improving the security of IoT applications in the 5G and beyond era by developing an intrusion detection system that employs DRL algorithms. Our approach involves a distributed Q-learning algorithm that observes the behavior of connected devices and predicts anomalous actions. Additionally, to overcome the challenges associated with bandwidth utilization and device behavior, we introduce a bandwidth allocation problem based on a reputation mechanism that allocates bandwidth to only trustworthy devices. Finally, we evaluate our proposed intrusion detection system on the selected indicators. The numerical results demonstrate that our proposed approach outperforms the referenced solutions on the selected indicators.","['Internet of Things', 'Security', 'Intrusion detection', 'Behavioral sciences', '5G mobile communication', 'Bandwidth', 'Reliability']","['5G and beyond', 'intrusion detection system (IDS)', 'deep reinforcement learning (DRL)', 'Internet of Things (IoT)']"
"The use of AI on Smart applications and in the organization of the network edge presents a rapidly advancing research field, with a great variety of challenges and opportunities. This article aims to provide a holistic review of studies from 2019 to 2021 related to the Intelligent Edge, a concept comprising both the use of AI to organize edge networks (Edge Intelligence) and Smart applications in the edge. An introduction is given to the technologies required to understand the state of the art of AI in edge networks, and a taxonomy is provided with “Enabling Technology” for Edge Intelligence, “Organization” of the edge using AI, and AI “Applications” in the edge as its main topics. Research trend data from 2015 to 2020 is presented for various subdivisions of these topics, showing both absolute and relative research interest in each subtopic. The “Organization” aspect, being the main focus of this article, has a more fine-grained subdivision, explaining all contributing factors in detail. The trends indicate an exponential increase in research interest in nearly all subtopics, but significant differences between them. For each subdivision of the taxonomy a number of selected studies from 2019 to 2021 are gathered to form a high-level illustration of the state of the art of Edge Intelligence. From these selected studies and the trend data, a number of short-term challenges and high-level visions for Edge Intelligence are formulated, providing a basis for future work.","['Artificial intelligence', 'Software', 'Market research', 'Genomics', 'Taxonomy', 'Logistics', 'Training']","['Fog computing', 'fog networks', 'edge networks', 'edge computing', 'artificial intelligence', 'review', 'trends']"
"Technology solutions must effectively balance economic growth, social equity, and environmental integrity to achieve a sustainable society. Notably, although the Internet of Things (IoT) paradigm constitutes a key sustainability enabler, critical issues such as the increasing maintenance operations, energy consumption, and manufacturing/disposal of IoT devices have long-term negative economic, societal, and environmental impacts and must be efficiently addressed. This calls for self-sustainable IoT ecosystems requiring minimal external resources and intervention, effectively utilizing renewable energy sources, and recycling materials whenever possible, thus encompassing energy sustainability. In this work, we focus on energy-sustainable IoT during the operation phase, although our discussions sometimes extend to other sustainability aspects and IoT lifecycle phases. Specifically, we provide a fresh look at energy-sustainable IoT and identify energy provision, transfer, and energy efficiency as the three main energy-related processes whose harmonious coexistence pushes toward realizing self-sustainable IoT systems. Their main related technologies, recent advances, challenges, and research directions are also discussed. Moreover, we overview relevant performance metrics to assess the energy-sustainability potential of a certain technique, technology, device, or network, together with target values for the next generation of wireless systems, and discuss protocol, integration, and implementation issues. Overall, this paper offers insights that are valuable for advancing sustainability goals for present and future generations.","['Internet of Things', 'Sustainable development', 'Green products', 'Wireless communication', 'Smart grids', 'Energy efficiency', 'Economics']","['Energy efficiency', 'energy harvesting', 'energy sustainability', 'energy transfer', 'green wireless communication', 'IoT', 'machine learning']"
"A sensor device can be used to detect target events at low cost. Moreover, there is a significant risk of sensor nodes being compromised or captured within large wireless sensor networks (WSNs). The transmission of valid event messages to users by a WSN can be hindered by network congestion due to false event messages in a compromised node. Several methods are available for detecting false event messages. However, they rely on long bits of authentication codes and hence do not provide a fundamental solution to prevent network congestion. In the proposed method, various hashing vectors, which are spaceefficient data structures that can determine whether the given data are an element of a set, are created based on the correct combination of authentication codes and placed in each node in advance. An event message contains an XOR of the authentication codes, and each node verifies it based on its hashing vector. If a node is acquired illegally, the information of the hashing vector and the XOR information of the authentication codes assigned to the correct event message is compromised, so we propose an algorithm to update the information securely. Compared to existing research, the number of hops required to detect a false event message increases by only about one hop, but the amount of traffic that a malicious node can generate can be reduced by about 60% or more. In other words, the proposed method effectively reduces the amount of traffic an attacker can generate with false event messages, which also reduces the overall network congestion.","['Wireless sensor networks', 'Codes', 'Authentication', 'Clustering algorithms', 'Sensors', 'Filtering', 'Event detection']","['Energy-efficient protocol', 'false event detection', 'security', 'wireless sensor networks']"
"We consider the problem of minimizing the total energy consumption due to the computation and communication tasks of federated learning (FL) under bandwidth and latency constraints. To avoid channel state information (CSI) feedback to the transmitter, we adopt outage probability as an additional constraint in the energy minimization problem. First, we define a feasibility metric based on the system design parameters to exclude slow clients (stragglers). Then, we propose a novel client selection algorithm, after excluding stragglers, based on dividing the remaining clients into clusters, where clients within the same cluster collaborate in a communication round to train their local models. For each communication round, one client cluster is selected in a round-robin fashion. Furthermore, we formulate and solve a resource allocation problem to optimize the transmit power, clock frequency, allocated bandwidth, and communication latency of clients within each cluster to minimize the total energy subject to total bandwidth, latency, and outage constraints. Moreover, we extend our FL design framework to the case of no CSI at both the client and server ends using differential transmission to eliminate CSI estimation pilot overhead and complexity at comparable total energy consumption and learning accuracy to coherent transmission. We test our proposed algorithms over MNIST and Fashion-MNIST datasets in iid and non-iid settings. Our proposed client selection algorithm reduces the number of participating clients per communication round by 41% compared to the baseline while maintaining the same learning accuracy. Moreover, our results demonstrate that increasing the number of receive antennas at the server from one to four can reduce the number of communication rounds required to reach a predetermined testing accuracy level by up to 53% for the Fashion-MNIST dataset.","['Servers', 'Bandwidth', 'Energy consumption', 'Wireless communication', 'Computational modeling', 'Resource management', 'Federated learning']","['Federated learning', 'outage probability', 'CSI', 'client selection', 'and resource allocation']"
"Simultaneous wireless information and power transfer (SWIPT) constitutes an emerging paradigm that prolongs the lifetime of energy-constrained devices, such as wireless sensors and Internet-of-Things (IoT) nodes. Its frequency-domain (FD) variant enables energy harvesting (EH) by using (low-power) local oscillators/mixers. In this paper, a novel FD-SWIPT waveform design that minimizes the multitone interference induced to the information signal by the energy signal, thus eliminating the need for using receive filters to this end, is described. The inherent interference suppression of the proposed strategy allows also for applying modulation classification (MC). This functionality is highly desirable in contemporary networks, where the access points utilize adaptive transmission. In this context, we analytically derive the average error probability of the proposed waveform over a Rayleigh fading channel for various modulation schemes under non-zero interference and frequency synchronization errors. Furthermore, we optimize the power of the energy tones, such that the signal-to-interference-ratio at the information signal is maximized subject to the EH and transmission power constraints. In addition, we investigate the coexistence of SWIPT with blind MC under this framework. Numerical simulation results reveal that the proposed approach substantially increases both the data rate and the harvested power in comparison to the conventional power-splitting method at the cost of a negligibly higher error probability. Also, they indicate that the employed MC scheme achieves a high success rate even in the low signal-to-noise-ratio regime. The proposed concept is validated experimentally in a realistic indoor environment by using a testbed based on software-defined radio units.","['Modulation', 'Wireless communication', 'Receivers', 'Wireless sensor networks', 'Frequency-domain analysis', 'Interference', 'OFDM']","['SWIPT', 'multitone energy signal', 'waveform design', 'optimization', 'experimental validation']"
"Detection for high-dimensional multiple-input multiple-output (MIMO) and massive MIMO (MMIMO) systems is an active field of research in wireless communications. While most works consider spatially uncorrelated channels, practical MMIMO channels are correlated. This paper investigates the impact of correlation on Sphere Decoder (SD), for both single-user and multi-user scenarios. The complexity of SD is mainly determined by the initial radius method and the number of visited nodes during detection. This paper introduces a new constraint on the evaluation process of the partial distance thus modifying the conventional tree searching algorithm. This significantly decreases the number of visited nodes and renders SD feasible for large-scale systems. In addition, a proposed hardware implementation featured with a one-node-per-cycle architecture, minimizes the latency of the detection process. Trade-offs between bit error rate performance and computational complexity are presented. The trade-offs are achieved by either modifying the backtracking mechanism or limiting the number of radius updates. Simulation results prove that the proposed optimizations are effective for both correlated and uncorrelated channels, regardless of the level of noise. The decoding gain of SD compared to the low-complexity linear detectors is higher in the presence of correlation than in the uncorrelated case. However, as expected, spatial correlation adversely affects the performance and the complexity of SD. Simulation results reported here also confirm that correlation at the side equipped with more antennas is less detrimental. Hardware implementation aspects are examined for both a Virtex-7 field-programmable gate array device and a 28-nm application-specific integrated circuit technology.","['Correlation', 'Detectors', 'Hardware', 'Modulation', 'Large-scale systems', 'Bit error rate', 'Receiving antennas']","['Correlation', 'initial radius', 'massive MIMO', 'maximum likelihood decoding', 'MIMO detection', 'multiple-input multiple-output', 'sphere decoding', 'hardware architecture']"
"We study the outage probability performance of hybrid protocols in relay-assisted, non-orthogonal multiple access (NOMA)-based power line communication. In particular, we consider two networks with source, relay and destination nodes, namely, cooperative relaying NOMA (CR NOMA) and single-stage NOMA (SS NOMA). We derive closed-form expressions for the outage probabilities of three hybrid relaying techniques, namely, hybrid decode- and amplify-and-forward (HDAF), incremental HDAF (IHADF) and IHDAF with maximal ratio combining (MRC) protocols. The HDAF protocol overcomes the limitations of decode-and-forward (DF) and amplify-and-forward (AF) protocols, by combining the advantages of both. In IHDAF protocol, the advantage of successful decoding at the receiver is exploited using the direct link between source and destination nodes, with the added benefit due to HDAF. Through an extensive simulation study, we (a) validate our analysis, (b) show that IHDAF-MRC outperforms IHDAF, HDAF, DF and AF, and (c) CR NOMA outperforms SS NOMA when IHDAF-MRC scheme is employed.","['NOMA', 'Relays', 'Protocols', 'Symbols', 'Receivers', 'Probability', 'Power system reliability']","['Cooperative relaying', 'hybrid relaying protocols', 'maximal ratio combining', 'non-orthogonal multiple access', 'power line communication']"
"Due to the low received power of Global Navigation Satellite Signals (GNSS), the performance of GNSS receivers can be disrupted by anthropogenic radio frequency interferences, with intentional jamming and spoofing activities being among the most critical threats. It is reported in the literature that modern, GNSS-equipped Android smartphones are generally resistant to simplistic spoofing, and many recent contributions support such a biased belief. In this paper, we present the results of a test campaign designed to further stress the resilience of such devices to simplistic spoofing attacks and highlight their actual vulnerability. We then propose an effective spoofing detection technique, that exploits the spatial and temporal correlation of the counterfeit signals by leveraging the statistical analysis of raw GNSS measurements. By not requiring access to the low signal processing level of the GNSS receiver, the proposed solution applies to any device embedding a GNSS receiver that provides raw GNSS measurements, such as current Android smartphones. Vulnerability analysis and validation of the proposed technique were conducted in a controlled environment by transmitting realistic, counterfeit Global Positioning System L1/CA navigation signals to a variety of Android smartphones embedding also different GNSS chipsets. We show that, under proper conditions, the devices were vulnerable to the attacks and that the effects were visible through their raw measurements, i.e., Carrier-to-noise ratio (C/N_{0}), pseudo-range measurements, and position estimates. In particular, the study demonstrates that cross-correlation between the C/N_{0}time series provided by each device for different GNSS satellites increases under spoofing conditions, thus constituting an effective metric to detect the attack within a few seconds.","['Global navigation satellite system', 'Smart phones', 'Receivers', 'Semiconductor device measurement', 'Position measurement', 'Operating systems', 'Frequency measurement']","['Radio frequency interference', 'simplistic spoofing', 'global navigation satellite system (GNSS)', 'GNSS receiver', 'smartphone', 'GNSS raw measurements']"
"In recent years there has been a growing interest in reconfigurable intelligent surfaces (RISs) as enablers for the realization of smart radio propagation environments which can provide performance improvements with low energy consumption in future wireless networks. However, to reap the potential gains of RIS it is crucial to jointly design both the transmit precoder and the phases of the RIS elements. Within this context, in this paper we study the use of multiple RIS panels in a parallel or multi-hop configuration with the aim of assisting a multi-stream multiple-input multiple-output (MIMO) communication. To solve the nonconvex joint optimization problem of the precoder and RIS elements targeted at maximizing the achievable rate, we propose a novel iterative algorithm based on the monotone accelerated proximal gradient (mAPG) method which includes an extrapolation step for improving the convergence speed and monitoring variables for ensuring sufficient descent of the algorithm. Based on the sufficient descent property we then present a detailed convergence analysis of the algorithm which includes expressions for the step size. Simulation results in different scenarios show that the use of multiple RIS panels combined with the proposed algorithm can be an effective solution for improving the achievable rates.","['Array signal processing', 'Optimization', 'MIMO communication', 'Reflection', 'Iterative methods', 'Complexity theory', 'Convergence']","['Reconfigurable intelligent surface (RIS)', 'achievable rate', 'multiple-input multiple-output (MIMO)', 'proximal minimization']"
"Multicast communication over wireless networks has many potential applications, such as real-time audiovisual content distribution or digital signage systems with multiple remote terminals. However, today’s common 802.11 networks cannot fully support such applications at the link layer due to the technical challenges in achieving both high transmission rate and high reliability for multicast data transfers. Those challenges primarily emerge from the inapplicability of immediate acknowledgment mechanisms of unicast transmissions to the multicast case and the need for finding a common transmission rate for all receivers with diverse channel conditions. The research literature in this domain mainly addresses the problem at the higher layers of the protocol stack. In this article, we present a novel approach that combines wireless link rate selection with an adaptive packet-level Forward Error Correction (FEC) mechanism in order to achieve high-rate and highly reliable multicast in 802.11 wireless networks. The integration of FEC into the 802.11 MAC layer allows direct interaction between the transmission rate and the coding scheme. As a result, potential packet losses caused by higher link rates can be compensated by the adjustable redundancy provided by FEC. In combination with an aggregated receiver feedback mechanism, this yields improved transmission efficiency and reliability for wireless multicast. We investigate this approach in a simulation environment under a realistic wireless channel model in various application scenarios with up to 50 receivers. The results represent significant performance improvements, in terms of throughput, channel utilization, and packet loss, over the state-of-the-art methods for reliable wireless multicast.","['Reliability', 'IEEE 802.11 Standard', 'Forward error correction', 'Receivers', 'Unicast', 'Protocols', 'Standards']","['802.11', 'forward error correction', 'network coding', 'rate sampling', 'reliable multicast', 'transmission rate control', 'wireless networks']"
"Latency to end-users and regulatory requirements push cloud providers to operate many datacenters all around the globe to host their cloud services. An emerging problem under such geo-distributed architecture is to assign each user request to an appropriate datacenter to benefit both cloud providers (e.g., low bandwidth cost) and end-users (e.g., low latency)-known as request allocation. However, prior request allocation solutions have significant limitations: they either focus only on optimizing the benefits for one entity (e.g., providers or users), or ignore some practical yet indispensable factors (e.g., heterogeneous latency requirements of different users and diverse per unit bandwidth cost among different datacenters) when optimizing benefits for both entities. In this paper, we study the problem of minimizing the total bandwidth cost for cloud service providers while guaranteeing the latency requirement for end-users. Specifically, we formulate an integer programming with consideration of the diversities in both the delay of requests and per unit bandwidth cost of datacenters. To efficiently and practically solve this problem, we first relax the integer programming into a continuous convex optimization and then take the advantages of random sampling to enforce the solution to be a feasible one for the original integer programming. We have conducted rigorous theoretical analysis to prove that our algorithm can provide a considerable good competitive ratio. Extensive simulations demonstrate that our proposed algorithm can reduce the total bandwidth cost by 30% while guaranteeing the latency requirements of all requests, as compared to conventional methods.","['Bandwidth', 'Resource management', 'Delays', 'Linear programming', 'Convex functions', 'Mathematical model', 'Optimization']","['Cloud services', 'request allocation', 'latency', 'bandwidth cost', 'random sampling and rounding']"
"In a mmWave mobile device, power consumption resulting from the high sampling rate is a primary concern for angle of arrival (AoA) estimation solutions. In this paper, we provide a power scalable solution for AoA estimation with structured waveforms in a narrowband channel. We design a set of pilot sequences that maintain orthogonality in sub-Nyquist sampling domains. We leverage the sequences’ structure to develop a variable rate decoupling algorithm to separate multiple sources at the receiver using partial knowledge about the pilots. The decoupling enables a feasible, low-complexity AoA estimation for digital architectures with flexible antenna array design. In this paper, we provide one such AoA estimation solution named ADELA for a linear antenna array design. Simulation results show that AoA estimation performance reaches the Cramer-Rao-Bounds (CRBs) for a range of SNRs. The proposed estimator with subsampling factors of 8 or less outperforms two examples of full rate virtual array AoA estimators for unknown waveforms: 2-level nested array and coprime filter bank estimators. Compared to these two examples of virtual array, our method offers an 8 times lower ADC power consumption, and a significantly lower computational complexity.","['Estimation', 'Receivers', 'Power demand', 'Linear antenna arrays', 'Wireless communication', 'Signal to noise ratio', 'Computer architecture']","['Sub-Nyquist AoA estimation', 'mmWave', 'digital receiver', 'pilot design', 'waveform structure']"
"We consider a system that integrates positioning and single-user millimeter wave (mmWave) communication, where the communication part adopts wavelength division multiplexing (WDM) and orbital angular momentum (OAM). This paper addresses the multi-dimensional constellation design in short-range line-of-sight (LOS) environment, with stable communication links. We propose a map-assisted method to quantify the system parameters based on positions and reduce real-time computing overhead. We explore the possibility of using a few patterns in the maps, and investigate its performance loss. We first investigate the features of OAM beams, and find that the link gain ratio between any two sub-channels remains unchanged at some specific positions. Then, we prove that a fixed constellation can be adopted for the positions where the link gain matrices are sufficiently close to be proportional. Moreover, we prove that the system can adopt a fixed power vector to generate a multi-dimensional constellation if the difference between fixed power vector and optimal power vector is small. Finally, we figure out that the constellation design for all receiver positions can be represented by a few constellation sets.","['Millimeter wave communication', 'Wavelength division multiplexing', 'Transmitting antennas', 'Receiving antennas', 'Optimization', 'Symbols', 'Real-time systems']","['Millimeter wave communication', 'multi-dimensional constellation', 'orbital angular momentum', 'wavelength division multiplexing', 'short-range line-of-sight']"
"The deployment of 5G networks is approaching a mature phase in many countries across the world. However, little efforts have been done so far to scientifically compare ElectroMagnetic Field (EMF) exposure and traffic levels before and after the activation of 5G service over the territory. The goal of this work is to provide a sound comparative assessment of exposure and traffic, by performing repeated measurements before and after 5G provisioning service. Our solution is based on an EMF meter and a spectrum analyzer that is remotely controlled by a measurement algorithm. In this way, we dissect the contribution of each pre-5G and 5G band radiating over the territory. In addition, we employ a traffic chain to precisely characterize the achieved throughput levels. Results, derived from a set of measurements performed on a commercial deployment, reveal that the provisioning of 5G service over mid-band frequencies has a limited impact on the exposure. In parallel, the measured traffic is more than doubled when 5G is activated over mid-bands, reaching levels above 200 [Mbps]. On the other hand, the provisioning of 5G over sub-GHz bands does not introduce a substantial increase in the traffic levels. Eventually, we demonstrate that EMF exposure is impacted by the raw-land reconfiguration to host the 5G panels, which introduces changes in the sight conditions and in the power received from the main lobes.","['5G mobile communication', 'Throughput', 'Monitoring', 'Frequency measurement', 'Base stations', 'Antenna measurements', 'Poles and towers']","['5G technology', 'EMF measurement', 'traffic measurement', 'comparative analysis']"
"This paper develops mathematical models for molecule harvesting transmitters in diffusive molecular communication (MC) systems. In particular, we consider a communication link consisting of a spherical transmitter nano-machine and a spherical receiver nano-machine suspended in a fluid environment. The transmitter and the receiver exchange information via signaling molecules. The transmitter is equipped with molecule harvesting units on its surface. Signaling molecules which come into contact with the harvesting units may be re-captured by the transmitter. For this system, we derive closed-form expressions for the channel impulse response and harvesting impulse response. Furthermore, we extend the harvesting transmitter model to the case of continuous signaling molecule release. In particular, we derive closed-form expressions for the average received signal at the receiver and the average harvested signal at the transmitter for different temporal release rates namely, constant, linearly increasing, and linearly decreasing release rates. Finally, we validate the accuracy of the derived mathematical expressions via particle-based simulations.","['Transmitters', 'Receivers', 'Mathematical models', 'Chemicals', 'Analytical models', 'Numerical models', 'Neurons']","['Transmitter modeling', 'molecule harvesting', 'energy harvesting transmitter', 'diffusive molecular communication']"
"This paper studies a bandwidth-limited federated learning (FL) system where the access point is a central server for aggregation and the energy-constrained user equipments (UEs) with limited computation capabilities (e.g., Internet of Things devices) perform local training. Limited by the bandwidth in wireless edge systems, only a part of UEs can participate in each FL training round. Selecting different UEs could affect the FL performance, and selected UEs need to allocate their computing resource effectively. In wireless edge FL systems, simultaneously accelerating FL training and reducing computing-communication energy consumption are of importance. To this end, we formulate a multi-objective optimization problem (MOP). In MOP, the model training convergence is difficult to calculate accurately. Meanwhile, MOP is a combinatorial optimization problem, with the high-dimension mix-integer variables, which is proved to be NP-hard. To address these challenges, a multi-objective evolutionary algorithm for the bandwidth-limited FL system (MOEA-FL) is proposed to obtain a Pareto optimal solution set. In MOEA-FL, an age-of-update-loss method is first proposed to transform the original global loss function into a convergence reference function. Then, MOEA-FL divides MOP intoNsingle objective subproblems by the Tchebycheff approach and optimizes the subproblems simultaneously by evolving a population. Extensive experiments have been carried out on MNIST dataset and a medical case called TissueMNIST dataset for both the i.i.d and non-i.i.d data setting. Experimental results demonstrate that MOEA-FL performs better than other algorithms and verify the robustness and scalability of MOEA-FL.","['Training', 'Convergence', 'Energy consumption', 'Optimization', 'Wireless communication', 'Computational modeling', 'Bandwidth']","['Federated learning', 'multi-objective evolutionary algorithm', 'combinatorial optimization', 'differential evolution', 'decomposition']"
"In this paper, we examine the use of intelligent reflective surface (IRS) to reconfigure the wireless propagation environment which assists the physical-layer multicast transmission with simple single-antenna radios in the network. Different phase-shift strategies are employed to adjust the reflection of the IRS. On the one hand, the max-min criterion is utilized to enhance the achievable rate of the receive nodes inside the multicast-target group. On the other hand, for the nodes outside the multicast-target group in the same network, we take advantage of another min-max criterion to let them all be less interfered with by the specific multicast communication. Multiobjective optimization is then set to reveal the compromises between the effect on the nodes of the two different groups. Simulation results show that the proposed IRS-aided physical-layer multicasting can fulfill the above-stated task, and can let the nodes in either group gain better results.","['Multicast communication', 'Optimization', 'Wireless communication', 'Wireless sensor networks', 'Simulation', 'Tuning', 'Task analysis']","['Intelligent reflective surface (IRS)', 'physical-layer multicasting', 'max-min criterion', 'min-max criterion', 'convex optimization', 'semidefinite relaxation (SDR)', 'multiobjective optimization']"
"In this paper, we study the throughput utility functions in buffer-equipped monostatic backscatter communication networks with multi-antenna Readers. In the considered model, the backscatter nodes (BNs) store the data in their buffers before transmission to the Reader. We investigate three utility functions, namely, the sum, the proportional and the common throughput. We design online admission policies, corresponding to each utility function, to determine how much data can be admitted in the buffers. Moreover, we propose an online data link control policy for jointly controlling the transmit and receive beamforming vectors as well as the reflection coefficients of the BNs. The proposed policies for data admission and data link control jointly optimize the throughput utility, while stabilizing the buffers. We adopt the min-drift-plus-penalty (MDPP) method in designing the control policies. Following the MDPP method, we cast the optimal data link control and the data admission policies as solutions of two independent optimization problems which should be solved in each time slot. The optimization problem corresponding to the data link control is non-convex and does not have a trivial solution. Using Lagrangian dual and quadratic transforms, we find a closed-form iterative solution. Finally, we use the results on the achievable rates of finite blocklength codes to study the system performance in the cases with short packets. As demonstrated, the proposed policies achieve optimal utility and stabilize the data buffers in the BNs.","['Throughput', 'Backscatter', 'Array signal processing', 'Internet of Things', 'Impedance', 'Downlink', 'Buffer storage']","['Backscatter communication', 'radio frequency identification', 'fairness', 'min-drift-plus-penalty', 'Lyapunov optimization', 'max-min throughput', 'proportional throughput', 'sum throughput', 'finite blocklength analysis', 'wireless energy transfer', 'energy harvesting', 'green communications', 'Internet of Things']"
"We consider multiple-input multiple output (MIMO) non-orthogonal multiple access (NOMA) downlink channels with zeroforcing beamforming transmission. The base station has multiple transmit antennas while all mobile devices have single receive antenna. With a limited feedback rate, channel direction information (CDI) needs to be quantized and fed back from mobile users to a base station. Thus, the accuracy of the quantized channel state information at the transmitter (CSIT) will depend on the feedback rate. To increase spectral efficiency, 2 active users in some clusters share the same beamforming vector and thus, will interfere fully with each other. Given a total-feedback rate, we analyze the feedback allocation for clusters with single active user and 2 active users. The objective is to either minimize the maximum outage probability or maximize the minimum rate among all active users in a cell. We show that the proposed feedback allocation is close to the optimum. Some numerical examples show that the resulting rate performance with the proposed feedback allocation is increased by 100% over that with the uniform feedback allocation.","['Resource management', 'Base stations', 'NOMA', 'Transmitting antennas', 'Power system reliability', 'Probability', 'Array signal processing']","['Non-orthogonal multiple access (NOMA)', 'multiple-input multiple-output (MIMO)', 'channel state information at transmitter (CSIT)', 'random vector quantization (RVQ)', 'zeroforcing', 'beamforming', 'feedback']"
"The non-orthogonal coexistence between the enhanced mobile broadband (eMBB) and the ultra-reliable low-latency communication (URLLC) in the downlink of a multi-cell massive MIMO system is rigorously analyzed in this work. We provide a unified information-theoretic framework blending an infinite-blocklength analysis of the eMBB spectral efficiency (SE) in the ergodic regime with a finite-blocklength analysis of the URLLC error probability relying on the use of mismatched decoding, and of the so-called saddlepoint approximation. Puncturing (PUNC) and superposition coding (SPC) are considered as alternative downlink coexistence strategies to deal with the inter-service interference, under the assumption of only statistical channel state information (CSI) knowledge at the users. eMBB and URLLC performances are then evaluated over different precoding techniques and power control schemes, by accounting for imperfect CSI knowledge at the base stations, pilot-based estimation overhead, pilot contamination, spatially correlated channels, the structure of the radio frame, and the characteristics of the URLLC activation pattern. Simulation results reveal that SPC is, in many operating regimes, superior to PUNC in providing higher SE for the eMBB yet achieving the target reliability for the URLLC with high probability. Moreover, PUNC might cause eMBB service outage in presence of high URLLC traffic loads. However, PUNC turns to be necessary to preserve the URLLC performance in scenarios where the multi-user interference cannot be satisfactorily alleviated.","['Ultra reliable low latency communication', 'Massive MIMO', 'Reliability', 'Downlink', 'Uplink', 'Error probability', 'Fading channels']","['Enhanced mobile broadband', 'error probability', 'massive MIMO', 'mismatched decoding', 'network availability', 'non-orthogonal multiple access', 'puncturing', 'saddlepoint approximation', 'spectral efficiency', 'superposition coding', 'ultra-reliable low-latency communications']"
"The quality of service (QoS) and quality of experience (QoE) metrics are vital performance indicators in cellular systems. This article presents an analytical study on indoor non-orthogonal multiple access (NOMA) systems empowered by intelligent reflecting surfaces (IRS) in generalizedκ−μfading channels. In particular, we consider the network’s QoS through various performance metrics such as outage probability (OP), ergodic capacity (EC), average bit error rate (BER), and system throughput, for which we derive exact and asymptotic closed-form expressions. Furthermore, in contrast to measuring the conventional QoE of a user, we propose to apply the mean score opinion (MOS) factor, in which the objective technical criterion is mutated into a subjective user-recognized quality. To quantify this, a MOS-based QoE evaluation model is applied to an interactive Web browsing service. The results demonstrate the impact of IRS on the QoS and QoE of the indoor NOMA users, as well as the effect of manipulating the channel fading components, power allocation coefficients, and path loss factors. It will be shown that careful manipulation of such parameters, coupled with the use of IRS, produces spectacular improvements in system performance. Monte Carlo simulations are performed to corroborate the accuracy of the derived analytical results.","['NOMA', 'Quality of service', 'Quality of experience', 'Fading channels', 'Throughput', 'Resource management', 'Symbols']","['Non-orthogonal multiple access (NOMA)', 'quality of service (QoS)', 'quality of experience (QoE)', 'and intelligent reflecting surface (IRS)']"
"In this paper, we characterize the combined channel state information (CSI) and re-transmission feedback on wireless communication. Feedback, often assumed free, has traditionally been used to either learn the channel state, or to request the re-transmission of a failed reception. We propose here a generalized framework for feedback that captures the channel time-variation. We consider two-way communication model where all the resources needed for training, feedback (CSI and automatic repeat request), and data. In order to maximize the throughput and optimize resource allocations, we provide the expressions of outage probabilities for different rate/power adaptation and re-transmission protocols. The simulation results show that, besides signal-to-noise ratio (SNR) and Doppler, the optimal pilot overhead depends on whether we are using rate/power adaptation and the number of re-transmissions. We also prove that in slow-fading channels with high SNR regimes, varying the rate provides the best throughput. When dealing with fast time-varying channels and low SNR regimes, using constant power and rate gives the best throughput performance.","['Receivers', 'Protocols', 'Throughput', 'Training', 'Channel estimation', 'Transmitters', 'Wireless communication']","['Feedback', 'training', 'ARQ', 'HARQ', 'CSI', 're-transmission', 'two-way communication', 'throughput', 'optimization']"
"In this paper, we study resource allocation algorithm design for multi-user orthogonal frequency division multiple access (OFDMA) ultra-reliable low latency communication (URLLC) in mobile edge computing (MEC) systems. To meet the stringent end-to-end delay and reliability requirements of URLLC MEC systems, we employ joint uplink-downlink resource allocation and finite blocklength transmission. Furthermore, we propose a partial time overlap between the uplink and downlink frames to minimize the end-to-end delay, which introduces a new time causality constraint. The proposed resource allocation algorithm is formulated as an optimization problem for minimization of the total weighted power consumption of the network under a constraint on the number of URLLC user bits computed within the maximum allowable computation time, i.e., the end-to-end delay of a computation task of each user. Despite the non-convexity and the complicated structure of the formulated optimization problem, we develop a globally optimal solution using a branch-and-bound approach based on discrete monotonic optimization theory. The branch-and-bound algorithm minimizes an upper bound on the total power consumption until convergence to the globally optimal value. Furthermore, to strike a balance between computational complexity and performance, we propose two efficient suboptimal algorithms. For the first suboptimal scheme, the optimization problem is reformulated in the canonical form of difference of convex programming. Then, successive convex approximation (SCA) is used to determine a locally optimal solution. For the second suboptimal scheme, we use a high signal-to-noise ratio approximation for the channel dispersion. Then, via novel transformations, we convert the non-convex quality-of-service constraints of the original problem into equivalent second-order-cone constraints. Our simulation results reveal that the proposed resource allocation algorithm design facilitates URLLC in MEC systems, and yields significant power savings compared to three baseline schemes. Moreover, our simulation results show that the proposed suboptimal algorithms offer different trade-offs between performance and complexity and attain an excellent performance at comparatively low complexity.","['Resource management', 'Ultra reliable low latency communication', 'Task analysis', 'Downlink', 'Delays', 'Uplink', 'Optimization']","['Branch-and-bound', 'finite blocklength transmission', 'mobile edge computing', 'successive convex approximation', 'ultra-reliable low latency communication']"
"Owing to various applications in the field of wireless communication systems, in this paper, using vector-based interior-point algorithm, we propose a generic methodology which optimizes simple exponential based approximations of the Gaussian Q function (GQF) yielding extremely accurate optimized approximations. We optimize the relative error (RE) which is considered as one of the key metrics used to evaluate the performance of these approximations. Precisely, we target the points of local~maximas where the RE is high, defining a new set of optimized coefficients yielding reduced RE at these points of concern. We also optimize the points of local~minimas ; however at these points the percentage reduction in RE is not that significant. We then compute the harmonic mean of all these optimal coefficients which makes the originally proposed bounds much tighter, for the entire performance range of the GQF. We further illustrate the tightness of the optimized approximation by facilitating the accurate computation of the error performance metrics like symbol error probability of various coherent digital modulation schemes like square quadrature amplitude modulation (SQAM), rectangular-QAM, cross-QAM and hexagonal-QAM over the versatile \kappa -\mu shadowed fading channel. The analysis is also validated with the help of Monte-Carlo simulations.","['Fading channels', 'Digital modulation', 'Performance analysis', 'MIMO communication', 'Cost function', 'Approximation algorithms', 'Symbols']","['Optimization', 'vector-based interior-point algorithm', 'Gaussian Q function', 'κ–μ shadowed fading channel', 'digital modulation techniques', 'wireless communication systems', 'approximate computing']"
"Loading methods for wireline and wireless transmission systems are explained and reviewed to allow examination of fundamental equivalences between approaches to optimum transmission with linear cross-dimensional interference. Shannon’s famous water-filling method is then re-interpreted by examining the difference between mutual information and the constellation size used on each tone. Results reinforce that highest performance in all methods retains use of Shannon’s water-filling. However, results also find that a constant-size constellation on all energized water-filling dimensions, when used with a good code and an appropriate joint maximum-likelihood decoder, is sufficient to approximate this highest performance. This means that tone-dependent constellation variation is not necessary, which increases receiver complexity but reduces the need for channel-state feedback. This then leads to some adaptive MCS (modulation and coding scheme) equivalent loading methods that can be applied generally to wireline and wireless transmission.","['Adaptive modulation', 'Codes', 'OFDM', 'Load management']","['Adaptive modulation', 'coded modulation', 'discrete multitone (DMT)', 'modulation and coding scheme (MCS)', 'orthogonal frequency-division multiplexing (OFDM)']"
"With the recent explosive growth in online classes and virtual meetings, real-time video communication has quickly become essential to everyday life. Despite its widespread deployment, our investigation revealed that current protocols, ranging from industry standards such as WebRTC to state-of-the-art research such as Salsify, frequently perform sub-optimally in the presence of competing flows at the same bottleneck. For example, WebRTC's throughput can degrade from 73% to a mere 8% of the available bandwidth when competing with just two TCP flows. We tackle this problem in this work by introducing a novel PIBES protocol for real-time video applications to operate in the presence of competing TCP traffic. PIBES employs a new inband bandwidth estimation method that can quickly and accurately measure the bottleneck link bandwidth even with competing flows. Moreover, PIBES can detect the absence or presence of competing flows, which enables it to maximize video quality when there is no competing flow and to maintain acceptable video quality while sharing bandwidth with competing flows. Experiments demonstrate that PIBES achieves throughput and delay comparable to the state-of-art protocols, but outperforms them significantly in the presence of competing TCP flows.","['Streaming media', 'Bandwidth', 'Protocols', 'Delays', 'Estimation', 'WebRTC']","['Real-time', 'video communication', 'congestion control', 'bitrate control', 'bandwidth estimation']"
"Energy efficient array processing is critical to implement feasible solutions for directional communication in a mmWave channel. MmWave channels are highly susceptible to blockage and require frequent angle of arrival (AoA) estimation. An AoA estimation solutions with a fully digital architecture offers a low latency, high performance and flexible solution suitable for the stringent requirements of 5 G. However, the large number of high speed converters in a digital receiver are the dominant power consuming elements. Alternative analog or hybrid architectures use fewer high speed converters, but require sweeping measurements to estimate AoAs over the angular space, and thus adds latency to the estimation process. In this paper, we present a variable rate sub-Nyquist decoupling solution that leverages pilot design. The pilot's subsequence properties allow decoupling the source waveforms at fractions of the Nyquist rate. We leverage this concept to scale power consumption by the converters. We preprocess the received signals at the antenna array with the variable rate sub-Nyquist decoupling algorithm and use a few well known digital estimators for AoA estimation including DEML, KR-MUSIC based two level nested array and coprime filter bank. In addition to scalable power consumption, our research indicates some other benefits of the decoupling, including reduced complexity algorithm implementation and improved performance estimation for the non maximum likelihood estimators.","['Estimation', 'Power demand', 'Channel estimation', 'Wireless communication', 'Radio frequency', 'Indexes', 'Array signal processing']","['AoA estimation', 'digital receiver', 'mmWave', 'pilot design', 'scalable power']"
"The coexistence of diverse services with heterogeneous requirements is a fundamental feature of 5G. This necessitates efficient Radio Access Network (RAN) slicing , defined as sharing of the wireless resources among diverse services while guaranteeing their throughput, timing, and/or reliability requirements. In this paper, we investigate RAN slicing for an uplink scenario in the form of multiple access schemes for two user types: (1) broadband users with throughput requirements and (2) intermittently active users with timing requirements, expressed as either Latency-Reliability (LR) or Peak Age of Information (PAoI). Broadband users transmit data continuously, hence, are allocated non-overlapping parts of the spectrum. We evaluate the trade-offs between the achievable throughput of a broadband user and the timing requirements of an intermittent user under Orthogonal Multiple Access (OMA) and Non-Orthogonal Multiple Access (NOMA), considering capture. Our analysis shows that NOMA, in combination with packet-level coding, is a superior strategy in most cases for both LR and PAoI, achieving a similar LR with only a slight 2% decrease in throughput with respect to the case where an independent channel is allocated to each user. The latter solution leads to the upper bound in performance but requires double the amount of resources than the considered OMA and NOMA schemes. However, there are extreme cases where OMA achieves a slightly greater throughput than NOMA at the expense of an increased PAoI.","['NOMA', 'Broadband communication', 'Throughput', 'Time division multiple access', 'Timing', 'Uplink', 'Frequency division multiaccess']","['Age of information (AoI)', 'heterogeneous services', 'non-orthogonal multiple access (NOMA)', 'reliability', 'slicing']"
"This study explores the feasibility of allocating finite resources beyond fifth generation networks for extended reality applications through the implementation of enhanced security measures via offloading analysis (RLIS). The quantification of resources is facilitated through the utilization of parameters, namely energy, capacity, and power, which are equipped with proximity constraints. These constraints are then integrated with activation functions in both multilayer perceptron and long short term memory models. Furthermore, the system model has been developed using vision-based computing, which involves managing data queues in terms of waiting periods to minimize congestion for data transmission with limited resources. The major significance of the proposed method is to utilize allocated spectrums for future generation networks by allocating necessary resources and therefore high usage of resources by all users can be avoided. In addition the advantage of the proposed method is secure the networks that operate beyond 5G where more number of users will try to share the allocated resources that needs to be provided with high security conditions.","['Resource management', 'Extended reality', 'Communication networks', 'Security', 'Bandwidth', 'Task analysis', 'Real-time systems']","['Deep learning algorithm', 'extended reality applications', 'fifth generation networks', 'limited resource', 'visual systems']"
"Rate-Splitting Multiple Access (RSMA) has recently found favor in high-mobility scenarios due to the benefits of relaxing the accuracy of Channel State Information at the Transmitter (CSIT) while maintaining high spectral efficiency. These benefits are particularly important in Satellite-Vehicular Networks (SVNs), where the mobility of the vehicles significantly affects the estimation accuracy of the CSIT. To tackle this challenge, we propose an RSMA-based satellite-vehicular communication system for reliable data transmission. Specifically, we investigate the outage probability of the RSMA-based satellite-vehicular communication system under the Shadowed-Rician model. Considering the satellite-vehicular communication outage problem, an optimization problem is formulated to maximize the Weighted Sum Rate (WSR) of the communication system. However, due to communication overhead and privacy concerns, not all vehicles are willing to participate in interference management. In this study, we exploit an incentive mechanism to efficiently solve this problem. Specifically, we formulated a non-cooperative game to motivate users to report the CSIT and participate in the power allocation. In addition, we have proved the existence and uniqueness of Nash Equilibrium (NE) in the game formulated. Our simulation results show that the proposed non-cooperative game theoretical approach achieves the effectiveness of the RSMA-based satellite-vehicular system. The game theoretical framework is shown to motivate users to participate in interference management, so as to maximize their transmission rates.","['Satellites', 'Games', 'Communication systems', 'Wireless communication', 'Low earth orbit satellites', 'Interference', 'Optimization']","['Nash Equilibrium (NE)', 'Non-cooperative Game', 'Non-Terrestrial Networks (NTNs)', 'Rate-Splitting Multiple Access (RSMA)']"
"The ability of reconfigurable intelligent surfaces (RIS) to produce complex radiation patterns in the far-field is determined by various factors, such as the unit cell’s design, spatial arrangement, tuning mechanism, the communication and control circuitry’s complexity, and the illuminating source’s type (point/planewave). Research on RIS has been mainly focused on two areas: first, the optimization and design of unit cells to achieve desired electromagnetic responses within a specific frequency band, and second, exploring the applications of RIS in various settings, including system-level performance analysis. The former does not assume any specific full radiation pattern on the surface level, while the latter does not consider any particular unit cell design. Both approaches largely ignore the complexity and power requirements of the RIS control circuitry. As we progress toward the fabrication and use of RIS in real-world settings, it is becoming increasingly necessary to consider the interplay between the unit cell design, the required surface-level radiation patterns, the control circuit’s complexity, and the power requirements concurrently. In this paper, we propose a benchmarking framework comprising a set of simple and complex radiation patterns. Using full-wave simulations, we compare the relative performance of various RISs made from unit cell designs that use PIN diodes as control elements in producing the full radiation patterns in the far-field of the RIS under point/planewave source assumptions. We also analyze the control circuit complexity and power requirements and explore the tradeoffs of various designs.","['Antenna radiation patterns', 'Receivers', 'Wireless communication', 'PIN photodiodes', 'Complexity theory', 'Benchmark testing', 'Radio transmitters']","['6G', 'RIS', 'unit cell']"
"A promising solution for massive Multiple-Input Multiple-Output (m-MIMO) systems is Hybrid digital-analogue (HDA) beamforming as it offers a balanced trade-off between energy efficiency (EE) and spectral efficiency (SE). The lack of practical demonstrations in the open literature is primarily because those most existing works require a large number of phase shifters (PSs), radio frequency (RF) switches, power dividers (PDs), and power combiners which contribute to high hardware complexity and energy consumption (due to insertion loss). In this paper, we introduce a practical dynamic subarray m-MIMO structure that is based on reconfigurable power dividers (RPDs). The extensive system simulation results, considering hardware imperfection extracted from a practical RPD implementation, indicate that the proposed RPD-based dynamic HDA m-MIMO outperforms the fixed subarray counterpart.","['Radio frequency', 'Array signal processing', 'Antennas', 'Hardware', 'Baseband', 'Power dividers', 'MIMO communication']","['Hybrid digital-analog beamforming', 'dynamic subarray', 'reconfigurable power dividers']"
"The combination of radio-frequency (RF) communication and underwater optical wireless communication (UOWC) plays a vital role in the underwater Internet of Things (UIoT). This correspondence proposes a dual-hop hybrid satellite underwater system that exploits non-orthogonal multiple access (NOMA) as a spectrum-efficient access technique. The RF link from the satellite to the relay on an oil platform is presumptively subject to a Shadowed-Rician (SR) fading, while the UOWC channels from the relay to the underwater destinations are suggested to follow Exponential-Generalized Gamma (EGG) distributions. The reliability of the system is characterized in terms of both underwater destinations and system outage probabilities (OPs). We derive new closed-form expressions for the OPs under imperfect successive interference cancellation (SIC) conditions. Furthermore, the asymptotic OP and the diversity order (DO) are obtained to learn more about the system’s performance. The results are verified through an extensive representative Monte-Carlo simulation. Also, we investigate the performance against the turbulence of the salty water, air bubbles level (BL), temperature gradients (TG), shadowing parameters, and satellite pointing errors due to satellite motion, even if the beam is pointed at the center of the directive antenna relay, the beam will randomly oscillate. Finally, we contrast our approach with the conventional orthogonal multiple access (OMA) scheme to demonstrate its superiority.","['Radio frequency', 'NOMA', 'Satellites', 'Relays', 'Signal to noise ratio', 'Interference cancellation', 'Satellite broadcasting']","['Satellite', 'underwater optical wireless communication', 'non-orthogonal multiple access', 'outage probability']"
"Mobile Edge Computing (MEC) in 5G networks has emerged as a promising technology to enable efficient and low-latency services for mobile users. In this paper, we present a novel synthetic data generation approach tailored for evaluating MEC in 5G networks. Our methodology incorporates resource-efficient techniques to generate realistic synthetic datasets that capture the spatio-temporal patterns of mobile traffic and user behavior. By leveraging advanced modeling techniques, including multi-head attention and bidirectional LSTM, we accurately model the complex dependencies in the data while optimizing computational resources. The proposed synthetic data generator enables the creation of diverse datasets that closely resemble real-world scenarios, facilitating the evaluation of MEC performance and optimizing resource utilization. Through extensive experiments and evaluations, we demonstrate the effectiveness of our approach in enabling accurate assessments of MEC in 5G networks. Our work contributes to the field by providing a robust methodology for synthetic data generation specifically tailored for MEC evaluation, addressing the need for resource-efficient evaluation frameworks in the context of emerging technologies. The results of our study provide valuable insights for the design and optimization of MEC systems in real-world deployments.","['Synthetic data', 'Generators', '5G mobile communication', 'Data models', 'Behavioral sciences', 'Computational modeling', 'Performance evaluation']","['Generative adversarial network', '5G', 'mobile edge computing', 'synthetic data generation', 'resource efficiency', 'performance evaluation']"
"This paper presents a novel approach to analyse the traffic efficiency of infrastructureless networks. Wireless communication is rapidly moving towards more infrastructureless routing schemes, where the devices encounter frequent topology changes and communicate with each other anonymously and sporadically. Nodes do not have the opportunity to choose an optimum routing path, they do rather obtain a valid routing path on the fly during the data exchange by cooperating with the nearest neighbours. Packets emanated from an infrastructureless node are disseminated through each node found in the vicinity. This produces a vast number of packet duplicates within the entire communication environment, causing the problem of increased congestion, increased packet delay, more packet drops, and undesirable energy consumption compared to the infrastructured networks. Data exchange and packet routing operations in large networks are highly stochastic. Thus, the performance analysis of routing operations is modeled as a stochastic queueing system, where the input and output traffics and throughput operations of routing systems are precisely formulated in the form of stochastic processes. A hypothetical routing algorithm is also defined here as a reference model in order to perform a comparative analysis of the infrastructureless routing scheme.","['Routing', 'Wireless sensor networks', 'Ad hoc networks', 'Routing protocols', 'Wireless communication', 'Analytical models', 'Heuristic algorithms']","['Ad-hoc routing', 'collaborative routing', 'energy sustainability', 'infrastructureless networks', 'routing optimization']"
"The limited transmitter-to-receiver stop-band isolation of the duplexers in long term evolution (LTE) and 5G/NR frequency division duplex transceivers induces leakage signals from the transmitter(s) (Tx) into the receiver(s) (Rx). These leakage signals are the root cause of a multitude of self-interference (SI) problems in the receiver path(s) diminishing a receiver’s sensitivity. Traditionally, these effects are counteracted by the use of various different SI cancellation (SIC) architectures which typically solely target one specific problem. In this paper, we propose two novel neural networks based architectures that can handle a variety of different SI effects without the need for a different architecture for each effect. We additionally show the suitability of the proposed architecture on SI effects occurring in in-band full duplex transceivers. Further, we introduce two novel low-cost training algorithms to enable online adaptation (as opposed to offline training currently proposed in literature). The combination of these two concepts is shown to not only beat existing algorithms in their cancellation performance, but also to provide sufficiently low computational complexity allowing on-chip implementations.","['Radio frequency', 'Transceivers', 'Interference cancellation', 'Computer architecture', 'Mixers', 'Harmonic analysis', 'Training']","['4G', 'machine learning', 'online learning', 'self-interference cancellation']"
"6G and beyond networks tend towards fully intelligent and adaptive design in order to provide better operational agility in maintaining universal wireless access and supporting a wide range of services and use cases while dealing with network complexity efficiently. Such enhanced network agility will require developing a self-evolving capability in designing both the network architecture and resource management to intelligently utilize resources, reduce operational costs, and achieve the coveted quality of service (QoS). To enable this capability, the necessity of considering an integrated vertical heterogeneous network (VHetNet) architecture appears to be inevitable due to its high inherent agility. Moreover, employing an intelligent framework is another crucial requirement for self-evolving networks to deal with real-time network optimization problems. Hence, in this work, to provide a better insight into network architecture design in support of self-evolving networks, we highlight the merits of integrated VHetNet architecture while proposing an intelligent framework for self-evolving integrated vertical heterogeneous networks (SEI-VHetNets). The impact of the challenges associated with SEI-VHetNet architecture, on network management is also studied considering a generalized network model. Furthermore, the current literature on network management of integrated VHetNets along with the recent advancements in artificial intelligence (AI)/machine learning (ML) solutions are discussed. Accordingly, the core challenges of integrating AI/ML in SEI-VHetNets are identified. Finally, the potential future research directions for advancing the autonomous and self-evolving capabilities of SEI-VHetNets are discussed.","['Computer architecture', 'Autonomous aerial vehicles', 'Optimization', 'Adaptive systems', 'Resource management', 'Real-time systems', 'Heterogeneous networks']","['SEI-VHetNet', 'network management', 'optimization problems', 'AI/ML solutions']"
"The conversion efficiency of a harvesting component can hinder device self-sustainability. Countering this inefficiency can be a rigorous task that requires either a modified input waveform at the harvesting component or an improved design of the harvesting component itself. This paper considers a selective orthogonal frequency division multiplexing (OFDM) system that is designed for simultaneous wireless information and power transfer (SWIPT). This paper introduces an enhanced transmitter architecture designed to condition the transmit signal for improved conversion efficiency. Leveraging the structure of the OFDM signal, the cyclic prefix plus a portion of the information signal are conditioned and used for harvesting. Then, a rectifier model and prototypes are designed to further improve conversion efficiency. System performance is evaluated with the analytical derivation of the rate-energy (R-E) tradeoff and an optimized transmission strategy for system self-sustainability. Monte Carlo simulations show increased energy gains and full self-sustainability with the introduction of signal conditioning.","['Wireless communication', 'Monte Carlo methods', 'Transmitters', 'OFDM', 'System performance', 'Rectifiers', 'Prototypes']","['Conversion efficiency', 'orthogonal frequency-division multiplexing', 'Schottky diode', 'selective transmission', 'self-sustainability', 'signal conditioning', 'simultaneous wireless information and power transfer']"
"We present a cooperative, dual-stage (DS) non-orthogonal multiple access (NOMA) scheme for power line communication (PLC) systems, where three PLC modems are served in two time slots. The network consists of a source (S) modem, two relay (R) modems and a destination (D) modem. In each of the two time slots, S communicates with one R using NOMA, and the other R communicates with D using the decode-and-forward technique. We consider both scenarios where a direct link may or may not exist between S and D. We derive expressions for the approximate average sum rate and overall outage probability of the network. Next, we formulate an optimization problem to find the optimal power allocation coefficients at S, such that the sum rate is maximized. We establish the accuracy of our approximations using Monte Carlo simulations. Furthermore, we show that our scheme outperforms the two-user single-stage and the DS schemes, and the three-user NOMA scheme proposed in the earlier literature, in terms of sum rate and outage probability.","['NOMA', 'Modems', 'Power system reliability', 'Receivers', 'Symbols', 'Relays', 'Probability density function']","['Cooperative power line communication', 'dual-stage', 'non-orthogonal multiple access', 'outage probability', 'sum rate']"
"Applications such as virtual reality and online gaming require low delays for acceptable user experience. A key task for over-the-top (OTT) service providers who provide these applications is sending traffic through the networks to minimize delays. OTT traffic is typically generated from multiple data centers which are multi-homed to several network ingresses. However, information about the path characteristics of the underlying network from the ingresses to destinations is not explicitly available to OTT services. These can only be inferred from external probing. In this paper, we combine network tomography with machine learning to minimize delays. We consider this problem in a general setting where traffic sources can choose a set of ingresses through which their traffic enter a black box network. The problem in this setting can be viewed as a reinforcement learning problem with strict linear constraints on a continuous action space. Key technical challenges to solving this problem include the high dimensionality of the problem and handling constraints that are intrinsic to networks. Evaluation results show that our methods achieve up to 60% delay reductions in comparison to standard heuristics. Moreover, the methods we develop can be used in a centralized manner or in a distributed manner by multiple independent agents.","['Delays', 'Tomography', 'Routing', 'Reinforcement learning', 'Probes', 'Data centers', 'Neural networks']","['Load distribution', 'segment routing', 'reinforcement learning', 'machine learning']"
"The definition of the fundamental concepts and the design of the architectural framework for network slicing in fifth-generation communication systems have been successfully concluded; the standardization activities are almost over; and the commercial deployment has already commenced worldwide. To compete for digital supremacy and to be seen as front-runners in the international technological race, researchers from various regions and countries have begun exploring the technical requirements, envisioning potential applications, identifying innovative enablers, developing testbeds for the preliminary validation of several terrestrial and non-terrestrial technologies, and conceptualizing the architectural design for the next generation of mobile communication systems – the sixth-generation (6G) – aiming to connect the human, physical, and digital worlds with a high level of intelligence and openness for the 2030s. In support of such an ambitious vision, this article extends the end-to-end network slicing concepts, methods, solutions, and functioning architectures towards 6G. To this intent, the study first presents several decisive motivating trends behind such an extension of network slicing in order to make forthcoming mobile networks fully slicing-aware. Following that, the paper attempts to highlight the intelligentization of a number of key enabling technologies that will bring a renaissance to network slicing in the next decade. It then proposes a unified architectural framework and its principal building blocks in several layers, paving the way for the implementation of an open and intelligent network and network slicing in 6G. The proposed architectural solution harmonizes the most recent specifications of the relevant de jure and de facto standards development organizations in their applicable layers with the aim of architecting a pre-standard-compliant and preliminary framework for slicing the 6G network. Finally, the article is intended to spur interest and lay the groundwork for further investigations and subsequent research and development by highlighting a number of open research challenges and directions in this flourishing field.","['6G mobile communication', 'Network slicing', 'Computer architecture', 'Automation', 'Optimization', 'Industries', 'Next generation networking']","['6G', 'automation', 'cloudification', 'end-to-end architecture', 'intelligentization', 'intelligent networks', 'intelligent network slice', 'management and orchestration', 'mobile communication systems', 'network architecture', 'network exposure', 'network slicing', 'network slice', 'openness', 'open network', 'open network slice', 'softwarization', 'standardization', 'standards', 'virtualization']"
"In this paper we demonstrate the application of Fully Convolutional Neural Network (FCN) for Frame Synchronization (FS) in bursty single carrier transmissions, commonly used in wireless sensor networks and Internet of Things (IoT) applications. Our approach shows greatly improved performance compared to noncoherent correlation-based methods under carrier phase and frequency offsets, especially for shorter preambles. Using a fully convolutional architecture allows the training of a deep filter, which we believe is more suited to signal processing tasks than more commonly used deep learning architectures with fully connected layers. In terms of deployment within a wider communications system, it could be treated similarly to a typical signal processing filter, which means it can be deployed to inputs of arbitrary length. Additionally, because the proposed model is composed only of convolutional layers, the entire model benefits from the weight sharing property of convolutional filters, and results in a greatly reduced memory footprint compared to that of similar models containing fully connected layers.","['Convolutional neural networks', 'Correlation', 'Synchronization', 'Training', 'Symbols', 'Wireless communication', 'Receivers']","['Deep learning', 'fully convolutional neural network', 'frame synchronization', 'Internet of Things']"
"Content caching at base stations is an effective solution to cope with the unprecedented data traffic growth by prefetching contents near to end-users. To proactively servicing users, it is of high importance to extract predictive information from data requests. In this paper, we propose an accurate content request prediction algorithm for improving the performance of edge caching systems. In particular, we develop a Bayesian dynamical model through which a complex nonlinear latent temporal trend structure in the content requests can be accurately tracked and predicted. The dynamical model also leverages tensor train decomposition to capture content-location interactions to further enhance the accuracy of predictions. To infer the model’s parameters, we derive an approximation of the posterior distribution based on variational Bayes (VB) method with an embedded Kalman smoother algorithm. Based on the predictions of the proposed model, we design a cost-efficient proactive cooperative caching policy which adaptively utilizes network resources and optimizes the content delivery. The advantage of the proposed caching scheme is demonstrated via numerical results using two real-world datasets, which show that the developed Bayesian dynamical model substantially outperforms reference methods that ignore the temporal trends and content-location interactions.","['Market research', 'Bayes methods', 'Predictive models', 'Tensors', 'Prediction algorithms', 'Numerical models', 'Heuristic algorithms']","['Proactive caching', 'content request prediction', 'tensor train decomposition', 'Bayesian modeling', 'temporal trend']"
"The Long Range (LoRa) modulation enables low-cost and low-power communications, serving as the foundation for the widely adopted terrestrial low-power wide-area network (LPWAN) technology known as LoRaWAN. Owing to its effectiveness, this modulation scheme is emerging as a potential option to provide direct-to-satellite (DtS) connectivity supporting Internet-of-Things (IoT) applications in remote or hard-to-reach areas, and complementing existing terrestrial networks. Besides the link budget and interference, the Doppler effect is one of the main challenges in LoRa DtS connectivity. Earlier studies have extensively investigated the link budget and the network scalability aspects, confirming the feasibility of integrating LoRa with Low Earth Orbit (LEO) satellites. However, only a few studies examine the influence of the Doppler effect on LoRa DtS performance. Specifically, the majority of the available literature report empirical studies that analyze the Doppler effect solely for a specific set of communication parameters. There remains a need for extensive and comprehensive examination of LoRa DtS performance under a strong Doppler effect in the LEO scenario. In this paper, we discuss and thoroughly investigate the impact of the Doppler effect on the reliability of LoRa satellite links. In particular, we analytically study packet losses, distinguishing the effect of Doppler shift from Doppler rate, the latter being caused by the variation in the relative speed of LEO satellites with respect to a terrestrial IoT end-device. Our analysis accounts for the effects of key communications parameters and settings, such as bandwidths, carrier frequency, MAC payload, LEO satellite’s orbital height, and LoRaWAN low data rate optimization (LDRO). Notably, the results identify the LoRa boundaries for direct to LEO satellite connectivity and can facilitate the selection of suitable parameters for future system designs. Specifically, our results demonstrate that the packet delivery ratio of the most vulnerable spreading factor, i.e., SF12, exceeds 82% when using 125 kHz bandwidth, 433 MHz carrier frequency, and 59 bytes payload for a satellite orbiting at 560 km height.","['Low earth orbit satellites', 'Satellites', 'Orbits', 'Satellite broadcasting', 'Modulation', 'Doppler shift', 'Internet of Things']","['Doppler effect', 'direct-to-satellite', 'LoRa', 'LoRaWAN', 'LPWAN', 'IoT', 'LEO', 'LDRO', 'orbit', 'satellite']"
"In a mobile millimeter wave (mmWave) communication system, blockages cause disconnections or serious degradation of communications. Several techniques have been proposed to control radio links between multiple base stations based on blockage prediction using camera images to avoid these problems. However, blockage prediction requires continuously determining the position of user equipment (UE) with decimeter precision, which is difficult when sensors and resources on the UE side are not available, and there are many moving objects around the UE. To resolve this problem, we propose a UE tracking method that uses a received signal strength indicator (RSSI) and RGB-D camera images from the base station. The proposed method consists of a combination of visual tracking and reidentification of the UE using radio information and camera images. We use the synchronization of the RSSI variation with the occlusion on the image by movements of the UE and objects for reidentification. We evaluated the proposed method experimentally in an outdoor environment by simulating a communication area formed by mmWave band base stations. The proposed method achieved an 11.5% improvement in tracking accuracy compared with conventional visual tracking.","['Millimeter wave communication', 'Radar tracking', 'Cameras', 'Visualization', 'Array signal processing', 'Sensors', '5G mobile communication']","['Image analysis', 'millimeter wave communication', 'object tracking', 'received signal strength indicator']"
"This paper presents a 3 dimensional (3D)-hierarchical modulation technique for optical wireless communication (OWC) systems in which high, medium and low priority data are encoded on the frequency, phase and amplitude of a subcarrier signal, respectively. The error performance of the 3D-hierarchical modulation is derived and evaluated under different channel conditions. A proof-of-concept experiment is then presented to demonstrate the feasibility of this approach. The results presented in this study show that encoding high and medium priority data on the frequency and phase of a subcarrier signal offers resilience against the deleterious effects of the OWC channel. The amplitude dimension of the subcarrier signal can then be utilised to further increase the system spectral efficiency if the channel conditions allow for it.",[],[]
"Advanced Indoor Positioning Systems (IPS) based on Received Signal Strength (RSS) fingerprints have been paramount in 6G network research and commercial exploitation due to their cost-effectiveness and simplicity. Despite their popularity, the advent of 6G has prompted a shift towards exploring Deep Learning algorithms to further enhance their performance and precision. Deep Learning research typically demands large datasets, leading to reliance on data augmentation and crowdsourcing techniques for data collection. However, the traditional centralization of data in crowdsourcing poses privacy risks, and here is where Federated Learning (FL) comes into play. In light of this, our study introduces FL to bridge this divide in a decentralized way, eliminating the need for servers to acquire labeled data directly from users. This approach aims to minimize localization error in RSS fingerprints, preserve user privacy, and reduce system latency, all key goals for 6G networks. Moreover, we explore the use of power transmission techniques to further decrease the latency in the FL system. Our simulation outcomes confirm the superiority of FL over traditional Stochastic Gradient Descent (SGD) methods considering critical evaluation metrics like localization error and global loss, paving the way for efficient 6G implementation.",[],[]
"Both massive multiple-input multiple-output (MIMO) and millimeter wave (mmWave) systems are receiving much attention for 5G and beyond-5G wireless access. When using a large antenna array, there are numerous challenges in designing low-power and cost-effective hardware. Most notably, the traditional approach of using high-resolution analog-to-digital converters (ADCs) at each element leads to both costly and high-power consumption devices. This has motivated research on multiple antenna arrays that use a low-resolution ADC at each element. We present a new framework for designing a multiple antenna array using a low-resolution ADC at each element, with the low-resolution ADC composed of simple shift and modulo analog processing and a sign quantizer. This modulo technique, called folding, has previously been utilized in the design of high-speed high-resolution ADCs. Using theory from folded ADCs, we show how to jointly design the modulo processing used across the elements. We show that our framework can provide substantial rate improvements including providing a linear growth in rate with the number of antennas at high SNR, as opposed to the logarithmic growth with the number of antennas provided by a sign-based low-resolution ADC array. We also explore how antenna ordering strategies can be used. We utilize Monte Carlo simulations to back our analysis.",[],[]
"This paper presents a geometry-based three-dimensional Probability of Line of Sight (PLoS) model for generic urban environments. The existing PLoS models are mostly empirical, based on measurements or ray-tracing simulations at specific environments. To enable Unmanned Aerial Vehicle (UAV) communication in various environments, it is urgently necessary to develop an analytical model with high parameter scalability. Through introducing the ITU model that describes a built-up environment with three parameters, we derive a closed-form PLoS model with the inputs of city built-up parameters, UAV altitude, elevation θ and azimuth φ angles between UAV and User Equipment (UE). Furthermore, we develop a geometry-based simulator to validate the proposed analytical model. The results show good agreements between the simulated results, analytical results, and 3GPP PLoS measurements. We demonstrate that the existing models provide less accurate results, which calls for re-evaluating the use cases and performance of UAV-mounted communication equipment in urban environments.",[],[]
"Communications are realized as a result of successive decisions at the physical layer, from modulation selection to multi-antenna strategy, and each decision affects the performance of the communication systems. Future communication systems must include extensive capabilities as they will encompass a wide variety of devices and applications. Conventional physical layer decision mechanisms may not meet these requirements, as they are often based on impractical and oversimplifying assumptions that result in a trade-off between complexity and efficiency. By leveraging past experiences, learning-driven designs are promising solutions to present a resilient decision mechanism and enable rapid response even under exceptional circumstances. The corresponding design solutions should evolve following the lines of learning-driven paradigms that offer more autonomy and robustness. This evolution must take place by considering the facts of real-world systems and without restraining assumptions. In this paper, the common assumptions in the physical layer are presented to highlight their discrepancies with practical systems. As a solution, learning algorithms are examined by considering the implementation steps and challenges. Furthermore, these issues are discussed through a real-time case study using software-defined radio nodes to demonstrate the potential performance improvement. A cyber-physical framework is presented to incorporate future remedies.",[],[]
"A major bottleneck in distributed learning is the communication overhead of exchanging intermediate model update parameters between the worker nodes and the parameter server. Recently, it is found that local gradients among different worker nodes are correlated. Therefore, distributed source coding (DSC) can be applied to increase communication efficiency by exploiting such correlation. However, it is highly non-trivial to exploite the gradient correlations in distributed learning due to the unknown and time-varying gradient correlation. In this paper, we first propose a DSC framework, named successive Wyner-Ziv coding, for distributed learning based on quantization and Slepian-Wolf (SW) coding. We prove that the proposed framework can achieve the theoretically minimum communication cost from an information theory perspective. We also propose a low-complexity and adaptive DSC for distributed learning, including a gradient statistics estimator, rate controller, and a log-likelihood ratio (LLR) computer. The gradient statistics estimator estimates the gradient statistics online based only on the quantized gradients at previous iterations, hence it does not introduce extra communication cost. The computation complexity of the rate controller and the LLR computer is reduced to a linear growth in the number of worker nodes by introducing a semi-analytical Monte Carlo simulation. Finally, we design a DSC-based distributed learning process and find that the extra delay introduced by DSC does not scale with the number of worker nodes.",[],[]
"This paper proposes a memory-efficient deep neural network (DNN) framework-based symbol level precoding (SLP). We focus on a DNN with realistic finite precision weights and adopt an unsupervised deep learning (DL) based SLP model (SLP-DNet). We apply a stochastic quantization (SQ) technique to obtain its corresponding quantized version called SLP-SQDNet. The proposed scheme offers a scalable performance vs memory trade-off, by quantizing a scalable percentage of the DNN weights, and we explore binary and ternary quantizations. Our results show that while SLP-DNet provides near-optimal performance, its quantized versions through SQ yield∼3.46×and∼2.64×model compression for binary-based and ternary-based SLP-SQDNets, respectively. We also find that our proposals offer∼20×and∼10×computational complexity reductions compared to SLP optimization-based and SLP-DNet, respectively.",[],[]
"In order to satisfy the higher demand raised by 6G, the major part of which refers to further extension of the coverage and achievements of ubiquitously massive connections, the integration of Non-Terrestrial Networks (NTNs) and the evolved version of Narrow Bandwidth Internet of Things (NB-IoT) are introduced as a potential solution. However, one of the main challenges still open is the Random Access (RA) procedure which refers to preamble detection and uplink synchronization. With the large Time of arrival (ToA) and Carrier frequency offset (CFO), traditional methods designed for Terrestrial Networks (TNs) can not provide accurate estimation performance. Some studies adopt Global Navigation Satellite System (GNSS) to pre-compensate the large ToA and CFO but it may not work well for NB-IoT devices in practice. Consequently, a complete system without GNSS is designed for RA in this paper. A system-level method is firstly applied to pre-compensate the CFO. Then we focus on tackling the ToA and residual CFO. A pre-process method for received signal is proposed and its phase series are analyzed. Then we propose a novel random access scheme based on change point detection (CPD) of phase series with machine learning to estimate ToA and CFO. Simulation results demonstrate that the designed overall system is practical in NTNs. Meanwhile, the proposed random access scheme achieves higher estimation accuracy of ToA and CFO compared with existing methods with the same range of ToA and CFO under low signal-noise ratio (SNR) conditions.",[],[]
"A new bound on the error probability of coding with limited code length over additive white Gaussian noise (AWGN) channels is proposed. The developed bound is proved to be universal for two connected encoding ways. On the one hand, we conceive folding the conventional codes, such as Hadamard and binary random ones, in order to adapt shorter code length. On the other hand, we further extend the above folded structure to Gaussian random coding and hence to bound its error probability. Finally, we demonstrate that the bound of the above two constructions can be unified aslog2e2πnC−−−−√2−n(C/2−R)whereCrepresents the capacity of the AWGN channel,nandRstand for the code length and rate, respectively. This theoretical contribution confirms that, in the context of short code length and low rate, the developed two constructions exhibit excellent performance even close to the Shannon bound on AWGN channels.",[],[]
"Long Range Wide Area Network (LoRaWAN) is currently one of the leading communication technologies for the Internet of Things (IoT) connectivity. It offers long-range and wide-area communication at low-power, low cost and low data rate. However, several studies demonstrated that LoRaWAN exhibits excessive collisions and thus performance degradation especially at large scale. This is mainly due to the ALOHA-based channel access technique adopted for uplink communications in LoRaWAN. In this work, we present a new schedule-based schema which allows a deterministic allocation of time, channel and spreading factor, for a collision-free uplink communication in LoRaWAN. For the sake of interoperability, our schema does not involve any additional synchronization phase for end-devices, or major changes in LoRaWAN specification as suggested in most of the existing studies in the literature. The performance evaluation of our proposed scheme proved an outstanding improvement of the network performance in terms of latency and energy-consumption. For instance, for an inter-packet transmission interval equal to 1800s (one packet each 30 min) and a cell size equal to 1000 end-devices, results show that using the proposed schedule-based schema, the uplink communication latency and energy consumption are reduced respectively by 89% and 78%, compared to the original LoRaWAN legacy class A.",[],[]
"The essential characteristics of blockchain, such as immutability and auditability, are leading to its increasing incorporation into society. The Internet of Things (IoT) aims to create a decentralized framework for IoT and enhance security. The processing of transactions for different IoT applications poses challenges for the convergence of IoT and blockchain due to varying performance requirements. In addition, it is important to note that the dispersed IoT system’s membership may change when an IoT device adds or departs from it. As the system becomes dynamic, it presents new difficulties for managing devices. Accordingly, a dynamic application block generation (DABG) scheme for blockchain-enabled IoT with dynamic device management and conditional traceability is proposed. To begin, it is necessary to construct a framework for an IoT system based on a consortium blockchain. This framework should consist of structures for dynamic application transactions and blocks, as well as a consensus mechanism. Miners are presented in distinct ways to adaptively process urgent and routine application transactions. A protocol based on group signature, known as DABG, is proposed for this framework. It is possible to achieve nonframeability, traceability, and anonymity by the use of the group signature. The suggested method may quickly validate transactions, manage devices in a dynamic manner, enable conditional traceability while maintaining data security, and keep users’ privacy intact. This is made possible by merging time-bound keys in the group signatures and node in the blockchain. Extensive testing has shown that the strategy in question has the potential to achieve very high levels of effectiveness.",[],[]
"Geographical Information System (GIS), as an Information System (IS), helps public/private organizations represent changes in natural/human context and discover patterns/relationships for social and economic progress through the examination of spatial/non-spatial data. In GIS applications, like smart communities services, rapid updating of great amount of data and expenditure of resources (hardware, human, financial) are significant, making traditional analysis on a single computer impractical due to computing limitations. Over the years, GIS has transformed from a tool that merely visualizes data to one that can model future scenarios. So, at first Cloud Computing (CC) emerged as an Internet-based paradigm offering dynamic and scalable solutions. Then, a new ecosystem has been defined Multi-Access Edge Computing (MEC), enabling CC capabilities and Information Technology (IT) services environment to the edge of the network, running applications, and performing related processing tasks closer to the customers. In this way network congestion is reduced and applications perform better. Therefore, a transition from CC to MEC is recommended for GIS. In this survey we provide a clear and structured overview of GIS applications proposed to date, who is using GIS and for what, to take advantage of its capabilities from integration with emerging technologies, CC first and MEC later, critically considering the problems, strengths and challenges inherent in these approaches, to support intelligent resource management and to fully grasp the opportunity offered by fifth-generation (5G) connectivity. The survey concludes by presenting some recommendations and future research directions.",[],[]
"Anomaly detection for the Internet of Things (IoT) is a major intelligent service required by many fields, including intrusion detection, state monitoring, device-activity analysis, and security supervision. However, the heterogeneous distribution of data and resource-constrained end nodes in ubiquitous IoT systems present challenges for existing anomaly detection models. Due to the advantages of flexible deployment and multi-dimensional resources, high altitude platform stations (HAPSs) and unmanned aerial vehicles (UAVs), which are important components of vertical heterogeneous networks (VHetNets), have significant potential for sensing, computing, storage, and communication applications in ubiquitous IoT systems. In this paper, we propose a novel VHetNet-enabled asynchronous federated learning (AFL) framework by adopting the compound-action actor-critic (CA2C) algorithm for UAV selection, which enables decentralized UAVs to collaboratively train a global anomaly detection model based on their local sensory data from IoT devices. In the VHetNet-enabled AFL framework, the UAV selection process aims to prevent UAVs with low local model quality and large energy consumption from affecting the learning efficiency and model accuracy. Due to the wide coverage as well as strong storage and computation capabilities, a HAPS operates as a central aerial server for aggregating local models of UAVs asynchronously and making decisions intelligently. Moreover, we propose a CA2C-based joint device association, UAV selection, and UAV placement algorithm to further enhance the overall federated execution efficiency and detection model accuracy under UAV energy constraints. Extensive experimental evaluation on real-world datasets demonstrates that the proposed algorithm can achieve high detection accuracy with short federated execution time and low energy consumption.",[],[]
"This paper investigates the application of artificial intelligence (AI) to wireless technology, specifically in the context of beam management (BM) in the advanced 5th-generation (5G) communication system. Our focus lies in aligning our study with the ongoing discussions within the Third Generation Partnership Project (3GPP) as of December 2022. Instead of evaluating the performance of specific AI models, we take user equipment (UE) receiver (Rx) beam prediction as an illustrative example of AI-based BM. We explore various aspects of AI model management, including model selection, monitoring, and activation/deactivation operations, from a 3GPP perspective. For model selection, we propose deploying distinct AI models for different propagation environments, categorized based on base station (BS) transmitter (Tx) beam measurement results. Reference Signal Received Power (RSRP) serves as a pivotal key performance index (KPI) for model performance monitoring. Our simulation results indicate that, instead of training one all-encompassing AI model with numerous layers for universal application, transitioning between domain-specific AI models with fewer layers yields superior performance. Model activation/deactivation procedures determine whether AI-based BM or traditional BM should be employed in a given scenario. We also introduce the use of AI for predicting the performance of both AI-based BM and traditional BM. By comparing the performance of these strategies, we can ascertain whether link performance degradation results from AI output errors or UE movement into challenging propagation environments. This approach enables the effective management of model switching between AI-based BM and traditional BM. The simulation shows that we can reduce the number of unnecessary switches by 10%.",[],[]
"6G is a next-generation cellular communication technology that builds up on existing 5G networks which are currently rolled out worldwide. Through incorporation of artificial intelligence (AI) and machine learning (ML), the core 5G network is advanced into an intelligent 6G network. The 6G Artificial Intelligence Radio Access Network (AI-RAN) is anticipated to offer advanced features like reduced latency, improved bandwidth, data rates and coverage. Furthermore, AI-RAN is expected to support complex use cases such as extreme connectivity, multi-user communications and dynamic spectrum access. This paper provides a detailed survey and thorough assessment of AI-RAN’s vision and state-of-the-art challenges. We first present a concise introduction to 6G AI-RAN followed by background information on the current 5G RAN and its challenges that must be overcome to implement 6G AI-RAN. The paper then examines trending research issues in AI-RAN i.e. challenges related to spectrum allocation, network architecture, and resource management. We discuss the methods to overcome these challenges which include the adoption of advanced machine learning and edge computing technologies to boost the performance of 6G AI-RAN. We conclude by stating open research directions.",[],[]
"Synchronizing the local oscillators in multibeam satellites with the objective of coherent communications is still an open challenge. It has to be addressed to implement full-frequency reuse approaches, such as precoding techniques using the already deployed multibeam satellites. This article addresses the required phase synchronization to enable precoding techniques in multibeam satellite systems. It contains the detailed design of a frequency and phase compensation loop based on the proportional-integral controller, which deals with the phase drift introduced by the hardware components. Specifically, the phase noise of the local oscillators used for up and down conversion at each system element (gateway, satellite, and user terminals). The implementation of the two-state phase noise model used to emulate this phase drift is included in the article. Besides, a comparative analysis of several methods to combine the frequency and phase measurements obtained from the user terminals is also included. Finally, the performance of the proposed closed-loop synchronization method is validated through simulations using our in-house developed MIMO end-to-end satellite emulator based on SDR platforms.",[],[]
"The traditional approach to distributed deep neural network (DNN) inference in edge computing systems is data-distributed inference. In this paradigm, each worker has a pre-trained DNN model. Using the DNN model, the worker processes the data that is offloaded to itself. The data-distributed inference approach (i) has high communication cost especially when the size of data is large, and (ii) is not efficient in terms of memory as the whole model should be stored and computed in each worker. Model-distributed inference is emerging as a promising solution, where a DNN model is distributed across workers. Although there is a huge amount of work on model-distributed training, the benefit of model distribution for inference is not understood well. In this paper, we analyze the potential of model-distributed inference in edge computing systems. Then, we develop an Adaptive and Resilient Model-Distributed Inference (AR-MDI) algorithm based on our optimal model allocation formulation. AR-MDI performs model allocation in a lightweight and decentralized way and it is resilient against delayed workers and failures. We implement AR-MDI in a real testbed consisting of NVIDIA Jetson TX2s and show that AR-MDI improves the inference time significantly as compared to baselines when the size of data is large, such as ImageNet.","['Computational modeling', 'Adaptation models', 'Data models', 'Training', 'Costs', 'Servers', 'Resource management']","['distributed deep neural networks', 'edge computing']"
"In this paper, we analyze the average age of information (AoI) and the average peak AoI (PAoI) of a multiuser mobile edge computing (MEC) system where a base station (BS) generates and transmits computation-intensive packets to user equipments (UEs). In this MEC system, we focus on three computing schemes: (i) The local computing scheme where all computational tasks are computed by the local server at the UE, (ii) The edge computing scheme where all computational tasks are computed by the edge server at the BS, and (iii) The partial computing scheme where computational tasks are partially allocated at the edge server and the rest are computed by the local server. Considering exponentially distributed transmission time and computation time and adopting the first come first serve (FCFS) queuing policy, we derive closed-form expressions for the average AoI and average PAoI. To address the complexity of the average AoI expression, we derive simple upper and lower bounds on the average AoI, which allow us to explicitly examine the dependence of the optimal offloading decision on the MEC system parameters. Aided by simulation results, we verify our analysis and illustrate the impact of system parameters on the AoI performance.",[],[]
"With the increasing use of Internet of Things (IoT)-enabled drones for various purposes, including photography, delivery, and surveillance, concerns related to privacy and security have arisen. Drones have the potential to capture sensitive information, invade privacy, and cause security breaches. Therefore, the need for advanced technology for the automated detection of drones has become crucial. In this paper, we propose an ensemble-based IoT-enabled drones detection scheme (in short, EDDSBS). The presented model is part of a computer vision-based module and uses transfer learning for improved performance. Transfer learning allows the reuse of pre-trained models and their knowledge in a different but related domain, enabling better performance with less training data. To evaluate the performance of the proposed EDDSBS, we test it on benchmark datasets, including the Drone–vs–Bird Dataset and the UAVDT dataset. The proposed EDDSBS outperforms the existing schemes of drone detection (i.e., in terms of accuracy). The results of the presented scheme demonstrate the potential of deep learning-based technology for automated drone detection in critical areas, such as airports, military bases, and other high-security areas. Thus the paper introduces a comprehensive process methodology for drone detection that can be applied in real-world settings for a sustainable and secure environment, which is required for a safe community.",[],[]
"We consider the distributed detection problem of a temporally correlated random radio source signal using a wireless sensor network capable of measuring the energy of the received signals. It is well-known that optimal tests in the Neyman-Pearson setting are based on likelihood ratio tests (LRT), which, in this set-up, evaluate the quotient between the probability density functions (PDF) of the measurements when the source signal is present and absent. When the source is present, the computation of the joint PDF of the energy measurements at the nodes is a challenging problem. This is due to the statistical dependence introduced to the received signals by the propagation through fading channels of the radio signal emitted by the source. We deal with this problem using the characteristic function of the (intractable) joint PDF, and proposing an approximation to it. We derive bounds for the approximation error in two wireless propagation scenarios, slow and fast fading, and show that the proposed approximation is exponentially tight with the number of nodes when the time-bandwidth product is sufficiently high. The approximation is used as a substitute of the exact joint PDF for building an approximate LRT, which performs better than other well-known detectors, as verified by Monte Carlo simulations.",[],[]
"Optical intelligent reflecting surfaces (OIRS) have the potential to enhance the capacity of the next generation optical wireless communication systems by re-configuring the propagation environment of optical waves. In this work, a submerged OIRS and a planar mirror surface (PMS) are used to build up a non-line of sight (NLOS) underwater wireless optical communication (UWOC) system. An NLOS UWOC setup, which makes use of the total internal reflection (TIR) phenomenon, serves as a benchmark against which the OIRS/PMS-assisted UWOC systems are compared. With the help of the Mellin inverse transform, Meijer’s G function, and Fox’s H-function, the closed-form expressions for the probability density function (PDF), cumulative distribution function (CDF), average spectral efficiency (SE), average energy efficiency (EE), outage probability, and average bit error rate (BER) of the proposed systems are derived. Further, an asymptotic analysis of average SE is performed to illustrate a better understanding of these proposed system’s performance at high SNR. The simulation results indicate that the OIRS-assisted UWOC system shows improved performance in comparison to benchmark scenario. Additionally, the efficient zones of OIRS deployment are analyzed.",[],[]
"As a specific category of artificial intelligence (AI), generative artificial intelligence (GenAI) generates new content that resembles what humans create. The rapid development of GenAI systems has created a huge amount of new data on the Internet, posing new challenges to current computing and communication frameworks. Currently, GenAI services rely on the traditional cloud computing framework due to the need for large computation resources. However, such services will encounter high latency because of data transmission and a high volume of user requests. On the other hand, edge-cloud computing can provide adequate computation power and low latency at the same time through the collaboration between edges and the cloud. Thus, it is attractive to build GenAI systems at scale by leveraging the edge-cloud computing paradigm. In this overview paper, we review recent developments in GenAI and edge-cloud computing, respectively. Then, we use two exemplary GenAI applications to discuss technical challenges in scaling up their solutions using edge-cloud collaborative systems. Finally, we list design considerations for training and deploying GenAI systems at scale and point out future research directions.",[],[]
"Symbol Level Precoding (SLP) has attracted significant research interest due to its ability to exploit interference for energy-efficient transmission. This paper proposes an unsupervised deep-neural network (DNN) based SLP framework. Instead of naively training a DNN architecture for SLP without considering the specifics of the optimization objective of the SLP domain, our proposal unfolds a power minimization SLP formulation based on the interior point method (IPM) proximal ‘log’ barrier function. Furthermore, we extend our proposal to a robust precoding design under channel state information (CSI) uncertainty. The results show that our proposed learning framework provides near-optimal performance while reducing the computational cost from \mathcal{O}(n^{7.5})to \mathcal{O}(n^{3})for the symmetrical system case where n=\text {number of transmit antennas}=\text {number of users}. This significant complexity reduction is also reflected in a proportional decrease in the proposed approach’s execution time compared to the SLP optimization-based solution.",[],[]
"High speed trains (HST) have gradually become an essential means of transportation, where given our digital world, it is expected that passengers will be connected all the time. More specifically, the on-board passengers require fast mobile connections, which cannot be provided by the currently implemented cellular networks. Hence, in this article, we propose an analogue radio over fiber (A-RoF) aided multi-service network architecture for high-speed trains, in order to enhance the quality of service as well as reduce the cost of the radio access network (RAN). The proposed design can simultaneously support sub-6GHz as well as millimetre wave (mmWave) communications using the same architecture. Explicitly, we design a photonics aided beamforming technique in order to eliminate the bulky high-speed electronic phase-shifters and the hostile broadband mmWave mixers while providing a low-cost RAN solution. Finally, a beamforming range of 180° is demonstrated with a high resolution using our proposed system.",[],[]
"In this paper, we investigate the performance of multiple-input and multiple-output (MIMO) unmanned air vehicle (UAV) based multi-user communication systems over generalized Nakagami-m fading channels subject to channel feedback delays. The impact of decode-and-forward (DF) based relaying and outdated channel state information is assessed at the receiver nodes. To reduce the complexity and retain the MIMO gains, the transmit antenna selection (TAS) strategy is used to select the best antenna at the source and UAV nodes. A generic framework in the form of closed-form analytical expressions for outage probability (OP), asymptotic OP, and the average symbol error rate of higher-order quadrature amplitude modulation (QAM) schemes such as hexagonal QAM, rectangular QAM, and cross QAM are derived. In addition, energy efficiency analysis is also performed for the considered system model. In this framework, altitude and location-dependent path loss modeling are considered for the air-to-ground links. Optimization of UAV location and altitude is performed through a limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) algorithm to attain minimum OP (MOP). Results illustrate optimal performance dependent on the channel correlation parameters, antenna elements, and fading conditions. Monte-Carlo simulations are performed to validate the derived analytical results and compared with the existing works.",[],[]
"In this paper, with the aid of the mathematical tool of stochastic geometry, we introduce analytical and computational frameworks for the distribution of three different definitions of delay, i.e., the time that it takes for a user to successfully receive a data packet, in large-scale cellular networks. We also provide an asymptotic analysis of one of the delay distributions, which can be regarded as the packet loss probability of a given network. To facilitate the computation of the obtained analytical formulations, we propose efficient numerical approximations based on the numerical inversion method, the Riemann sum, and the beta distribution. Finally, we demonstrate the accuracy of the obtained analytical formulations and the corresponding approximations against Monte Carlo simulations, and unveil insights on the delay performance with respect to several design parameters, such as the decoding threshold, the transmit power, and the deployment density of the base stations. The proposed methods can facilitate the analysis and optimization of cellular networks, subject to reliability constraints on the network packet delay, that are not restricted to the local (average) delay, e.g., in the context of delay sensitive applications.",[],[]
"Indoor communication and positioning are significant fields of applications for indoor Internet of Things (IoT) given the rapid growth of IoT in smart cities, smart grids, and smart industries. Visible light positioning (VLP) has become more and more attractive for researchers to provide indoor location-aware IoT services. Additionally, artificial intelligence (AI) has attracted considerable research effort to address the challenges in visible-light communication (VLC) systems. This is an emerging technology in next-generation wireless networks. However, despite the rapid progress, the use of AI in localization, navigation, and position estimation is still underexplored in VLC systems, and various research challenges are still open. This methodological review summarizes the research efforts regarding the use of AI in the field of VLP, to improve the position estimation accuracy in both two-dimensional (2D) and three-dimensional (3D) indoor IoT applications. This treatise also presents open issues and potential future directions for motivating further research in the field. Various databases were utilized in this paper: Scopus, Google Scholar, and IEEE Xplore; obtained 88 papers from 2017 to early 2023. Most (68%) of the AI articles in VLP systems are machine learning (ML) methods applied for localization and position estimation in VLC systems, while the other 32% of the research articles focussed on evolutionary algorithms. ML and evolutionary models may present limitations in terms of complexity and time-consuming nature but offer highly accurate, robust, reliable, and cost-effective results in terms of position estimation over conventional approaches.",[],[]
"The massive digital information generated in conjunction with the ever-increasing phasor measurement data in the power grid has led to a tremendous constraint on the analysis and timely processing of real-time data. Under these conditions, leveraging Artificial Intelligence (AI) can play a crucial role in assisting more efficient data processing and analysis. In this paper an AI-assisted power grid event classification method is proposed, which aims at improving the overall power grid system performance. Furthermore, an edge cloud sharing scheme is introduced for a large-scale power grid system. To balance the load and reduce the maximum processing time, a multiple edge cloud node-based scheme is developed. The simulation results verify that the proposed AI-assisted event classification method, together with the edge cloud sharing scheme, can significantly improve the overall performance of the system.",[],[]
"In this paper, the dominant factor affecting the performance of active intelligent reflecting surface (IRS) aided wireless communication networks in Rayleigh fading channel, namely the average signal-to-noise ratio (SNR)γ0at IRS, is studied. Making use of the weak law of large numbers, the simple asymptotic expressions for the received SNR at user and the average SNR at IRS are derived as the numberNof IRS elements goes to medium-scale and large-scale. WhenNtends to large-scale, the asymptotic received SNR at user is proved to be a linear increasing function of a product ofγ0andN. Subsequently, when the base station (BS) transmit power is fixed, there exists an optimal limited reflective power at IRS. At this point, more IRS reflect power will degrade the SNR performance. Additionally, under the total power sum constraint of the BS transmit power and the power reflected by the IRS, an optimal power allocation (PA) strategy is derived and shown to achieve 0.83 bit rate gain over equal PA. Finally, an IRS with finite phase shifters being taken into account, generates phase quantization errors, and further leads to a degradation of receive performance. The corresponding closed-form performance loss expressions for user’s asymptotic SNR, achievable rate (AR), and bit error rate (BER) are derived for active IRS. Numerical simulation results show that a 3-bit discrete phase shifter is required to achieve a trivial performance loss for a large-scale active IRS.","['Signal to noise ratio', 'Finite element analysis', 'Wireless networks', 'Quantization (signal)', 'Phase shifters', 'Array signal processing', 'Channel estimation']","['Active IRS', 'finite phase shifter', 'quantization error', 'performance loss', 'the law of large numbers']"
"The Industrial Internet of Things needs wireless communication with bounded latency and stronger robustness. Nodes employing the Time Slotted Channel Hopping (TSCH) MAC operate according to a schedule, and recent work on flow-based autonomous schedulers has shown they can guarantee dedicated resources to each flow of traffic. However, these works assume all nodes transmit toward one destination. Industrial applications such as process control require heterogeneous traffic patterns, e.g., for sensor-to-actuator. We investigate how autonomous flow-based scheduling may support heterogeneous traffic patterns. We have previously proposed the Layered scheduler that emphasized flow scheduling and spatial reuse. In this work, we extend Layered to support heterogeneous traffic patterns. The extension includes a novel mechanism where the first application traffic packet is sent in a shared cell to inherently signal the need for scheduling dedicated cells. In adapting to heterogeneous traffic patterns, we encountered seven challenges. These include, e.g., the schedule adapting to packets later found as invalid at the routing layer and MAC queues leading to packets signaling outdated routing information to neighbors. We identify a set of mitigations and key parameters to address these challenges, and we evaluate their impact using the Cooja simulator and the FIT IoT-LAB testbed. The mitigation mechanisms are essential to ensure predictable performance under all conditions. Shared cell capacity was crucial as insufficient capacity can have a detrimental impact. Lastly, the scheduler was compared to the autonomous scheduler Orchestra. In scenarios with heterogeneous traffic patterns, we found the extended Layered scheduler retained performance independent of the number of flows. However, it comes at the cost of energy per goodput. Compared to Orchestra, Layered requires approximately twice the energy to maintain the schedule, yet Layered’s higher capacity allows for comparable efficiency as application traffic increases.",[],[]
"Edge-caching is an effective solution to cope with the unprecedented data traffic growth by storing contents in the vicinity of end-users. In this paper, we formulate a hierarchical caching policy where the end-users and cellular base station (BS) are equipped with limited cache capacity with the objective of minimizing the total data traffic load in the network. The caching policy is a nonlinear combinatorial programming problem and difficult to solve. To tackle the issue, we design a heuristic algorithm as an approximate solution which can be solved efficiently. Moreover, to proactively serve the users, it is of high importance to extract useful information from data requests and predict user interest about contents. In practice, the data often contain implicit feedback from users which is quite noisy and complicates the reliable prediction of user interest. In this regard, we introduce a Bayesian Poisson matrix factorization model which utilizes the available side information about contents to effectively filter out the noise in the data and provide accurate prediction. Subsequently, we design an efficient Markov chain Monte Carlo (MCMC) method to perform the posterior approximation. Finally, a real-world dataset is applied to the proposed proactive caching-prediction scheme and our results show significant improvement over several commonly-used methods. For example, when the BS and the users have caches with storage of 25% and 10% of the total contents size respectively, our approach yields around 8% improvement with respect to the state-of-the-art approach in terms of caching performance.",[],[]
"Proximity estimation forms the backbone of contact tracing solutions, as quarantining potentially infected individuals is essential for controlling the spread of epidemics. It is also essential for device discovery in Device-to-Device (D2D) and Vehicle-to-Vehicle (V2V) communications, which will be critical in future 6G networks. Despite the widespread coverage of cellular networks, no previous work has evaluated cellular-based proximity estimation in an experimental setting. In this paper, we collect Channel State Information (CSI) measurements from an actual cellular network and utilize them to train and evaluate two proposed solutions. Capitalizing on CSI spatial correlation, we propose a data-driven method to classify devices based on their respective proximity. The proposed neural network has an accuracy of 91.18% when classifying devices as within 5 meters from one another or not, from only 10 seconds of CSI measurements. This method is complemented by a model-based approach that provides a solid theoretical model for estimating proximity. The model-based approach uses Bayesian inference of the conditional distribution of power correlation across devices, assuming spatially-correlated shadowing and stochastic geometry tools. The proposed Bayesian regressor fits our dataset better than the standard exponentially-decaying correlation model, reaching a2.8×lower RMSE while requiring fine-tuning of only two more parameters. A Bayesian classifier is also proposed and reached a 91.67% accuracy on the binary case while also outperforming the data-driven model significantly at higher distances.",[],[]
"Modern communication systems employ multi-domain modulation and coding techniques for effectively exploiting all available resources. Hence in such systems the transmit and receive signals have an inherent multi-domain structure which can be represented using tensors. This work considers the capacity of higher order tensor channels associated with such multi-domain communication systems when the elements of the input tensor are constrained to be drawn from discrete signalling constellations. We establish a relationship between the tensor gradient of the mutual information and the error covariance tensor associated with the minimum mean squared error estimator at the receiver. This relation is used to iteratively find a multi-linear precoder at the input which achieves capacity of the tensor channel under the signalling constellation constraints. Through numerical examples, we show the convergence behavior of the proposed precoder, and compare the capacity achieved under different constellations with the capacity when the input is Gaussian. Further, we exploit the tensor formulation of the problem to find the channel capacity under a variety of different power constraints spanning across several domains. At high SNR, the constellation constraints saturate the capacity while at low SNRs, the constellation constraints are not too relevant, and the power constraints dominate and limit the performance. The capacity saturation level depends on the input order and distribution.",[],[]
"The provisioning of 5G technology does not only involve mobile terminals, but also new services such as Fixed Wireless Access (FWA). The aim of this study is to examine the ElectroMagnetic Field (EMF) and throughput performance of a FWA deployment utilizing Standalone technology operating at 3.5 GHz. To address the unique characteristics of 5G FWA signals, an innovative framework has been designed based on the measurement of 5G FWA spectrum using four independent chains and an additional traffic generation chain to saturate the radio link capacity at the measurement location. Methodologies for evaluating 5G FWA exposure under conservative conditions, such as measurements of exposure during active traffic generation and maximum power extrapolations, are also introduced. Results from real measurements taken at a baseball stadium show that 5G FWA exposure is consistently low, typically below 0.4 [V/m], with an upper bound of 0.59 [V/m], while the achieved throughput is up to 250 [Mbps]. Additionally, the measured 5G exposure levels are a small fraction compared to those emitted by other technologies such as 4G. Furthermore, the values estimated by simulation from the output power counters of the base station are found to be in close agreement with the measured exposure levels.",[],[]
"Massive multiple-input multiple-output (mMIMO) technology is a way to increase spectral efficiency and provide access to the Internet of Things (IoT) and machine-type communication (MTC) devices. To exploit the benefits of large antenna arrays, accurate channel estimation through pilot signals is needed. Massive IoT and MTC systems cannot avoid pilot reuse because of the enormous numbers of connected devices. We propose a pilot reuse algorithm based on channel charting (CC) to mitigate pilot contamination in a multi-sector single-cell mMIMO system having spatially correlated channels. We show that after creating an interference map via CC, a simple strategy to allocate the pilot sequences can be implemented. The simulation results show that the CC-based pilot reuse strategy improves channel estimation accuracy, which subsequently improves the symbol detection performance and increases the spectral efficiency compared to other existing schemes. Moreover, the performance of the CC pilot assignment method approaches that of exhaustive search pilot assignment for small network setups.","['Contamination', 'Channel estimation', 'Interference', 'Resource management', 'Covariance matrices', 'Symbols', 'Partial transmit sequences']","['Channel charting', 'massive MIMO', 'pilot reuse', 'pilot contamination']"
"As wireless communications and interconnected networks become ubiquitous and relied upon, they must also remain secure. Advanced communication systems that use techniques to improve data throughput and minimize latency lend themselves to physical-layer authentication. The stochastic and dynamic nature of the wireless mobile channel provides features that can be extracted through deep learning. We propose a novel method to authenticate transmitters at the physical layer by leveraging channel state information to predict future channel impulse responses. Specifically, we compare the use of recurrent neural networks (RNNs) using long-short term memory (LSTM) and gated recurrent unit (GRU) cells with variations of a conditional generative adversarial network (CGAN) to authenticate transmitters in a mobile environment. Our evaluation shows that standalone RNNs using LSTM and GRU cells are adept at predicting future channel responses, however a CGAN-trained discriminator using GRU cells is able to match the authentication accuracy of a standalone network without using a predefined channel prediction error threshold. Using a discriminator trained by a CGAN with binary cross entropy loss in the discriminator and mean squared error loss in the generator, the neural network was able to authenticate at a 98.5% rate.","['Authentication', 'Transmitters', 'Recurrent neural networks', 'Wireless communication', 'Generative adversarial networks', 'Gallium nitride', 'Radio transmitters']","['Channel prediction', 'channel state information', 'generative adversarial network', 'physical-layer authentication', 'recurrent neural networks']"
"Several experiments and field trials have shown that deterministic components characterize the Power Line Communication (PLC) noise across multiple-conductors. This article aims at understanding the main origins of these noise components and evaluate several methods to characterize them statistically. The correlation, distance-correlation, mutual information estimation, and a proposed linear coefficient ratio, are used to analyze the noise time-series statistically. The analysis of the PLC noise deterministic components allow us to develop an enhanced noise model that includes them. The presence of deterministic noise fosters the development of coding and decoding algorithms that aim at mitigating the noise effect. New decoding techniques called Quasi-Deterministic-Decoding are detailed and tested with single-input-multiple-output (SIMO) 1×2 and multiple-input-multiple-output (MIMO) 2×2 transmission configurations using real PLC noise and channel realizations. The proposed model leads to a better representation of the PLC noise traces and, consequently, the development of improved coding algorithms for reliable data transmission at low signal-to-noise ratios.",[],[]
"Due to its superior performance, Orthogonal Chirp Division Multiplexing (OCDM) has recently gained attention as a potential replacement for Orthogonal Frequency Division Multiplexing (OFDM) in beyond-5G systems. In this paper, we provide an analytical characterization of OCDM signals, elucidating the theoretical principles that enable their numerical generation through the Inverse Discrete Fresnel Transform (IDFnT), despite the presence of severe frequency-domain aliasing that substantially distorts the signal at the transmitter output. Furthermore, in light of the proposed utilization of the THz band in beyond-5G systems, we investigate the performance of OCDM in this frequency range in the presence of thermal, molecular, and phase noise. To model the latter, which is expected to be a significant challenge at THz frequencies, we take as a reference an actual Phase Locked Loop (PLL) oscillator operating at 237.7 GHz. The numerical results reveal the achievable performance of OCDM as a function of several key factors, including the modulation order, the bandwidth, the number of chirps constituting the signal, the oscillator parameters, the channel model, and the use of techniques aimed at mitigating the impact of phase noise. The findings are compared with those of OFDM, which is regarded as a benchmark due to its adoption in 4G and 5G systems, and demonstrate the superior performance of OCDM also in the presence of significant phase noise.",[],[]
"In this study, we propose an over-the-air computation (OAC) scheme based on chirps to detect the majority votes (MVs) in a wireless network for federated edge learning (FEEL) and distributed localization. With the proposed approach, a group of votes is mapped to an index of a linear chirp at each edge device (ED). From superposed chirp signals, the corresponding MVs at the edge server (ES) are then detected non-coherently with a set of energy comparators by exploiting the bit representation of the indices. The proposed scheme is power-efficient and has low out-of-band emission while it does not use the channel state information (CSI) at the EDs and ES. Hence, it paves the way for long-distance FEEL and distributed localization based on MVs in a wireless sensor network with low-complexity devices. For FEEL, we comprehensively demonstrate the efficacy of the proposed approach under heterogeneous data distribution. For localization, we propose iterative refinements and multiple repetitions to improve the localization performance. We show that the proposed strategies minimize the distance between the root-mean-square error (RMSE) error and quantization bound.","['Location awareness', 'Chirp', 'Symbols', 'Wireless sensor networks', 'Distance learning', 'Computer aided instruction', 'Spectral efficiency']","['Chirp', 'distributed learning', 'distributed localization', 'over-the-air computation', 'PMEPR']"
"One of the main problems with WSNs is that most sensor nodes in wireless sensor networks (WSNs) are motorized by energy-constrained, which significantly affects the system’s effectiveness, dependability, and lifespan. Numerous clustering strategies have been created to enhance the energy efficiency of WSNs in 5G and 6G transmission. To overcome these issues, we suggest a collaborative energy-efficient routing protocol (CEEPR) for sustainable communication in 5G/6G wireless sensor networks (WSNs). Initially, this study gathered and collected the data at the sink node. The network’s nodes are clustered using the reinforcement learning technique (R.L.). Cluster head selection is employed for better data transmission using residual energy (RE) based cluster head selection algorithm. A collaborative energy-efficient routing protocol (CEERP) is proposed. We use a multi-objective improved seagull algorithm (MOISA) as an optimization technique to enhance the system’s performance. Finally, the presentation of the system is analyzed. Compare with the existing methods, the primary metrics are throughput, energy consumption, network lifetime, packet transmission, routing overhead, and transmission speed. The proposed approach uses 50% less energy while improving network lifespan and energy efficiency compared to the current protocols.","['Wireless sensor networks', 'Routing', 'Energy efficiency', 'Wireless communication', '5G mobile communication', 'Routing protocols', 'Internet of Things']","['Wireless sensor networks (WSNs)', '5G/6G transmission', 'reinforcement algorithm (RL)', 'residual energy (RE) based cluster head selection', 'collaborative energy-efficient routing protocol (CEERP)', 'multi-objective improved seagull algorithm (MOISA)']"
"Supporting delay-constrained traffic becomes more and more critical in multimedia communication systems, tactile Internet, networked control systems, and cyber-physical systems, etc. In delay-constrained traffic, each packet has a hard deadline. When it is not delivered before the hard deadline, it becomes useless and will be removed from the system. This feature is completely different from that of traditional delay-unconstrained traffic and brings new challenge to network protocol design. In this work, we study the widely-used (slotted) ALOHA and CSMA wireless access protocols but under the new delay-constrained setting. Our goal is to compare delay-constrained ALOHA and CSMA for different system settings and thus give network operators guidelines on protocol selection. We use two Markov chains to analyze delay-constrained ALOHA and CSMA, respectively. However, the number of states of Markov chains increases exponentially with respect to the number of users in the network. Therefore, we can only compare the exact performance of delay-constrained ALOHA and CSMA for small-scale networks. To address the curse of dimensionality, we design a single-user parameterized ALOHA (resp. CSMA) system, where the parameters are to be learned to approximate the original multi-user ALOHA (resp. CSMA) system. In addition, our low-complexity approach preserves the Markov-chain structure of the systems and thus enables us to compute some other interested performance metrics such as average delivery time. We use our low-complexity approach to reveal the conditions under which ALOHA (resp. CSMA) outperforms CSMA (resp. ALOHA) in the delay-constrained setting via extensive simulations.",[],[]
"With the exponential growth of Internet of Things (IoT) devices, IoT has become a transformative technology with applications spanning various domains. It encompasses a wide range of public and industrial vertical services that come with diverse and stringent Quality of Service (QoS) requirements. Traditional networks often struggle to meet the demands of these diverse IoT services. As a result, the introduction of 5G and Beyond 5G (B5G) networks holds promise in accommodating these diverse IoT services through network slicing technology. Network slicing involves partitioning a single physical network infrastructure into multiple logically isolated networks and ensures dedicated resources to each service as per QoS requirements. Additionally, Multi-Access Edge Computing (MEC) in B5G networks presents an innovative solution to facilitate low-latency communication for IoT services. However, the automatic provisioning and management of end-to-end (e2e) network slicing for IoT services across multi-domain infrastructures pose significant challenges, including manual error-prone resource configuration, network slice template preparation, and human intervention. This paper proposes an automated Artificial Intelligence (AI) and MEC-enabled solution for provisioning and managing network slice resources across multiple domains specifically tailored for IoT services. Our solution provides an abstraction layer that generates slice templates for each domain and automates the deployment of resources based on the specified QoS requirements. It automates the slice resource configuration process, reduces human intervention, and manages the complete lifecycle of IoT slices. We have conducted several tests with our system, creating multiple IoT slices, and have observed stable performance in slice design, resource provisioning, slice isolation, and management.",[],[]
"In this paper, a deep learning-based cooperative semantic communication system is proposed on relay channels. In order to enhance the reliability and adaptability of the system to varying channel conditions, an on-demand semantic forwarding framework is established, where the relay attempts to recover and re-transmit the source semantic information, as required by the destination. To be specific, for determining whether a semantic forwarding is needed from the relay, a semantic similarity check (SSC) is proposed, based on which the degree of semantic information recovery at the destination can be accurately estimated. On the other hand, in order to effectively merge the semantic information received through different paths, a semantic combining (SeC) method is proposed by combining the semantic features abstracted from both the direct link and the relay link. For achieving a desirable performance trade-off between the degree of semantic information recovery and the transmit energy consumption, a new metric of semantic energy efficiency (SEE) is proposed. Simulation results verify the performance gains achieved by the proposed cooperative semantic communication system with on-demand semantic forwarding, as compared to the state-of-the-art schemes employing separate source-channel coding in low-to-medium signal-to-noise ratio (SNR) regimes. Furthermore, as compared to the case with always-forwarding, almost the same performance is achieved by the proposed on-demand forwarding, but with energy consumption.",[],[]
"Due to the power consumption and high circuit cost in antenna arrays, the practical application of massive multiple-input multiple-output (MIMO) in the sixth generation (6G) and future wireless networks is still challenging. Employing low-resolution analog-to-digital converters (ADCs) and hybrid analog and digital (HAD) structure is two low-cost choices with acceptable performance loss. In this paper, the combination of the mixed-ADC architecture and HAD structure employed at receiver is proposed for direction of arrival (DOA) estimation, which will be applied to the beamforming tracking and alignment in 6G. By adopting the additive quantization noise model, the exact closed-form expression of the Cramér-Rao lower bound (CRLB) for the HAD architecture with mixed-ADCs is derived. Moreover, the closed-form expression of the performance loss factor is derived as a benchmark. In addition, to take power consumption into account, energy efficiency is also investigated in our paper. The numerical results reveal that the HAD structure with mixed-ADCs can significantly reduce the power consumption and hardware cost. Furthermore, that architecture is able to achieve a better trade-off between the performance loss and the power consumption. Finally, adopting 2–4 bits of resolution may be a good choice in practical massive MIMO systems.",[],[]
"Human-Centric Sensing (HCS), a novel approach in the evolution of the Next Generation Internet of Things (NG-IoT), exploits the ubiquity of diverse smart devices, including smartphones or wearable devices, in conjunction with their enhanced sensing capabilities to collect information, leveraging human intelligence for the common benefit of the crowd. The main feature of HCS is the involvement of mobile users in data collection, processing, analysis and sharing. Thus, the main challenge in HCS systems is to ensure users’ participation and trustworthiness as well as data quality. The aim of this work is, as a first step, to identify and discuss the factors that affect data quality in HCS-based NG-IoT systems, as well as elaborate on their interrelation. Furthermore, potential solutions that could be adopted to ensure the highest possible degree of data quality are highlighted, in conjunction with critical aspects that should be considered, proposing a novel classification with three major categories: task assignment, reputation mechanisms and blockchain technology. Finally, a trust-aware task assignment model is proposed to effectively address the data quality challenge in HCS-based IoT systems, reflecting users’ trustworthiness, willingness, experience, and ability to collect and share high-quality data contributions. The proposed trust-aware task assignment model exploits a reputation mechanism and is designed using blockchain and smart contract technologies to enable the decentralized provision of trustworthy services among entities and preserve users’ privacy, harnessing the decentralization, transparency and immutability offered by blockchain. Trust-based task assignment offers an effective solution for trustworthy users’ selection while ensuring high-quality contributions and users’ privacy.","['Task analysis', 'Data integrity', 'Sensors', 'Surveys', 'Resource management', 'Blockchains', 'Data privacy']","['Blockchain', 'data quality', 'Internet of Things (IoT)', 'human centric sensing (HCS)', 'reputation mechanism', 'task assignment']"
"This study focuses on optimizing the performance of an uplink pairwise Non-Orthogonal Multiple Access (NOMA) scenario with and without the support of a relayer, while subject to jamming attacks. We consider two different relaying protocols, one where the sources and the destination are within range of each other and one where they are not. The relay node can be mobile, e.g., a mobile base station, an unmanned aerial vehicle (UAV) or a stationary node that is chosen as a result of a relay selection procedure. We also benchmark with a NOMA retransmission protocol and an Orthogonal Multiple Access (OMA) scheme without a relayer. We analyze, adjust and compare the four protocols for different settings using outage analysis, which is an efficient tool for establishing communication reliability for both individual nodes and the overall wireless network. Closed-form expressions of outage probabilities can be adopted by deep reinforcement learning (RL) algorithms to optimize wireless networks online. Accordingly, we first derive closed-form expressions for the individual outage probability (IOP) of each source node link and the relayer link using both pairwise NOMA and OMA. Next, we analyze the IOP for one packet (IOPP) for each source node considering all possible links between the source node to the destination, taking both phases into account for the considered protocols when operating in Nakagami-mfading channels. The overall outage probability for all packets (OOPP) is defined as the maximum IOPP obtained among the source nodes. This metric is useful to optimize the whole wireless network, e.g., to ensure fairness among the source nodes. Then, we propose a method using deep RL where the OOPP is used as a reward function in order to adapt to the dynamic environment associated with jamming attacks. Finally, we discuss valuable guidelines for enhancing the communication reliability of the legitimate system.",[],[]
"Low-complexity fusion rules relying on hybrid combining are proposed for decision fusion in frequency selective millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) sensor networks (SNs). Both centralized (C-MIMO) and distributed (D-MIMO) antenna architectures are considered, where the error-prone local sensor decisions are transmitted over orthogonal subcarriers to a fusion center (FC) employing a large antenna array. Fusion rules are designed for the FC, followed by closed-form expressions of the false alarm and detection probabilities to comprehensively characterize the performance of distributed detection. Furthermore, efficient transmit signaling vectors are designed for optimizing the detection performance. Both the asymptotic performance analysis and the pertinent power reduction laws are presented for the large antenna regime considering both the C-MIMO and D-MIMO topologies, which potentially lead to a significant transmit power reduction. Low-complexity fusion rules and their analyses are also given for the realistic scenario of incorporating channel state information (CSI) uncertainty, where the sparse Bayesian learning (SBL) framework is utilized for the estimation of the sparse frequency selective mmWave massive MIMO channel. Finally, the performance of the proposed low-complexity detectors is characterized through extensive simulation results for different scenarios.",[],[]
"5G and beyond networks are considered a catalyst for emerging IoT applications and services by providing ultra-reliable connectivity and massive connections to billions of IoT sensors and devices. However, the scalable deployment of such services requires reduced cost, an open ecosystem for IoT application developers and service providers, and a multi-tenant deployment model enabling the 5G and beyond network infrastructure to host multiple IoT services while preserving the service level agreement (SLA) requirements. AI brings intelligence to the network infrastructure to automate several network functions and predict the service’s workload to ensure network function scaling and adaptation. 5G brings AI to the radio access network (RAN) to reduce the operation cost, decrease power consumption and boost service quality. With this evolution towards AI-based features in the network, the Open RAN (ORAN) specification expanded the network functions virtualization to the RAN intelligence by introducing RAN Intelligent Controller (RIC) to enable AI applications for the network functions. This paper focuses on the RAN intelligence ecosystem and presents an intelligent network application (xApp) for network slicing for the RAN using AI and Deep Learning techniques. We evaluated the xApp with a near Real-Time RAN Intelligent Controller (near-RT RIC) and showed the network slicing functionality in an automated and intelligent fashion. We show how intelligent network slicing enables emerging IoT services to co-exist while meeting the required SLAs.",[],[]
"Orthogonal frequency-division multiplexing (OFDM) is a key technology for cellular and Wi-Fi systems, but its performance may be degraded by hardware impairments. Existing works focus mostly on single hardware impairment in OFDM systems, without considering the joint effect of hardware impairments on the entire system. In this paper, hardware impairments including nonlinear power amplification, clipping, in-phase/quadrature-phase (IQ) imbalance, phase noise, carrier frequency offset, and sampling clock offset in OFDM systems are simultaneously considered. We propose end-to-end deep learning-based designs, which jointly optimize transmitter and receiver, to effectively mitigate the performance loss due to hardware impairments. For single-antenna systems and2×2multiple-input and multiple-output (MIMO) systems, the proposed design featuring the dense layer neural network (DLNN) significantly outperforms traditional impairment-mitigating methods under both the additive white Gaussian noise (AWGN) channel and the Rayleigh fading channel. Meanwhile, the complexity of the proposed scheme is six times smaller. For2×4MIMO systems, the proposed design featuring the residual dense convolution dense neural network (ResNet-DCDNN) outperforms the traditional methods by a large margin. Additionally, transfer learning is applied to effectively address the issue of time-varying impairment levels.",[],[]
"The world population is rapidly increasing, which in turn increases food needs. On the other hand, food production is the main cause of water withdrawal. Precision agriculture, based on the Internet of Underground Things (IoUT), has recently been proposed to reduce water withdrawals. IoUT comprises sensors and communication devices that are partially or fully submerged beneath the ground surface. However, the current technology used in IoUT is challenged by inefficient spectral and energy efficiency. This paper proposes a deep learning (DL)-based index modulation technique to increase spectral efficiency without raising the system’s bit error rate. The article decreases the peak-to-average power ratio (PAPR) and enhances energy efficiency by employing the X-transform time-domain synchronous index modulation (TDS-IM) orthogonal frequency division multiplexing (OFDM) in underground communication. Unlike the radio frequency channel, the underground channel is environment-dependent. Therefore, in this paper, we propose a DL receiver detector to eliminate such environmental dependency and simplify the system’s complexity. This proposal can establish new identification parameters, achieving accurate estimation of the modulated symbols even in harsh communication channels. The results of the simulation indicate the superior performance of the proposed scheme in terms of both spectral and energy efficiency compared to the benchmarks, as well as its ability to improve the system’s bit error rate.",[],[]
"The radio frequency fingerprint (RFF)-based device identification is a promising physical layer authentication technique. However, the wireless channel significantly affects the RFF features of the wideband wireless devices. In this paper, we extensively investigate the impact of channel variation on RFF identification using 20 MHz IEEE 802.11 signal. A time domain least mean square (LMS) equalization-based feature extraction method has been proposed. This method progressively restores the transmitted signal and preserves more details of RFF features than the classical frequency domain equalization (FDE) method. Moreover, a hybrid identifier is proposed to take advantage of both LMS-based and FDE-based methods. With the equalized samples, a four-layer convolutional neural network is designed for device identification. An experimental system has been set up to capture the waveform of 68 802.11 devices at different positions. The experimental results show that the LMS-based method outperforms others when the acquisition positions of the training dataset are the same as those of the testing dataset. On the other hand, the FDE-based method is shown to be more effective when the acquisition positions of the training dataset do not fully include those of the testing dataset. Moreover, the hybrid identifier achieves an improvement of 2% for overall identification accuracy.",[],[]
"In this paper, we devise a deep SARSA reinforcement learning (DSRL) user scheduling algorithm for a base station (BS) that uses a high-altitude platform station (HAPS) as a backup to serve multiple users in a wireless cellular network. Considering a realistic scenario, we assume that only the outdated channel state information (CSI) of the terrestrial base station (TBS) is available in our defined user scheduling problem. We model this user scheduling problem using a Markov decision process (MDP) framework, aiming to maximize the sum-rate while minimizing the number of active antennas at the HAPS. Our performance analysis shows that the sum-rate obtained with our proposed DSRL algorithm is close to the optimal sum-rate achieved with an exhaustive search method. We also develop a heuristic optimization method to solve the user scheduling problem at the BS. We show that for a scenario where perfect CSI is not available, our proposed DSRL algorithm outperforms the heuristic optimization method.",[],[]
"In this paper, we study the DoF of the time-selective M\times N wireless X -network assisted by an IRS. It is well-known that the DoF of the M\times N wireless X -network is {}\frac {MN}{M+N-1} . We show that the maximum DoF of \min \{M,N\} can be achieved when the IRS has enough elements. We consider two kinds of active and passive IRSs. We also consider two different scenarios, where the channel coefficients for IRS elements are either independent or correlated. For the M\times N wireless X -network assisted by an active IRS with independent channel coefficients, we derive the inner and outer bounds on the DoF region and the lower and upper bounds on the sum DoF. We show that the maximum value for the sum DoF, i.e., \min (M,N) , is achievable if the number of elements is more than a threshold for the active IRS, which is equal to the approximate capacity of \min \{M,N\}\log (\rho +1)+o(\log (\rho)) for the IRS-assisted X -network, where \rho is the transmission power. For the M\times N wireless X -network assisted by a passive IRS with the assumption of independent and correlated channel coefficients for IRS elements, we introduce probabilistic inner and outer bounds on the DoF region, and the probabilistic lower and upper bounds on the sum DoF and show that the proposed lower bound for the sum DoF asymptotically approaches \min (M,N) with an order of at least O\left({{}\frac {1}{Q}}\right) for independent channel coefficients (i.e., the sum DoF is \min \{M,N\}\left({1-O\left({\frac {1}{Q}}\right)}\right) ), which is equal to the approximate capacity of \min \{M,N\}\left({1-O\left({{}\frac {1}{Q}}\right)}\right)\log (\rho +1)+o(\log (\rho)) and O\left({{}\frac {1}{\sqrt {Q}}}\right) for correlated channel coefficients (i.e., the sum DoF is \min \{M,N\}\left({1-O\left({{}\frac {1}{\sqrt {Q}}}\right)}\right) , which is equal to the approximate capacity of \min \{M,N\}\left({1-O\left({{}\frac {1}{\sqrt {Q}}}\right)}\right)\log (\rho +1)+o(\log (\rho))) , where Q is the number of IRS elements. Thus, this decrement in the order of convergence shows the performance loss for correlated IRS elements. In addition, we extend the lower bound of the sum DoF proposed for the active IRS with independent channel coefficients to the scenario with correlated channel coefficients, i.e., the sum DoF is the same as independent IRS elements for \min \{M,N\}\le 5 and Q\le 20 , and for other cases, the sum DoF converges to \min \{M,N\} with an order of at least O\left({{}\frac {1}{\sqrt {Q}}}\right) .",[],[]
"Conventional coherent chaos-based communication systems require synchronization of chaotic signals, which is still practically unattainable in a noisy environment. Moreover, in non-coherent schemes, a part of the bit duration is spent sending non-information-bearing reference samples, which deteriorates the Bit Error Rate performance (BER) of these systems. To tackle these problems, this paper designs anM-ary Deep Learning Chaos Shift Keying(M-ary DLCSK) system. The proposed receiver uses a Convolutional Neural Network (CNN)-based classifier that recoversM-ary modulated data. The trained NN model grasps different chaotic maps, estimates channels, and classifies the received signals effectively. Moreover, we consider a Transfer Learning (TL) framework that enhances the noise performance and classification results. Due to the generalization capabilities of TL, the trained NN can work in different Signal-to-Noise Ratio (SNR) conditions without the need for re-training. We compare the BER performance, complexity, and bandwidth efficiency of theM-ary DLCSK receiver with existing receivers. The results demonstrate that theM-ary DLCSK receiver is the first practical system that achieves the theoretical BER performance of the coherent CSK systems under Rayleigh fading channels. Moreover, the proposed system provides a considerable performance advantage compared to the existing DL-based receivers under Rayleigh fading channels. For example, the BER performance of 8-ary DLCSK shows a gain of 0.1 over the Long Short-Term Memory (LSTM)-aided DNN systems at the targetEb/N0=14dB. These features makeM-ary DLCSK an attractive candidate for several applications, such as Massive Multiple-Input Multiple Output (MIMO), Vehicle-to-everything (V2X), Quantum, and optical communication systems.",[],[]
"We propose an optimal destination scheduling scheme to improve the physical layer security (PLS) of a power-line communication (PLC) based Internet-of-Things system in the presence of an eavesdropper. We consider a pinhole (PH) architecture for a multi-node PLC network to capture the keyhole effect in PLC. The transmitter-to-PH link is shared between the destinations and an eavesdropper which correlates all end-to-end links. The individual channel gains are assumed to follow independent log-normal statistics. Furthermore, the additive impulsive noise at each node is modeled by an independent Bernoulli-Gaussian process. Exact computable expressions for the average secrecy capacity (ASC) and the probability of intercept (POI) performance over many different networks are derived. Approximate closed-form expressions for the asymptotic ASC and POI are also provided. We find that the asymptotic ASC saturates to a constant level as transmit power increases. We observe that the PH has an adverse effect on the ASC. Although the shared link affects the ASC, it has no effect on the POI. We show that by artificially controlling the impulsive to background noise power ratio and its arrival rate at the receivers, the secrecy performance can be improved.",[],[]
"We propose and evaluate a technique that learns the probability of a network transmission link experiencing a fault by using outlier flows (in the performance sense) as training data. This technique autonomously determines the most likely links causing performance degradation in a communications network; a critical feature of zero-touch network management. Our new Network Link Outlier Factor (NLOF) with most likely links (NLOF:MLL) is experimentally compared to the existing literature (including our original NLOF) using classification performance measures: recall, precision,F1-score, and time-to-detection. We utilize inferential statistics and a wide set of Mininet experiments to determine statistically significant performance differences. We find that our NLOF:MLL outperforms the existing literature wrt the importantF1-score while exhibiting a competitive time-to-detection.",[],[]
"In this manuscript, the problem of detecting multiple targets and jointly estimating their spatial coordinates (namely, the range, the Doppler and the direction of arrival of their electromagnetic echoes) in a colocated multiple-input multiple-output radar system employing orthogonal frequency division multiplexing is investigated. It is well known its optimal solution, namely the joint maximum likelihood estimator of an unknown number of targets, is unfeasible because of its huge computational complexity. Moreover, until now, sub-optimal solutions have not been proposed in the technical literature. In this manuscript a novel approach to the development of reduced complexity solutions is illustrated. It is based on the idea of separating angle estimation from range-Doppler estimation, and of exploiting known algorithms for solving these two sub-problems. A detailed analysis of the accuracy and complexity of various detection and estimation methods based on this approach is provided. Our numerical results evidence that one of these methods is able to approach optimal performance in the maximum likelihood sense with a limited computational effort in different scenarios.",[],[]
"Multiple-Input Multiple-Output (MIMO) systems have been proposed to increase the capacity and transmission distance of sub-THz and THz communication systems. Although much work has been done to pave the way for arrays and arrays of subarrays using omnidirectional or semi-omnidirectional antenna elements, few works explore the benefits of directional antenna elements in (sub-)THz MIMO systems. This work presents a brief survey of what has been done for sub-THz and THz multiple antenna systems and builds on the previous work to suggest using more directional antenna elements in sub-THz and THz MIMO systems. The first experimental implementation of a sub-THz diversity scheme is also used to verify the findings. Both the numerical and experimental analyses demonstrate that directional antenna elements in sub-THz and THz MIMO can increase the achievable data rates due to their focusing power and the shape of their radiation pattern.",[],[]
"Federated Learning (FL) is a state-of-the-art paradigm used in Edge Computing (EC). It enables distributed learning to train on cross-device data, achieving efficient performance, and ensuring data privacy. In the era of Big Data, the Internet of Things (IoT), and data streaming, challenges such as monitoring and management remain unresolved. Edge IoT devices produce and stream huge amounts of sample sources, which can incur significant processing, computation, and storage costs during local updates using all data samples. Many research initiatives have improved the algorithm for FL in homogeneous networks. However, in the typical distributed learning application scenario, data is generated independently by each device, and this heterogeneous data has different distribution characteristics. As a result, the data stream, often characterized as Big Data, used by each device for local learning is unbalanced and is not independent or identically distributed. Such data heterogeneity can degrade the performance of FL and reduce resource utilization. In this paper, we present the DSS-Edge-FL, a Dynamic Sample Selection optimization algorithm that aims to optimize resources and address data heterogeneity. The extensive results of the experiment demonstrate that our proposed approach outperforms the resource efficiency of conventional training methods, with a lower convergence time and improved resource efficiency.","['Dynamic scheduling', 'Data models', 'Training', 'Heuristic algorithms', 'Big Data', 'Resource management', 'Optimization']","['Federated learning', 'edge computing', 'intelligent edge', 'data streaming', 'big data', 'dynamic resource allocation']"
"A dynamic multi-symbol (DM) flipping scheme is proposed for various symbol flipping decodings of non-binary low-density parity-check (NB-LDPC) codes. This new approach divides the whole decoding process into multiple stages according to the number of iterations and allows a different maximum number of symbols to be flipped in each stage. Numerical analysis reveals that the proposed multi-symbol flipping scheme can yield a higher probability of correct flipping with the dynamic flipping threshold. The proposed DM scheme can be highly parallelized, extensively improving the decoder throughput over existing symbol flipping decoding algorithms based on prediction (SFDP). Numerical results show that the DM scheme can significantly enhance the error correction capability and achieve faster convergence speed with little increase in average computational complexity.",[],[]
"Traditional voting systems mainly comprise of paper polling, electronic ballot system (EVM), mechanical devices, etc., and demand the physical presence of the voters. In the new age of digitization, the electronic voting system has come up with a unique facility to cast votes from any discreet place. However, the e-voting system has to face several challenges regarding security and privacy. To overcome such obstructions, blockchain is introduced in e-voting applications that preserve anonymity, security, and consistency of voter-related information with the help of Merkle tree and hash digest. Hence, any discrepancy can immediately be detected whenever the hash values of the respective block have been modified and consequently, the whole block is discarded. In this research, a novel e-voting scheme is proposed following the decentralized service-oriented architecture of Exonum private blockchain, hybrid consensus algorithm, and Elliptic Curve Diffie-Helmen (ECDH) protocol to agree upon a secure session key among different participants. Moreover, the proposed scheme (ECC-EXONUM-eVOTING) employs a zero-knowledge protocol and is customized to work over idemix technologies with a blind signature scheme. Numerous well-known cryptographic attacks are analyzed formally using the probabilistic random oracle model and informally for validating the security strength of ECC-EXONUM-eVOTING. As a result, it is found that the proposed scheme is well-defended against all potential security concerns. Furthermore, the scheme is simulated using both Automated Validation of Internet Security Protocols and Applications (AVISPA) and Scyther tools to demonstrate the proposed scheme is not prone to any security attacks. Finally, it is concluded that the proposed scheme is well-suited for secure e-voting applications.",[],[]
"Low-density lattice codes (LDLCs) achieve near-capacity performance on additive white Gaussian noise (AWGN) channels. TheM-Gaussian decoder is the state-of-the-art message passing decoder for LDLCs in terms of the error performance. However, this decoder has complexityO(Md−1)with messages represented by Gaussian mixtures, wheredis the degree of an LDLC andMis the number of Gaussian functions for approximating each check node message. In this paper, we establish the correspondence between Gaussian functions for approximating a variable node message and points of a certain lattice. Based on this lattice viewpoint, the problem of approximating a variable node message is formulated as a lattice point enumeration (LPE) problem. Then, an LPE decoder with linear complexityO(d)is proposed. Our simulation results validate that the LPE decoder achieves almost the same error performance as theM-Gaussian decoder.",[],[]
"Secure storage and sharing of Personal Health Records (PHRs) in Internet of Medical Things (IoMT) is one of the significant challenges in the healthcare ecosystem. Due to the high value of personal health information, PHRs are one of the favourite targets of cyber attackers worldwide. Over the years, many solutions have been proposed; however, most solutions are inefficient for practical applications. For instance, several existing schemes rely on the bilinear pairings, which incur high computational costs. To mitigate these issues, we propose a novel PHR-sharing scheme that is dynamic, efficient, and practical. Specifically, we combine searchable symmetric encryption, blockchain technology and a decentralized storage system, known as Inter-Planetary File System (IPFS) to guarantee confidentiality of PHRs, verifiability of search results, and forward security. Moreover, we provide formal security proofs for the proposed scheme. Finally, we have conducted extensive test-bed experiments and the results demonstrate that the proposed scheme can be used in practical scenarios related to IoMT environment.",[],[]
"To achieve low probability of intercept (LPI) in radar networks for multiple target detection, it is necessary to find the optimal assignment of distributed radars to targets. The multi-radar to multi-target assignment (MRMTA) problem aims to find the best radar combination, but its brute-force (BF)-based approach over all possible sensor combinations has exponential complexity, making it challenging to implement in networks with a large number of radars or targets. This limits the implementation of the BF approach in networks that prioritize low latency and complexity. To address this challenge, we propose a supervised machine-learning (ML)-based solution for the MRMTA problem. Our proposed implementation scheme performs the training procedure offline, leading to a significant reduction in assignment complexity and processing latency. We conducted extensive numerical simulations to design an ML structure with high accuracy, convergence speed, and scalability. Simulation results demonstrate the efficiency and effectiveness of our proposed ML-based MRMTA solution, which achieves near-optimal LPI performance with considerably lower computation time than benchmark schemes. Our proposed solution has the potential to optimize the assignment of distributed radars to targets in LPI radar networks and improve the performance of complex networks with low latency and complexity requirements.",[],[]
"The rapidly growing demand for vast numbers of Internet of Things (IoT), in both urban and rural areas, necessitates their ceaseless and automatic energy supply. This is particularly vital in cases where the IoT sensors are deployed in distant or dangerous locations outside human reach. In this direction, unmanned aerial vehicles (UAVs) with wireless power transfer (WPT) capabilities can address this issue, due to their flexible deployment. To this end, we devise an architecture in which a UAV swarm covers the energy demand of an IoT network, while concurrently, the UAVs fulfil their energy needs through a charging station (CS) infrastructure. A practical energy model is considered, which takes into account the UAVs’ battery level, energy consumption due to transition to different locations, hovering, and WPT. Also, to capture the UAV-CS interaction, an economic model is introduced. The UAVs aim to maximize their profit by transferring energy to the IoT, while the CSs aim to maximize their profit by recharging the UAVs. To ensure a profit-wise stable CS-UAV association, while providing energy coverage to the IoT, we formulate a many-to-one matching game. Due to inter-dependencies between UAVs’ utilities, i.e., externalities, a matching algorithm with two-sided exchange-stability is proposed. To further evaluate the considered system, we design an optimization scheme which performs the UAV-CS assignment towards maximizing the energy coverage of the IDs. Numerical results showcase the matching algorithm’s ability to provide near-optimal energy coverage to the IDs, while balancing fairness among the competing agents’ profit, compared to the optimization scheme.",[],[]
"Direct digital synthesis (DDS) architectures are becoming more prevalent as modern digital-to-analog converter (DAC) and programmable logic devices evolve to support higher bandwidths. The DDS architecture provides the benefit of digital control but at a cost of generating spurious content in the spectrum. The generated spurious content may cause intermodulation distortion preventing proper demodulation of the received signal. The distortion may also interfere with the neighboring frequency bands. This article presents the various DDS architectures and explores the DDS architecture which provides the most digital reconfigurability with the lowest spurious content. End-to-end analytical equations, numerical and mathematical models are developed to determine the location and power levels of spurs. Afterwards, the analytical equations, numerical and mathematical models are shown to be consistent with the experimental data. A developer can use the information to design a DDS architecture that meets their minimum requirements.",[],[]
"Many rural and remote areas around the world still lack access to the Internet due to the high deployment and maintenance costs, the lack of infrastructure, and the low income compared to urban regions. Therefore, cost-effective wireless backhaul links are required to connect these areas to the Internet core network. A wireless non-line-of-sight (NLOS) backhaul via diffraction in rural areas is a promising low-cost solution alternative to the traditional LOS backhaul. A reliable and cost-effective NLOS backhaul can be achieved if accurate propagation modelling and performance evaluation are performed before deployment. In this work, we first examine the accuracy of various terrain-based propagation models in predicting diffraction in different rural sites at unlicensed 5.8 GHz band. Although the classic irregular terrain model (ITM) is widely implemented in many radio frequency (RF) design software, we find that it overestimates the actual diffraction loss over small diffraction angles with an average error of −24 dB and RMSE value of 23 dB. However, a less complex rounded-obstacle diffraction model shows very high prediction accuracy compared to measured data with a negligible average error of −0.01 dB and RMSE value of 2.7 dB. Further, we evaluate the feasibility of an end-to-end NLOS backhaul link via simulation based on the IEEE 802.11ac standard using the practical characteristics of commercial RF equipment. The link performance is evaluated via end-to-end simulation in terms of the achieved throughput and the packet error rate (PER) over various antenna heights, modulation, and coding (MCS) schemes, and channel bandwidths. The results show that an NLOS backhaul at a distance of 11 km is viable with a throughput of 100–175 Mbps and PER of 0-0.1.",[],[]
"Aerial base stations can be realized using unmanned aerial vehicles (UAVs)-mounted access points, which offer flexible and rapid network access in scenarios where terrestrial cellular networks are challenged by high traffic loads or insufficient coverage. Visible light communication (VLC) is an emerging technology that can be integrated with UAVs to offer simultaneous communication and illumination services while having low interference, high energy efficiency, and high data rates. To achieve an efficient operation of VLC-enabled UAVs, the power consumption of the UAVs due to their mobility should be considered, as it constitutes a significant portion of the total power consumption of the UAVs, especially when compared to that of the communication needs. This paper proposes a framework to optimize the trajectories of VLC-enabled UAVs, considering a multi-objective optimization problem that jointly maximizes the sum-rate and rate fairness of the users and minimizes the power consumption of the UAVs using particle swarm optimization (PSO) algorithm. The results show that the proposed optimization can effectively improve the performance of the system under various user mobility scenarios and UAV deployment altitudes.",[],[]
"Rate-Splitting Multiple Access (RSMA) has been recognized as an effective technique to reconcile the tradeoff between decoding interference and treating interference as noise in 6G and beyond networks. In this paper, in line with the need for network sustainability, we study the energy-efficient power and rate allocation of the common and private messages transmitted in the downlink of a single-cell single-antenna RSMA network. Contrary to the literature that resorts to heuristic approaches to deal with the joint problem, we transform the formulated energy efficiency maximization problem into a multi-agent Deep Reinforcement Learning (DRL) problem, based on which each transmitted private message represents a different DRL agent. Each agent explores its own state-action space, the size of which is fixed and independent of the number of agents, and shares its gained experience by exploration with a common neural network. Two DRL algorithms, namely the value-based Deep Q-Learning (DQL) and the policy-based REINFORCE, are properly configured and utilized to solve it. The adaptation of the proposed DRL framework is also demonstrated for the treatment of the considered network’s sum-rate maximization objective. Numerical results obtained via modeling and simulation verify the effectiveness of the proposed DRL framework to conclude a solution to the joint problem under both optimization objectives, outperforming existing heuristic approaches and algorithms from the literature.",[],[]
"Non-terrestrial networks (NTNs) complement their terrestrial counterparts in enabling ubiquitous connectivity globally by serving unserved and/or underserved areas of the world. Supporting enhanced mobile broadband (eMBB) data over NTNs has been extensively studied in the past. However, focus on massive machine type communication (mMTC) over NTNs is currently growing. Evidence for this are the work items included into the 3rd generation partnership project (3GPP) agenda for commissioning standards for Internet-of-Things (IoT) communications over NTNs. Supporting mMTC in non-terrestrial cellular IoT (C-IoT) networks requires jointly addressing the unique challenges introduced in NTNs and C-IoT communications. In this paper, we tackle one such issue caused due to the extended round-trip time and increased path loss in NTNs resulting in a degraded network throughput. We propose smarter transport blocks scheduling methods that can increase the efficiency of resource utilization. We conduct end-to-end link-level simulations of C-IoT traffic over NTNs. Our numerical results of throughput show the improvement in performance achieved using our proposed solutions against legacy scheduling methods.",[],[]
"The next evolutionary step in human-computer interfaces will bring forward immersive digital experiences that submerge users in a 3D world while allowing them to interact with virtual or twin objects. Accordingly, various collaborative extended reality (XR) applications are expected to emerge, imposing stringent performance requirements on the underlying wireless connectivity infrastructure. In this paper, we examine how novel multi-antenna coded caching (CC) techniques can facilitate high-rate low-latency communications and improve users’ quality of experience (QoE) in our envisioned multi-user XR scenario. Specifically, we discuss how these techniques make it possible to prioritize the content relevant to wireless bottleneck areas while enabling the cumulative cache memory of the users to be utilized as an additional communication resource. In this regard, we first explore recent advancements in multi-antenna CC that facilitate the efficient use of distributed in-device memory resources. Then, we review how XR application requirements are addressed within the third-generation partnership project (3GPP) framework and how our envisioned XR scenario relates to the foreseen use cases. Finally, we identify new challenges arising from integrating CC techniques into multi-user XR scenarios and propose novel solutions to address them in practice.",[],[]
"mmWave communications are paving the way for next-generation cellular networks due to their inherent ability to provide high data rates and mitigate interference. Coupled with this are the enormous potential and challenges posed by eXtended Reality (XR) applications which are becoming increasingly ubiquitous. In this paper, we leverage the unique characteristics of mmWave networks to re-think and re-design fundamental network architecture and functions in order to meet the strict requirements of deadline-driven XR applications. We propose a multi-tiered multi-connectivity architecture that allows users (UEs) to connect to multiple base stations (gNBs) simultaneously and switch rapidly between them in case of blockages. By replicating UE data at multiple gNBs close to the UE, we ensure that we satisfy strict Quality of Service (QoS) constraints even with unpredictable, dynamic blockages of the mmWave links. We show through extensive system-level simulations that our network architecture allows us to shield UEs from high handover delays and minimizes data plane interruptions in case of blockages. Moreover, we note that existing algorithms for network functions such as gNB selection and scheduling are not optimized for the multi-connectivity paradigm, nor do they specifically cater to strict deadline constraints or intermittent wireless links. We propose a Deep Reinforcement Learning framework that selects gNBs for data replication by explicitly optimizing to meet strict deadline constraints of XR traffic. Our Deep Learning agent analyzes global state information and predicts the best selection of gNBs to preemptively replicate data for future transmissions. Furthermore, we propose a scheduler based on maximal weight matching, dubbed $\beta -$ MWM, which is specifically tailored to exploit multi-connectivity. We show that our Deep Learning based Data Replication Predictor and $\beta -$ MWM scheduler perform better than existing, conventional algorithms and result in markedly better performance for XR applications with strict deadlines.",[],[]
"Communications in the mmWave and THz bands will be a key technological pillar for next-generation wireless networks. However, the increase in frequency results in an increase in path loss, which must be compensated for by using large antenna arrays. This introduces challenging issues due to power consumption, signalling overhead for channel estimation, hardware complexity, and slow beamforming and beam alignment schemes, which are in contrast with the requirements of next-generation wireless networks. In this paper, we propose the adoption of a retro-directive antenna array (RAA) at the user equipment (UE) side, where the signal sent by the base station (BS) is reflected towards the source after being conjugated and phase-modulated according to the UE data. By making use of modified Power Methods for the computation of the eigenvectors of the resulting round-trip channel, it is shown that, in single and multi-user multiple-input multiple-output (MIMO) scenarios, ultra-low complexity UEs can establish parallel communication links automatically with the BS in a very short time. This is done in a blind way, also by tracking fast channel variations while communicating, without the need for ADC chains at the UE as well as without explicit channel estimation and time-consuming beamforming and beam alignment schemes.",[],[]
"Communications and information theory use the GaussianQ-function, a positive and decreasing function, across the literature. Its approximations were created to simplify mathematical study of the GaussianQ-function expressions. This is important since theQ-function cannot be represented in closed-form terms of elementary functions. In a noise model with the Gaussian distribution function and various digital modulation schemes, closed-form approximations of the GaussianQ-function are used to predict a digital communications system’s symbol error probability (SEP) or bit error probability (BEP). Another significant scenario pertains to fading channels, whereby it is important to accurately determine, through a closed-form expression, the precise evaluations of complex integrals involved in the computations of SEP or BEP. In addition to the aforementioned scenarios, it is imperative for a communications system designer to ascertain the requisite operational signal-to-noise ratio for the specific application, based on the target SEP (or BEP). In this scenario, the crucial role of the explicit invertibility of the GaussianQ-function approximation is of significant importance in achieving this objective. In this paper we propose a survey of the approximations of the GaussianQ-function found in the literature, reviewing also the approximations originally given for the 4 classical special functions related to it, restricting the analysis to the explicitly invertible ones, and classifying them on the basis of their accuracy (on the significant range), simplicity, and easiness of inversion, also distinguishing the bounds from approximations. We also list the inverses of some of them, already published or newly found in this research.",[],[]
"The efficient administration of network resources in Multi-Access Edge Computing (MEC) is an active research topic these days. Task sharing, in particular, is one of the fundamental problems regarding MEC architectures although the existing literature approaches the topic mostly from the mobile user point of view. In this article, we present the chronicles of EdgeChain network, a proof of concept demo of a blockchain-based model for secure and private task sharing collaboration specially designed for edge computing servers. EdgeChain operates on a decentralized approach that counts on the blockchain services provided by the Hyperledger Fabric platform. The network offers enhanced security features that leverage the permissioned nature of Fabric, more specifically, it relies on Fabric’s membership service to validate identities and allowed behavior of participant nodes in the network. This design choice restrains external attackers from interfering with the normal operation of the task sharing scheme. The network also offers enhanced privacy features powered by a smart contract design that makes use of multiple decoupled Fabric channels with separate operation rules, ledgers, and peer-to-peer communication networks. This design choice guarantees that the computational tasks circulating in the network are only exposed to servers participating in task sharing services. The trait prevents other servers in the network to have access to private tasks and their data. The article goes through the multiple stages of the design and construction process of the proof of concept demo, from the blockchain-based task sharing framework and system model, all the way to the implementation details of the network.",[],[]
"This paper addresses the problem of scheduling the uplink bandwidth of Low Earth Orbit (LEO) satellites among multiple Internet of Things (IoT) slices with diverse Quality of Service (QoS) requirements. The scheduling process involves twofold decisions—the amount of bandwidth allocated and the allocation duration. Resource scheduling for satellite IoT services is challenging because of limited bandwidth availability during a satellite pass, especially for LEO satellites. Another challenge is to compute a fair allocation schedule for IoT services with different latency demands, packet transmission frequency, required data volume, and the number of IoT devices. To address these challenges, we propose a fairness-aware inter-slice scheduler for satellite IoT services in this work. The proposed scheduler computes service priority based on respective traffic demands. We propose two algorithms for the scheduler based on weighted greedy and Simulated Annealing (SA), respectively. The weighted greedy algorithm schedules the services greedily based on the priority order. The SA algorithm enhances the greedy solution by ensuring the allocated bandwidth is proportional to the respective priority values. The simulation results show that the proposed SA algorithm achieves up to 21.11% more proportional fairness than the Simulated Annealing and Monte Carlo (SA-MC) benchmark scheme.",[],[]
"Today’s communication system design heavily depends on computer simulation for performance evaluation. The burgeoning ultra-reliable communication systems, however, pose a significant simulation challenge as such systems operate at very low packet error rates (PERs) whereas the required simulation time of the conventional Monte Carlo (MC) method is many times the inverse of the PER. Various importance sampling-type techniques have been developed for more efficient simulation of channel-coded transmission, but they typically rely on exploiting code weaknesses (for the generation of error-causing noise samples) and most works on soft-decision decoding only treat binary signaling. In this paper, we propose to use a function, termed the noise gauging function (NGF), that roughly measures the error-causing propensity of noise samples and we present a way to adaptively optimize the noise sampling under such a function for simulation efficiency. Both binary and nonbinary signalings are considered. And the proposed technique does not require detailed knowledge of the code weaknesses, although some high-level understanding of the code properties can benefit the design of efficient NGFs. We investigate the application of the proposed technique to several common channel codes. Numerical results indicate an approximately 10- to 1,000-fold speedup versus MC.",[],[]
"Cell-free massive MIMO precoding leverages the large number of antennas and dense access point (AP) deployment to concurrently serve multiple users in a wide coverage area, thus promising efficient interference mitigation and significant network capacity enhancement. Most existing centralized precoding methods for cell-free massive MIMO systems, whether optimization-based or learning-based, suffer from high computational complexity or expensive communication overhead, introducing additional latency to the network in practical applications. To address the above issues, in this paper we propose two decentralized precoding methods based on the horizontal federated learning (HFL) and vertical federated learning (VFL) frameworks, respectively. In the HFL-based precoding method, we design a low-cost residual global channel state information (CSI) feature acquisition mechanism called RFAM at each AP to create local datasets. RFAM eliminates the need for point-to-point CSI exchange between APs, resulting in reduced communication overhead. In the VFL-based precoding method, each AP utilizes its own CSI for precoding scheme design, thereby eliminating the communication overhead associated with obtaining CSI from other APs. Furthermore, the computational complexity is considerably reduced due to the low overhead of model inference. Experimental results conducted in various channel environments show that the proposed HFL-based method achieves faster convergence rate and outperforms traditional decentralized methods in terms of sum-rate performance. The results also show that the proposed VFL-based method achieves similar sum-rate performance to centralized schemes.",[],[]
"Massive multiple-input-multiple-output (MIMO) is a key enabler for obtaining higher data rates in the next generation wireless technology. While it has the power to transform cellular communication, with potential for spatial diversity and multiplexing, a bottleneck that often gets overlooked is the fronthaul capacity. The fronthaul link that connects a massive MIMO Remote Radio Head (RRH) and carries in-phase and quadrature (IQ) samples to the Baseband Unit (BBU) of the base station can throttle the network capacity/speed if appropriate data compression techniques are not applied, particularly in the uplink. This paper proposes an iterative technique for fronthaul load reduction in the uplink for massive MIMO systems utilizing the convolution structure of the received signals. The proposed algorithm provides compression ratios of about 30- 50\times. This work provides extensive analysis of the performance of the proposed method for a plethora of practical scenarios and constraints, such as different channel parameters and models, receive antenna correlation, and under imperfect channel information. It also discusses the numerical convergence and complexity of the proposed algorithm and compares the performance against other existing compression techniques.",[],[]
"The high altitude platform (HAP) network has been regarded as a cost-efficient solution for providing network access to rural or remote areas. Apart from network connectivity, rural areas are predicted to have demands for diverse real-time intelligent communication services, such as smart agriculture and digital forestry. The effectiveness of real-time decision-making applications depends on the timely updating of sensing data measurements used in generating decisions. As a performance metric capable of quantifying the freshness of transmitted information, the age of information (AoI) can evaluate the freshness-aware performance of the process of updating sensory data. However, unlike urban areas, the available communication resources in rural areas may not allow for maintaining dedicated infrastructures for different types of services, e.g., conventional non-freshness-aware services and freshness-aware real-time services, thereby requiring the proper resource allocation among different services. In this article, we first introduce the anticipated services and discuss the advances of rural networks. Next, a case study on the efficient resource allocation across heterogeneous services characterized by AoI and data rate in HAP networks is presented. We also explore the potential of employing the free-space optical (FSO) backhaul framework to enhance the performance of multi-layer HAP networks. To strike a balance between the AoI and data rate, we develop both static and deep reinforcement learning (DRL)-based dynamic resource allocation schemes to allocate the communication resources provided by HAP networks. The simulation results show that the proposed dynamic DRL-based method outperforms the heuristic algorithm and can surpass the performance ceiling achieved by the proposed static allocation scheme. In particular, our presented method can improve performance by nearly 2.5 times more than the ant colony optimization (ACO) method in terms of weighted sum performance improvements. Some insights on system design and promising future research directions are also given.",[],[]
"Massive multi-input multi-output (MIMO) has been recognized as a key technology for fifth generation (5G) networks. Indeed, it enables to meet the challenging requirements of increased coverage, capacity and massive connectivity. Nevertheless, its performance is limited by channel estimation overhead which scales in time division duplexing (TDD) systems with the number of active users, consequently limiting the system’s efficiency. In this paper, we address the bottleneck of channel estimation in TDD Massive MIMO through optimized lean carrier. We propose an adaptive uplink training scheme that exploits the heterogeneous Doppler spreads of the different users in order to reduce the periodicity of uplink sounding signal transmission. The idea is to enable the network to plan its uplink training decisions for long time periods while taking into consideration user mobility. To this end, we formulate a two time-scale control problem that takes into account the different rates of the wireless channel and location changes. In the fast time scale, an optimized uplink training policy is derived based on estimated user locations. In the slow time scale, positioning decisions are optimized. Simulation results show that the optimized training policies provide considerable improvement in the system efficiency even with partial location knowledge.",[],[]
"Sixth Generation (6G) transceivers are envisioned to feature massively large antenna arrays compared to its predecessor. This will result in even higher spectral efficiency (SE) and multiplexing gains. However, immense concerns remain about the energy efficiency (EE) of such transceivers. This work focuses on partially connected hybrid architectures, with the primary aim of enhancing the EE of the system. To achieve this objective, the study proposes a combined approach of joint antenna selection and precoding, which holds the potential to further optimize the system’s EE while maintaining a satisfactory SE performance levels. The proposed approach incorporates antenna selection based on a meta-heuristic cyclic binary particle swarm optimization algorithm along with successive interference cancellation-based precoding. The results indicate that the proposed solution, in terms of SE and EE, performs very close to the optimal exhaustive search algorithm. This study also investigates the trade-off between SE and EE in a low and high signal-to-noise ratio (SNR) regimes. The robustness of the proposed scheme is also demonstrated when the channel state information is imperfect. In conclusion, this work presents a lower complexity approach to enhance EE in 6G transceivers while maintaining SE performance and along with a reduction in power consumption.",[],[]
"Virtual routers (vRouters) in cloud systems are in great demand due to the ever-increasing network bandwidth. To meet this demand, hardware offload technologies are mandatory for vRouters. Furthermore, offload hardware needs new architecture to achieve a throughput of several hundred Gbps, because memories that store routing tables will create a bottleneck. Therefore, we propose a new hardware architecture using high bandwidth memories (HBMs) and an acceleration engine equipped with multi-pipeline processing. In this paper, we improved the performance of the random memory-accesses from multiple pipelines by distributing to the memory channels of HBMs. In our prototype, vRouter packet processing performances were achieved 320 M packet per second(pps) and 250 Gbit per second (bps). Moreover, the prototype showed low and stable latency of packet transfers that was not affected by background traffic.",[],[]
"This study delves into the potential of harnessing the orbital angular momentum (OAM) property of electromagnetic waves in near-field and line-of-sight scenarios by utilizing large intelligent surfaces, in the context of holographic multiple-input multiple-output (MIMO) communications. The paper starts by characterizing OAM-based communications and investigating the connection between OAM-carrying waves and optimum communication modes recently analyzed for communicating with smart surfaces. Subsequently, it proposes implementable strategies for generating and detecting OAM-based communication signals using intelligent surfaces and optimization methods that leverage focusing techniques. Then, the performance of these strategies is quantitatively evaluated through simulations. The numerical results show that OAM waves while constituting a viable and more practical alternative to optimum communication modes are sub-optimal in terms of achievable capacity.",[],[]
"Beyond 5G (B5G) networks rapidly growing to connect billions of Internet of Things (IoT) devices and the dense deployment of IoT devices leads the large-scale network conflict and obstacles the resource-efficient, which brings a great challenge for network resource management (NRM). To tackle this problem, hypergraph based resource-efficient collaborative reinforcement learning (CRL) was proposed for B5G massive IoT. Firstly, the hypergraph theory based network conflict model was formulated to quantify the conflict degree of the B5G massive IoT. Then, since the conflict-free resource management problem is a combinatorial optimization problem with NP-hard, the resource management based Markov decision process (MDP) model was built for NRM in B5G massive IoT. To reduce the computational load by distributing the training overhead throughout the entire B5G massive IoT and achieve distributed collaborative learning, the federated averaging advantage Actor-Critic (FedAvg-A2C) based resource management is proposed to handle the network conflict-free resource management problem and accelerate the training process. Simulation results show the proposed scheme has high network throughput and the resource-efficient in B5G massive IoT.",[],[]
"The surging capacity demands of 5G networks and the limited coverage distance of high frequencies like millimeter-wave (mmW) and sub-terahertz (THz) bands have led to consider the upper 6 GHz (U6G) spectrum for radio access. However, due to the presence of the existing satellite (SAT) services in these bands, it is crucial to evaluate the impact of the interference of terrestrial U6G stations to SAT systems. A comprehensive study on the aggregated U6G-to-SAT interference is still missing in the literature. In this paper, we propose a stochastic model of interference (SMI) to evaluate the U6G-to-SAT interference, including the statistical characterization of array gain and clutter-loss and considering different interference modes. Furthermore, we propose an approximate geometrical-based stochastic model of interference (GSMI) as an alternative method to SMI when the clutter-loss distribution is unavailable. Our results indicate that given the typical international mobile telecommunication (IMT) parameters, the aggregated interference power toward the geostationary (GEO) SATs, is well below the relevant protection criterion, and we prove numerically that the GSMI method overestimates the aggregated interference power with only 2 dB compared to the SMI method.",[],[]
"Intelligent reflecting surfaces (IRS) have emerged as a promising technology to enhance the performance of wireless communication systems. By actively manipulating the wireless propagation environment, IRS enables efficient signal transmission and reception. In recent years, the integration of IRS with full-duplex (FD) communication has garnered significant attention due to its potential to further improve spectral and energy efficiencies. IRS-assisted FD systems combine the benefits of both IRS and FD technologies, providing a powerful solution for the next generation of cellular systems. In this manuscript, we present a novel approach to jointly optimize active and passive beamforming in a multiple-input-multiple-output (MIMO) FD system assisted by an IRS for weighted sum rate (WSR) maximization. Given the inherent difficulty in obtaining perfect channel state information (CSI) in practical scenarios, we consider imperfect CSI and propose a statistically robust beamforming strategy to maximize the ergodic WSR. Additionally, we analyze the achievable WSR for an IRS-assisted MIMO FD system under imperfect CSI by deriving both the lower and upper bounds. To tackle the problem of ergodic WSR maximization, we employ the concept of expected weighted minimum mean squared error (EWMMSE), which exploits the information of the expected error covariance matrices and ensures convergence to a local optimum. We evaluate the effectiveness of our proposed design through extensive simulations. The results demonstrate that our robust approach yields significant performance improvements compared to the simplistic beamforming approach that disregards CSI errors, while also outperforming the robust half-duplex (HD) system considerably.",[],[]
"A novel implicit transmission with bit flipping (ITBF) technique is introduced to transmit a coded stream implicitly while transmitting a coded stream explicitly over a channel. ITBF flips a set of chosen parity bits of the explicitly transmitted stream according to an implicit stream. Numerical results obtained using the low density parity check (LDPC) code employed in the 5G standard show that ITBF can transmit an implicit stream up to 13.19% of the rate of transmission of the explicit stream without significantly sacrificing performance, or increasing the decoding complexity or the decoding delay. The ITBF is combined with collection of punctured code decoding (CPCD) to form implicit transmission with collection decoding (ITCD) schemes that can further increase the rate of transmission on the implicit stream without increasing the decoding delay, however, with a slight increase in the decoding complexity. It is demonstrated with the LDPC code in the WiFi standard that ITCD can transmit an implicit stream at up to 25% of the rate of transmission of the explicit stream.",[],[]
"Capacity of uncoordinated multiple access channel (MAC) with single-user demodulation has been studied in literature under various assumptions for symbol-synchronous transmission. This paper considers symbol-asynchronous uncoordinated transmission on a binary continuous-time noiseless MAC. It first proposes and develops a new multi-user interference analysis approach that in turn allows exact evaluation of the mutual information on the multi-access channel. Consequently, the sum-capacity of the considered binary symbol-asynchronous uncoordinated MAC is determined.",[],[]
"From an information-theoretic perspective, we investigate performance of asymmetrically clipped optical OFDM (ACO-OFDM) in intensity modulated optical wireless systems. Our focus is how much the information rate of ACO-OFDM can be boosted by receivers which utilize information contained in even-indexed subcarriers rather than only observing the modulated (odd-indexed) subcarriers as in the conventional receiver. In the presence of time-dispersion, the maximum achievable gain of such improved receivers in information rate is characterized by a multi-letter (vector) conditional mutual information, which is difficult to evaluate. Under independent complex Gaussian input symbols, we derive computationally tractable lower and upper bounds on the maximum achievable gain, which contain only single-letter expressions of conditional mutual information or conditional differential entropy. At low SNR, the gap between our lower and upper bounds vanishes asymptotically. By numerical studies, it is shown that time dispersion significantly reduces the maximum achievable gain at low SNR. At high SNR, our lower bounds and asymptotic analysis reveal that the achievable gain is still evident, measured in terms of the proportion to that in the case of no time dispersion.",[],[]
"Network slicing enables operators to cost-efficiently support diverse applications on a common physical infrastructure. The ever-increasing densification of network deployment leads to complex and non-trivial inter-cell interference, which requires more than inaccurate analytic models to dynamically optimize resource management for network slices. In this paper, we develop a DRIP algorithm with multiple deep reinforcement learning (DRL) agents to cooperatively optimize resource partition in individual cells to fulfill the requirements of each slice, based on two alternative reward functions with max-min fairness and logarithmic utility. Nevertheless, existing DRL approaches usually tie the pretrained model parameters to specific network environments with poor transferability, which raises practical deployment concerns in large-scale mobile networks. Hence, we design a novel transfer learning-aided DIRP (TL-DIRP) algorithm to ease the transfer of DRIP agents across different network environments in terms of sample efficiency, model reproducibility, and algorithm scalability. The TL-DIRP algorithm first centrally trains a generalized model and then transfers the “generalist” to each local agent (a.k.a., the “specialist”) with distributed finetuning and execution. TL-DIRP consists of two steps: 1) centralized training of a generalized distributed model, and 2) transferring the “generalist” to each local agent with distributed finetuning and execution. We comprehensively investigate different types of transferable knowledge: model transfer, instance transfer, and combined model and instance transfer. We evaluate the proposed algorithms in a system-level network simulator with 12 cells. The numerical results show that not only DIRP outperforms existing baseline approaches in terms of faster convergence and higher reward, but more importantly, TL-DIRP significantly improves the service performance, with reduced exploration cost, accelerated convergence rate, and enhanced model reproducibility. As compared to a traffic-aware baseline, TL-DIRP provides about 15% less violation ratio of the quality of service (QoS) for the worst slice service and 8.8% less violation on the average service QoS.",[],[]
"This paper examines the separation of wireless communication and radar signals, thereby guaranteeing cohabitation and acting as a panacea to spectrum sensing. First, considering that the channel impulse responses were known by legitimate receivers (communication and radar), we showed that the optimizing beamforming weights mitigate the interference caused by signals and improve the physical layer security (PLS) of the system. Furthermore, when the channel responses were unknown, we designed an interference filter as a low-complex noise and interference cancellation autoencoder. By mitigating the interference on the legitimate users, the PLS was guaranteed. Results showed that even for a low signal-to-noise ratio, the autoencoder produces low root-mean-square error (RMSE) values.",[],[]
"Reuse-1 systems operating in the sub-1 GHz UHF band are limited by substantial co-channel interference (CCI). In such orthogonal frequency division multiple access (OFDMA) cellular systems, the inter-sector or inter-tower interference (ITI) makes accurate signal recovery quite challenging as sub-1 GHz bands only support single-input single-output (SISO) links. Interference-aware receiver algorithms are essential to mitigate the ITI in such low-frequency bands. Such algorithms enable ubiquitous mobile broadband access over the entire homeland, say with >95% geographical coverage with quality of service guarantees. One element of the interference-aware signal recovery is the least-squares-based joint channel estimation scheme that uses non-orthogonal pilot subcarriers. This estimator is then compared with a variant that uses orthogonal pilot subcarriers to bring out the advantage of this joint estimator. It is shown that the proposed joint estimator requires fewer pilots to be well-determined when compared to its under-determined orthogonal counterpart. Moreover, it is easy to implement and does not require any knowledge of channel statistics. This work also derives a compensation factor needed for the interference-aware detector in the presence of inter-carrier interference (ICI) originating from multiple transmitters. Simulation results show that the proposed joint channel estimator outperforms traditional estimators at moderate to high frequency selectivity. The proposed compensation factor to the joint detector is found to be essential for recovering the transmitted signal in the absence of phase-tracking pilots.",[],[]
"Intelligent reflecting surface (IRS) is a promising concept for 6G wireless communications that allows tuning of the wireless environments to increase spectral and energy efficiency. Many optimization techniques have been proposed in literature to deal with the joint passive and active beamforming design problem, but without any optimality guarantees for the multiple access points (APs), multiple IRSs, and multiple users scenario. Moreover, the multiple access problem is also considered with the beamformer design which has not been addressed in literature, except in the context of joint transmission, which is not considered herein. To further maximize ground based and support non-terrestrial communications, the joint aerial IRS (AIRS) positioning and beamformer design problem is also considered. In the first part of the paper, an algorithm considering predefined AP-user pairing is proposed, which allows beamforming vectors to be designed distributively at each access point by using Generalized Bender Decomposition (GBD), consequently resulting in certain level of optimality. The problem can be transformed via mathematical manipulation and semidefinite relaxation (SDR) into a convex problem and solve using semidefinite programming (SDP). Another algorithm was developed to solve for optimal AP-user pairing at the same time by introducing additional binary variables, making the problem into a mixed-integer SDP (MISDP) problem, which is solved using GBD-MISDP solver, albeit with higher computational and time complexity than the GBD for the original problem. A heuristic pairing algorithm, called GBD-iterative link removal (GBD-ILR), is proposed to combat this problem and it is shown to achieve solution close to that of the GBD-MISDP method. A joint AIRS positioning and beamformer design problem is solved in the second part by using the proposed successive convex approximation-alternating direction of method of multipliers-GBD (SAG) method. Simulation results show the effectiveness of all proposed algorithms for joint beamformer design, joint beamformer design with AP-user pairing in a multiple access points system, and the joint AIRS positioning and beamformer design. In addition to simulation results, an analysis of communication overhead incurred due to use of the IRS is also given.",[],[]
"Joint communication and sensing (JCAS) is of special interest due to densification in various contexts like wireless networks, airspace, and transportation networks. In this paper, a JCAS approach using a single multi-mode multi-port antenna (M3PA) element is proposed. Towards this goal, circular beamforming is applied in order to separate communication and sensing channels, using the unique capabilities of M3PA s. The sensing possibilities using only a single M3PA element are evaluated assuming compressive sensing and MIMO radar signal processing including the polarization domain. Simulation results for an unmanned aerial vehicle scenario indicate a good separation of the communication link and sensing system at the cost of a slightly increased angular error.",[],[]
"With the growth of terminal devices and data traffic, privacy concerns have inspired an innovative edge learning framework, called federated learning (FL). Over-the-air computation (OAC) has been introduced to reduce communication overhead for FL, however, requires stringent time alignment. Misaligned OAC has been proposed by recent research where the symbol-timing misaligned superimposed signal can be recovered via whitening matched filtering and sampling (WMFS), followed by maximum likelihood (ML) estimation. Similarly to aligned OAC, misaligned OAC also suffers from the straggler issue, leading to FL’s poor performance under low EsN0. To solve this issue, we propose a novel framework of misaligned OAC FL for accurate model aggregation on wireless networks. First, we analyze the effect of aggregation error on the convergence of FL. Then, we formulate an optimization problem to minimize the distortion of the aggregation measured by mean square error (MSE) w.r.t. the transmitter equalization and receiver combining. Finally, a successive convex approximation (SCA)-based optimization algorithm is further developed to solve the resulting quadratic constrained quadratic programming. Comprehensive experiments show that the proposed algorithm achieves substantial learning performance improvement compared to existing baseline schemes and achieves the near-optimal performance of the ideal benchmark with aligned and noiseless aggregation.",[],[]
"The downlink (DL) of a reconfigurable intelligent surface (RIS)-aided multi-user (MU) millimeter wave (mmWave) multiple-input multiple-output (MIMO) system relying on a non-diagonal RIS (NDRIS) phase shift matrix is considered. A max-min fairness (MMF) problem is formulated under the total transmit power constraint while employing joint active hybrid beamforming (HBF) both at the base station (BS) as well as at each user equipment (UE), and passive beamforming at the NDRIS. To solve this non-convex problem, a sequential optimization method is conceived, wherein the UE having the poorest channel is identified first, which is termed as the worst-case UE. Then the phase shifter coefficients of the NDRIS are optimized using the alternating direction method of multipliers (ADMM) followed by the hybrid transmit precoder (TPC) and receiver combiner (RC) design using the Karcher mean, the least squares and the regularized zero forcing (RZF) principles. Finally, the optimal power allocation is computed using the path-following algorithm. Simulation results show that the proposed NDRIS-HBF system yields an improved worst-case UE rate in comparison to its conventional diagonal RIS (DRIS)-HBF counterpart, while approaching the half-duplex relay (HDR)-HBF benchmark for large values of the number of reflecting elements (REs). Furthermore, the energy efficiency (EE) of the NDRIS structure is significantly higher than that of the DRIS, HDR systems, while being higher than that achieved by the full-duplex relay (FDR) system at high SNR.",[],[]
"The reliability of ultrasonic intra-body communication (IBC) is crucial for medical monitoring, diagnosis, and treatment. Recently, ultrasonic splitting receivers that jointly use coherent detection (CD) and non-coherent energy detection (ED) obtain superior bit-error rate (BER) performance compared to a single CD or ED receiver for reliable IBCs. However, this paper demonstrates that existing splitting receivers have performance gains in terms of mutual information and BER in the case of independent channel noises for CD and ED, while no performance gains exist in the case of correlated channel noises. To this end, we propose an ultrasonic index modulation-splitting-maximum ratio combination (UsIM-S-MRC) system. UsIM activates a part of transmission frames whilst S-MRC only processes active frames and treats inactive frames as zero, reducing the average channel noise in all frames. Therefore, the joint-designed UsIM-S-MRC effectively mitigates the impact of channel noise correlation on splitting detection. The theoretical mutual information, BER, and optimal splitting ratio are also derived. Extensive simulations validate the theoretical analysis and reveal that UsIM-S-MRC can attain performance gain in both independent and correlated channel noises, providing a reliable receiver structure for future ultrasonic IBCs.",[],[]
"Hierarchical Federated Learning (HFL) has emerged to overcome the shortcomings of conventional Federated Learning (FL) due to communication obstacles between the end users and the cloud server and the congestion at the backhaul of wireless network implementations. In this paper, we consider a wireless user-edge-cloud HFL network where the transmissions of the users’ local model parameters to the edge are multiplexed via the Non-Orthogonal Multiple Access (NOMA) technique. The joint problem of association and uplink transmission power allocation of the users to the edge is formulated and solved as a non-cooperative game in satisfaction form. Diverging from the prevailing research that proposes centralized solution concepts, each user makes autonomous decisions regarding its association and power level so as to attain a minimum acceptable tradeoff of three vital network factors. The latter includes the global model’s training accuracy and the users’ consumed energy and time during transmission. Different types of equilibria are explored, i.e., the Satisfaction Equilibrium (SE) and Minimum Efficient Satisfaction Equilibrium (MESE) which not only fulfills users’ minimum tradeoff but also minimizes the overall network’s cost. Algorithms based on Reinforcement Learning (RL) and Best Response Dynamics (BRD) are, then, devised to conclude the SE and MESE points. The proposed framework is evaluated via modeling and simulation, verifying its efficiency in achieving an equitable balance in the network.",[],[]
"We consider the multi-user detection (MUD) problem in uplink grant-free non-orthogonal multiple access (NOMA), where the access point has to identify the total number and correct identity of the active Internet of Things (IoT) devices and decode their transmitted data. We assume that IoT devices use complex spreading sequences and transmit information in a random-access manner following the burst-sparsity model, where some IoT devices transmit their data in multiple adjacent time slots with a high probability, while others transmit only once during a frame. Exploiting the temporal correlation, we propose an attention-based bidirectional long short-term memory (BiLSTM) network to solve the MUD problem. The BiLSTM network creates a pattern of the device activation history using forward and reverse pass LSTMs, whereas the attention mechanism provides essential context to the device activation points. By doing so, a hierarchical pathway is followed for detecting active devices in a grant-free scenario. Then, by utilising the complex spreading sequences, blind data detection for the estimated active devices is performed. The proposed framework does not require prior knowledge of device sparsity levels and channels for performing MUD. The results show that the proposed network achieves better performance compared to existing benchmark schemes.",[],[]
"In free-space optical satellite networks (FSOSNs), satellites connected via laser inter-satellite links (LISLs), latency is a critical factor, especially for long-distance inter-continental connections. Since satellites depend on solar panels for power supply, power consumption is also a vital factor. We investigate the minimization of total network latency (i.e., the sum of the network latencies of all inter-continental connections in a time slot) in a realistic model of a FSOSN, the latest version of the Starlink Phase 1 Version 3 constellation. We develop mathematical formulations of the total network latency over different LISL ranges and different satellite transmission power constraints for multiple simultaneous inter-continental connections. We use practical system models for calculating network latency and satellite optical link transmission power, and we formulate the problem as a binary integer linear program. The results reveal that, for satellite transmission power limits set at 0.5 W, 0.3 W, and 0.1 W, the average total network latency for all five inter-continental connections studied in this work levels off at 339 ms, 361 ms, and 542 ms, respectively. Furthermore, the corresponding LISL ranges required to achieve these average total network latency values are 4500 km, 3000 km, and 1731 km, respectively. Different limitations on satellite transmission power exhibit varying effects on average total network latency (over 100 time slots), and they also induce differing changes in the corresponding LISL ranges. In the absence of satellite transmission power constraints, as the LISL range extends from the minimum feasible range of 1575 km to the maximum feasible range of 5016 km, the average total network latency decreases from 589 ms to 311 ms.",[],[]
"Matrix multiplication is a fundamental operation in various algorithms for big data analytics and machine learning. As the size of the dataset increases rapidly, it is now a common practice to distribute the computation on multiple servers. As straggling servers are inevitable in a distributed infrastructure, various coding schemes have been proposed to tolerate potential stragglers. However, as resources are shared with other jobs in a distributed infrastructure and their performance can change dynamically, the optimal way of encoding the input matrices is not static. So far, all existing coding schemes require encoding input matrices in advance, and cannot change the coding schemes or adjust their parameters flexibly. In this paper, we propose a framework that can change the coding schemes and/or their parameters by locally re-encoding the coded task on each server. We first present this framework for entangled polynomial codes, which changes the coding parameters with marginal overhead and saves job completion time. We then extend the framework for matrices with bounded entries, achieving a higher level of flexibility for local re-encoding while maintaining better numerical stability.",[],[]
"When fully implemented, sixth generation (6G) wireless systems will constitute intelligent wireless networks that enable not only ubiquitous communication but also high-accuracy localization services. They will be the driving force behind this transformation by introducing a new set of characteristics and service capabilities in which location will coexist with communication while sharing available resources. To that purpose, this survey investigates the envisioned applications and use cases of localization in future 6G wireless systems, while analyzing the impact of the major technology enablers. Afterwards, system models for millimeter wave, terahertz and visible light positioning that take into account both line-of-sight (LOS) and non-LOS channels are presented, while localization key performance indicators are revisited alongside mathematical definitions. Moreover, a detailed review of the state of the art conventional and learning-based localization techniques is conducted. Furthermore, the localization problem is formulated, the wireless system design is considered and the optimization of both is investigated. Finally, insights that arise from the presented analysis are summarized and used to highlight the most important future directions for localization in 6G wireless systems.",[],[]
"Recently, long reach x-digital subscriber line (LR-xDSL) has been proposed to extend the reach of conventional DSL systems. The extended loop lengths are characterized by a longer channel impulse response (CIR), which requires a similarly longer cyclic prefix (CP) to successfully eliminate the inter-symbol interference (ISI) between successive time-domain discrete multi-tone (DMT) symbols and inter-carrier interference (ICI) between the carriers or tones of the same DMT symbol. This adds a large overhead to the transmitted symbols and results in throughput loss. A per-tone equalizer (PTEQ) is an attractive alternative to deal with extended loop lengths. However, it imposes a large initialization computational complexity and memory requirement, hindering the use of a PTEQ in practical multiple-input multiple-output (MIMO) scenarios. To tackle this problem, a specific structure in the MIMO DSL channel, namely that the combined ISI and ICI signal power from the crosstalk channels is significantly lower than the desired and combined ISI and ICI signal power from the direct channels, may be exploited in deriving a novel low complexity/memory solution, here referred as sparse MIMO PTEQ, with negligible impact (≈ 0.5% drop) on performance compared to a full MIMO PTEQ. For a conventional DSL binder size of 16 lines and a PTEQ order of 3, the proposed sparse MIMO PTEQ performs at 42% of the initialization computational complexity and 29.7% of the memory requirement, with negligible performance degradation, compared to a full MIMO PTEQ. The initialization computational complexity and memory requirement is further reduced by the proposed diagonal MIMO PTEQ which operates at 0.4% of the initialization computational complexity of a full MIMO PTEQ and requires 6.25% memory compared to a full MIMO PTEQ, with some degradation in the performance compared to the full MIMO PTEQ. The diagonal MIMO PTEQ has the additional benefit that it can be applied in both upstream and downstream scenarios, in contrast to the full and sparse MIMO PTEQ which can be used only in upstream scenarios.",[],[]
"This paper presents a machine learning-based framework for the predictive deployment of unmanned aerial vehicles (UAVs) as flying base stations (BSs) to offload heavy traffic from ground BSs. To account for time-varying traffic distribution, a long short-term memory (LSTM)-based prediction algorithm is introduced to predict future cellular traffic. A joint K-means and expectation maximization (EM) algorithm based on Gaussian mixture models (GMM) is proposed to determine the service area of each UAV based on the predicted user service distribution. Based on the predicted traffic, the optimal positions of UAVs are derived, and four multiple access techniques, namely, rate splitting multiple access (RSMA), frequency domain multiple access (FDMA), time domain multiple access (TDMA), and non-orthogonal multiple access (NOMA), are compared to minimize the total transmit power. Simulation results show that the proposed method can reduce up to 24% of the total power consumption compared to the conventional method without traffic prediction. Furthermore, RSMA is found to require the lowest transmit power among the four multiple access techniques. Therefore, this paper focuses on the comparison of multiple access techniques for UAV deployment, which is essential for the efficient and effective use of UAVs as flying BSs.",[],[]
"Motivated by the demand of cheap hardware and low computational complexity in massive connectivity, we propose a scheme in which transmitters send data encoded by Bloom filter with On-Off Keying (OOK) modulation and receiver performs hard-decision envelope detection on received signals. For frequency-flat fading scenarios with inter-user synchronization (IUS), we develop a Noisy-Combinatorial Orthogonal Matching Pursuit (NCOMP) decoding strategy, and for frequency-selective fading scenarios with IUS, we propose an improved decoding strategy, called multipath-NCOMP (MNCOMP). In addition, we propose a sliding window method to adapt the two decoding strategies to scenarios without IUS. Based on a many-access channel (MnAC) model, we study the asymptotic performance of the proposed scheme for activity recognition and message transmission tasks. Theoretical analysis guarantees that the error probability of our scheme vanishes asymptotically with the number of users, and also shows that the MNCOMP strategy is superior to the NCOMP strategy in frequency-selective fading MnACs. In addition, the proposed sliding window method can ensure that our scheme is immune to the lack of IUS. Numerical experiments corroborate our asymptotic analytical results, for finite number of users.",[],[]
"The effect of electromagnetic radiation on public health is a recurring topic in the societal and political discourse, peaking with the introduction of every new generation of cellular technology. With the introduction of 5G, promising high peak download speeds as well as high-power beams, there is a need to revisit traditional measurement approaches. ICNIRP, a.k.a the International Committee on Non-Ionising Radiation Protection, offers a useful guideline to evaluate the electromagnetic field exposure of living tissues and provides some limits to keep exposure well below the threshold where it is considered harmful. However, modern packet radio technologies such as 5G or Wi-Fi are different from old broadcasting technologies. They deliver high power in very short bursts, spread over a wide band, thus increasing the difficulty of measuring electric fields with traditional instruments, such as spectrum analyzers. In addition, 5G promises a high spatial focusing performance, which means that the field can vary significantly even in a small area. Hence, measurements with a higher spatial density than we can achieve with expensive and bulky spectrum analyzers are urgently needed. Software-defined radios (SDRs), as a size- and cost-efficient alternative, can be used to capture signals in the time domain and thus increase measurement accuracy. However, software-defined radios are not designed to be used as RF power meters. They require accurate calibration and data analysis to ensure the measured power is correct. The aim of this work is to provide a general framework to calibrate SDRs, enabling them to measure RF power and extract the corresponding electric field value. Subsequently, the influence of the SDR parameters on the accuracy of the electric field measurement is investigated. To assess the performance of the proposed calibration framework in a real-life scenario, we rely on our private 5G network with a calibrated SDR to measure the RF power from a 5G network. Our measurements show that the average electric field exposure of 5G networks is well below 1 V/m.",[],[]
"If a wireless channel is said wide-sense stationary (WSS), the mean and autocorrelation of its small-scale fading are position invariant. This occurs when the channel is rich in propagation paths – their complex summation averages out any position-specific characteristics. Extensive measurement campaigns have validated the microwave channel to be WSS – it is inherently rich in diffracted paths, the 1G to 4G systems that operate there employ omnidirectional antennas which detect paths from all directions, and the systems feature narrow bandwidths which sum paths over long sample periods. Popular millimeter-wave (mmWave) channel models assume the channel is WSS without measurement-based validation even though the channel is inherently sparse due to weak diffraction, and the 5G systems that operate there employ pencilbeam antennas and feature ultrawide bandwidths. In fact, a recent measurement campaign showed the 60 GHz channel is non-WSS at narrow beamwidths and wide bandwidths, however the campaign considered only one measured channel and only a discrete set of beamwidths and bandwidths. For comprehensive analysis, in this paper we measured 88 channels over three indoor and two outdoor environments with our 60 GHz channel sounder and varied the beamwidth and bandwidth continuously to find the crossover points when the channel transitions from WSS to non-WSS.",[],[]
"This paper studies the application of neural networks to Viterbi detection of FTN signals in an intersymbol interference (ISI) channel. The main contribution of this paper is to propose a receiver structure for detecting FTN signals in unknown static ISI channel. In particular, we propose a novel low-complexity neural network structure for calculating the branch metrics, and we explore its suitability for FTN signalling with channel uncertainty. We compare the proposed network, which we call the Metric Net (MetNet), to a benchmark neural network-based technique for metric calculation, the ViterbiNet, which was originally designed for ISI channels. The simulation results confirm that the MetNet outperforms the ViterbiNet, with two orders of magnitude lower complexity, and is much more resilient to channel uncertainty than the traditional Viterbi detector, which uses Euclidean distance for metric calculations. We further show that the MetNet exhibits robustness to being trained at mismatched SNR values and FTN pulse acceleration factors, meaning that the number of trained models required can be significantly reduced. Additionally, the results show that the proposed MetNet remains a favorable alternative at much higher levels of channel uncertainties. The results also reflect that we can generalize the MetNet to work with different channel models defined by different decaying factors. Finally, we show that we succeed in achieving a bandwidth efficiency gain of 33% due to FTN by using the MetNet in the presence of channel uncertainty.",[],[]
"Guaranteeing operational connectivity in emergency situations by means of prompt on-demand network re-configuration is crucial to carry out effective public protection and disaster relief actions. This article analyzes network configuration options enhanced by the integrated access backhaul (IAB) feature for 5G-Advanced and beyond mission-critical services. We specifically aim to investigate the possible interplay of sidelink communications, directional unicast transmissions, and multicasting. To this end, we offer a practical methodology based on a fluid model approximation to capture the time dynamics of mission-critical services in their transient phase. Simulation results show that the interplay of multicasting, unicasting, and sidelink is a highly effective solution for mission-critical communications. In contrast, standalone unicast, multicast, and sidelink transmissions are unable to support such services. A further emerging aspect is that, in a mixed unicast-multicast-sidelink configuration, optimal results are obtained when multicast exploits most of the available resources, specifically more than 70%.",[],[]
"The choice of modulation techniques for next-generation communications in the sub-terahertz and terahertz bands remains largely unresolved. A variety of traditional and new schemes show promise, but the question remains open on the illustrative comparison process for realistic terahertz systems. While there are some preliminary studies in this area, we emphasize that the peculiarities of terahertz hardware necessitate a scheme that (i) is resistant to system-wide phase noise (PN) and (ii) has a low peak-to-average power ratio (PAPR). Therefore, in this article, we present a comprehensive methodology to carefully model and jointly study the impacts of PN and PAPR on the performance of candidate modulations for terahertz links. We first deliver a mathematical model for sub-terahertz/terahertz phase noise impairments at 130 GHz, 225 GHz, and 1.02 THz based on measurements from actual terahertz hardware. We then introduce the PAPR penalty – an approach for fair comparison of bit error rate (BER) and spectral efficiency (SE) of modulation schemes with different PAPRs. We finally combine these two effects to comprehensively study the characteristics of single and multi-carrier modulation schemes for terahertz communications. Our study reveals that analyzing PAPR and PN jointly is paramount: accounting for only one leads to major deviations in the numerical results and misleading conclusions on best modulation choice(s). The delivered framework and evaluation should facilitate further studies, leading to a well-motivated selection of the most suitable modulation scheme(s) for prospective sub-terahertz and terahertz radio systems.",[],[]
"The use of filter banks for implementing multicarrier spread spectrum systems leads to a class of effective waveforms that are highly resilient to partial-band interferers. Such waveforms can be also designed to keep the peak-to-average power ratio (PAPR) of the resulting signal at a minimum level. The use of multiple spreading gain vectors (known as multicodes), on the other hand, is an effective method for increasing the data rate in spread spectrum systems, in general. This paper presents a detailed analysis of a class of filter bank multicarrier spread spectrum (FBMC-SS) waveforms and demonstrates an effective receiver implementation of them when multicodes are applied. Application of the developed multicode waveform for communications over high-frequency (HF) skywave channels is also explored, and the benefits that it provides are studied both numerically, through computer simulations, and experimentally, by examining the receivers performance over a variety of skywave links.","['Receivers', 'Peak to average power ratio', 'Transmitters', 'Complexity theory', 'Synchronization', 'Symbols', 'Government']","['Filter bank multicarrier', 'spread spectrum communications', 'HF communications', 'multicode methods']"
"Generalized Spatial Modulation (GSM) enables a trade-off between very high spectral efficiencies and low hardware costs for massive MIMO systems. This is achieved by transmitting information via the selection of active antennas from a set of available antennas besides the transmission of conventional data symbols. GSM systems have been investigated concerning various aspects like suitable signal constellations, efficient detection algorithms, hardware implementations, spatial precoding, and error control coding. On the other hand, determining the capacity of GSM is challenging because no closed-form expressions have been found so far. This paper investigates the mutual information for different GSM variants. We consider a multilevel coding approach, where the antenna selection and IQ modulation are encoded independently. Combined with multistage decoding, such an approach enables low-complexity capacity-achieving coded modulation. The influence of the data symbols on the mutual information is illuminated. We analyze the portions of mutual information related to antenna selection and the IQ modulation processes which depend on the GSM variant and the signal constellation. Moreover, the potential of spatial modulation for massive MIMO systems with many transmit antennas is investigated. Especially in systems with many transmit antennas much information can be conveyed by antenna selection.",[],[]
"Extremely large-scale antenna arrays, tremendously high frequencies, and new types of antennas are three clear trends in multi-antenna technology for supporting the sixth-generation (6G) networks. To properly account for the new characteristics introduced by these three trends in communication system design, the near-field spherical-wave propagation model needs to be used, which differs from the classical far-field planar-wave one. As such, near-field communication (NFC) will become essential in 6G networks. In this tutorial, we cover three key aspects of NFC. 1) Channel Modelling: We commence by reviewing near-field spherical-wave-based channel models for spatially-discrete (SPD) antennas. Then, uniform spherical wave (USW) and non-uniform spherical wave (NUSW) models are discussed. Subsequently, we introduce a general near-field channel model for SPD antennas and a Green’s function-based channel model for continuous-aperture (CAP) antennas. 2) Beamfocusing and Antenna Architectures: We highlight the properties of near-field beamfocusing and discuss NFC antenna architectures for both SPD and CAP antennas. Moreover, the basic principles of near-field beam training are introduced. 3) Performance Analysis: Finally, we provide a comprehensive performance analysis framework for NFC. For near-field line-of-sight channels, the received signal-to-noise ratio and power-scaling law are derived. For statistical near-field multipath channels, a general analytical framework is proposed, based on which analytical expressions for the outage probability, ergodic channel capacity, and ergodic mutual information are obtained. Finally, for each aspect, topics for future research are discussed.","['Antennas', 'Antenna arrays', '6G mobile communication', 'Transmitting antennas', 'Channel models', 'Wireless networks', 'Tutorials']","['Antenna architecture', 'beamforcusing', 'channel modelling', 'near-field communications', 'performance analysis']"
"Index assignment (IA) is a low-complexity joint source-channel coding technique that has the potential for use in low-latency and low-power applications, such as wireless sensor networks (WSNs). Though binary IA has been extensively studied for assigning binary indices to quantized codewords (or symbols) under the assumption of binary symmetric channels (BSCs), real-world scenarios often useM-ary modulations. Directly applying binary IAs designed for BSCs toM-ary modulations results in suboptimal performance. In this paper, we investigate theM-ary IA, which assignsM-ary labels to quantized codewords (or symbols), assuming the use of a equiprobable lattice quantizer. For such a system, we derive a tight performance bound and propose a near-optimal IA scheme based on a two-step design. In addition, we propose explicit IA constructions for practical modulation schemes, including PAM, QAM, and PSK. Our proposed IA design is rigorously proven to be optimal for 3-PSK and QPSK, whereas for larger modulation orders, the proposed IA constructions approach the bounds within small gaps. Our simulations show that the constructed IA scheme can achieve significant energy savings compared to the conventional binary IA scheme. Specifically, in some WSN scenarios, the proposed IA for 16-QAM is shown to achieve significant reductions in energy consumption relative to the conventional binary counterpart.",[],[]
"In this paper, the problem of active eavesdropper detection is considered for a cell-free massive multiple-input multiple-output (m-MIMO) system which is attacked by an active eavesdropper within the uplink training phase, also called pilot spoofing attack. Two methods based on log-likelihoodratio tests (LLRT), one in a centralized and the other in a decentralized fashion, are proposed to detect the signal abnormality. The methods take advantage of a special protocol in which the legitimate users switch to an off-mode irregularly, without significantly affecting the spectral efficiency of the data transmission. The protocol is directly applicable to environments with low to moderate mobility, and can be extended to high mobility through a simple rearrangement of available pilot sequences among users if needed. More importantly, the proposed methods impose low fronthaul overhead which is imperative for a cellfree m-MIMO system with a large number of access points (APs). A closed-form expression for the joint probability density function (PDF) of the processed received signals conditioned on the alternative hypothesis, which is essential for the implementation of LLRT-based detection methods, is also derived. Through an asymptotic analysis, it is shown for the proposed methods that the detection and false-alarm probabilities approach to one and zero, respectively as the number of APs goes to infinity. Numerical results reveal that both methods significantly outperform a recent approach in terms of false-alarm rate with negligible degradation in the per user uplink spectral efficiency.",[],[]
"The use of multiple-input multiple-output (MIMO) non-orthogonal multiple access (NOMA) based communication protocols is proposed and investigated for the uplink of wireless networks with buffered data-sources, which is the basis of the introduced medium access control (MAC)-layer protocol. To this end, the long-term average throughput is maximized by optimizing the set of users that transmit information at each time slot and their transmit power, the number of packets that are admitted in each user’s queue, and the transmission rates, assuming that the instantaneous channel state information is not available at the transmitters. Also, considering a receiver with multiple antennas, two detection techniques are used to mitigate the interference when two users are chosen to simultaneously transmit information in the same resource block, namely successive interference cancellation (SIC) and joint decoding (JD). More specifically, the outage probability for both considered techniques is derived in closed-from, which is a prerequisite for the derivation and the optimization of the throughput. The formulated multi-dimensional long-term stochastic optimization problem is solved by using the Lyapunov framework. Finally, simulation results verify the gains by using MIMO-NOMA as the basis of the next generation multiple access and illustrate the superiority of JD compared to SIC with respect to the number of the receiver’s antennas.",[],[]
"Smart cities have seen a growing interest among governments, researchers, and industries. Smart cities use digital technologies to enhance the quality of life for residents while promoting sustainability and efficient resource management. By integrating various technologies such as Information and Communication Technologies (ICT), Artificial Intelligence (AI), and Internet of Things (IoT), smart cities can improve the delivery of public services, optimize transportation systems, reduce energy consumption, and enhance public safety, among other benefits. Smart cities focus on automating different disciplines, including smart environment, smart home, smart economy, smart mobility, and smart governance. As a result, multi-agent driven smart cities have received tremendous attention from the research community for obtaining intelligent solutions to complex problems in different disciplines by subdividing responsibilities into multiple agents and empowering agents through AI. In this regard, it is vital to explore the usage of multi-agent systems in different critical application areas of smart cities. In this paper, a detailed description of the multi-agent process for smart city application areas is provided, along with resources and future research directions. Four different application areas: smart home, smart governance, smart environment, and smart mobility are discussed in detail.",[],[]
"Non-fungible tokens (NFTs) have become an exciting technology that provides a fresh perspective on asset ownership, provenance, and value exchange. NFTs, a blockchain-based technology, are distinct and indivisible cryptographic tokens used to confirm and record the ownership of digital and physical assets in an immutable and transparent way. The fundamental block of NFT is a smart contract built on a blockchain network. This contract contains specific information about the asset it represents, such as its unique identifier, metadata, and ownership details. The information is kept private and tamper-proof due to the decentralized and distributed structure of the blockchain, boosting faith in the token’s authenticity. The NFT is gaining popularity, but it is still in the developing stage. There is a need for a comprehensive survey to guide future research and development in NFTs. Thus, this paper presents the technical components of NFTs, their features, and the minting process. Further, this survey paper describes different token standards for NFTs. It presents various applications of NFTs in healthcare, supply chain, gaming, identity verification, agriculture, intellectual property, smart cities, charity and donation, and education. The article also emphasizes the significant difficulties faced currently in implementing NFT technology from the viewpoints of ownership, governance, and property rights, as well as security, privacy, and environmental effects. This work also elucidates the future directions to overcome the challenges in adopting NFTs in various applications.",[],[]
"Massive machine type communication (mMTC) is expected to support the connections of billion devices, where a sporadic short packet transmission is a key feature for mMTC applications. Conventional pilot-based coherent schemes suffer from a huge resource overhead as the pilot occupies a large portion in the short packet transmission. To avoid the overhead and latency caused by pilot symbols and mitigate the adverse effect of wireless fading, we develop a noncoherent single-input multiple-output (SIMO) framework with modulation on conjugate-reciprocal zeros (MOCZ) for short packet transmission. A novel low-complexity noncoherent Viterbi-like detector is proposed, which can exploit diversity of receive antennas by jointly testing the zeros of the received MOCZ symbol over independent channel realizations. Simulation results demonstrate that the proposed detector can outperform the pilot-based OFDM systems and get better performance than the direct zero-testing detector for short packet transmissions in frequency-selective fading environments.",[],[]
"In this work, we delve into the typical errors that frequently arise in communication and storage systems, including deletion, insertion, substitution, and adjacent transposition errors. To effectively address these errors, a novelq-ary code construction(q≥2)which consists of three constraints is proposed. Significantly, the devised codes mark the initial venture intoq-ary code design, exhibiting2logqn+4redundancy symbols in rectifying a single deletion, insertion, substitution, or adjacent transposition error. This work also provides a meticulous mathematical analysis of the design of the proposed code, especially how our proposed code can distinguish the substitution and adjacent transposition error scenarios. In addition, a comprehensive decoding procedure for all error scenarios is also proposed.",[],[]
"We study communication systems over band-limited Additive White Gaussian Noise (AWGN) channels in which the transmitter’s output is constrained to be symmetric binary (bipolar). We improve the available Ozarow-Wyner-Ziv (OWZ) lower bound on capacity which is based on peak-power constrained pulse-amplitude modulation, by introducing new schemes (achievability) with two advantages over the studied OWZ schemes. Our schemes achieve a moderately improved information rate and they do so with much fewer sign transitions of the binary signal. The gap between the known upper bound, which is based on spectral constrains of bipolar signals, and our new achievable lower bound is reduced to 0.93 bits per Nyquist interval at high SNR.",[],[]
"We consider transmission of packets over queue-length sensitive unreliable links, where packets are randomly corrupted through a noisy channel whose transition probabilities are modulated by the queue-length. The goal is to characterize the capacity of this channel. We particularly consider multipleaccess systems, where transmitters dispatch encoded symbols over a system that is a superposition of continuous-time GIk/GI/1 queues. A server receives and processes symbols in order of arrivals with queuelength dependent noise. We first determine the capacity of single-user queue-length dependent channels. Further, we characterize the best and worst dispatch processes for GI/M/1 queues and the best and worst service processes for M/GI/1 queues. Then, the multiple-access channel capacity is obtained using point processes. When the number of transmitters is large and each arrival process is sparse, the superposition of arrivals approaches a Poisson point process. In characterizing the Poisson approximation, we show that the capacity of the multiple-access system converges to that of a single-user M/GI/1 queue-length dependent system, and an upper bound on the convergence rate is obtained. This implies that the best and worst server behaviors of single-user M/GI/1 queues are preserved in the sparse multiple-access case.",[],[]
"In this work, we present symbol error performance analysis of probabilistic shaping (PS) for wireless communications in noise-limited and fading channels. Two fading models considered are the Rayleigh and log-normal fading channels. The results are corroborated with simulation and compared with the conventional uniformly distributed input symbols. In all channel conditions, PS results in significant reductions in the SNR required to achieve a specific error probability compared to the conventional uniformly shaped symbols. For example, in a noise-limited channel, PS based quadrature amplitude modulation (QAM) signal results in SNR gains of 1.16 dB, 1.41 dB, and 1.52 dB compared to the uniformly distributed QAM symbols at equal entropy rates of 4, 6, and 8 bit/symbol and a symbol error ratio (SER) of1×10−3.",[],[]
"The high data rates required for next generation applications necessitate the use of millimeter wave and terahertz bands where bandwidth is abundant. Due to high path loss in these bands, antenna arrays (AAs) are needed to focus the signal in highly directional beams in the desired directions. However, highly directional communication links in these bands are vulnerable to misalignment and blockages due to mobility. Thus, both mobile blockers and user equipment (UE) rotations can significantly increase the handover (HO) frequency, while HO delays and HO failures jeopardize system latency and reliability. Furthermore, the scanning angle of each AA is limited by the orientation of the device, the mounting angle of the AA, the element spacing and the grating sidelobes formed during beamforming, which is referred as Field-of-View (FoV). In scenarios with UE rotational mobility, limited FoV may lead to loss of connection with the source next-generation NodeB (gNB). In this paper, we analyze current HO and radio link monitoring protocols in scenarios with UE rotational mobility and mobile blockers. We use a Markov Chain based analytical model as well as MATLAB based system level simulations to show how user rotation can significantly increase the outage duration under various deployment configurations. We propose enhancements to enable faster HO under user rotations and demonstrate significant performance improvements using simulations.",[],[]
"It has been known that the insufficiency of linear coding in achieving the optimal rate of the general index coding problem is rooted in its rate’s dependency on the field size. However, this dependency has been described only through the two well-known matroid instances, namely the Fano and non-Fano matroids, which, in turn, limits its scope only to the fields with characteristic two. In this paper, we extend this scope to demonstrate the reliance of linear index coding rate on fields with characteristic three. By constructing two index coding instances of size 29, we prove that for the first instance, linear coding is optimal only over the fields with characteristic three, and for the second instance, linear coding over any field with characteristic three can never be optimal. Then, a variation of the second instance is designed as the third index coding instance of size 58. For this instance, it is proved that while linear coding over any field with characteristic three cannot be optimal, there exists a nonlinear code over the fields with characteristic three, which achieves its optimal rate. Connecting the first and third index coding instances in two specific ways, called no-way and two-way connections, will lead to two new index coding instances of size 87 and 91, for which linear coding is outperformed by nonlinear codes. Another main contribution of this paper is the reduction of the key constraints on the space of the linear coding for the first and second index coding instances, each of size 29, into a matroid instance with the ground set of size 9, whose linear representability is dependent on the fields with characteristic three. The proofs and discussions provided in this paper through using these two relatively small matroid instances will shed light on the underlying reason causing the linear coding to become insufficient for the general index coding problem.",[],[]
"This work investigates the performance of a reconfigurable intelligent surface (RIS) aided communication system under ultra-reliable low-latency communication (URLLC) constraints, where the secrecy performance for communication with multiple legitimate users(D), scheduled one at a time, in presence of eavesdropper(E)is analyzed. The outage probability and block error rate (BLER) atDandEare derived for infinite and finite blocklength transmissions assuming that the direct communication links between source(S)−DandS−Eexist. The expressions for the asymptotic outage probability, secrecy capacity, secrecy outage probability and secure BLER are also obtained. The new expressions for the probability density function (PDF) and the cumulative distribution function (CDF) for the difference of phases of two Nakagami−mdistributed channel envelopes are derived. To validate the correctness of the derived analytical expressions and to analyze the impact of various system parameters including the number of RIS meta-atoms, the magnitude of reflection coefficient, transmit signal-to-noise ratio (SNR) threshold, and quantized phase-shifts, Monte-Carlo simulations are used. The performance of the proposed system is compared with that of the decode and forward relay-based system. It is also shown that RIS significantly improves the performance atD, whereas degrading the same forE.",[],[]
"Intelligent reflecting surface (IRS)-assisted communications technology is currently considered a key enabler for various wireless applications. The maximum gain of IRS is achieved when the phases of the reflected signals are optimally selected to maximize signal-to-noise ratio (SNR). However, practical hurdles such as imperfect phase estimation and hardware limitations such as phase quantization can reduce the potential gain of the IRS deployment. Internet of Things applications are more vulnerable to such limitations due to restrictions on device size, energy, cost, and computational power. Therefore, this work evaluates the joint impact of quantization and imperfect phase estimation where the probability density function (PDF) of the estimated and quantized phase is derived. Then, using the sinusoidal addition theorem, the PDF of the received signal envelope is derived and used to derive exact analytic expressions for the symbol error rate and outage probability. The analytical and simulation results obtained show that the impact of the joint estimation and quantization imperfections depends on the SNR and number of IRS elements. In particular, it is shown that increasing the number of IRS elements can effectively mitigate the impact of phase estimation and quantization problems. Furthermore, the results show that the impact of phase quantization increases as the accuracy of phase estimation decreases.",[],[]
"The extensive availability of spectrum resources and the remarkably high data transmission rate of millimeter-wave (mmWave) technology have propelled its significance as a vital component in the advancement of mobile communications, including fifth generation (5G) networks. However, the intermittent nature of mmWave links and their interaction with transport layer protocols pose several challenges, which bring inadequate performance, due to fluctuations in high-frequency channels. Consequently, although these features of mmWave might be advantageous, they can actually hinder the performance. Although these issues have been studied in the literature with TCP, there are few works that have studied how QUIC behaves over this kind of channels. This paper aims to compare the performance of TCP and QUIC over mmWave channels, studying the impact at the application level. We conduct extensive performance evaluations, based on traces that are obtained by means of a detailed simulation of different mmWave scenarios, using the ns-3 simulator. We analyze key performance indicators, such as delay, throughput, and bottleneck buffer. The results evince that QUIC outperforms TCP in highly fluctuating mmWave channels, showing better performance for both throughput and delay.",[],[]
"In this research, we present a cooperative communication network based on two energy-harvesting (EH) decode-and-forward (DF) relays that contain the harvest-store-use (HSU) architecture and may harvest energy from the surrounding environment by utilizing energy buffers. To improve the performance of this network, an opportunistic routing (OR) algorithm is presented that takes into account channel status information, relay position, and energy buffer status, as well as using the maximal ratio combining (MRC) at the destination to combine the signals received from the source and relays. The theoretical expressions for limiting distribution of energy stored in infinite-size buffers are derived from the discrete-time continuous-state space Markov chain model (DCSMC). In addition, the theoretical expressions for outage probability, throughput, and per-packet time slot cost in the network are obtained utilizing both the limiting distributions of energy buffers and the probabilities of transmitter candidates set. Through numerous simulation results, it is demonstrated that simulation results match with corresponding theoretical results.",[],[]
"A critical concern within the realm of visible light communications (VLC) pertains to enhancing system data rate, particularly in scenarios where the direct line-of-sight (LoS) connection is obstructed by obstacles. The deployment of meta-surface-based simultaneous transmission and reflection reconfigurable intelligent surface (STAR-RIS) has emerged to combat challenging LoS blockage scenarios and to provide 360∘ coverage in radio-frequency wireless systems. Recently, the concept of optical simultaneous transmission and reflection reconfigurable intelligent surface (OSTAR-RIS) has been promoted for VLC systems. This work is dedicated to studying the performance of OSTAR-RIS in detail and unveiling the VLC system performance gain under such technology. Specifically, we propose a novel multi-user indoor VLC system that is assisted by OSTAR-RIS. To improve the sum rate performance of the proposed system, both power-domain non-orthogonal multiple access (NOMA) and rate splitting multiple access (RSMA) are investigated in this work. To realize this, a sum rate maximization problem that jointly optimizes the roll and yaw angles of the reflector elements as well as the refractive index of the refractor elements in OSTAR-RIS is formulated, solved, and evaluated. The maximization problem takes into account practical considerations, such as the presence of non-users (i.e., blockers) and the orientation of the recipient’s device. The sine-cosine meta-heuristic algorithm is employed to get the optimal solution of the formulated non-convex optimization problem. Moreover, the study delves into the sum energy efficiency optimization of the proposed system. Simulation results indicate that the proposed OSTAR-RIS RSMA-aided VLC system outperforms the OSTAR-RIS NOMA-based VLC system in terms of both the sum rate and the sum energy efficiency.",[],[]
"The development of cellular networks is driving the rapid growth of wireless communications. With the advent of the 5th Generation (5G) towards the future of the 6th Generation (6G) dedicated to achieving strong growth in traffic while reducing energy consumption, there is a need to solve the problems facing leveraging of these networks’ advantages and support both operators and mobile users. The main challenges for wireless communications are power consumption, Quality of Service (QoS), and the blind areas of a Non-Line-Of-Sight (NLOS) between mobile users and the Base Station (BS). The Intelligent Reflective Surface (IRS) of a reconfigurable meta material is a promising solution for solving some of the challenges of wireless communications. Additionally, it enhances the QoS of the received signals without the need for a power source to operate. Hence, it does not constitute an additional burden as it consists of passive elements. From the other hand, it provides the perfect solution to cover mobile users in blind areas without the need to deploy extra expensive BSs. In this work, we propose to equip buses by IRS allowing them to act as mobile IRS. These buses will become a relay for the surrounding moving vehicles, represented as taxies in the performance section of this paper. Practically speaking, not all buses have to be IRS equipped. We propose various approaches for selecting the best buses equipped with IRS. In the first optimization approach, we adapt the classical IRS selections methods used in static context to the mobile case. It uses a Multi Integer Linear Programming (MILP) which gives optimal results but with a very long processing time. Thus, we propose a neural-network to learn the result of the MILP. As an alternative solution, another approach is proposed using a Markov decision problem (MDP) relying on Long Short-Term Memory (LSTM) to predict the positions of the surrounding moving vehicles. It is used to solve the optimization problem with the performance criteria targeted for each session. The performance of the proposed approaches are validated based on bus and taxi dataset for the city of Rome in Italy.",[],[]
"In this paper, a dynamic unmanned aerial vehicle (UAV)-based heterogeneous network (HetNet) equipped with directional terahertz (THz) antennas is studied to solve the problem of transferring massive traffic of distributed small cells to the core network. To this end, we first characterize a detailed three-dimensional (3D) modeling of the dynamic UAV-assisted HetNet, by taking into account the random positions of small cell base stations (SBSs), spatial angles between THz links, real antenna pattern, and UAV’s vibrations in the 3D space. We then formulate the problem for UAV trajectory to minimize the maximum outage probability (OP) of directional THz links. Then, using geometrical analysis and deep reinforcement learning (RL) method, we propose several algorithms to find the optimal trajectory and select an optimal pattern during the trajectory. For a network with slow time changes, we also propose a deep RL framework to solve the joint optimal UAV positioning and antenna pattern control. The simulation results confirm that the UAV trajectory or antenna pattern control is not enough to achieve acceptable performance, and the UAV should control its antenna patterns during the trajectory to manage the interference.",[],[]
"High-bandwidth signals are needed in many applications like radar, sensing, measurement and communications. Especially in optical networks, the sampling rate and analog bandwidth of digital-to-analog converters (DACs) is a bottleneck for further increasing data rates. To circumvent the sampling rate and bandwidth problem of electronic DACs, we demonstrate the generation of wide-band signals with low-bandwidth electronics. This generation is based on orthogonal sampling with sinc-pulse sequences inNparallel branches. The method not only reduces the sampling rate and bandwidth, at the same time the effective number of bits (ENOB) is improved, dramatically reducing the requirements on the electronic signal processing. In proof of concept experiments the generation of analog signals, as well as Nyquist shaped and normal data will be shown. In simulations we investigate the performance of 60 GHz data generation by 20 and 12 GHz electronics. The method can easily be integrated together with already existing electronic DAC designs and would be of great interest for all high-bandwidth applications.",[],[]
"This paper derives approximate outage probability (OP) expressions for uplink cell-free massive multiple-input-multiple-output (CF-mMIMO) systems with and without pilot contamination. The system’s access points (APs) are considered to have imperfect channel state information (CSI). The signal-to-interference-plus-noise ratio (SINR) of the CF-mMIMO system is approximated via a Log-normal distribution using a two-step moment matching method. OP and ergodic rate expressions are derived with the help of the approximated Log-normal distribution. For the no-pilot contamination scenario, an exact expression is first derived using conditional expectations in terms of a multi-fold integral. Then, a novel dimension reduction method is used to approximate it by the sum of single-variable integrations. Both the approximations derived for the CF-mMIMO systems are also useful for single-cell collocated massive MIMO (mMIMO) systems and lead to closed-form expression. The derived expressions closely match the simulated numerical values for OP and ergodic rate.",[],[]
"This paper investigates the gain of parallel processing, when mobile edge computing (MEC) is implemented in dense wireless networks. In this scenario users connect to several access points (APs), and utilize the computation capability of multiple servers at the same time. This allows a balanced load at the servers, with the eventual cost of decreased spectrum efficiency. The problem of sum transmission energy minimization under response time constraints is considered and proved to be non-convex. The complexity of optimizing a part of the system parameters is investigated, and based on these results an iterative resource allocation procedure is proposed that converges to a local optimum. The performance of the joint resource allocation is evaluated by comparing it to lower and upper bounds defined by less or more flexible multi-cell MEC architectures. The results show that the free selection of the AP is crucial for achieving decent system performance. The average level of parallel processing is in general low in dense systems, but it is an important option for the rare, highly unbalanced instances.",[],[]
"Radar based sensing and communication systems sharing a common spectrum have become a potential research problem in recent years due to spectrum scarcity. The spectrum sharing radar (SSR) is a new technology that uses the total available bandwidth (BW) for both radar based sensing and communication. Unlike traditional radar, the SSR divides the total available BW into radar-only and mixed-use bands. In a radar-only band, only radar sensor signals can be transmitted and received. In contrast, radar and communication signals can both be transmitted and received in the mixed-use band. Taking such BW sharing into account, this paper investigates the performance of SSR in an information-theoretic sense. To evaluate performance, mutual information (MI), spectral efficiency (SE) and capacity (C) metrics are used. Initially, this paper considered a clean environment (no multipath) in order to evaluate performance metrics in the mixed-use band with and without successive interference cancellation. Following that, this paper addresses the performance of BW allocation by allocating low to high BW in mixed-band. Furthermore, the performance metrics are extended to account for the multipath environment, and the same analogy as in a clean environment is used. In addition, the MI and SE of traditional radar system is taken into account when comparing the performance of SSR with and without the use of the SIC. Finally, MI and capacity results show that using the SIC scheme in a mixed-use band yields performance comparable to traditional radar and communication system. In terms of SE, the SSR with SIC scheme outperforms traditional radar and communication system.",[],[]
"As an essential technology in the evolution towards full duplex, the subband full duplex (SBFD) operation has attracted significant attention from both the academic and industrial communities, as well as global standardization bodies for its capability to increase the user perceived throughput (UPT) and the uplink coverage, reduce the transmission latency and support various traffics with different requirements in the same cell in time division duplexing (TDD) bands. In this paper, the model of a SBFD based communication system is clearly presented, along with the interference. To facilitate the early deployment of SBFD in 5G-Advanced and 6G networks, comprehensive performance evaluations on UPT, latency and coverage under different scenarios, different SBFD patterns and different traffics are conducted to explore the potential of the SBFD operation. Furthermore, a field test is carried out to verify the practical performance of SBFD in a typical outdoor scenario. Results obtained from both the simulations and field tests demonstrate significant performance gain for SBFD compared with traditional division duplexing schemes, especially for uplink transmissions.",[],[]
"Underwater optical wireless communication (UOWC) systems face significant challenges due to oceanic turbulence and pointing errors (PE), which can degrade system performance. Moreover, interference is another challenge in UOWC, considering the consistent increase in underwater optical devices. In this study, the performance of rate splitting multiple access (RSMA)-based UOWC systems is investigated over an exponential-generalized gamma (EGG)-distributed oceanic turbulence channel with generalized PE. The RSMA scheme is employed to facilitate communication between a source and multiple users in the UOWC system while accounting for the combined effects of oceanic turbulence and generalized PE. The statistical characterization of the UOWC system is analyzed, including the probability density function (PDF) of the signal-to-noise ratio (SNR), outage probability, throughput, and sum ergodic capacity. Additionally, closed-form expressions for the outage probability and throughput are derived, and asymptotic expressions for the outage probability in the high SNR regime are provided. Moreover, the diversity order of the system is evaluated, and the impact of different parameters on the system performance is discussed. Our results demonstrate the effectiveness of the RSMA-based UOWC system in mitigating the adverse effects of oceanic turbulence and PE while achieving improved performance in terms of capacity and outage probability. The findings of this study provide valuable insights for the design and optimization of UOWC systems in challenging underwater environments.",[],[]
"We propose frequency-domain interleaver for discrete Fourier transform spread orthogonal division multiplexing (DFT-s-OFDM) based on a linear- or quadratic permutation polynomial (LPP/QPP). Interleaving the Fourier coefficients (i.e., the DFT precoder output) implies that the modulation symbols become transmitted over both time- and frequency domain, which is beneficial over time-frequency selective channels. Despite that the single-carrier property is lost due to the interleaving, the peak-to-average-power ratio (PAPR) can be improved. The results show that a QPP can suppress the error floor of the bit/block error rate (BER/BLER) which occurs on channels with large Doppler spread and simultaneously reduce the PAPR. An LPP primarily decreases the PAPR, especially for BPSK, where the gain is several dB. We derive criteria of how to analytically determine the QPPs and the LPPs.",[],[]
"In this paper, a novel waveform with low peak-to-average power ratio (PAPR) and high robustness against phase noise (PN) is presented. It follows the discrete Fourier transform spread orthogonal frequency division multiplexing (DFT-s-OFDM) signal model. This scheme, called 3MSK, is inspired by continuous-phase frequency shift keying (CPFSK), but it uses three frequencies in the baseband model – specifically, 0 and $\pm f_{symbol}/4$ , where $f_{symbol}$ is the symbol rate – which effectively constrains the phase transitions between consecutive symbols to 0 and $\pm \pi /2$ rad. Motivated by the phase controlled model of modulation, different degrees of phase continuity can be achieved, allowing to reduce the out-of-band (OOB) emissions of the transmitted signal, while supporting receiver processing with low complexity. Furthermore, the signal characteristics are improved by generating an initial time-domain constant envelope signal at higher than the symbol rate. This helps to reach smooth phase transitions between 3MSK symbols, while the information is encoded in the phase transitions. Also the possibility of using excess bandwidth is investigated by transmitting additional non-zero frequency bins outside the active frequency bins of the basic DFT-s-OFDM model, which provides the capability to greatly reduce the PAPR. The most critical tradeoffs of the oversampled schemes are that improved PAPR is achieved with the cost of somewhat reduced link performance and, in case of excess band, also the spectrum efficiency is reduced. Due to the fact that the information is encoded in the phase transitions, a receiver model that tracks the phase variations without needing reference signals is developed. To this end, it is shown that this new modulation is well-suited for non-coherent receivers, even under strong phase noise (PN) conditions, thus allowing to reduce the overhead of reference signals. Evaluations of this physical-layer modulation and waveform scheme are performed in terms of transmitter metrics such as PAPR, OOB emissions and achievable output power after the power amplifier (PA), using a practical PA model. Finally, coded radio link evaluations are also provided, demonstrating that 3MSK has a similar bit error rate (BER) performance as that of traditional quadrature phase-shift keying (QPSK), but with significantly lower PAPR, higher achievable output power, and the possibility of using non-coherent receivers.",[],[]
"In this paper, we design a positioning information (PI) based codebook (PI-CB) for reconfigurable intelligent surface (RIS) passive beamforming (PBF), in online stage. In which, corresponding to each reflection pattern (RP), the current user equipment (UEs) PI, which are widely and easily available in communication networks, are stored. Moreover, for further overhead reduction, and based on the stored PI, we propose a partial PI-CB scheme for PBF of the RIS, where a group of candidate RPs, that are highly probable to serve the UEs, is determined depending on a distance metric. Consequently, the system can select the best RP by training only this group of RPs instead of searching the entire PI-CB. The proposed full and partial PI-CB schemes highly reduce the overhead and complexity, especially in large RIS. Also, they outperform channel estimation and alternating optimization (CE&AO) based and randomly generated CB based schemes in terms of effective achievable rate, particularly in rapidly changing channels. The proposed schemes can obtain about 35% increase in the effective achievable rate compared to CE&AO based scheme in 4 users’ scenario with extremely low complexity. In addition, using the partial PI-CB, further reduction in overhead and complexity can be achieved with a slight decrease in performance due to the positioning error.",[],[]
"The current 5th Generation-New Radio (5G-NR) systems are designed to meet the demands of various applications, offering high data rates, low latency, and high reliability. In the current 5G-NR systems, the Discrete Fourier Transform-spread-Orthogonal Frequency Division Multiplexing (DFT-s-OFDM) waveform is the most commonly employed waveform to transmit user data in uplink transmissions, specifically in coverage-limited scenarios. To enable coherent data demodulation, pilot signals, commonly referred to as Demodulation Reference Signals (DMRS), are transmitted along with data-carrying DFT-s-OFDM symbols. However, in the current DFT-s-OFDM architecture, the DMRS and data are transmitted on distinct OFDM symbols. This time separation necessitates the demodulation process to commence only after receiving the initial DMRS symbol, resulting in significant processing delays. Furthermore, the existing architecture consumes more DMRS resources and necessitates complex time interpolation techniques to support users with high mobility. To address these challenges, we propose an improved DFT-s-OFDM architecture that enables instantaneous data demodulation, leading to a substantial reduction in processing delays. Furthermore, despite not utilizing any time interpolation techniques, the proposed method effectively caters to high-speed users, thereby conserving computational resources and hence contributing to minimizing the system complexity and latency. We thoroughly investigate the proposed architecture and evaluate its performance in different simulation settings. The results demonstrate that the proposed architecture significantly improves packet error performance, particularly in high Doppler scenarios, while using 16% lesser DMRS overhead. This reduction in DMRS overhead frees up additional resources for data transmission, ultimately enhancing data rates.",[],[]
"In this paper, we consider a decode-and-forward (DF) buffer-aided (BA) multi-relay system using double differential (DD) encoding and decoding for the transmission and reception of data packets. The proposed system does not require carrier frequency offset (CFO) information and channel state information (CSI) at any communicating node. A priority-based max link selection protocol has been adopted to select links based on the buffer status and channel quality. The Markov-chain approach is used for developing the state transition matrix, using which the steady state probability of the system is obtained. The outage probability and average bit error rate (ABER) expressions are derived using the steady-state probability. The proposed setup performance is then compared with the conventional coherent BA system. It is established that the considered setup is affected by a signal-to-noise ratio (SNR) penalty of≈3dB, which is considerably lower than the well-known 6 dB SNR penalty existing for DD modulation compared to coherent modulation. Also, the proposed setup outcompetes the coherent max-link approach in the presence of a CFO.",[],[]
"This work investigates the proactive eavesdropping based legitimate surveillance for integrated satellite-terrestrial relay networks with multiple proactive monitors. To study the eavesdropping non-outage probability, we propose three different presentative proactive eavesdropping cases/modes, where the legitimate monitors could change its roles between eavesdropping and jamming, namely, Case I: Eavesdropping then Jamming, Case II: Jamming then Eavesdropping, Case III: Always Eavesdropping. Particularly, we get the accurate expressions of the eavesdropping non-outage probability for the proposed three proactive eavesdropping modes in the presence of multiple monitors. To derive further insights, we provide the asymptotic analysis of the three eavesdropping non-outage probabilities. Besides, the colluding proactive eavesdropping scenario is considered, in which all the monitors cooperate to overhear or jam the legitimate users. Finally, numerical results are obtained to verify the rightness of the analytical analysis.",[],[]
"As the cloud moves from monolithic infrastructure to a self-isolated cloud native microservice environment, automation is becoming an important aspect for the management of the application life cycle. In this context, there are many tools available that can monitor these applications and raise alarms. However, automated orchestration is still in its early stages, and the available solutions are not capable of monitoring the whole system from application to hardware levels and performing automated operations within the system. Moreover, IP Multimedia Subsystem (IMS), which is the core part of the Telecom industry, has switched to a microservice environment. These IMS services are critical and need to be proactively monitored to provide automated orchestration operations. In this paper, we address the aforementioned problem by proposing a new scheme for monitoring the metrics from different sources and proactively and automatically performing orchestration using machine learning while improving the scalability of the cloud native Virtual IMS. Experiments carried out with a real cloud-native IMS running in a kubernetes cluster explore the relevance, efficiency and scalability of the proposed scheme.",[],[]
"Line-of-sight link blockages represent a key challenge for the reliability and latency of millimeter wave (mmWave) and terahertz (THz) communication networks. To address this challenge, this paper leverages mmWave and LiDAR sensory data to provide awareness about the communication environment and proactively predict dynamic link blockages before they occur. This allows the network to make proactive decisions for hand-off/beam switching, enhancing the network reliability and latency. More specifically, this paper addresses the following key questions: (i) Can we predict a line-of-sight link blockage, before it happens, using in-band mmWave/THz signal and LiDAR sensing data? (ii) Can we also predict when this blockage will occur? (iii) Can we predict the blockage duration? And (iv) can we predict the direction of the moving blockage? For that, we develop machine learning solutions that learn special patterns of the received signal and sensory data, which we call pre-blockage signatures , to infer future blockages. To evaluate the proposed approaches, we build a large-scale real-world dataset that comprises co-existing LiDAR and mmWave communication measurements in outdoor vehicular scenarios. Then, we develop an efficient LiDAR data denoising algorithm that applies some pre-processing to the LiDAR data. Based on the real-world dataset, the developed approaches are shown to achieve above 95% accuracy in predicting blockages occurring within 100 ms and more than 80% prediction accuracy for blockages occurring within one second. Given this future blockage prediction capability, the paper also shows that the developed solutions can achieve an order of magnitude saving in network latency, which further highlights the potential of the developed blockage prediction solutions for wireless networks.",[],[]
"This paper studies the coexistence between a downlink multiuser massive multi-input-multioutput (MIMO) communication system and MIMO radar. The performance of the massive MIMO system with maximum ratio (MR), zero-forcing (ZF), and protective ZF (PZF) precoding designs is characterized in terms of spectral efficiency (SE) and by taking the channel estimation errors and power control into account. The idea of PZF precoding relies on the projection of the information-bearing signal onto the null space of the radar channel to protect the radar against communication signals. We further derive closedform expressions for the detection probability of the radar system for the considered precoding designs. By leveraging the closed-form expressions for the SE and detection probability, we formulate a power control problem at the radar and base station (BS) to maximize the detection probability while satisfying the per-user SE requirements. This optimization problem can be efficiently tackled via the bisection method by solving a linear feasibility problem. Our analysis and simulations show that the PZF design has the highest detection probability performance among all designs, with intermediate SE performance compared to the other two designs. Moreover, by optimally selecting the power control coefficients at the BS and radar, the detection probability improves significantly.",[],[]
"This study investigates the resource management problem for millimeter-wave-based switched beam (SWB) full-duplex small cell networks with the consideration of user equipment’s (UE’s) quality of experience (QoE) requirement and time-varying wireless channels. An optimization problem is formulated to maximize the long-term QoE by implementing beam assignment (BA) and rate control (RC) under short-term beam and long-term energy efficiency constraints. By leveraging the Lyapunov optimization technique, the original problem is converted into a series of BA and RC problems in each time slot. To solve the converted problem with affordable complexity, novel closed-form solutions for BA and RC are first derived by considering beam constraints in SWB systems. A decomposition-based BA and RC (DBR) algorithm with only polynomial computational complexity is then proposed based on the derived closed-form solutions. The simulation results demonstrate that the proposed DBR method can effectively[‘effectively’ appears to be a more suitable word in this case.] balance the performance and complexity because the DBR scheme outperforms the benchmark scheme and achieves nearly optimal performance in terms of system delay and QoE.",[],[]
"This paper considers the motion energy minimization problem for a robot that uses millimeter-wave (mm-wave) communications assisted by an intelligent reflective surface (IRS). The robot must perform tasks within given deadlines, and it is subject to quality of service (QoS) constraints. This problem is crucial for fully automated factories governed by the binomial of autonomous robots and new generations of mobile communications, i.e., 5G and 6G. In this new context, robot energy efficiency and communication reliability remain fundamental problems that couple in optimizing robot trajectory and communication QoS. More precisely, to account for the mutual dependency between robot position and communication QoS, robot trajectory and beamforming at the IRS and access point all need to be optimized. We present a solution that can decouple the two problems by exploiting mm-wave channel characteristics. Then, a closed-form solution is obtained for the beamforming optimization problem, whereas the trajectory is optimized by a novel successive-convex optimization-based algorithm that can deal with abrupt line-of-sight (LOS) to non-line-of-sight (NLOS) transitions. Specifically, the algorithm uses a radio map to avoid obstacles and poorly covered areas. We prove that the algorithm can converge to a solution satisfying the Karush-Kuhn-Tucker conditions. The simulation results show a fast convergence rate of the algorithm and a dramatic reduction of the motion energy consumption with respect to methods that aim to find maximum-rate trajectories. Moreover, we show that passive IRSs represent a powerful solution to improve the radio coverage and motion energy efficiency of robots.","['Robots', 'Service robots', 'Energy consumption', 'Robot kinematics', 'Robot sensing systems', 'Quality of service', 'Array signal processing']","['Energy efficient motion', 'intelligent reflective surface', 'millimeter-waves', 'robot path planning']"
"Transfer learning (TL) has proven to be a transformative technology for computer vision (CV) and natural language processing (NLP) applications, offering improved generalization, state-of-the-art performance, and faster training time with less labelled data. As a result, TL has been identified as a key research area in the budding field of radio frequency machine learning (RFML), where deployed environments are constantly changing, data is hard to label, and applications are often safety-critical. TL literature and theory shows that TL is generally successful when the source and target domains and tasks are similar, but the term similar is not sufficiently defined. Therefore, quantifying dataset similarity is of importance for analyzing and potentially predicting TL performance, and also has further application in RFML dataset design. This work offers a dataset similarity metric, specifically designed for raw RF datasets, based on expert-defined features andχ2tests, and systematically evaluates the proposed metric using synthetic datasets with carefully curated signal-to-noise ratios (SNRs), frequency offsets (FOs), and modulation types. Results show that the proposed dataset similarity metric intuitively quantifies the notion of similar signal sets, so long as the expert-features used to construct the metric are well suited to the application.",[],[]
"We propose a general framework for noncoherent communication using techniques from the field of quantum error correction (QEC). We first propose an approach for analyzing a classical communication channel as a quantum channel, and develop an extension of the QEC conditions to the classical case. We derive a quantum analogue of the noncoherent multiantenna wireless channel. Restricting to the case in which the number of transmitter and receiver antennas are equal and a power of two, we use the framework to develop a family of space-time block codes for noncoherent multiple-input multiple-output (MIMO) communication. Under a Rayleigh fading assumption, we derive the optimal decoder and bound the probability of symbol detection error. We compare our performance to comparable coherent and noncoherent approaches and achieve competitive performance for various antenna geometries and rates.",[],[]
"A comprehensive coverage of the state-of-the-art in quantum machine learning (QML) methodologies, with a unique perspective on their applications for wireless communications, is presented. The paper begins by delving into the fundamental principles of quantum computing, and then goes through different operations and techniques that are involved in QML deployments. Subsequently, it provides an in-depth look at various methods peculiar to quantum computing, such as quantum search algorithms, and discusses their potentials towards maximizing the performance of wireless systems. The integration of quantum-based learning models into the existing machine learning methodologies, such as within the frameworks of unsupervised learning and reinforcement learning, are then examined. Taking the viewpoint of wireless communications, diverse studies in the literature that employ QML-based optimization methods are also highlighted. Finally, to ensure the applicability and feasibility of QML for optimizing wireless systems, potential solutions for deployment challenges are addressed.",[],[]
"The distribution of entangled qubit pairs to end nodes is the key requirement of a quantum network to enable qubit state transmission through quantum teleportation. Existing protocols for entanglement distribution fix a specific swapping order on the involved quantum repeaters and delegate entanglement purification to upper-layer protocols. This limitation is problematic because entangled states tend to degrade due to quantum noise, and they cannot be purified if their fidelity (i.e. quality) falls below a certain threshold. It is therefore of the utmost importance to co-plan entanglement swapping and purification to achieve a target end-to-end fidelity. In this work, we present the Ranked Entanglement Distribution Protocol (REDiP), which overcomes the aforementioned limits by including the ""ranks"" mechanism to configure the ordering of both purification and entanglement swapping steps. We show how REDiP can easily be configured to implement custom entanglement swapping and purification strategies, including (but not restricted to) those adopted in two recent works. We also propose an algorithm to estimate the bandwidth to allocate on every link of the REDiP path, and we provide a set of guidelines on how REDiP ranks can be configured depending on user requirements and hardware configuration. Such guidelines are driven by original insights into purification performance. We conduct simulations to verify our results and assess the impact of different REDiP configurations on the performance of a repeater network, in terms of throughput and fidelity.",[],[]
"The L-band Digital Aeronautical Communications System (LDACS) is a cellular-based broadband, secure digital aeronautical communications system designed to enhance the safety and efficiency of air traffic management (ATM) through the facilitation of innovative ATM paradigms, such as 4D trajectory-based operations (TBO). LDACS is in its final stages of standardization by the International Civil Aviation Organization (ICAO) and the European Organisation for Civil Aviation Equipment (EUROCAE). Furthermore, it has been introduced to not only the aviation industry but also the Internet community in the form of an informational Request For Comments (RFC). Rapidly creating prototypes of an international communications standard by multiple organizations is crucial to the effective deployment of that standard. This approach enables validation and interoperability testing across various countries’ prototypes. Therefore, first, we create an LDACS prototype through a software/hardware co-design strategy by Software Defined Radio (SDR) and High-Level Synthesis (HLS). This approach expedites and economically streamlines the development process. Second, we show the alignment of our prototype with the LDACS specification through preliminary and cell entry tests. Third, we demonstrate the efficacy of LDACS’ Quality of Service (QoS) and security features via end-to-end IPv6 connectivity and security tests. Finally, the soundness and clarity of the LDACS specification is evidenced via interoperability tests between our prototype and a European counterpart.",[],[]
"The evolving landscape of beyond 5G and 6G wireless communication systems in smart urban environments faces numerous interference-related challenges posed by legitimate and illicit devices. In this context, Intelligent Reflecting Surfaces (IRS) have emerged as a promising solution to mitigate interference caused by obstacles and unknown jamming devices. Existing techniques mainly focus on mitigating the impact of a single jammer in IRS-assisted communications systems, which affects both stationary and mobile devices. Additionally, these approaches target a single objective, such as minimizing the energy, enhancing the transmission rate, or maximizing the Signal-to-Interference-plus-Noise Ratio (SINR), which restrains the performance of the system. This paper offers a comprehensive anti-jamming solution for securing wireless communications in a smart city urban environment comprising diverse public events such as sporting events, parades, festivals, and exhibitions. The focus is on maintaining essential services like security, law enforcement, logistics, emergency response, crowd management, and public health. We introduce a Reinforcement Learning-based technique for UAV-mounted IRS, optimizing trajectory and phase shift beamforming to counteract the disruptive impact of jammers, ensuring reliable communication in dynamic, security-sensitive settings Our approach also seeks to achieve multi-objective optimization by striking a balance between transmission rate and energy consumption in this highly challenging environment. The formulated optimization is computationally complex due to its combinatorial nature. Hence, we leverage the light-weight Deep Reinforcement Learning (DRL) technique called Deep Deterministic Policy Gradient (DDPG) to optimize trajectory and IRS phase shifts and achieve multiple objectives jointly. Experimental results demonstrate the effectiveness of our proposed DDPG-based approach in outperforming other RL algorithms. It achieves a near-optimal solution compared to the benchmark technique within the close gap and improves both achievable transmission rates and energy efficiency compared to related works by 50-70%.",[],[]
"Aiming at protecting device data privacy, Federated Learning (FL) is a framework of distributed machine learning in which devices’ local model parameters are exchanged with a centralized server without revealing the actual data. Hierarchical Federated Learning (HFL) framework was introduced to improve FL communication efficiency where devices are clustered and seek model consensus with the support of edge servers (e.g., base stations). Devices in a cluster submit their local model updates to their assigned local edge server for aggregation at each iteration. The edge servers transmit the aggregated models to a centralized server and establish a global consensus. However, similar to FL, adversaries may threaten the security and privacy of HFL. The client devices within a cluster may deliberately provide unreliable local model updates through poisoning attacks or poor-quality model updates due to inconsistent communication channels, increased device mobility, or inadequate device resources. To address the above challenges, this paper investigates the client selection problem in the HFL framework to eliminate the impact of unreliable clients while maximizing the global model accuracy of HFL. Each FL edge server is equipped with a Deep Reinforcement Learning (DRL)-based reputation model to optimally measure the reliability and trustworthiness of FL workers within its cluster. A Multi-Agent Deep Deterministic Policy Gradient (MADDPG) is utilized to enhance the accuracy and stability of the HFL global model, given the workers’ dynamic behaviors in the HFL environment. The experimental results indicate that our proposed MADDPG improves the accuracy and stability of HFL compared with the conventional reputation model and single-agent DDPG-based reputation model.",[],[]
"This paper investigates resource sharing strategies for point-to-multipoint (P2MP) distribution in next-generation digital subscriber line (DSL) networks. The latest DSL ITU standard, multi-gigabit fast access to subscriber terminals (MGfast) or G.9711, supports P2MP transmission as a new feature, which allows resource sharing among multiple customer premises equipments (CPEs). It offers an optimized user experience by efficiently utilizing available resources, thereby reducing the cost of service per user. The resource sharing can be done by allocating part of the available bandwidth to each CPE connected to the same MGfast transceiver unit (MTU-O) at the distribution point unit (DPU). An optimal solution to the grouping and per-group frequency-division multiple access (FDMA) allocation is necessary to optimally exploit the network resources. In this scenario, computing the optimal solutions involves significant computational complexity, especially when the network is dynamic, i.e., CPEs are frequently changing their activity status and traffic demands. Therefore, to overcome these issues, it is necessary to employ heuristic strategies that can provide comparable performance but with significantly reduced computational complexity. This paper proposes the optimal solutions to the grouping and per-group FDMA allocation for both upstream (US) and downstream (DS) P2MP transmission. Additionally, heuristic strategies with significantly lower computational complexity are proposed based on the optimal solutions. These heuristic strategies are shown to achieve comparable performance to the optimal solutions.",[],[]
"The advancement of millimeter wave (mmWave) technology can meet the requirements of large amounts of data communications among intelligent devices for indoor scenarios. However, reducing interference in an enclosed region is still challenging because the human body becomes the main blockage in indoor scenarios. In this paper we develop a robust system to exploit the self-blockage model for analysing the side effect of the human body and adopting a multi-ball Line of Sight (LOS) link state model to describe the conventional blockage. The combination of self-blockage and link state models will provide a more comprehensive and accurate expression for indoor obstacles that disturb the communication robustness. Besides, we formulate the resource allocation as a robust optimization problem, where the proposed optimization function is to achieve the maximum throughput by minimizing interference. We have given a closed expression of coverage rate to analyze the system performance. The simulation results show that the proposed model can accurately describe the distribution of blockages for indoor cases.",[],[]
"LoRa has been considered as a key enabler for the next generation Internet of Things (IoT) networks. However, the low spectral efficiency (SE) of chirp spread spectrum (CSS) modulation used in LoRa is a fatal drawback for its extensive applications in the sixth generation (6G) enabled high-data-rate IoT era. In this paper, we propose SSK PSK-LoRa (slope-shift-keying and phase-shift-keying LoRa) modulation, which can achieve higher SE and better energy efficiency (EE) than the conventional LoRa modulation. In particular, the transceiver architecture of our proposed scheme is presented along with both coherent and semi-coherent detection methods. Moreover, the orthogonality of SSK PSK-LoRa symbols is analyzed and the closed-form approximations for bit error rate (BER) in both additive white Gaussian noise (AWGN) and Rayleigh fading channels are derived. Numerical results demonstrate the superiority of our proposed modulation scheme, which outperforms most classical counterparts in terms of BER and effective throughput.",[],[]
"This paper proposes the joint use of caching and simultaneous wireless information and power transfer (SWIPT) to enhance the performance of cell-edge users of down-uplink simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-empowered non-orthogonal multiple access (NOMA) systems. The main idea is to leverage the available caching at nearby users to establish cooperative communication while taking into account the problem of energy issues through SWIPT mechanisms, namely time-switching (TS) and power-splitting (PS) strategies. To improve the communication quality of users, STAR-RIS with an energy-splitting (ES) protocol is established and phase-shift design is formulated. Closed-form approximations and asymptotic expressions for users’ outage probability (OP) and effective ergodic capacity (EC) are derived. Based on the attained expressions, useful insights into system design, such as the impact of power allocation, imperfect successive interference cancellation, the number of transmit/reflect components on the system performance, trade-offs between the network requirement and the system performance, and trade-offs between the energy harvesting process and data transmission quality, are deduced. Numerical examples verify our derivations and show that: i) the performance of cell-edge users is improved significantly; ii) exploiting SWIPT-PS shows better OP and EC performance for nearby users compared to SWIPT-TS, while both provide comparable performance for cooperative communication; iii) as the performance of cell-edge users is mostly guaranteed, the communication quality of nearby users in downlink and source node in uplink can be improved by optimizing either the power budget or the energy harvesting factors; iv) adopting STAR-RIS with ES protocol outperforms mode-switching and time-splitting ones in terms of both OP and EC; and v) STAR-RIS-empowered NOMA has better EC performance than orthogonal-multiple access counterparts when SIC operation is properly designed.",[],[]
"An advantage of using the composite fading models (CFMs) is their ability to concurrently address the impact of multi-path and shadowing phenomena on the system performance in wireless communications. A Fisher-Snedecor (FS) \mathcal{F}CFM has been recently proposed as an experimentally verified and tractable fading model that can be efficiently applied for 5G and beyond 5G wireless communication systems. This paper provides second-order (s-order) performance analysis of the product of Nindependent but not identically distributed (i.n.i.d) FS \mathcal{F}random variables (RVs). In particular, accurate and closedform approximations for level crossing rate (LCR) and average fade duration (AFD) of the product of Ni.n.i.d FS \mathcal{F}(N-{\mathrm {FS}}~ \mathcal{F})RVs are successfully derived by exploiting a general property of a Laplace approximation method for evaluation of the N-folded integral-form LCR expression. Based on the obtained s-order statistical results, the burst error rate and maximum symbol rate of the N-FS \mathcal{F}distribution are addressed and thoroughly examined. The numerical results of the considered performance measures are discussed in relation to the N-\mathrm{FS}~ \mathcal{F}multi-path and shadowing severity parameters. Moreover, the impact of the number of hops (N)of the N-FS \mathcal{F}CFM on the s-order metrics, the burst error rate and maximum symbol rate are numerically evaluated and investigated. The derived s-order statistical results can be used to address the cooperative relay-assisted (RA) communications for vehicular systems. Monte-Carlo (\mathrm{M}-\mathrm{C})simulations for the addressed statistical measures are developed in order to confirm the provided theoretical results.",[],[]
"In cognitive radio (CR) networks, a secondary user access control (SUAC) technique has been designed to enhance spectrum efficiency, in which a jamming signal is deliberately injected to maintain reliable sensing performance of authorized secondary users (A-SUs) and degrades unauthorized secondary users (UA-SUs) spectrum sensing results. We consider the problem of jamming signal design in massive multiple-input multiple-output (MIMO) CR networks wherein each primary user has a larger number of antennas that coexists with multiple secondary users. In this paper, we propose a jamming signal design framework that combines maximizing the jammer’s influences on UA-SUs and minimizing on A-SUs. The resulting problem is a non-convex quadratically constrained quadratic programming (QCQP) problem, and a semidefinite relaxation (SDR) method can be one of the approximate solutions but cannot meet our stringent constraints and lacks jamming efficiency. We propose a novel optimization algorithm based on theK-best methodology to design the jamming signal. Simulation results show the effectiveness of the proposedK-best based SUAC method in improving the spectrum sensing performance of A-SUs.",[],[]
"Recent developments in Unmanned Aerial Vehicles (UAVs) technology have paved the way to their utilization in different fields and applications. Among these applications, UAVs have been widely investigated as a good candidate to serve as mobile base-stations that can provide prompt and temporary coverage of wireless connectivity for specific areas. Recent studies that address different UAVs’ performance aspect lack of considering more practical channel models. Therefore, in this paper, based on a recent three dimensional channel modeling of UAV-based links, the secrecy analysis is in detail conducted for UAV-based links operating on the millimeter wave band. Specifically, the average channel capacity, the average secrecy capacity and the probability of strictly positive secrecy capacity are all investigated between two UAVs in the presence of another UAV-based eavesdropper. Our results include closed form expressions of the addressed secrecy metrics along with simulation results that reveal the impact of different operational parameters on the secrecy performance.",[],[]
"Information security and self-sustainability are crucial to resource-limited wireless networks. In this paper, we assess the secrecy performance of the legitimate link without having eavesdropper's channel state information (CSI) while considering an energy harvesting (EH) jammer with finite and infinite energy buffer sizes. The EH jammer utilizes accumulated energy from its buffer and transmits artificial noise (AN) to ensure secure communication. To manage the uncertainty in harvested energy and consumption, we analyze on-off (OO) jamming and always-on (AO) jamming schemes. The analytical expressions are obtained for secrecy outage probability (SOP). This analysis models the energy arrival and departure process by a discrete-time continuous-state Markov chain to eliminate the computational complexity and inaccuracy of a discrete-state Markov chain. Both the schemes are compared using SOP and secure throughput. This analysis shows the adequacy of a finite buffer for an EH jammer and also the fact that the availability of jamming power has a more pronounced effect on SOP as compared to a larger number of antennas at the jammer.",[],[]
"Decentralized machine learning enables multiple devices to train a global model collaboratively and is a promising paradigm to realize ubiquitous intelligence for the Internet of Vehicles (IoV). Existing work mainly focused on either the data privacy protection techniques or efficient topology orchestration of decentralized machine learning. However, these techniques cannot be directly applied to IoV due to possible accuracy degradations and insufficient topology adaptability, not to mention the joint secure and efficient decentralized learning designs. This paper proposes a secure and efficient hierarchical decentralized learning framework for IoV networks with multiple fog nodes and mobile vehicles. The proposed framework combines federated learning and distributed consensus for vehicle-fog and inter-fog collaborative learning, respectively, and integrates masking with local training to protect data privacy. We propose the network-level masking mechanism and consensus matrix optimization for signaling-efficient implementations in IoV. The network-level masking can eliminate the masking pairing requirements of inter-fog handover of mobile vehicles and is proved to be canceled via distributed consensus. Experimental results on two popular datasets validate the superiority of the proposed framework in terms of learning accuracy, data protection, and signaling efficiency, compared to the existing approaches.",[],[]
"Traditional multiple access schemes, as well as more recent preamble-based schemes, cannot achieve the extremely low latency, complexity, and collision probability required by the next-generation Internet-of-Things (IoT) networks to operate. To address such issues and further reduce the latency and packet loss, we introduce a novel semi-grant-free multiple access protocol for short packet transmission, the partial-information multiple access (PIMA) scheme. PIMA transmissions are organized in frames, and in the partial information acquisition (PIA) sub-frame of each frame, the base station (BS) estimates the number of active devices, i.e., the devices having packets waiting for transmission in their queue. Based on this estimate, the BS chooses both the total number of slots to be allocated in the data transmission (DT) sub-frame and the respective user-to-slot assignment. Although collisions may still occur due to multiple users assigned to the same slot, they are drastically reduced with respect to slotted ALOHA-based schemes, while achieving lower latency than both time-division multiple-access (TDMA) and preamble-based protocols, due to the extremely reduced overhead of the PIA sub-frame. We assess the performance of PIMA under various activation statistics, proving the robustness of the proposed solution to the traffic intensity, also with traffic bursts.",[],[]
"A key challenge for common waveforms for Integrated Sensing and Communications – widely regarded as a resource-efficient way to achieve high performance for both functionalities – lies in leveraging information-bearing channel-coded communications signal(s) (c.c.s) for sensing. In this paper, we investigate the range-Doppler sensing performance of c.c.s in multi-user interference-limited scenarios, and show that it is affected by sidelobes whose form depends on whether the c.c.s modulates a single-carrier or OFDM waveform. While uncoded signals give rise to asymptotically zero sidelobes due to the law of large numbers, it is not obvious that the same holds for c.c.s, as structured codes (e.g., linear block codes) induce dependence across codeword symbols. In this paper, we show that c.c.s also give rise to asymptotically zero sidelobes – for both single-carrier and OFDM waveforms – by deriving upper bounds for the tail probabilities of the sidelobe magnitudes that decay asexp(−O(code rate×block length)). Consequently, for any code rate, c.c.s are effective sensing signals that are robust to multi-user interference at sufficiently large block lengths, with negligible difference in performance based on whether they modulate a single-carrier or OFDM waveform. We verify the latter implication through simulations, where we observe the sensing performance (i.e., the detection and false-alarm probabilities) of a QPSK-modulated c.c.s (code rate = 120/1024, block length = 1024 symbols) to match that of a comparable interference-free FMCW waveform even at high interference levels (SIR of −11dB), for both single-carrier and OFDM waveforms.",[],[]
"This paper investigates serially concatenated (SC) and interleaved transmission schemes for a new class of continuous phase modulation (CPM), which has the property being a single sideband (SSB). This new CPM is commonly called Single Side-Band Frequency Shift Keying (SSB-FSK). Due to the ample space of SSB-FSK parameters, finding good serially concatenated schemes using SSB-FSK with desired spectral and energy efficiencies as well as reasonable complexity, is challenging. We present a systematic, simulation-based extrinsic information transfer (EXIT) chart approach to evaluate this multi-objective problem (improve spectral and energy efficiencies and reduce complexity) and subsequently design a competitive serially concatenated SSB-FSK based scheme. We show that iterative decoding of SC schemes using SSB-FSK (SC-SSB-FSK) provides close to optimal performance. Furthermore, the bit error rate (BER) of SC-SSB-FSK is compared to that using raised cosine (RC) based CPM signals. It has been shown that the proposed technique outperforms the state-of-the-art serially concatenated systems.",[],[]
"Network operators can operate services in a flexible way with virtual network functions thanks to the network function virtualization technology. Flow partition allows aggregated traffic to be split into multiple parts, which increases the flexibility. This paper proposes a service deployment model with flow partition to minimize the service deployment cost with meeting service delay requirements. A virtual network function of a service is allowed to have several instances, each of which hosts a part of flows and can be shared among different services, to reduce the initial and proportional cost. We provide the mathematical formulation for the proposed model and transform it to a special case as a mixed integer second-order cone programming (MISOCP) problem. A heuristic algorithm, which is called a flow partition heuristic (FPH), is introduced to solve the original problem in practical time by decomposing it into several steps; each step handles a convex problem. We compare the performances of proposed model with flow partition and conventional model without flow partition. We consider the formulated MISOCP problem with adopting a strategy of even splitting to divide flows in a special case, which is called an even spitting heuristic (ESH). The performances of FPH and ESH are compared in a realistic scenario. We also consider the formulated MISOCP problem as an original problem and compare it to an FPH-based heuristic algorithm with the even-splitting strategy (FPH-ES), in both realistic and synthetic scenarios. The numerical results reveal that the proposed model saves the service deployment cost compared to the conventional one. It improves the maximum admissible traffic scale by 23% in average in our examined cases. We observe that FPH outperforms ESH and ESH outperforms FPH-ES in terms of the service deployment cost in their own focused problems, respectively.",[],[]
"One of the fundamental challenges in 5G and beyond technologies is to support short packet transmissions while ensuring ultra-reliable communication. Due to the distributed nature of the networks, such as machine-to-machine (M2M) communications, interference is unavoidable. The impact of interference on the system’s performance must be better understood when users are constrained to transmit short packets. In addition, users’ traffic is bursty. Thus, they may not always have data to send. This work considers a two-user Z-interference channel (Z-IC) under Rayleigh fading. The work characterizes the stability region corresponding to prominent interference mitigation schemes such as treating interference as noise, successive interference cancellation, and joint decoding schemes using the finite block-length information theory framework. The developed results consider the packet length, rate, and underlying channel model. Evaluating stability region involves determining the probability of successful decoding for the various interference mitigation techniques. The different probabilities of successful decoding are characterized for various interference mitigation techniques. These results are not explored in the existing literature in the context of Z-IC. The developed results also help to explore the impact of interference on average delay and the average age of information for various interference mitigation techniques.",[],[]
"Currently, with the widespread of the intelligent Internet of Things (IoT) in beyond 5G, wireless federated learning (WFL) has attracted a lot of attention to enable knowledge construction and sharing among a huge amount of distributed edge devices. However, under unstable wireless channel conditions, existing WFL schemes exist the following challenges: First, learning model parameters will be disturbed by bit errors because of interference and noise during wireless transmission, which will affect the training accuracy and the loss of the learning model. Second, traditional edge devices with CPU acceleration are inefficient due to the low throughout computation, especially in accelerating the encoding and decoding process during wireless transmission. Third, current hardware-level GPU acceleration methods cannot optimize complex operations, for instance, complex wireless coding in the WFL environment. To address the above challenges, we propose a software-defined GPU-CPU empowered efficient WFL architecture with embedding LDPC communication coding. Specifically, we embed wireless channel coding into the server weight aggregation and the client local training process respectively to resist interruptions in the learning process and design a GPU-CPU acceleration scheme for this architecture. The experimental results show its anti-interference ability and GPU-CPU acceleration ability during wireless transmission, which is 10 times the error control capability and 100 times faster than existing WFL schemes.",[],[]
"The ever-increasing demand for wireless communication has driven the development of innovative technologies to address the issues that conventional communication networks meet. To support autonomous devices in the sixth-generation (6G) and provide seamless wireless connectivity in inaccessible and hard-to-reach areas, the combination of software-defined networking (SDN) with unmanned aerial vehicles (UAVs) has already shown promises. As the traditional UAV-based cellular network encompasses challenges such as lack of centralized supervision, robust decision-making ability, and dynamic network management, the SDN platform could be an excellent alternative to offer aerial networking for future 6G ecosystem. However, the SDN-assisted UAV paradigm faces a set of challenges and critical issues. For instance, the network management and orchestration, resource allocation and optimization, handover mechanism, spectrum management, energy efficiency, integration of heterogeneous networks, and reliability, thus causing the decrease of quality of services of the SDN-enables UAV networks. To effectively address all these issues and identify the trends of software-defined UAV (SDUAV) networks, more research activities are needed. Unfortunately, there are not any specific guidelines in the literature that address each of these requirements, enabling technologies, emerging techniques, and future research directions in the context of SDN-based UAV networks to enable next-generation 6G systems. Therefore, motivated by this fact, this paper offers a comprehensive evaluation of the state of the SDUAV network and architecture, highlighting its various requirements and different prospects and applications in 6G systems. Besides, this paper also addresses why the combination of SDN and UAV-enabled networks is becoming essential in the 6G networking context and how it can offer more flexibility, scalability, reliability, and efficient connectivity than the conventional approaches to provide features like intelligence, automation, programmability, and centralized controllability. This paper furthermore outlines the difficulties of implementing SDUAV networks, such as security, adaptability, interference management, interoperability, and lack of standardization. Finally, to accomplish the goals, this article provides a roadmap for future trends and research directions in SDUAV networks to support the 6G systems.",[],[]
"Sparse multipath channel impulse response (CIR) estimation schemes are conceived for optical orthogonal frequency division multiplexing (O-OFDM) visible light communication (VLC) systems. We commence by deriving the input-output models for both asymmetrically clipped optical OFDM (ACO-OFDM) and direct current-biased optical OFDM (DCO-OFDM) systems. A multipath CIR model is derived that captures both the diffusive as well as specular reflections of the VLC channel. Next, we introduce both the sparsity-agnostic conventional least square (LS) and the linear minimum mean square error (LMMSE) channel estimation (CE) techniques. This is followed by the orthogonal matching pursuit (OMP)-based sparse recovery technique, which exploits the delay-domain sparsity of the CIR. Furthermore, a novel sparse multipath CIR estimation scheme is proposed using the Bayesian learning (BL) framework, which requires only a limited number of pilot subcarriers, hence resulting in a reduced pilot overhead as compared to other state-of-the-art (SoA) CE techniques. The Bayesian Cramer Rao lower bound (BCRLB) as well as the Oracle-minimum mean squared error (O-MMSE) estimator are also derived for benchmarking the estimation performance of the proposed BL-based framework. Our simulation results demonstrate that the proposed BL method outperforms other existing sparse and conventional CE methods in terms of various metrics, such as the normalized mean-square-error (NMSE), the outage probability (OP), and the bit error-rate (BER) despite its reduced pilot overhead.",[],[]
"Non-orthogonal multiple access (NOMA) has emerged as an effective technology, aiding in latency reduction, improving security and reliability, and enhancing the spectral efficiency and capacity in Internet of Things (IoT)-enabled communication systems. In this work, an implementation of co-operative secure multiple-input single-output NOMA IoT framework is proposed and examined in a two-fold manner. First, under the consideration of standalone links, a user selection criteria which relies on the characterization of sum ergodic capacity (SEC) is investigated in the presence of an eavesdropper (ED) and beamforming constraints. Next, using the optimized beamformers, a user selection criteria which relies on the outage probability analysis is performed under cooperative relaying considerations. We demonstrate that our optimization approach achieves reduced capacity and coverage for the ED while maximizing the overall SEC and coverage of the network under multiple network configurations. The proposed optimization solution demonstrates superior performance compared to the benchmarked schemes. Derived closed-form analytical expressions are validated through simulation means.",[],[]
"Networked robots have become crucial for unmanned applications since they can collaborate to complete complex tasks in remote/hazardous/depopulated areas. Due to the cost inefficiency of deploying cellular network infrastructure in these areas, hybrid satellite-UAV networks emerge as a promising solution. These networks provide seamless and on-demand connectivity for multiple robots with various task requirements, and support computation-intensive and latency-sensitive services through mobile edge computing (MEC)-based offloading. However, to complete tasks in limited times, the rapid collective movement of mobile robots may cause frequent service migration, and a large number of gathered robots may compete for limited bandwidth resources in satellite and UAV communications. As a result, offloading latency may increase significantly. To address this issue, the average completion time of multi-robot offloading in task-oriented satellite-UAV networks with MEC is formulated as an optimization problem. Unlike conventional mobility-aware MEC-based offloading schemes, joint optimization of mobility control, data offloading, and resource allocation is proposed using velocity control of multiple robots. According to Lyapunov optimization, the original optimization problem is simplified into minimizing the average completion time of offloading for all robots within UAV and satellite coverage. A multi-agent Q-learning algorithm, including multi-group dual-agent Q-learning, is proposed based on local state observation and global reward calculation. In each dual-agent Q-learning, one agent is responsible for velocity control and communication resource allocation, while the other is responsible for data offloading and computational resource allocation. The convergence of the proposed multi-agent Q-learning algorithm is also theoretically analyzed. Simulation results show that the proposed scheme can effectively reduce the offloading latency by up to 35% in the multi-robot environment over its conventional counterparts.",[],[]
"Terahertz (THz) wireless communication has emerged as a promising technology for high-speed, ultra-broadband, and low-latency data transmission. However, the performance of THz systems is significantly affected by various channel and system parameters. Existing literature often models THz transmission links usingα−μfading with pointing error, which is suitable for optical communication systems or a specific type of THz antennas. A new pointing error model designed for THz antennas has recently been proposed. The fluctuating two-ray (FTR) fading model emerges as a very suitable choice for THz transmission, as it considers the line-of-sight components and their random phase distributions, in addition to multipath and non-linearity of the propagation medium. In this paper, we model a THz transmission link considering FTR short-term fading, channel path loss, and the recently proposed pointing error, with two standard uniformN×NTHz antenna arrays, one mounted at the unstable transmitter and one at the unstable receiver (instability accounts for the dynamic nature of drone-to-drone communication scenarios, particularly, dynamic drones fluctuating in yaw, roll, and pitch directions). Novel exact analytical closed-form expressions for the probability density function, the cumulative distribution function, and moment generating function of the received instantaneous signal-to-noise ratio are derived for the aforementioned system using standard mathematical functions. Next, closed-form expressions for performance metrics, namely, the outage probability, the average bit error rate, and the average channel capacity, are derived. Furthermore, thenth moment of the received instantaneous signal-to-noise ratio and diversity order of the system are derived. The effects of various channel and system parameters on the system performance are studied, and the accuracy of the derived analytical expressions is confirmed by simulation results.",[],[]
"Given the necessity of connecting the unconnected, covering blind spots has emerged as a critical task in the next-generation wireless communication network. A direct solution involves obtaining a coverage manifold that visually showcases network coverage performance at each position. Our goal is to devise different methods that minimize the absolute error between the estimated coverage manifold and the actual coverage manifold (referred to as accuracy), while simultaneously maximizing the reduction in computational complexity (measured by computational latency). Simulation is a common method for acquiring coverage manifolds. Although accurate, it is computationally expensive, making it challenging to extend to large-scale networks. In this paper, we expedite traditional simulation methods by introducing a statistical model termed line-of-sight probability-based accelerated simulation. Stochastic geometry is suitable for evaluating the performance of large-scale networks, albeit in a coarse-grained manner. Therefore, we propose a second method wherein a model training approach is applied to the stochastic geometry framework to enhance accuracy and reduce complexity. Additionally, we propose a machine learning-based method that ensures both low complexity and high accuracy, albeit with a significant demand for the size and quality of the dataset. Furthermore, we describe the relationships between these three methods, compare their complexity and accuracy as performance verification, and discuss their application scenarios.",[],[]
"An unmanned aerial vehicle-mounted base station (UAV-BS), also known as an aerial base station (ABS), is a viable technology for the next 6G wireless networks due to its adaptability and affordability. Furthermore, the concept of tethered UAVs (TUAVs), can be used to circumvent the limited network operating time of UAV-BS networks. TUAVs are UAVs powered by a ground energy source via a tether that restrain their mobility while providing unlimited power. In this work, we propose a system where ABSs are deployed in user hotspots to offload the traffic and assist terrestrial base stations (TBSs). We will analyze three different scenarios and compare them in terms of coverage performance and energy efficiency. For a more realistic system, we offer a system model that considers the dynamic spatial distribution of users. First of all, we start by determining the optimal locations of TUAVs that minimize the average pathloss for each scenario. Next, using tools from stochastic geometry and an approach of dividing the space into concentric rings and slices to quantify the locations and orientations of ground stations (GSs), we analyze both coverage and energy performance for each scenario. We verify our findings using Monte-Carlo simulations and draw multiple useful insights. For instance, we show that deploying a TUAV with attachment and detachment capability for each pair of clusters outperforms deploying a normal TUAV for each cluster in terms of energy efficiency but not in terms of coverage performance.",[],[]
"This paper proposes a novel approach that utilizes differential encoding to overcome the channel estimation problem in communication systems with low-resolution quantization receivers. For differentially encoded data, we derive the maximum likelihood detection rule for the canonical block-2 detectors, employing just two consecutive quantized observations at the channel output and without any receiver-side channel state information. We establish the optimality of this maximum likelihood detection rule within the class of block-Ldetectors, whereL≥3, under the condition thatn=log2M, withnandMdenoting the number of quantization bits and input alphabet size, respectively. The derived detector has a simple and easily implementable structure, comparing the quantization region indices of consecutive observations to determine the transmitted message index. By leveraging the structure of the derived optimum detector, we obtain the expression for the message error probability in Rayleigh fading wireless channels. Through asymptotic analysis in the high signal-to-noise ratio regime, we reveal a crucial finding that achieving the same diversity order as infinite bit quantization with full channel knowledge requires an additional two bits at the quantizer, in addition to the minimum requirement oflog2Mbits. One bit compensates for the low-resolution effect, while the other addresses the lack of channel knowledge. Finally, we conduct an extensive simulation study to demonstrate the performance of the optimum detectors and quantify the performance loss resulting from the absence of channel knowledge at the receiver.",[],[]
"BGP, the de-facto standard protocol for exchanging routes on a network-wide basis called AS employs invalid routes. Recently, a data object called Autonomous System Provider Authorization (ASPA) was proposed as a new specification for verifying PATH information in BGP security. In this paper, we shed light on the effectiveness of ASPAs in a partial deployment alongside the conventional BGP through experiments based on a real AS topology. To this end, we also present a novel simulation tool, LOTUS, for BGP route exchange, including ASPAs. We then evaluate deployments of ASPAs and their verification with LOTUS for two cases on network topology in Japan: the case in deployment from ASes whose number of connections with other ASes is large, i.e., deployment from top ASes, and the case in deployment from ASes at the end of the network topology, i.e., deployment from leaf-node ASes. As a result, we confirm that the number of victim ASes decreases in the former case, while ASPAs provide no advantage in the latter case. Notably, the number of victim ASes decreases by about 96% on average by deploying the verification with ASPAs in the top-eight ASes. Based on these results, we further conduct extensive experiments in the deployment from the top ASes, whereby ASes outside the network topology advertise malicious routes to the victim ASes. We also discuss a case whereby an adversary tries to leverage ASPAs. Our promising results show that the adversary will no longer obtain an advantage even by leveraging ASPAs.",[],[]
"We describe and experimentally evaluate the performance of our Network Link Outlier Factor (NLOF) for locating faults in communication networks. The NLOF is a unique outlier score assigned to each link in a network. It is computed using four distinct stages in a data analytics pipeline. The input to the pipeline are flow records (e.g., NetFlow) and network topology data (e.g., Link Layer Discovery Protocol (LLDP)). In the first stage, flow record throughput values are clustered in two sub-stages: using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and then our novel domain-specific ThroughPut Cluster (TPCluster) technique. In the second stage, flow outlier scores are determined within each cluster using a measure of proximity to a selected performance exemplar. In the third stage, flows are associated with network links using topology data. Finally, in the fourth stage the flow outliers are used to compute the outlier factor or score for each network link. The network link outlier scores are used with a detection rule to locate faults. We present the results of a wide set of Mininet experiments that appraise the fault detection/localization performance of NLOF. We find that NLOF allows for the detection of errors on edge links with a simple detection rule and the detection of errors on core links with a rule that includes topology relationships. NLOF is also compared to an abrupt change detection technique; while both have roughly the same detection power, the precision of NLOF is 42% higher and NLOF required 40% less time to detect failures on average.","['Fault detection', 'Switches', 'Machine learning', 'Anomaly detection', 'Training data', 'Fault diagnosis']","['Network management', 'clustering', 'outlier detection', 'fault detection', 'fault localization']"
"This article aims to investigate the performance of the IEEE 802.11-based tree-topology network, where a wireless node is within the others' carrier sensing ranges. In such a network, the concurrent transmission is a dominant cause of frame collisions. Moreover, the relay nodes (RN) (i.e., in the tree) likely cause the coexistence of non-saturated and saturated nodes in the networks. Those conditions have not been addressed in the previous works yet. As a solution, this work proposes new analytical expressions of delay and throughput in the investigated scenario. The presented analytical model incorporates the Bianchi model and airtime concept to formulate operations of the IEEE 802.11 nodes. First, by leveraging Bianchi-based analysis, the proposed model gives the frame collision probability caused by concurrent transmission. Second, by using airtime concept analysis, the ratio of frame numbers of each flow in a relay node (RN) is expressed to represent the buffer state of RN. As a result, we can obtain the network throughput, the throughput, and the delay of each flow. The validity of the analytical expressions is confirmed by the quantitative agreement between the analytical and simulation results.",[],[]
"The upcoming sixth generation (6G) mobile networks require integration between terrestrial mobile networks and non-terrestrial networks (NTN) such as satellites and high altitude platforms (HAPs) to ensure wide and ubiquitous coverage, high connection density, reliable communications and high data rates. The main challenge in this integration is the requirement for line-of-sight (LOS) communication between the user equipment (UE) and the satellite. In this paper, we propose a framework based on actor-critic reinforcement learning and generative models for LOS estimation and traffic scheduling on multiple links connecting a user equipment to multiple satellites in 6G-NTN integrated networks. The agent learns to estimate the LOS probabilities of the available channels and schedules traffic on appropriate links to minimise end-to-end losses with minimal bandwidth. The learning process is modelled as a partially observable Markov decision process (POMDP), since the agent can only observe the state of the channels it has just accessed. As a result, the learning agent requires a longer convergence time compared to the satellite visibility period at a given satellite elevation angle. To counteract this slow convergence, we use generative models to transform a POMDP into a fully observable Markov decision process (FOMDP). We use generative adversarial networks (GANs) and variational autoencoders (VAEs) to generate synthetic channel states of the channels that are not selected by the agent during the learning process, allowing the agent to have complete knowledge of all channels, including those that are not accessed, thus speeding up the learning process. The simulation results show that our framework enables the agent to converge in a short time and transmit with an optimal policy for most of the satellite visibility period, which significantly reduces end-to-end losses and saves bandwidth. We also show that it is possible to train generative models in real time without requiring prior knowledge of the channel models and without slowing down the learning process or affecting the accuracy of the models.",[],[]
"High-precision positioning of two underwater mobile robots is investigated in this work. To achieve good performance in underwater communication, control algorithms are implemented to maintain the position of the receiver robot aligned with that of the transmitter in the presence of measurement noise and process uncertainty. Although recent research works have successfully integrated control algorithms with Extended Kalman Filter (EKF) estimator to track the desired position of the transmitter, other aspects besides the convergence to the equilibrium point such as operational constraints and input constraints were not taken into account within these controllers. Such inability of these control algorithms may degrade the performance of the controlled process. Motivated by the above considerations, a tracking Model Predictive Control (MPC) with an EKF-based estimator is developed to both estimate the process states online and drive the actual system to the desired equilibrium point while meeting input and state constraints. The closed-loop stability and the recursive feasibility of the proposed tracking MPC scheme are rigorously proved. To demonstrate the applicability of the proposed control design, the performance of the tracking MPC with that of the conventional Proportional (P), Proportional Integral Derivative (PID), and Linear Quadratic Regulator (LQR) controllers are compared.",[],[]
"Adaptor signature is extended from the standard digital signature, which conceals a secret value in the “pre-signature”. The one who knows the secret value can transform it into a completed signature. Adaptor signatures are useful in addressing blockchain scalability issues, and have been used to build blockchain scaling protocols, such as payment channels or atomic swaps. Recently, someone propose two-party adaptor signature to enhance the privacy in application of adaptor signatures. However, we notice that there is no identity-based two-party adaptor signature scheme so far. Based on IEEE P1363 standard identity-based signature scheme, a secure two-party adaptor signature is constructed in this paper. Moreover, under the random oracle model, we analyze the security of our proposed P1363-based two-party adaptor signature. Finally, we make a comparison between our scheme and other adaptor signature schemes in computation and communication overheads, the result shows the costs of our scheme are acceptable.",[],[]
"Beyond fifth generation (B5G) / sixth generation (6G) wireless systems require full utilization of all spectrums. However, hardware devices operating in the high-frequency bands bring a series of non-ideal behaviors, including nonlinear distortion caused by high-power amplifiers and phase noise generated by radio-frequency oscillators, which will lead to performance degradation of traditional signal modulation technology. To this end, this paper proposes a 5G new radio-compatible constellation design method using projection over quadrature amplitude modulation (QAM). First, to address the nonlinear distortion caused by power amplifiers, the pseudo-amplitude-phase-shift keying (APSK) constellation with a low peak-to-average power ratio is proposed. The signal points are selected from QAM by minimum Euclidean distance with APSK. Second, for a phase noise channel, the pseudo-spiral constellation is designed with the projection of Spiral constellation points onto QAM by the proposed minimum radial-angular weighted distance. Simulation results show that the proposed pseudo-APSK modulation outperforms traditional QAM at medium and high code rates considering nonlinear distortion produced by a power amplifier. Besides, the proposed pseudo-spiral modulation exhibits significant performance gain and improved robustness compared to QAM under phase noise-affected channels, particularly in medium-high signal-to-noise ratio conditions.",[],[]
"Millimeter-wave (mmWave) communication has been recognized as a key technology to improve the rate performance of the next-generation wireless networks. However, the mmWave signals suffer from severe path loss and penetration attenuation due to the extremely high frequency characteristics. As a result, a massive number of small base stations (SBSs) would be deployed in mmWave networks to provide seamless coverage. To overcome obstacle blockages, the multi-connectivity technology that enables a user to associate with multiple SBSs simultaneously has been proposed by 3GPP, which brings new challenges to resource allocation. This paper investigates the joint user association and power allocation problem in such a multi-connectivity enabled mmWave network aiming at maximizing the system sum rate while considering the backhaul capacity limit and users’ individual rate requirements. The optimization problem is highly non-convex and NP-hard to solve. By decoupling it into user association subproblem and power allocation subproblem, we first propose a novel swapping algorithm on top of the classic many-to-many matching scheme to perform user association, and then apply the difference of convex functions (D.C.) programming approach to transform the non-convex power allocation subproblem into a convex one and solve it by an iterative algorithm. Simulation results demonstrate the superior performance of the proposed multi-connectivity user association and power allocation schemes over the existing ones.",[],[]
"Vision-aided wireless communication is attracting increasing interest and finding new use cases in various wireless communication applications. These vision-aided communication frameworks leverage visual data captured, for example, by cameras installed at the infrastructure or mobile devices to construct some perception about the communication environment through the use of deep learning and advances in computer vision and visual scene understanding. Prior work has investigated various problems such as vision-aided beam, blockage, and hand-off prediction in millimeter wave (mmWave) systems and vision-aided covariance prediction in massive MIMO systems. This prior work, however, has focused on scenarios with a single object (user) in front of the camera. In this paper, we define the user identification task as a key enabler for realistic vision-aided communication systems that can operate in crowded scenarios and support multi-user applications. The objective of the user identification task is to identify the target communication user from the other candidate objects (distractors) in the visual scene. We develop machine learning models that process either one frame or a sequence of frames of visual and wireless data to efficiently identify the target user in the visual/communication environment. Using the large-scale multi-modal sense and communication dataset, DeepSense 6G, which is based on real-world measurements, we show that the developed approaches can successfully identify the target users with more than 97% accuracy in realistic settings. This paves the way for scaling the vision-aided wireless communication applications to real-world scenarios and practical deployments.",[],[]
"In this article, we present a twin-tiered deep learning (DL) symbol detector architecture for a multi-user multiple-input multiple-output (MIMO) visible light communication (VLC) system, based on rate-splitting multiple access (RSMA). The DL architecture exploits long short-term memory (LSTM), by utilizing two identical LSTM-based deep neural networks (DNNs) to concurrently decode the common and private streams, within a multicarrier framework. Termed as VLC with RSMA and LSTM-based DNN (ViRSMALNet), this detector overcomes the limitations in successive interference cancellation (SIC) receivers, and decodes the RSMA messages from the combined signal in a single step. Extensive simulations based on Monte Carlo methods are executed to assess the symbol error rate (SER) performance of ViRSMALNet across diverse modulation schemes. The results establish that ViRSMALNet outperforms SIC-based least squares (LS) and minimum mean square error (MMSE) detectors, and closely attains the performance of the optimal maximum likelihood (ML) detector. Further, this study also investigates the influence of factors such as the number of light-emitting diodes per fixture, the number of photodetectors per user, and the DNNs’ hyperparameters on the SER performance. Moreover, the impact of the distance between the PDs on the system’s sum rate performance is studied.",[],[]
"Distributed machine learning at the network edge has emerged as a promising new paradigm. Various machine learning (ML) technologies will distill Artificial Intelligence (AI) from enormous mobile data to automate future wireless networking and a wide range of Internet-of-Things (IoT) applications. In distributed edge learning, multiple edge devices train a common learning model collaboratively without sending their raw data to a central server, which not only helps to preserve data privacy but also reduces network traffic. However, distributed edge training and edge inference typically still require extensive communications among devices and servers connected by wireless links. As a result, the salient features of wireless networks, including interference and channels’ heterogeneity, time-variability, and unreliability, have significant impacts on the learning performance.",[],[]
"The Proliferation of connected and embedded devices has played a critical role in the development of next generation Internet of Things (IoT), and consequently, the diffusion of low power networks. The sustainability of such networks entails energy aware devices that are capable of self-sustainable operations by harvesting and recycling energy from various sources. Accordingly, integration of wireless-powered networks (WPN) with ambient backscatter communications (AmBC), in which a backscatter transmitter conveys data to a destination by backscattering ambient signals, has recently emerged as a promising key technology for low power wireless networks. However, these technologies are not optimized and/or designed for large-scale networks and a massive number of IoT devices.",[],[]
"What will the wireless networks of the next generation look like? The latest advances in industrial and academic research have shed some light into that direction. With the finalization of the framework recommendation for the 6th generation (6G) networks by the International Telecommunication Union (ITU), the pillar usage scenarios, supporting capabilities and technical enablers are becoming clearer. The overarching motivation for the development of 6G is to continue to build an inclusive information society in a sustainable way. In this context, a range of user and application trends are foreseen to become an integral part of 6G, including ubiquitous intelligence, immersive multimedia and multi-sensory interactions, digital twins, digital health, smart industries, ubiquitous connectivity, integration of sensing and communication, as well as sustainability. It is worth noting that many of the trending demands do not come from the traditional markets for private mobile users and instead, they are driven by strong needs from vertical industries including manufacturing, transportation, and health care.",[],[]
"With the explosive growth of smart devices and development of wireless technology, numerous new applications such as Augmented Reality (AR), Virtual Reality (VR), Mixed Reality (MR), autonomous driving and intelligent manufactory enter our daily life and put stringent requirements on the current communications technologies. Boosted by multimedia applications and Internetof-Things (IoT), immersive communications is considered to bring a novel vision on the development of advanced communications systems and networks. Nevertheless, to fully explore the immersive experience and applications, ultrareliable, low latency and high data rate communications systems are required. However, the conventional access network can hardly accommodate such stringent requirements, due to limited capacity and long latency on the backhaul links. Novel approaches that bring various network functions and contents to the network edge, i.e., mobile edge computing and caching, are promising to tackle the aforementioned challenges.",[],[]
"The Increasing number of smart devices in different forms and capabilities combined with the worldwide adoption of advanced multimedia applications are contributing to the significant growth of the mobile data traffic. The fifth generation (5G) and beyond-5G wireless networks aim at providing wireless connectivity with very high data rates, low latency, high reliability, and scalability, and will support the emerging Internet-of-Things (IoT) applications and services with massively interconnected devices [items 1), 2) in the Appendix].",[],[]
"Conventional wireless communication systems operate in a half-duplex mode, i.e., current radios cannot transmit and receive at the same time and on the same frequency. Full-duplex wireless operation was generally assumed to be impossible due to the great difference in transmit and receive signal power levels. However, recent advances in antenna, hardware, and signal processing techniques have shown that full-duplex operation is practically feasible. Thanks to novel combinations of antenna, analog, and digital cancellation techniques, self-interference suppression of 80–110 dB can be made possible. The feasibility in building a practical full-duplex radio using off-the-shelf hardware and software-defined radios therefore alleviates many problems in wireless network design.",[],[]
"In recent years, vector-based machine learning algorithms, such as random forests, support vector machines, and 1-D convolutional neural networks, have shown promising results in hyperspectral image classification. Such methodologies, nevertheless, can lead to information loss in representing hyperspectral pixels, which intrinsically have a sequence-based data structure. A recurrent neural network (RNN), an important branch of the deep learning family, is mainly designed to handle sequential data. Can sequence-based RNN be an effective method of hyperspectral image classification? In this paper, we propose a novel RNN model that can effectively analyze hyperspectral pixels as sequential data and then determine information categories via network reasoning. As far as we know, this is the first time that an RNN framework has been proposed for hyperspectral image classification. Specifically, our RNN makes use of a newly proposed activation function, parametric rectified tanh (PRetanh), for hyperspectral sequential data analysis instead of the popular tanh or rectified linear unit. The proposed activation function makes it possible to use fairly high learning rates without the risk of divergence during the training procedure. Moreover, a modified gated recurrent unit, which uses PRetanh for hidden representation, is adopted to construct the recurrent layer in our network to efficiently process hyperspectral data and reduce the total number of parameters. Experimental results on three airborne hyperspectral images suggest competitive performance in the proposed mode. In addition, the proposed network architecture opens a new window for future research, showcasing the huge potential of deep recurrent networks for hyperspectral data analysis.","['Hyperspectral imaging', 'Recurrent neural networks', 'Logic gates', 'Support vector machines', 'Data models']","['Convolutional neural network (CNN)', 'deep learning', 'gated recurrent unit (GRU)', 'hyperspectral image classification', 'long short-term memory (LSTM)', 'recurrent neural network (RNN)']"
"The Soil Moisture and Ocean Salinity (SMOS) mission is European Space Agency (ESA's) second Earth Explorer Opportunity mission, launched in November 2009. It is a joint program between ESA Centre National d'Etudes Spatiales (CNES) and Centro para el Desarrollo Tecnologico Industrial. SMOS carries a single payload, an L-Band 2-D interferometric radiometer in the 1400-1427 MHz protected band. This wavelength penetrates well through the atmosphere, and hence the instrument probes the earth surface emissivity. Surface emissivity can then be related to the moisture content in the first few centimeters of soil, and, after some surface roughness and temperature corrections, to the sea surface salinity over ocean. The goal of the level 2 algorithm is thus to deliver global soil moisture (SM) maps with a desired accuracy of 0.04 m3/m3. To reach this goal, a retrieval algorithm was developed and implemented in the ground segment which processes level 1 to level 2 data. Level 1 consists mainly of angular brightness temperatures (TB), while level 2 consists of geophysical products in swath mode, i.e., as acquired by the sensor during a half orbit from pole to pole. In this context, a group of institutes prepared the SMOS algorithm theoretical basis documents to be used to produce the operational algorithm. The principle of the SM retrieval algorithm is based on an iterative approach which aims at minimizing a cost function. The main component of the cost function is given by the sum of the squared weighted differences between measured and modeled TB data, for a variety of incidence angles. The algorithm finds the best set of the parameters, e.g., SM and vegetation characteristics, which drive the direct TB model and minimizes the cost function. The end user Level 2 SM product contains SM, vegetation opacity, and estimated dielectric constant of any surface, TB computed at 42.5°, flags and quality indices, and other parameters of interest. This paper gives an overview of the algorithm, discusses the caveats, and provides a glimpse of the Cal Val exercises.","['Surface topography', 'Vegetation mapping', 'Sea surface', 'Ocean temperature', 'Land surface', 'L-band', 'Surface roughness']","['Cal/Val', 'model', 'SMOS', 'soil moisture', 'retrievals', 'vegetation opacity']"
"In this paper, we focus on tackling the problem of automatic accurate localization of detected objects in high-resolution remote sensing images. The two major problems for object localization in remote sensing images caused by the complex context information such images contain are achieving generalizability of the features used to describe objects and achieving accurate object locations. To address these challenges, we propose a new object localization framework, which can be divided into three processes: region proposal, classification, and accurate object localization process. First, a region proposal method is used to generate candidate regions with the aim of detecting all objects of interest within these images. Then, generic image features from a local image corresponding to each region proposal are extracted by a combination model of 2-D reduction convolutional neural networks (CNNs). Finally, to improve the location accuracy, we propose an unsupervised score-based bounding box regression (USB-BBR) algorithm, combined with a nonmaximum suppression algorithm to optimize the bounding boxes of regions that detected as objects. Experiments show that the dimension-reduction model performs better than the retrained and fine-tuned models and the detection precision of the combined CNN model is much higher than that of any single model. Also our proposed USB-BBR algorithm can more accurately locate objects within an image. Compared with traditional features extraction methods, such as elliptic Fourier transform-based histogram of oriented gradients and local binary pattern histogram Fourier, our proposed localization framework shows robustness when dealing with different complex backgrounds.","['Remote sensing', 'Feature extraction', 'Object detection', 'Proposals', 'Semantics', 'Search methods', 'Neural networks']","['Convolutional neural network (CNN)', 'object localization', 'remote sensing images', 'unsupervised score-based bounding box regression (USB-BBR)']"
"Cloud properties have been retrieved from the Moderate Resolution Imaging Spectroradiometer (MODIS) over 12 years of continuous observations from Terra and over nine years from Aqua. Results include the spatial and temporal distribution of cloud fraction, the cloud top pressure and cloud top temperature, and the cloud optical thickness and effective radius of both liquid water and ice clouds. Globally, the cloud fraction derived by the MODIS cloud mask is ~ 67%, with somewhat more clouds over land during the afternoon and less clouds over ocean in the afternoon, with very little difference in global cloud cover between Terra and Aqua. Overall, the cloud fraction over land is ~ 55%, with a distinctive seasonal cycle, whereas the ocean cloudiness is much higher, around 72%, with much reduced seasonal variation. Aqua and Terra have comparable zonal cloud top pressures, with Aqua having somewhat higher clouds (cloud top pressures lower by 100 hPa) over land due to afternoon deep convection. The coldest cloud tops (colder than 230 K) generally occur over Antarctica and the high clouds in the tropics. The cloud effective particle radius of liquid water clouds is significantly larger over ocean (mode 12-13 μm) than land (mode 10-11 μm), consistent with the variation in hygroscopic aerosol concentrations that provide cloud condensation nuclei necessary for cloud formation. We also find the effective radius to be 2-3 μm larger in the southern hemisphere than in the northern hemisphere, likely reflecting differences in sources of cloud condensation nuclei.","['Clouds', 'MODIS', 'Integrated optics', 'Optical sensors', 'Ice', 'Histograms', 'Satellites']","['Aqua', 'cloud remote sensing', 'clouds', 'Moderate Resolution Imaging Spectroradiometer (MODIS)', 'satellite applications', 'Terra', 'terrestrial atmosphere']"
"Data provided by most optical Earth observation satellites such as IKONOS, QuickBird, and GeoEye are composed of a panchromatic channel of high spatial resolution (HR) and several multispectral channels at a lower spatial resolution (LR). The fusion of an HR panchromatic and the corresponding LR spectral channels is called “pan-sharpening.” It aims at obtaining an HR multispectral image. In this paper, we propose a new pan-sharpening method named Sparse F usion of Images (SparseFI, pronounced as “sparsify”). SparseFI is based on the compressive sensing theory and explores the sparse representation of HR/LR multispectral image patches in the dictionary pairs cotrained from the panchromatic image and its downsampled LR version. Compared with conventional methods, it “learns” from, i.e., adapts itself to, the data and has generally better performance than existing methods. Due to the fact that the SparseFI method does not assume any spectral composition model of the panchromatic image and due to the super-resolution capability and robustness of sparse signal reconstruction algorithms, it gives higher spatial resolution and, in most cases, less spectral distortion compared with the conventional methods.","['Image reconstruction', 'Spatial resolution', 'Image fusion', 'Data integration']","['Data fusion', 'dictionary training', 'pan-sharpening', 'SL1MMER', 'sparse coefficients estimation', 'Sparse Fusion of Images (SparseFI)']"
"Change detection is one of the central problems in earth observation and was extensively investigated over recent decades. In this paper, we propose a novel recurrent convolutional neural network (ReCNN) architecture, which is trained to learn a joint spectral-spatial-temporal feature representation in a unified framework for change detection in multispectral images. To this end, we bring together a convolutional neural network and a recurrent neural network into one end-to-end network. The former is able to generate rich spectral-spatial feature representations, while the latter effectively analyzes temporal dependence in bitemporal images. In comparison with previous approaches to change detection, the proposed network architecture possesses three distinctive properties: 1) it is end-to-end trainable, in contrast to most existing methods whose components are separately trained or computed; 2) it naturally harnesses spatial information that has been proven to be beneficial to change detection task; and 3) it is capable of adaptively learning the temporal dependence between multitemporal images, unlike most of the algorithms that use fairly simple operation like image differencing or stacking. As far as we know, this is the first time that a recurrent convolutional network architecture has been proposed for multitemporal remote sensing image analysis. The proposed network is validated on real multispectral data sets. Both visual and quantitative analyses of the experimental results demonstrate competitive performance in the proposed mode.","['Feature extraction', 'Task analysis', 'Remote sensing', 'Convolutional neural networks', 'Earth', 'Data mining', 'Recurrent neural networks']","['Change detection', 'long short-term memory (LSTM)', 'multitemporal image analysis', 'recurrent convolutional neural network (ReCNN)']"
"Hyperspectral image (HSI) denoising is an essential preprocess step to improve the performance of subsequent applications. For HSI, there is much global and local redundancy and correlation (RAC) in spatial/spectral dimensions. In addition, denoising performance can be improved greatly if RAC is utilized efficiently in the denoising process. In this paper, an HSI denoising method is proposed by jointly utilizing the global and local RAC in spatial/spectral domains. First, sparse coding is exploited to model the global RAC in the spatial domain and local RAC in the spectral domain. Noise can be removed by sparse approximated data with learned dictionary. At this stage, only local RAC in the spectral domain is employed. It will cause spectral distortion. To compensate the shortcoming of local spectral RAC, low-rank constraint is used to deal with the global RAC in the spectral domain. Different hyperspectral data sets are used to test the performance of the proposed method. The denoising results by the proposed method are superior to results obtained by other state-of-the-art hyperspectral denoising methods.","['Noise reduction', 'Noise', 'Dictionaries', 'Spectral analysis', 'Noise measurement', 'Encoding', 'Indexes']","['Global redundancy and correlation (RAC)', 'hyperspectral image (HSI) denoising', 'local RAC', 'low rank', 'sparse representation']"
"Sentinel-1 (S-1) has an unparalleled mapping capacity. In interferometric wide swath (IW) mode, three subswaths imaged in the novel Terrain Observation by Progressive Scans (TOPS) SAR mode result in a total swath width of 250 km. S-1 has become the European workhorse for large area mapping and interferometric monitoring at medium resolution. The interferometric processing of TOPS data however requires special consideration of the signal properties, resulting from the ScanSAR-type burst imaging and the antenna beam steering in azimuth. The high Doppler rate in azimuth sets very stringent coregistration requirements, making the use of enhanced spectral diversity (ESD) necessary to obtain the required fine azimuth coregistration accuracy. Other unique aspects of processing IW data, such as azimuth spectral filtering, image resampling, and data deramping and reramping, are reviewed, giving a recipe-like description that enables the user community to use S-1 IW mode repeat-pass SAR data. Interferometric results from S-1A are provided, demonstrating the mapping capacity of the S-1 system and its interferometric suitability for geophysical applications. An interferometric evaluation of a coherent interferometric pair over Salar de Uyuni, Bolivia, is provided, where several aspects related to coregistration, deramping, and synchronization are analyzed. Additionally, a spatiotemporal evaluation of the along-track shifts, which are directly related to the orbital/instrument timing error, measured from the SAR data is shown, which justifies the necessity to refine the azimuth shifts with ESD. The spatial evaluation indicates high stability of the azimuth shifts for several slices of a datatake.","['Azimuth', 'Doppler effect', 'Synthetic aperture radar', 'Synchronization', 'Europe', 'Space vehicles', 'Monitoring']","['Coregistration', 'Interferometric SAR (InSAR)', 'Sentinel-1 (S-1)', 'synthetic aperture radar (SAR)', 'Terrain Observation by Progressive Scans (TOPS)']"
"We present a model function for the emissivity of the wind roughened ocean surface for microwave frequencies between 6 and 90 GHz. It is an update, refinement, and extension of model functions we had developed previously. The basis of our analysis are brightness temperature (TB) measurements from the spaceborne microwave radiometer WindSat and the Special Sensor Microwave/Imager, which are collocated with independent measurements of surface wind speeds and directions. This allows the determination of the emissivity model function for Earth incidence angles (EIA) around 55°. We demonstrate that an essential part in the model development is the absolute calibration of the radiometer measurements over the ocean to the computed TB of the radiative transfer model, one of whose components the emissivity model function is. We combine our results with other established measurements for lower EIA and finally obtain a model function which can be used over the whole EIA range between 0° and 65°. Results for both the isotropic, wind direction independent part as well as the four Stokes parameters of the wind direction signal are presented. Special emphasis is made on the behavior at high wind speeds between 20 and 40 m/s by conducting a comparison with data from the step frequency microwave radiometer.","['Wind speed', 'Sea surface', 'Ocean temperature', 'Sea measurements', 'Atmospheric modeling', 'Microwave radiometry']","['High wind speeds', 'microwave radiometer calibration', 'ocean surface emissivity', 'radiative transfer model (RTM)', 'Step Frequency Microwave Radiometer (SFMR)', 'Special Sensor Microwave/Imager (SSM/I)', ""Stokes' parameters"", 'WindSat']"
"So far, a large number of advanced techniques have been developed to enhance and extract the spatially semantic information in hyperspectral image processing and analysis. However, locally semantic change, such as scene composition, relative position between objects, spectral variability caused by illumination, atmospheric effects, and material mixture, has been less frequently investigated in modeling spatial information. Consequently, identifying the same materials from spatially different scenes or positions can be difficult. In this article, we propose a solution to address this issue by locally extracting invariant features from hyperspectral imagery (HSI) in both spatial and frequency domains, using a method called invariant attribute profiles (IAPs). IAPs extract the spatial invariant features by exploiting isotropic filter banks or convolutional kernels on HSI and spatial aggregation techniques (e.g., superpixel segmentation) in the Cartesian coordinate system. Furthermore, they model invariant behaviors (e.g., shift, rotation) by the means of a continuous histogram of oriented gradients constructed in a Fourier polar coordinate. This yields a combinatorial representation of spatial-frequency invariant features with application to HSI classification. Extensive experiments conducted on three promising hyperspectral data sets (Houston2013 and Houston2018) to demonstrate the superiority and effectiveness of the proposed IAP method in comparison with several state-of-the-art profile-related techniques. The codes will be available from the website: https://sites.google.com/view/danfeng-hong/data-code.","['Feature extraction', 'Hyperspectral imaging', 'Semantics', 'Data mining', 'Histograms']","['Attribute profile (AP)', 'feature extraction', 'Fourier', 'frequency', 'hyperspectral image', 'invariant', 'remote sensing', 'spatial information modeling', 'spatial???spectral classification']"
"Supervised approaches classify input data using a set of representative samples for each class, known as training samples. The collection of such samples is expensive and time demanding. Hence, unsupervised feature learning, which has a quick access to arbitrary amounts of unlabeled data, is conceptually of high interest. In this paper, we propose a novel network architecture, fully Conv-Deconv network, for unsupervised spectral-spatial feature learning of hyperspectral images, which is able to be trained in an end-to-end manner. Specifically, our network is based on the so-called encoder-decoder paradigm, i.e., the input 3-D hyperspectral patch is first transformed into a typically lower dimensional space via a convolutional subnetwork (encoder), and then expanded to reproduce the initial data by a deconvolutional subnetwork (decoder). However, during the experiment, we found that such a network is not easy to be optimized. To address this problem, we refine the proposed network architecture by incorporating: 1) residual learning and 2) a new unpooling operation that can use memorized max-pooling indexes. Moreover, to understand the “black box,” we make an in-depth study of the learned feature maps in the experimental analysis. A very interesting discovery is that some specific “neurons” in the first residual block of the proposed network own good description power for semantic visual patterns in the object level, which provide an opportunity to achieve “free” object detection. This paper, for the first time in the remote sensing community, proposes an end-to-end fully Conv-Deconv network for unsupervised spectral-spatial feature learning. Moreover, this paper also introduces an in-depth investigation of learned features. Experimental results on two widely used hyperspectral data, Indian Pines and Pavia University, demonstrate competitive performance obtained by the proposed methodology compared with other studied approaches.","['Hyperspectral imaging', 'Feature extraction', 'Training', 'Network architecture', 'Support vector machines']","['Convolutional network', 'deconvolutional network', 'hyperspectral image classification', 'residual learning', 'unsupervised spectral–spatial feature learning']"
"We analyze the fully-polarimetric Uninhabited Aerial Vehicle Synthetic Aperture Radar (UAVSAR) data acquired on June 23, 2010, from two adjacent, overlapping flight tracks that imaged the main oil slick near the Deepwater Horizon (DWH) rig site in the Gulf of Mexico. Our results show that radar backscatter from both clean water and oil in the slick is predominantly from a single surface scatterer, consistent with the tilted Bragg scattering mechanism, across the range of incidence angles from 26° to 60°. We show that the change of backscatter over the main slick is due both to a damping of the ocean wave spectral components by the oil and an effective reduction of the dielectric constant resulting from a mixture of 65-90% oil with water in the surface layer. This shows that synthetic aperture radar can be used to measure the oil volumetric concentration in a thick slick. Using the H/A/α parameters, we show that surface scattering is dominant for oil and water whenever the data are above the noise floor and that the entropy (H) and α parameters for the DWH slick are comparable to those from the clean water. The anisotropy, A, parameter shows substantial variation across the oil slick and a significant range-dependent signal whenever the backscatter in all channels is above the instrument noise floor. For slick detection, we find the most reliable indicator to be the major eigenvalue of the coherency matrix, which is approximately equal to the total backscatter power for both oil in the slick and clean sea water.","['Sea surface', 'Scattering', 'Surface waves', 'Radar', 'Backscatter', 'Surface tension']","['Oil spill', 'radar polarimetry', 'synthetic aperture radar']"
"Parametric thresholding algorithms applied to synthetic aperture radar (SAR) imagery typically require the estimation of two distribution functions, i.e., one representing the target class and one its background. They are eventually used for selecting the threshold that allows binarizing the image in an optimal way. In this context, one of the main difficulties in parameterizing these functions originates from the fact that the target class often represents only a small fraction of the image. Under such circumstances, the histogram of the image values is often not obviously bimodal and it becomes difficult, if not impossible, to accurately parameterize distribution functions. Here we introduce a hierarchical split-based approach that searches for tiles of variable size allowing the parameterization of the distributions of two classes. The method is integrated into a flood-mapping algorithm in order to evaluate its capacity for parameterizing distribution functions attributed to floodwater and changes caused by floods. We analyzed a data set acquired during a flood event along the Severn River (U.K.) in 2007. It is composed of moderate (ENVISAT-WS) and high (TerraSAR-X)-resolution SAR images. The obtained classification accuracies as well as the similarity of performance levels to a benchmark obtained with an established method based on the manual selection of tiles indicate the validity of the new method.","['Synthetic aperture radar', 'Backscatter', 'Histograms', 'Speckle', 'Distribution functions', 'Spatial resolution']","['Change detection (CD)', 'flood', 'hierarchical split-based approach (HSBA)', 'parametric thresholding', 'synthetic aperture radar (SAR)']"
"Intercalibration of satellite instruments is critical for detection and quantification of changes in the Earth's environment, weather forecasting, understanding climate processes, and monitoring climate and land cover change. These applications use data from many satellites; for the data to be interoperable, the instruments must be cross-calibrated. To meet the stringent needs of such applications, instruments must provide reliable, accurate, and consistent measurements over time. Robust techniques are required to ensure that observations from different instruments can be normalized to a common scale that the community agrees on. The long-term reliability of this process needs to be sustained in accordance with established reference standards and best practices. Furthermore, establishing physical meaning to the information through robust Système International d'unités traceable calibration and validation (Cal/Val) is essential to fully understand the parameters under observation. The processes of calibration, correction, stability monitoring, and quality assurance need to be underpinned and evidenced by comparison with “peer instruments” and, ideally, highly calibrated in-orbit reference instruments. Intercalibration between instruments is a central pillar of the Cal/Val strategies of many national and international satellite remote sensing organizations. Intercalibration techniques as outlined in this paper not only provide a practical means of identifying and correcting relative biases in radiometric calibration between instruments but also enable potential data gaps between measurement records in a critical time series to be bridged. Use of a robust set of internationally agreed upon and coordinated intercalibration techniques will lead to significant improvement in the consistency between satellite instruments and facilitate accurate monitoring of the Earth's climate at uncertainty levels needed to detect and attribute the mechanisms of change. This paper summarizes the state-of-the-art of postlaunch radiometric calibration of remote sensing satellite instruments through intercalibration.","['Instruments', 'Calibration', 'Satellites', 'Satellite broadcasting', 'Measurement uncertainty', 'Uncertainty', 'Standards']","['Calibration', 'comparison', 'constellations', 'correction', 'cross-calibration', 'Earth Observing (EO) System', 'infrared', 'intercalibration', 'international collaboration', 'microwave', 'monitoring', 'radiometric calibration', 'reflective solar band (RSB)', 'satellite', 'satellites', 'thermal infrared', 'traceability', 'validation', 'visible']"
"With a large amount of open satellite multispectral (MS) imagery (e.g., Sentinel-2 and Landsat-8), considerable attention has been paid to global MS land cover classification. However, its limited spectral information hinders further improving the classification performance. Hyperspectral imaging enables discrimination between spectrally similar classes but its swath width from space is narrow compared to MS ones. To achieve accurate land cover classification over a large coverage, we propose a cross-modality feature learning framework, called common subspace learning (CoSpace), by jointly considering subspace learning and supervised classification. By locally aligning the manifold structure of the two modalities, CoSpace linearly learns a shared latent subspace from hyperspectral-MS (HS-MS) correspondences. The MS out-of-samples can be then projected into the subspace, which are expected to take advantages of rich spectral information of the corresponding hyperspectral data used for learning, and thus leads to a better classification. Extensive experiments on two simulated HS-MS data sets (University of Houston and Chikusei), where HS-MS data sets have tradeoffs between coverage and spectral resolution, are performed to demonstrate the superiority and effectiveness of the proposed method in comparison with previous state-of-the-art methods.","['Hyperspectral imaging', 'Optimization', 'Satellites', 'Earth', 'Manifolds']","['Common subspace learning (CoSpace)', 'cross-modality learning', 'hyperspectral', 'landcover classification', 'multispectral (MS)', 'remote sensing']"
"The synthetic aperture radar (SAR) Doppler centroid has been used to estimate the scatter line-of-sight radar velocity. In weak to moderate ocean surface current environment, the SAR Doppler centroid is dominated by the directionality and strength of wave-induced ocean surface displacements. In this paper, we show how this sea state signature can be used to improve surface wind retrieval from SAR. Doppler shifts of C-band radar return signals from the ocean are thoroughly investigated by colocating wind measurements from the ASCAT scatterometer with Doppler centroid anomalies retrieved from Envisat ASAR. An empirical geophysical model function (CDOP) is derived, predicting Doppler shifts at both VV and HH polarization as function of wind speed, radar incidence angle, and wind direction with respect to radar look direction. This function is used into a Bayesian inversion scheme in combination with wind from a priori forecast model and the normalized radar cross section (NRCS). The benefit of Doppler for SAR wind retrieval is shown in complex meteorological situations such as atmospheric fronts or low pressure systems. Using in situ information, validation reveals that this method helps to improve the wind direction retrieval. Uncertainty of the calibration of Doppler shift from Envisat ASAR hampers the inversion scheme in cases where NRCS and model wind are accurate and in close agreement. The method is however very promising with respect of future SAR missions, in particular Sentinel-1, where the Doppler centroid anomaly will be more robustly retrieved.","['Doppler shift', 'Wind speed', 'Doppler radar', 'Sea surface']","['Doppler', 'surface wind', 'synthetic aperture radar (SAR)']"
"Coastlines, shoals, and reefs are some of the most dynamic and constantly changing regions of the globe. The emergence of high-resolution satellites with new spectral channels, such as the WorldView-2, increases the amount of data available, thereby improving the determination of coastal management parameters. Water-leaving radiance is very difficult to determine accurately, since it is often small compared to the reflected radiance from other sources such as atmospheric and water surface scattering. Hence, the atmospheric correction has proven to be a very important step in the processing of high-resolution images for coastal applications. On the other hand, specular reflection of solar radiation on nonflat water surfaces is a serious confounding factor for bathymetry and for obtaining the seafloor albedo with high precision in shallow-water environments. This paper describes, at first, an optimal atmospheric correction model, as well as an improved algorithm for sunglint removal based on combined physical and image processing techniques. Then, using the corrected multispectral data, an efficient multichannel physics-based algorithm has been implemented, which is capable of solving through optimization the radiative transfer model of seawater for bathymetry retrieval, unmixing the water intrinsic optical properties, depth, and seafloor albedo contributions. Finally, for the mapping of benthic features, a supervised classification methodology has been implemented, combining seafloor-type normalized indexes and support vector machine techniques. Results of atmospheric correction, remote bathymetry, and benthic habitat mapping of shallow-water environments have been validated with in situ data and available bionomic profiles providing excellent accuracy.","['Atmospheric modeling', 'Sea measurements', 'Satellites', 'Sea surface', 'Satellite broadcasting', 'Sensors', 'Atmospheric measurements']","['Atmospheric model', 'bathymetry mapping', 'benthic habitat mapping', 'high-resolution multispectral imagery', 'physical and image processing techniques', 'sunglint', 'WorldView-2 (WV2)']"
"We identify canopy species in a Hawaiian tropical forest using supervised classification applied to airborne hyperspectral imagery acquired with the Carnegie Airborne Observatory-Alpha system. Nonparametric methods (linear and radial basis function support vector machine, artificial neural network, and k-nearest neighbor) and parametric methods (linear, quadratic, and regularized discriminant analysis) are compared for a range of species richness values and training sample sizes. We find a clear advantage in using regularized discriminant analysis, linear discriminant analysis, and support vector machines. No unique optimal classifier was found for all conditions tested, but we highlight the possibility of improving support vector machine classification with a better optimization of its free parameters. We also confirm that a combination of spectral and spatial information increases accuracy of species classification: we combine segmentation and species classification from regularized discriminant analysis to produce a map of the 17 discriminated species. Finally, we compare different methods to assess spectral separability and find a better ability of Bhattacharyya distance to assess separability within and among species. The results indicate that species mapping is tractable in tropical forests when using high-fidelity imaging spectroscopy.","['Vegetation', 'Training', 'Accuracy', 'Image segmentation', 'Measurement', 'Hyperspectral imaging', 'Imaging']","['Carnegie Airborne Observatory (CAO)', 'hyperspectral imaging', 'image classification', 'tree species identification', 'tropical biodiversity']"
"Over the past few years, hyperspectral image classification using convolutional neural networks (CNNs) has progressed significantly. In spite of their effectiveness, given that hyperspectral images are of high dimensionality, CNNs can be hindered by their modeling of all spectral bands with the same weight, as probably not all bands are equally informative and predictive. Moreover, the usage of useless spectral bands in CNNs may even introduce noises and weaken the performance of networks. For the sake of boosting the representational capacity of CNNs for spectral-spatial hyperspectral data classification, in this work, we improve networks by discriminating the significance of different spectral bands. We design a network unit, which is termed as the spectral attention module, that makes use of a gating mechanism to adaptively recalibrate spectral bands by selectively emphasizing informative bands and suppressing less useful ones. We theoretically analyze and discuss why such a spectral attention module helps in a CNN for hyperspectral image classification. We demonstrate using extensive experiments that in comparison with state-of-the-art approaches, the spectral attention module-based convolutional networks are able to offer competitive results. Furthermore, this work sheds light on how a CNN interacts with spectral bands for the purpose of classification.","['Hyperspectral imaging', 'Logic gates', 'Task analysis', 'Convolution', 'Support vector machines']","['Attention module', 'convolutional neural network (CNN)', 'gating mechanism', 'hyperspectral image classification']"
"Over the past few years making use of deep networks, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), classifying hyperspectral images has progressed significantly and gained increasing attention. In spite of being successful, these networks need an adequate supply of labeled training instances for supervised learning, which, however, is quite costly to collect. On the other hand, unlabeled data can be accessed in almost arbitrary amounts. Hence it would be conceptually of great interest to explore networks that are able to exploit labeled and unlabeled data simultaneously for hyperspectral image classification. In this article, we propose a novel graph-based semisupervised network called nonlocal graph convolutional network (nonlocal GCN). Unlike existing CNNs and RNNs that receive pixels or patches of a hyperspectral image as inputs, this network takes the whole image (including both labeled and unlabeled data) in. More specifically, a nonlocal graph is first calculated. Given this graph representation, a couple of graph convolutional layers are used to extract features. Finally, the semisupervised learning of the network is done by using a cross-entropy error over all labeled instances. Note that the nonlocal GCN is end-to-end trainable. We demonstrate in extensive experiments that compared with state-of-the-art spectral classifiers and spectral-spatial classification networks, the nonlocal GCN is able to offer competitive results and high-quality classification maps (with fine boundaries and without noisy scattered points of misclassification).","['Hyperspectral imaging', 'Convolution', 'Task analysis', 'Recurrent neural networks', 'Semisupervised learning', 'Support vector machines']","['Graph convolutional network (GCN)', 'hyperspectral image classification', 'nonlocal graph', 'semisupervised learning']"
"The problem of automatically recognizing and fitting hyperbolae from ground-penetrating radar (GPR) images is addressed, and a novel technique computationally suitable for real-time on-site application is proposed. After preprocessing of the input GPR images, a novel thresholding method is applied to separate the regions of interest from background. A novel column-connection clustering (C3) algorithm is then applied to separate the regions of interest from each other. Subsequently, a machine learnt model is applied to identify hyperbolic signatures from outputs of the C3 algorithm, and a hyperbola is fitted to each such signature with an orthogonal-distance hyperbola fitting algorithm. The novel clustering algorithm C3 is a central component of the proposed system, which enables the identification of hyperbolic signatures and hyperbola fitting. Only two features are used in the machine learning algorithm, which is easy to train using a small set of training data. An orthogonal-distance hyperbola fitting algorithm for “south-opening” hyperbolae is introduced in this work, which is more robust and accurate than algebraic hyperbola fitting algorithms. The proposed method can successfully recognize and fit hyperbolic signatures with intersections with others, hyperbolic signatures with distortions, and incomplete hyperbolic signatures with one leg fully or largely missed. As an additional novel contribution, formulas to compute an initial “south-opening” hyperbola directly from a set of given points are derived, which make the system more efficient. The parameters obtained by fitting hyperbolae to hyperbolic signatures are very important features; they can be used to estimate the location and size of the related target objects and the average propagation velocity of the electromagnetic wave in the medium. The effectiveness of the proposed system is tested on both synthetic and real GPR data.","['Ground penetrating radar', 'Machine learning algorithms', 'Clustering algorithms', 'Image edge detection', 'Transforms', 'Partitioning algorithms', 'Real-time systems']","['Buried asset detection', 'column-connection clustering (C3) algorithm', 'ground-penetrating radar (GPR)', 'hyperbola recognition', 'machine learning', 'orthogonal-distance fitting']"
"Ground-penetrating radar (GPR) is a powerful and rapidly maturing technology for subsurface threat identification. However, sophisticated processing of GPR data is necessary to reduce false alarms due to naturally occurring subsurface clutter and soil distortions. Most currently fielded GPR-based landmine detection algorithms utilize feature extraction and statistical learning to develop robust classifiers capable of discriminating buried threats from inert subsurface structures. Analysis of these techniques indicates strong underlying similarities between efficient landmine detection algorithms and modern techniques for feature extraction in the computer vision literature. This paper explores the relationship between and application of one modern computer vision feature extraction technique, namely histogram of oriented gradients (HOG), to landmine detection in GPR data. The results presented indicate that HOG features provide a robust tool for target identification for both classification and prescreening and suggest that other techniques from computer vision might also be successfully applied to target detection in GPR data.",[],[]
"Land surface albedo is a critical parameter in surface-energy budget studies. Over the past several decades, many albedo products are generated from remote-sensing data sets. The Moderate Resolution Imaging Spectroradiometer (MODIS) bidirectional reflectance distribution function (BRDF)/Albedo algorithm is used to routinely produce eight day (16-day composite), 1-km resolution MODIS albedo products. When some natural processes or human activities occur, the land-surface broadband albedo can change rapidly, so it is necessary to enhance the temporal resolution of albedo product. We present a direct-estimation algorithm for mapping daily land-surface broadband albedo from MODIS data. The polarization and directionality of the Earth's reflectance-3/polarization and anisotropy of reflectances for atmospheric sciences coupled with observations from a Lidar BRDF database is employed as a training data set, and the 6S atmospheric radiative transfer code is used to simulate the top-of-atmosphere (TOA) reflectances. Then a relationship between TOA reflectances and land-surface broadband albedos is developed using an angular bin regression method. The robustness of this method for different angular bins, aerosol conditions, and land-cover types is analyzed. Simulation results show that the absolute error of this algorithm is∼0.009for vegetation, 0.012 for soil, and 0.030 for snow/ice. Validation of the direct-estimation algorithm against in situ measurement data shows that the proposed method is capable of characterizing the temporal variation of albedo, especially when the land-surface BRDF changes rapidly.",[],[]
"Object detection and semantic segmentation are two main themes in object retrieval from high-resolution remote sensing images, which have recently achieved remarkable performance by surfing the wave of deep learning and, more notably, convolutional neural networks. In this paper, we are interested in a novel, more challenging problem of vehicle instance segmentation, which entails identifying, at a pixel level, where the vehicles appear as well as associating each pixel with a physical instance of a vehicle. In contrast, vehicle detection and semantic segmentation each only concern one of the two. We propose to tackle this problem with a semantic boundary-aware multitask learning network. More specifically, we utilize the philosophy of residual learning to construct a fully convolutional network that is capable of harnessing multilevel contextual feature representations learned from different residual blocks. We theoretically analyze and discuss why residual networks can produce better probability maps for pixelwise segmentation tasks. Then, based on this network architecture, we propose a unified multitask learning network that can simultaneously learn two complementary tasks, namely, segmenting vehicle regions and detecting semantic boundaries. The latter subproblem is helpful for differentiating “touching” vehicles that are usually not correctly separated into instances. Currently, data sets with a pixelwise annotation for vehicle extraction are the ISPRS data set and the IEEE GRSS DFC2015 data set over Zeebrugge, which specializes in a semantic segmentation. Therefore, we built a new, more challenging data set for vehicle instance segmentation, called the Busy Parking Lot Unmanned Aerial Vehicle Video data set, and we make our data set available at http://www.sipeo.bgu.tum.de/downloads so that it can be used to benchmark future vehicle instance segmentation algorithms.","['Image segmentation', 'Semantics', 'Feature extraction', 'Vehicle detection', 'Remote sensing', 'Task analysis', 'Object detection']","['Boundary-aware multitask learning network', 'fully convolutional network (FCN)', 'high-resolution remote sensing image/video', 'instance semantic segmentation', 'residual neural network (ResNet)', 'vehicle detection']"
"In this paper, sea ice in the Central Arctic has been classified in synthetic aperture radar (SAR) images from ENVISAT using a neural network (NN)-based algorithm and a Bayesian algorithm. Since different sea ice types can have similar backscattering coefficients at C-band HH polarization, it is necessary to use textural features in addition to the backscattering coefficients. The analysis revealed that the most informative texture features for the classification of multiyear ice (MYI), deformed first-year ice (FYI) (DFYI), and level FYI (LFYI) and open water/nilas are correlation, inertia, cluster prominence, energy, homogeneity, and entropy, as well as third and fourth central statistical moments of image brightness. The optimal topology of the NN, trained for ENVISAT wide-swath SAR sea ice classification, consists of nine neurons in input layer, six neurons in hidden layer, and three neurons in output layer. The classification results for a series of 20 SAR images, acquired in the central part of the Arctic Ocean during winter months, were compared to expert analysis of the images and ice charts. The results of the NN classification show that the average correspondences with the expert analysis amount to 85%, 83%, and 80%for LFYI, DFYI, and MYI, respectively. The Bayesian pixel-based method can provide a higher resolution in the classified image and, therefore, better capability to identify leads compared to the NN method. Both methods may be effectively used in the Central Arctic where MYI is predominant.","['Synthetic aperture radar', 'Sea ice', 'Backscatter', 'Artificial neural networks', 'Algorithm design and analysis', 'Classification algorithms']","['Classification', 'neural network (NN) algorithm', 'sea ice', 'synthetic aperture radar (SAR)']"
"Canopy structure plays an essential role in biophysical activities in forest environments. However, quantitative descriptions of a 3-D canopy structure are extremely difficult because of the complexity and heterogeneity of forest systems. Airborne laser scanning (ALS) provides an opportunity to automatically measure a 3-D canopy structure in large areas. Compared with other point cloud technologies such as the image-based Structure from Motion, the power of ALS lies in its ability to penetrate canopies and depict subordinate trees. However, such capabilities have been poorly explored so far. In this paper, the potential of ALS-based approaches in depicting a 3-D canopy structure is explored in detail through an international benchmarking of five recently developed ALS-based individual tree detection (ITD) methods. For the first time, the results of the ITD methods are evaluated for each of four crown classes, i.e., dominant, codominant, intermediate, and suppressed trees, which provides insight toward understanding the current status of depicting a 3-D canopy structure using ITD methods, particularly with respect to their performances, potential, and challenges. This benchmarking study revealed that the canopy structure plays a considerable role in the detection accuracy of ITD methods, and its influence is even greater than that of the tree species as well as the species composition in a stand. The study also reveals the importance of utilizing the point cloud data for the detection of intermediate and suppressed trees. Different from what has been reported in previous studies, point density was found to be a highly influential factor in the performance of the methods that use point cloud data. Greater efforts should be invested in the point-based or hybrid ITD approaches to model the 3-D canopy structure and to further explore the potential of high-density and multiwavelengths ALS data.","['Vegetation', 'Benchmark testing', 'Three-dimensional displays', 'Remote sensing', 'Solid modeling', 'Geospatial analysis', 'Geography']","['Airborne laser scanning (ALS)', 'benchmark', 'canopy structure', 'crown class', 'individual tree detection (ITD)', 'LiDAR', 'point cloud', 'subordinate tree']"
"The long time series of nighttime light (NTL) data collected by DMSP/OLS sensors provides a unique and valuable resource to study changes in human activities. However, its full time series potential has not been fully explored, mainly due to inconsistencies in the temporal signal. Previous studies have tried to resolve this issue in order to generate a consistent NTL time series. However, due to geographic limitations with the algorithms, these approaches cannot generate a coherent NTL time series globally. The purpose of this study is to develop a methodology to create a consistent NTL time series that can be applied globally. Our method is based on a novel sampling strategy to identify pseudoinvariant features. We select data points along a ridgeline-the densest part of a density plot generated between the reference image and the target image-and then use those data points to derive calibration models to minimize inconsistencies in the NTL time series. Results show that the algorithm successfully calibrates DMSP/OLS annual composites and generates a consistent NTL time series. Evaluation of the results shows that the calibrated NTL time series significantly reduces the differences between two images within the same year and increases the correlations between the NTL time series and gross domestic product as well as with energy consumption, and outperforms the Elvidge et al. (2014) method. The methodology is simple, robust, and easy to implement. The quality-enhanced NTL time series can be used in a myriad of applications that require a consistent data set over time.","['Time series analysis', 'Calibration', 'Satellites', 'Sensors', 'Earth', 'Satellite broadcasting', 'Systematics']","['Calibration sites', 'intersensor calibration', 'ridge regression', 'ridge sampling and regression', 'ridgeline sampling regression (RSR)', 'satellite image normalization']"
"One of the main challenges in through-the-wall radar imaging (TWRI) is the strong exterior wall returns, which tend to obscure indoor stationary targets, rendering target detection and classification difficult, if not impossible. In this paper, an effective wall clutter mitigation approach is proposed for TWRI that does not require knowledge of the background scene nor does it rely on accurate modeling and estimation of wall parameters. The proposed approach is based on the relative strength of the exterior wall returns compared to behind-wall targets. It applies singular value decomposition to the data matrix constructed from the space-frequency measurements to identify the wall subspace. Orthogonal subspace projection is performed to remove the wall electromagnetic signature from the radar signals. Furthermore, this paper provides an analysis of the wall and target subspace characteristics, demonstrating that both wall and target subspaces can be multidimensional. While the wall subspace depends on the wall type and building material, the target subspace depends on the location of the target, the number of targets in the scene, and the size of the target. Experimental results using simulated and real data demonstrate the effectiveness of the subspace projection method in mitigating wall clutter while preserving the target image. It is shown that the performance of the proposed approach, in terms of the improvement factor of the target-to-clutter ratio, is better than existing approaches and is comparable to that of background subtraction, which requires knowledge of a reference background scene.","['Antenna arrays', 'Clutter', 'Radar imaging', 'Radar antennas', 'Indexes']","['Singular value decomposition (SVD)', 'subspace projection', 'target subspace', 'through-the-wall radar imaging (TWRI)', 'wall clutter removal', 'wall subspace']"
"With respect to recent advances in remote sensing technologies, the spatial resolution of airborne and spaceborne sensors is getting finer, which enables us to precisely analyze even small objects on the Earth. This fact has made the research area of developing efficient approaches to extract spatial and contextual information highly active. Among the existing approaches, morphological profile and attribute profile (AP) have gained great attention due to their ability to classify remote sensing data. This paper proposes a novel approach that makes it possible to precisely extract spatial and contextual information from remote sensing images. The proposed approach is based on extinction filters, which are used here for the first time in the remote sensing community. Then, the approach is carried out on two well-known high-resolution panchromatic data sets captured over Rome, Italy, and Reykjavik, Iceland. In order to prove the capabilities of the proposed approach, the obtained results are compared with the results from one of the strongest approaches in the literature, i.e., APs, using different points of view such as classification accuracies, simplification rate, and complexity analysis. Results indicate that the proposed approach can significantly outperform its alternative in terms of classification accuracies. In addition, based on our implementation, profiles can be generated in a very short processing time. It should be noted that the proposed approach is fully automatic.","['Remote sensing', 'Data mining', 'Feature extraction', 'Radio frequency', 'Spatial resolution', 'Earth']","['Attribute profile (AP)', 'extinction profile (EP)', 'image classification', 'random forest (RF)', 'remote sensing data']"
"This paper explores the use of full polarimetric synthetic aperture radar (PolSAR) images for tsunami damage investigation from the polarimetric viewpoint. The great tsunami induced by the earthquake of March 11th, 2011, which occurred beneath the Pacific off the northeastern coast of Japan, is adopted as the study case using the Advanced Land Observing Satellite/Phased Array type L-band Synthetic Aperture Radar multitemporal PolSAR images. The polarimetric scattering mechanism changes were quantitatively examined with model-based decomposition. It is clear that the observed reduction in the double-bounce scattering was due to a change into odd-bounce scattering, since a number of buildings were completely washed away, leaving relatively a rough surface. Polarization orientation (PO) angles in built-up areas are also investigated. After the tsunami, PO angle distributions from damaged areas spread to a wider range and fluctuated more strongly than those from the before-tsunami period. Two polarimetric indicators are proposed for damage level discrimination at the city block scale. One is the ratio of the dominant double-bounce scattering mechanism observed after-tsunami to that observed before-tsunami, which can directly reflect the amount of destroyed ground-wall structures in built-up areas. The second indicator is the standard deviation of the PO angle differences, which is used to interpret the homogeneity reduction of PO angles. Experimental results from after- and before-tsunami comparisons validate the efficiency of these indexes, since the built-up areas with different damage levels can be well discriminated. In addition, comparisons between before-tsunami pairs further confirm the stability of the two polarimetric indexes over a long temporal duration. These interesting results also demonstrate the importance of full polarimetric information for natural disaster assessment.","['Scattering', 'Tsunami', 'Buildings', 'Synthetic aperture radar', 'Urban areas', 'Spaceborne radar', 'Earthquakes']","['Damage assessment', 'model-based decomposition', 'natural disaster', 'polarimetry', 'polarization orientation angle', 'synthetic aperture radar (SAR)', 'urban area']"
"Anomalies usually refer to targets with a spot of pixels (even subpixels) that stand out from their neighboring background clutter pixels in hyperspectral imagery (HSI). Compared to backgrounds, anomalies have two main characteristics. One is the spectral anomaly, i.e., their spectral signatures are different from those associated to their surrounding backgrounds; another is the spatial anomaly, i.e., anomalies occur as few pixels (even subpixels) embedded in the local homogeneous backgrounds. However, most of the existing anomaly detection algorithms for HSI only employed the spectral anomaly. If the two characteristics are exploited in a detection method simultaneously, better performance may be achieved. The third-order (two modes for space and one mode for spectra) tensor representation of HSI has been proved to be an effective tool to describe the spatial and spectral information equivalently; therefore, tensor representation is convenient for exhibiting the two characteristics of anomalies simultaneously. In this paper, a new anomaly detection method based on tensor decomposition is proposed and divided into three steps. Three factor matrices and a core tensor are first estimated from the third-order tensor that is constructed from the HSI data cube by using the Tucker decomposition, and their major and minor principal components (PCs) are more likely to correspond to the spectral signatures of the backgrounds and the anomalies, respectively. In the second step, a reconstruction-error-based method is presented to find the first largest PCs along each mode to eliminate the spectral signatures of the backgrounds as much as possible, and thus, the remaining data may be modeled as the spectral signatures of the anomalies with a Gaussian noise. Finally, a CFAR test is implemented to detect the anomalies from the remaining data. Experiments with simulated, synthetic, and real HSI data sets reveal that the proposed method outperforms those spectral-anomaly-based methods with better detection probability and less false alarm rate.","['Tensile stress', 'Detection algorithms', 'Matrix decomposition', 'Detectors', 'Hyperspectral imaging', 'Computational modeling']","['Anomaly detection', 'hyperspectral imagery (HSI)', 'tensor representation', 'Tucker decomposition']"
"We present an advanced differential synthetic aperture radar (SAR) interferometry (DInSAR) processing chain, based on the Parallel Small BAseline Subset (P-SBAS) technique, for the efficient generation of deformation time series from Sentinel-1 (S-1) interferometric wide (IW) swath SAR data sets. We first discuss an effective solution for the generation of high-quality interferograms, which properly accounts for the peculiarities of the terrain observation with progressive scans (TOPS) acquisition mode used to collect S-1 IW SAR data. These data characteristics are also properly accounted within the developed processing chain, taking full advantage from the burst partitioning. Indeed, such data structure represents a key element in the proposed P-SBAS implementation of the S-1 IW processing chain, whose migration into a cloud computing (CC) environment is also envisaged. An extensive experimental analysis, which allows us to assess the quality of the obtained interferometric products, is presented. To do this, we apply the developed S-1 IW P-SBAS processing chain to the overall archive acquired from descending orbits during the March 2015-April 2017 time span over the whole Italian territory, consisting in 2740 S-1 slices. In particular, the quality of the final results is assessed through a large-scale comparison with the GPS measurements relevant to nearly 500 stations. The mean standard deviation value of the differences between the DInSAR and the GPS time series (projected in the radar line of sight) is less than 0.5 cm, thus confirming the effectiveness of the implemented solution. Finally, a discussion about the performance achieved by migrating the developed processing chain within the Amazon Web Services CC environment is addressed, highlighting that a two-year data set relevant to a standard S-1 IW slice can be reliably processed in about 30 h.The presented results demonstrate the capability of the implemented P-SBAS approach to efficiently and effectively process large S-1 IW data sets relevant to extended portions of the earth surface, paving the way to the systematic generation of advanced DInSAR products to monitor ground displacements at a very wide spatial scale.","['Synthetic aperture radar', 'Interferometry', 'Strain', 'Distributed databases', 'Time series analysis', 'Orbits']","['Cloud computing (CC)', 'deformation time series', 'differential synthetic aperture radar interferometry (DInSAR)', 'GPS', 'Parallel Small BAseline Subset (P-SBAS)', 'Sentinel-1']"
"Most current semantic segmentation approaches fall back on deep convolutional neural networks (CNNs). However, their use of convolution operations with local receptive fields causes failures in modeling contextual spatial relations. Prior works have sought to address this issue by using graphical models or spatial propagation modules in networks. But such models often fail to capture long-range spatial relationships between entities, which leads to spatially fragmented predictions. Moreover, recent works have demonstrated that channel-wise information also acts a pivotal part in CNNs. In this article, we introduce two simple yet effective network units, the spatial relation module, and the channel relation module to learn and reason about global relationships between any two spatial positions or feature maps, and then produce Relation-Augmented (RA) feature representations. The spatial and channel relation modules are general and extensible, and can be used in a plug-and-play fashion with the existing fully convolutional network (FCN) framework. We evaluate relation module-equipped networks on semantic segmentation tasks using two aerial image data sets, namely International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam data sets, which fundamentally depend on long-range spatial relational reasoning. The networks achieve very competitive results, a mean F 1 score of 88.54% on the Vaihingen data set and a mean F 1 score of 88.01% on the Potsdam data set, bringing significant improvements over baselines.","['Semantics', 'Image segmentation', 'Task analysis', 'Visualization', 'Remote sensing', 'Cognition', 'Convolution']","['Fully convolutional network (FCN)', 'high resolution aerial imagery', 'relation network', 'semantic segmentation']"
"Signal decorrelation poses a limitation to multipass SAR interferometry. In pursuit of overcoming this limitation to achieve high-precision deformation estimates, different techniques have been developed, with short baseline subset, SqueeSAR, and CAESAR as the overarching schemes. These different analysis approaches raise the question of their efficiency and limitation in phase and consequently deformation estimation. This contribution first addresses this question and then proposes a new estimator with improved performance, called Eigendecomposition-based Maximum-likelihood-estimator of Interferometric phase (EMI). The proposed estimator combines the advantages of the state-of-the-art techniques. Identical to CAESAR, EMI is solved using eigendecomposition; it is therefore computationally efficient and straightforward in implementation. Similar to SqueeSAR, EMI is a maximum-likelihood-estimator; hence, it retains estimation efficiency. The computational and estimation efficiency of EMI renders it as an optimum choice for phase estimation. A further marriage of EMI with the proposed Sequential Estimator by Ansari et al. provides an efficient processing scheme tailored to the analysis of Big InSAR Data. EMI is formulated and verified in relation to the state-of-the-art approaches via mathematical formulation, simulation analysis, and experiments with time series of Sentinel-1 data over the volcanic island of Vulcano, Italy.","['Electromagnetic interference', 'Time series analysis', 'Maximum likelihood estimation', 'Systematics', 'Synthetic aperture radar', 'Strain']","['Big Data', 'coherence matrix', 'covariance estimation', 'differential interferometric synthetic aperture radar', 'distributed scatterers (DS)', 'efficiency', 'error analysis', 'maximum-likelihood estimation', 'near real-time (NRT) processing']"
"The first products of the Global Space-based Inter-Calibration System (GSICS) include bias monitoring and calibration corrections for the thermal infrared (IR) channels of current meteorological sensors on geostationary satellites. These use the hyperspectral Infrared Atmospheric Sounding Interferometer (IASI) on the low Earth orbit (LEO) Metop satellite as a common cross-calibration reference. This paper describes the algorithm, which uses a weighted linear regression, to compare collocated radiances observed from each pair of geostationary-LEO instruments. The regression coefficients define the GSICS Correction, and their uncertainties provide quality indicators, ensuring traceability to the selected community reference, IASI. Examples are given for the Meteosat, GOES, MTSAT, Fengyun-2, and COMS imagers. Some channels of these instruments show biases that vary with time due to variations in the thermal environment, stray light, and optical contamination. These results demonstrate how inter-calibration can be a powerful tool to monitor and correct biases, and help diagnose their root causes.","['Instruments', 'Low earth orbit satellites', 'Calibration', 'Uncertainty', 'Monitoring', 'Meteorology']","['Calibration', 'earth observing system', 'infrared image sensors', 'international collaboration', 'meteorology', 'satellites']"
"Geosynchronous synthetic aperture radar (GEO SAR) has been studied for several decades but has not yet been implemented. This paper provides an overview of mission design, describing significant constraints (atmosphere, orbit, temporal stability of the surface and atmosphere, measurement physics, and radar performance) and then uses these to propose an approach to initial system design. The methodology encompasses all GEO SAR mission concepts proposed to date. Important classifications of missions are: 1) those that require atmospheric phase compensation to achieve their design spatial resolution; and 2) those that achieve full spatial resolution without phase compensation. Means of estimating the atmospheric phase screen are noted, including a novel measurement of the mean rate of change of the atmospheric phase delay, which GEO SAR enables. Candidate mission concepts are described. It seems likely that GEO SAR will be feasible in a wide range of situations, although extreme weather and unstable surfaces (e.g., water, tall vegetation) prevent 100% coverage. GEO SAR offers an exciting imaging capability that powerfully complements existing systems.","['Synthetic aperture radar', 'Orbits', 'Azimuth', 'System analysis and design', 'Atmospheric measurements', 'Low earth orbit satellites']","['Atmosphere', 'geosynchronous (GEO)', 'mission', 'synthetic aperture radar (SAR)', 'system']"
"Accurate flood mapping is important for both planning activities during emergencies and as a support for the successive assessment of damaged areas. A valuable information source for such a procedure can be remote sensing synthetic aperture radar (SAR) imagery. However, flood scenarios are typical examples of complex situations in which different factors have to be considered to provide accurate and robust interpretation of the situation on the ground. For this reason, a data fusion approach of remote sensing data with ancillary information can be particularly useful. In this paper, a Bayesian network is proposed to integrate remotely sensed data, such as multitemporal SAR intensity images and interferometric-SAR coherence data, with geomorphic and other ground information. The methodology is tested on a case study regarding a flood that occurred in the Basilicata region (Italy) on December 2013, monitored using a time series of COSMO-SkyMed data. It is shown that the synergetic use of different information layers can help to detect more precisely the areas affected by the flood, reducing false alarms and missed identifications which may affect algorithms based on data from a single source. The produced flood maps are compared to data obtained independently from the analysis of optical images; the comparison indicates that the proposed methodology is able to reliably follow the temporal evolution of the phenomenon, assigning high probability to areas most likely to be flooded, in spite of their heterogeneous temporal SAR/InSAR signatures, reaching accuracies of up to 89%.","['Synthetic aperture radar', 'Remote sensing', 'Rivers', 'Sensors', 'Data models', 'Probabilistic logic', 'Data integration']","['Bayesian networks (BNs)', 'data fusion', 'flood mapping', 'synthetic aperture radar (SAR) change detection', 'synthetic aperture radar (SAR)/interferometric SAR (InSAR) time series analysis']"
"One challenge to implementing spectral change detection algorithms using multitemporal Landsat data is that key dates and periods are often missing from the record due to weather disturbances and lapses in continuous coverage. This paper presents a method that utilizes residuals from harmonic regression over years of Landsat data, in conjunction with statistical quality control charts, to signal subtle disturbances in vegetative cover. These charts are able to detect changes from both deforestation and subtler forest degradation and thinning. First, harmonic regression residuals are computed after fitting models to interannual training data. These residual time series are then subjected to Shewhart X-bar control charts and exponentially weighted moving average charts. The Shewhart X-bar charts are also utilized in the algorithm to generate a data-driven cloud filter, effectively removing clouds and cloud shadows on a location-specific basis. Disturbed pixels are indicated when the charts signal a deviation from data-driven control limits. The methods are applied to a collection of loblolly pine ( Pinus taeda) stands in Alabama, USA. The results are compared with stands for which known thinning has occurred at known times. The method yielded an overall accuracy of 85%, with the particular result that it provided afforestation/deforestation maps on a per-image basis, producing new maps with each successive incorporated image. These maps matched very well with observed changes in aerial photography over the test period. Accordingly, the method is highly recommended for on-the-fly change detection, for changes in both land use and land management within a given land use.","['Satellites', 'Earth', 'Remote sensing', 'Harmonic analysis', 'Control charts', 'Time series analysis', 'Indexes']","['Degradation', 'statistical process control', 'thinning', 'trajectory']"
"Recently, sparse signal representation of image patches has been explored to solve the pansharpening problem. Although these proposed sparse-reconstruction-based methods lead to promising results, three issues remained unsolved: 1) high computational cost; 2) no consideration given to the possibility of mutually correlated information in different multispectral channels; and 3) requirement that the spectral responses of the panchromatic (Pan) image and the multispectral image cover the same wavelength range, which is not necessarily valid for most sensors. In this paper, we propose a sophisticated sparse image fusion algorithm, which is named “jointly sparse fusion of images” (J-SparseFI). It is based on the earlier proposed sparse fusion of images (SparseFI) algorithm and overcomes the aforementioned three drawbacks of the existing sparse image fusion algorithms. The computational problem is handled by reducing the problem size and by proposing a fully parallelizable scheme. Moreover, J-SparseFI exploits the possible signal structure correlations between multispectral channels by introducing the joint sparsity model (JSM) and sharpening the highly correlated adjacent multispectral channels together. This is done by exploiting the distributed compressive sensing theory that restricts the solution of an underdetermined system by considering an ensemble of signals being jointly sparse. J-SparseFI also offers a practical solution to overcome spectral range mismatch between the Pan and multispectral images. By means of sensor spectral response and channel mutual correlation analysis, the multispectral channels are assigned to primary groups of joint channels, secondary groups of joint channels, and individual channels. Primary groups of joint channels, individual channels, and secondary groups of joint channels are then reconstructed sequentially, by the JSM or by modified SparseFI, using a dictionary trained from the Pan image or previously reconstructed high-resolution multispectral channels. A recipe of how to choose appropriate algorithm parameters, including the most crucial regularization parameter, is provided. The algorithm is evaluated and validated using WorldView-2-like images that are simulated using very high resolution airborne HySpex hyperspectral imagery and further practically demonstrated using real WorldView-2 images. The algorithm's performance is compared with other state-of-the-art methods. Visual and quantitative analyses demonstrate the high quality of the proposed method. In particular, the analysis of the difference images suggests that J-SparseFI is superior in image resolution recovery.","['Sensors', 'Spatial resolution', 'Image sensors', 'Image fusion', 'Correlation']","['Data fusion', 'joint sparsity', 'jointly sparse fusion of images (J-SparseFI)', 'pansharpening', 'sparse fusion of images (SparseFI)']"
"The TropiSAR campaign has been conducted in August 2009 in French Guiana with the ONERA airborne radar system SETHI. The main objective of this campaign was to collect data to support the Phase A of the 7th Earth Explorer candidate mission, BIOMASS. Several specific questions needed to be addressed to consolidate the mission concept following the Phase 0 studies, and the data collection strategy was constructed accordingly. More specifically, a tropical forest data set was required in order to provide test data for the evaluation of the foreseen inversion algorithms and data products. The paper provides a description of the resulting data set which is now available through the European Space Agency website under the airborne campaign link. First results from the TropiSAR database analysis are presented with two in-depth analyses about both the temporal radiometric variation and temporal coherence at P-band. The temporal variations of the backscatter values are less than 0.5 dB throughout the campaign, and the coherence values are observed to stay high even after 22 days. These results are essential for the BIOMASS mission. The observed temporal stability of the backscatter is a good indicator of the expected robustness of the biomass estimation in tropical forests, from cross-polarized backscatter values as regarding environmental changes such as soil moisture. The high temporal coherence observed after a 22-day period is a prerequisite for SAR Polarimetric Interferometry and Tomographic applications in a single satellite configuration. The conclusion then summarizes the paper and identifies the next steps in the analysis.","['Biomass', 'Calibration', 'Spaceborne radar', 'Coherence', 'Laser radar', 'Vegetation']","['Forestry', 'interferometry', 'polarimetric synthetic aperture radar']"
"Radio frequency interference (RFI) is a known issue in low-frequency radar remote sensing. In synthetic aperture radar (SAR) image processing, RFI can cause severe degradation of image quality, distortion of polarimetric signatures, and an increase of the SAR phase noise level. To address this issue, a processing system was developed that is capable of reliably detecting, characterizing, and mitigating RFI signatures in SAR observations. In addition to being the basis for image correction, the robust RFI-detection algorithms developed in this paper are used to retrieve a wealth of RFI-related information that allows for mapping, characterizing, and classifying RFI signatures across large spatial scales. The extracted RFI information is expected to be valuable input for SAR-system design, sensor operations, and the development of effective RFI-mitigation strategies. The concepts of RFI detection, analysis, and mapping are outlined. Large-scale RFI mapping results are shown. In case studies, the benefit of detailed RFI information for customized RFI filtering and sensor operations is exemplified.","['Synthetic aperture radar', 'L-band', 'Time-frequency analysis', 'Azimuth', 'Interference', 'Noise', 'Detectors']","['Calibration', 'error correction', 'L-band', 'radar remote sensing', 'radio frequency interference (RFI)', 'synthetic aperture radar (SAR)']"
"The Japan Meteorological Agency (JMA) successfully launched the Himawari-8 (H-8) new-generation geostationary meteorological satellite with the Advanced Himawari Imager (AHI) sensor on October 7, 2014. The H-8/AHI level-2 (L2) operational cloud property products were released by the Japan Aerospace Exploration Agency during September 2016. The Voronoi light scattering model, which is a fractal ice particle habit, was utilized to develop the retrieval algorithm called “Comprehensive Analysis Program for Cloud Optical Measurement” (CAPCOM-INV)-ice for the AHI ice cloud product. In this paper, we describe the CAPCOM-INV-ice algorithm for ice cloud products from AHI data. To investigate its retrieval performance, retrieval results were compared with 2000 samples of the ice cloud optical thickness and effective particle radius values. Furthermore, AHI ice cloud products are evaluated by comparing them with the MODIS collection-6 (C6) products. As an experiment, cloud property retrievals from AHI measurements, with an observation interval time of 2.5 min and ground-based rainfall observation radar data (the latter of which is supplied by the JMA, with a 1-km grid mesh), are used to investigate the generation processes of deep convective (DC) cloud in the vicinity of the Kyushu island, Japan. It revealed that AHI measurements have the capability of monitoring the growth processes, including variation of the cloud properties and the precipitation in the DC cloud.","['Ice', 'Clouds', 'Cloud computing', 'Atmospheric modeling', 'Integrated optics', 'Optical imaging', 'Optical scattering']","['DC cloud', 'ice clouds', 'remote sensing', 'retrieval algorithm']"
"A new near-field radar modeling approach for wave propagation in planar layered media is presented. The radar antennas are intrinsically modeled using an equivalent set of infinitesimal electric dipoles and characteristic, frequency-dependent, global reflection, and transmission coefficients. These coefficients determine through a plane wave decomposition wave propagation between the radar reference plane, point sources, and field points. The interactions between the antenna and layered medium are thereby inherently accounted for. The fields are calculated using 3-D Green's functions. We validated the model using an ultrawideband frequency-domain radar with a transmitting and receiving Vivaldi antenna operating in the range 0.8-4 GHz. The antenna characteristic coefficients are obtained from near- and far-field measurements over a copper plane. The proposed model provides unprecedented accuracy for describing near-field radar measurements collected over a water layer, the frequency-dependent electrical properties of which were described using the Debye model. Layer thicknesses could be retrieved through full-wave inversion. The proposed approach demonstrated great promise for nondestructive testing of planar materials and digital soil mapping using ground-penetrating radar.","['Radar antennas', 'Dipole antennas', ""Green's function methods"", 'Transmitting antennas', 'Ground penetrating radar', 'Aperture antennas']","['Antenna modeling', ""Green's functions"", 'ground-penetrating radar (GPR)', 'near-field', 'planar layered media']"
"In recent years, new algorithms have been proposed to retrieve maximum available information in synthetic aperture radar (SAR) interferometric stacks with focus on distributed scatterers. The key step in these algorithms is to optimally estimate single-master (SM) wrapped phases for each pixel from all possible interferometric combinations, preserving useful information and filtering noise. In this paper, we propose a new method for SM-phase estimation based on the integer least squares principle. We model the SM-phase estimation problem in a linear form by introducing additional integer ambiguities and use a bootstrap estimator for joint estimation of SM-phases and the integer unknowns. In addition, a full error propagation scheme is introduced in order to evaluate the precision of the final SM-phase estimates. The main advantages of the proposed method are the flexibility to be applied on any (connected) subset of interferograms and the quality description via the provision of a full covariance matrix of the estimates. Results from both synthetic experiments and a case study over the Torfajökull volcano in Iceland demonstrate that the proposed method can efficiently filter noise from wrapped multibaseline interferometric stacks, resulting in doubling the number of detected coherent pixels with respect to conventional persistent scatterer interferometry.","['Estimation', 'Mathematical model', 'Synthetic aperture radar', 'Decorrelation', 'Phase estimation', 'Covariance matrices', 'Joining processes']","['Distributed scatterers (DS)', 'integer least squares (ILS)', 'interferometric synthetic aperture radar (InSAR)', 'phase linking', 'phase triangulation', 'radar interferometry']"
"The quantification of uncertainty in satellite-derived global surface albedo products is a critical aspect in producing complete, physically consistent, and decadal land property data records for studying ecosystem change. A challenge in validating albedo measurements acquired from space is the ability to overcome the spatial scaling errors that can produce disagreements between satellite and field-measured values. Here, we present the results from an accuracy assessment of MODIS and Landsat-TM albedo retrievals, based on collocated comparisons with tower and airborne Cloud Absorption Radiometer (CAR) measurements collected during the 2007 Cloud and Land Surface Interaction Campaign (CLASIC). The initial focus was on evaluating inter-sensor consistency through comparisons of intrinsic bidirectional reflectance estimates. Local and regional assessments were then performed to obtain estimates of the resulting scaling uncertainties, and to establish the accuracy of albedo reconstructions during extended periods of precipitation. In general, the satellite-derived estimates met the accuracy requirements established for the high-quality MODIS operational albedos at 500 m (the greater of 0.02 units or ±10% of surface measured values). However, results reveal a high degree of variability in the root-mean-square error (RMSE) and bias of MODIS visible (0.3-0.7 μm) and Landsat-TM shortwave (0.3-5.0 μm) albedos; where, in some cases, retrieval uncertainties were found to be in excess of 15 %. Results suggest that an overall improvement in MODIS shortwave albedo retrieval accuracy of 7.8%, based on comparisons between MODIS and CAR albedos, resulted from the removal of sub-grid scale mismatch errors when directly scaling-up the tower measurements to the MODIS satellite footprint.","['Satellites', 'MODIS', 'Remote sensing', 'Earth', 'Poles and towers', 'Clouds', 'Land surface']","['Biosphere', 'ecosystems', 'land surface', 'remote sensing']"
"Wide-swath synthetic aperture radar (SAR) missions with short revisit times, such as Sentinel-1 and the planned NISAR and Tandem-L, provide an unprecedented wealth of interferometric SAR (InSAR) time series. However, the processing of the emerging Big Data is challenging for state-of-the-art InSAR analysis techniques. This contribution introduces a novel approach, named Sequential Estimator, for efficient estimation of the interferometric phase from long InSAR time series. The algorithm uses recursive estimation and analysis of the data covariance matrix via division of the data into small batches, followed by the compression of the data batches. From each compressed data batch artificial interferograms are formed, resulting in a strong data reduction. Such interferograms are used to link the “older” data batches with the most recent acquisitions and thus to reconstruct the phase time series. This scheme avoids the necessity of reprocessing the entire data stack at the face of each new acquisition. The proposed estimator introduces negligible degradation compared to the Cramer-Rao lower bound under realistic coherence scenarios. The estimator may therefore be adapted for high-precision near-real-time processing of InSAR and accommodate the conversion of InSAR from an offline to a monitoring geodetic tool. The performance of the Sequential Estimator is compared to state-of-the-art techniques via simulations and application to Sentinel-1 data.","['Coherence', 'Maximum likelihood estimation', 'Time series analysis', 'Synthetic aperture radar', 'Monitoring', 'Earth']","['Big Data', 'coherence estimation error', 'data compression', 'differential interferometric synthetic aperture radar (DInSAR)', 'distributed scatterers', 'efficiency', 'error analysis', 'low-rank approximation', 'maximum-likelihood estimation (MLE)']"
"In this paper, we study self-taught learning for hyperspectral image (HSI) classification. Supervised deep learning methods are currently state of the art for many machine learning problems, but these methods require large quantities of labeled data to be effective. Unfortunately, existing labeled HSI benchmarks are too small to directly train a deep supervised network. Alternatively, we used self-taught learning, which is an unsupervised method to learn feature extracting frameworks from unlabeled hyperspectral imagery. These models learn how to extract generalizable features by training on sufficiently large quantities of unlabeled data that are distinct from the target data set. Once trained, these models can extract features from smaller labeled target data sets. We studied two self-taught learning frameworks for HSI classification. The first is a shallow approach that uses independent component analysis and the second is a three-layer stacked convolutional autoencoder. Our models are applied to the Indian Pines, Salinas Valley, and Pavia University data sets, which were captured by two separate sensors at different altitudes. Despite large variation in scene type, our algorithms achieve state-of-the-art results across all the three data sets.","['Feature extraction', 'Data mining', 'Data models', 'Sensors', 'Hyperspectral imaging', 'Training', 'Encoding']","['Autoencoder', 'deep learning', 'feature learning', 'hyperspectral imaging', 'independent component analysis (ICA)', 'self-taught learning']"
"This article investigates the presence of a new interferometric signal in multilooked synthetic aperture radar (SAR) interferograms that cannot be attributed to the atmospheric or Earth-surface topography changes. The observed signal is short-lived and decays with the temporal baseline; however, it is distinct from the stochastic noise attributed to temporal decorrelation. The presence of such a fading signal introduces a systematic phase component, particularly in short temporal baseline interferograms. If unattended, it biases the estimation of Earth surface deformation from SAR time series. Here, the contribution of the mentioned phase component is quantitatively assessed. The biasing impact on the deformation-signal retrieval is further evaluated. A quality measure is introduced to allow the prediction of the associated error with the fading signals. Moreover, a practical solution for the mitigation of this physical signal is discussed; special attention is paid to the efficient processing of Big Data from modern SAR missions such as Sentinel-1 and NISAR. Adopting the proposed solution, the deformation bias is shown to decrease significantly. Based on these analyses, we put forward our recommendations for efficient and accurate deformation-signal retrieval from large stacks of multilooked interferograms.","['Strain', 'Fading channels', 'Time series analysis', 'Synthetic aperture radar', 'Systematics', 'Decorrelation', 'Moisture']","['Big Data', 'deformation estimation', 'differential interferometric synthetic aperture radar (SAR) (DInSAR)', 'distributed scatterers (DSs)', 'error analysis', 'near real-time (NRT) processing', 'phase inconsistencies', 'signal decorrelation', 'time-series analysis']"
"Multispectral light detection and ranging (LiDAR) has the potential to recover structural and physiological data from arboreal samples and, by extension, from forest canopies when deployed on aerial or space platforms. In this paper, we describe the design and evaluation of a prototype multispectral LiDAR system and demonstrate the measurement of leaf and bark area and abundance profiles using a series of experiments on tree samples “viewed from above” by tilting living conifers such that the apex is directed on the viewing axis. As the complete recovery of all structural and physiological parameters is ill posed with a restricted set of four wavelengths, we used leaf and bark spectra measured in the laboratory to constrain parameter inversion by an extended reversible jump Markov chain Monte Carlo algorithm. However, we also show in a separate experiment how the multispectral LiDAR can recover directly a profile of Normalized Difference Vegetation Index (NDVI), which is verified against the laboratory spectral measurements. Our work shows the potential of multispectral LiDAR to recover both structural and physiological data and also highlights the fine spatial resolution that can be achieved with time-correlated single-photon counting.","['Laser radar', 'Wavelength measurement', 'Optical imaging', 'Physiology', 'Vegetation', 'Optical sensors', 'Instruments']","['Leaf area profile', 'leaf physiology', 'light detection and ranging (LiDAR)', 'multispectral', 'parameter inversion']"
"We present a first assessment of airborne laser and radar altimeter data over snow-covered sea ice, gathered during the National Aeronautics and Space Administration Operation IceBridge Mission. We describe a new technique designed to process radar echograms from the University of Kansas snow radar to estimate snow depth. We combine IceBridge laser altimetry with radar-derived snow depths to determine sea ice thickness. Results are validated through comparison with direct measurements of snow and ice thickness collected in situ at the Danish GreenArc 2009 sea ice camp located on fast ice north of Greenland. The IceBridge instrument suite provides accurate measurements of snow and ice thickness, particularly over level ice. Mean IceBridge snow and ice thickness agree with in situ measurements to within∼0.01 and∼0.05 m, respectively, while modal snow and ice thickness estimates agree to within 0.02 and 0.10 m, respectively. IceBridge snow depths were correlated with in situ measurements (R=0.7, for an averaging length of 55 m). The uncertainty associated with the derived IceBridge sea ice thickness estimates is 0.40 m. The results demonstrate the retrieval of both first-year and multiyear ice thickness from IceBridge data. The airborne data were however compromised in heavily ridged ice where snow depth, and hence ice thickness, could not be measured. Techniques developed as part of this study will be used for routine processing of IceBridge retrievals over Arctic sea ice. The limitations of the GreenArc study are discussed, and recommendations for future validation of airborne measurements via field activities are provided.","['Snow', 'Sea ice', 'Sea measurements', 'Laser radar', 'Asynchronous transfer mode']","['Altimetry', 'geophysical measurement techniques', 'remote sensing', 'sea ice', 'snow']"
"We introduce a total variation (TV) regularization model for synthetic aperture radar (SAR) image despeckling. A dual-formulation-based adaptive TV (ATV) regularization method is applied to solve the TV regularization. The parameter adaptation of the TV regularization is performed based on the noise level estimated via wavelets. The TV-regularization-based image restoration model has a good performance in preserving image sharpness and edges while removing noises, and it is therefore effective for edge preserve SAR image despeckling. Experiments have been carried out using optical images contaminated with artificial speckles first and then SAR images. A despeckling evaluation index (DEI) is designed to assess the effectiveness of edge preserve despeckling on SAR images, which is based on the ratio of the standard deviations of two neighborhood areas of different sizes of a pixel. Experimental results show that the proposed ATV method can effectively suppress SAR image speckles without compromising the edge sharpness of image features according to both subjective visual assessment of image quality and objective evaluation using DEI.","['Synthetic aperture radar', 'Image edge detection', 'TV', 'Noise', 'Estimation', 'Transforms', 'Indexes']","['Adaptive total variation (ATV) regularization', 'despeckling', 'evaluation index', 'synthetic aperture radar (SAR)']"
"We developed a scheme to improve hourly estimates of aerosol optical thickness (AOT) from the Advanced Himawari Imager (AHI) onboard Himawari-8, the Japan Meteorological Agency's latest geostationary satellite. Taking advantage of the sampling characteristics of the AHI (10-min temporal and subkilometer spatial resolution), we quantify temporal and spatial variability of AOT from the observations (AOToriginal) and utilize this information to develop an hourly algorithm that produces two sets of derived AOTs: AOTpure, derived by the application of strict cloud screening to AOToriginal, and AOTmerged, derived from spatial and temporal interpolations of AOTpure. The AOTs thus obtained from the hourly algorithm were validated against measurements from the aerosol robotic network. The root-mean-square errors (RMSEs) of the AOTpure and AOTmerged products were 0.14 and 0.11, respectively, providing improvement over an RMSE of 0.20 for AOToriginal.","['Aerosols', 'Spatiotemporal phenomena', 'Clouds', 'Pollution measurement', 'Land surface']","['Aerosols', 'algorithms', 'remote sensing', 'satellites']"
"As a new method of Earth observation, video satellite is capable of monitoring specific events on the Earth's surface continuously by providing high-temporal resolution remote sensing images. The video observations enable a variety of new satellite applications such as object tracking and road traffic monitoring. In this article, we address the problem of fast object tracking in satellite videos, by developing a novel tracking algorithm based on correlation filters embedded with motion estimations. Based on the kernelized correlation filter (KCF), the proposed algorithm provides the following improvements: 1) proposing a novel motion estimation (ME) algorithm by combining the Kalman filter and motion trajectory averaging and mitigating the boundary effects of KCF by using this ME algorithm and 2) solving the problem of tracking failure when a moving object is partially or completely occluded. The experimental results demonstrate that our algorithm can track the moving object in satellite videos with 95% accuracy.","['Videos', 'Satellites', 'Correlation', 'Object tracking', 'Monitoring', 'Optical filters']","['Correlation filter', 'motion estimation (ME)', 'object tracking', 'satellite videos']"
"The analysis of multitemporal very high spatial resolution imagery is too often limited to the sole use of pixel digital numbers which do not accurately describe the observed targets between the various collections due to the effects of changing illumination, viewing geometries, and atmospheric conditions. This paper demonstrates both qualitatively and quantitatively that not only physically based quantities are necessary to consistently and efficiently analyze these data sets but also the angular information of the acquisitions should not be neglected as it can provide unique features on the scenes being analyzed. The data set used is composed of 21 images acquired between 2002 and 2009 by QuickBird over the city of Denver, Colorado. The images were collected near the downtown area and include single family houses, skyscrapers, apartment complexes, industrial buildings, roads/highways, urban parks, and bodies of water. Experiments show that atmospheric and geometric properties of the acquisitions substantially affect the pixel values and, more specifically, that the raw counts are significantly correlated to the atmospheric visibility. Results of a 22-class urban land cover experiment show that an improvement of 0.374 in terms of Kappa coefficient can be achieved over the base case of raw pixels when surface reflectance values are combined to the angular decomposition of the time series.","['Sensors', 'Atmospheric measurements', 'Scattering', 'Geometry', 'Satellites', 'Spatial resolution', 'Time series analysis']","['Angular decomposition', 'bidirectional reflectance distribution function (BRDF)', 'multiangular analysis', 'multitemporal analysis', 'optical very high spatial resolution', 'QuickBird (QB)', 'surface anisotropy', 'surface reflectance', 'urban change detection', 'urban classification', 'WorldView-2']"
"Remote sensing data provide significant information to constrain the geometry of geological structures at depth. However, the use of intraformational geomorphologic features such as flatirons and incised valleys often calls for tedious user interaction during 3-D model building. We propose a new method to generate 3-D models of stratigraphic formations, based primarily on remote sensing images and digital elevation models. This method is based on interpretations of the main relief markers and interpolation of a stratigraphic property on a tetahedral mesh covering the domain of study. The tetrahedral mesh provides a convenient way to integrate available data during the interpolation while accounting for discontinuities such as faults. Interpretive expert input may be provided through constrained interactive editing on arbitrary cross-sections, and additional surface or subsurface data may also be integrated in the modeling. We demonstrate this global workflow on a structurally complex basin in the Sierra Madre Oriental, Northeastern Mexico.","['Data models', 'Geology', 'Computational modeling', 'Solid modeling', 'Surface topography', 'Surface treatment', 'Remote sensing']","['Applications', 'cartography', 'computer-aided design', 'earth and atmospheric sciences', 'graphical user interface', 'interpolation', 'model development', 'optimization']"
"Continuous, consistent, and long time-series from remote sensing are essential to monitoring changes on Earth's surface. However, analyzing such data sets is often challenging due to missing values introduced by cloud cover, missing orbits, sensor geometry artifacts, and so on. We propose a new and accurate spatio-temporal prediction method to replace missing values in remote sensing data sets. The method exploits the spatial coherence and temporal seasonal regularity that are inherent in many data sets. The key parts of the method are: 1) the adaptively chosen spatio-temporal subsets around missing values; 2) the ranking of images within the subsets based on a scoring algorithm; 3) the estimation of empirical quantiles characterizing the missing values; and 4) the prediction of missing values through quantile regression. One advantage of quantile regression is the robustness to outliers, which enables more accurate parameter retrieval in the analysis of remote sensing data sets. In addition, we provide bootstrap-based quantification of prediction uncertainties. The proposed prediction method was applied to a Normalized Difference Vegetation Index data set from the Moderate Resolution Imaging Spectroradiometer and assessed with realistic test data sets featuring between 20% and 50% missing values. Validation against established methods showed that the proposed method has a good performance in terms of the root-mean-squared prediction error and significantly outperforms its competitors. This paper is accompanied by the open-source R package gapfill, which provides a flexible, fast, and ready-to-use implementation of the method.","['Remote sensing', 'Prediction methods', 'Indexes', 'Uncertainty', 'MODIS', 'Open source software']","['Alaska', 'gap filling', 'imputation', 'interpolation', 'Moderate Resolution Imaging Spectroradiometer Normalized Difference Vegetation Index (MODIS NDVI)', 'quantile regression', 'R gapfill', 'TIMESAT', 'uncertainty']"
"Radiometric cross calibration of Earth observation sensors is a crucial need to guarantee or quantify the consistency of measurements from different sensors. Twenty desert sites, historically selected, are revisited, and their radiometric profiles are described for the visible to the near-infrared spectral domain. Therefore, acquisitions by various sensors over these desert sites are collected into a dedicated database, Structure d'Accueil des Données d'Etalonnage, defined to manage operational calibrations and the required SI traceability. The cross-calibration method over desert sites is detailed. Surface reflectances are derived from measurements by a reference sensor and spectrally interpolated to derive the surface and then top-of-atmosphere reflectances for spectral bands of the sensor to calibrate. The comparison with reflectances really measured provides an estimation of the cross calibration between the two sensors. Results illustrate the efficiency of the method for various pairs of sensors among AQUA-Moderate Resolution Imaging Spectroradiometer (MODIS), Environmental Satellite-Medium Resolution Imaging Spectrometer (MERIS), Polarization and Anisotropy of Reflectance for Atmospheric Sciences Couples With Observations From a Lidar (PARASOL)-Polarization and Directionality of the Earth Reflectances (POLDER), and Satellite pour l'Observation de la Terre 5 (SPOT5)-VEGETATION. MERIS and MODIS calibrations are found to be very consistent, with a discrepancy of 1%, which is close to the accuracy of the method. A larger bias of 3% was identified between VEGETATION-PARASOL on one hand and MERIS-MODIS on the other hand. A good consistency was found between sites, with a standard deviation of 2% for red to near-infrared bands, increasing to 4% and 6% for green and blue bands, respectively. The accuracy of the method, which is close to 1%, may also depend on the spectral bands of both sensor to calibrate and reference sensor (up to 5% in the worst case) and their corresponding geometrical matching.","['Sensors', 'Calibration', 'Standards', 'MODIS', 'Atmospheric measurements', 'Satellites']","['Calibration', 'radiometry', 'remote sensing', 'spectral analysis']"
"The Global Change Observation Mission 1st-Water (GCOM-W1) satellite, which was launched by the Japan Aerospace Exploration Agency in May 2012, contains an Advanced Microwave Scanning Radiometer-2 (AMSR2). The AMSR2 provides multifrequency measurements of microwave energy (brightness temperature) emitted by the Earth's surface and atmosphere. However, each frequency field of view (FOV) differs in size because of its hardware design. For the retrieval of more accurate geophysical parameters from multifrequency brightness temperatures, the brightness temperatures should be modified to be the same as measured in the same FOV. The Backus-Gilbert (BG) method is one of the antenna pattern matching techniques used for this modification. We applied the BG method to the AMSR2 data to define a new data set of modified brightness temperatures, a level 1R (L1R) product that is freely and widely available. We optimized the implementation of the BG method to obtain the L1R product, with smoothing factors dynamically determined for all modified brightness temperatures. This paper describes the implementation method, including the criterion used to determine the smoothing factors and presents a quality assessment of the L1R product.","['Microwave radiometry', 'Earth', 'Satellites', 'Antenna measurements', 'Antennas', 'Brightness temperature', 'Temperature measurement']","['Advanced Microwave Scanning Radiometer-2 (AMSR2)', 'Backus–Gilbert (BG) method', 'convolution', 'field of view (FOV)', 'microwave radiometer', 'water cycle monitoring']"
"This paper addresses the highly challenging problem of automatically detecting man-made structures especially buildings in very high-resolution (VHR) synthetic aperture radar (SAR) images. In this context, this paper has two major contributions. First, it presents a novel and generic workflow that initially classifies the spaceborne SAR tomography (TomoSAR) point clouds-generated by processing VHR SAR image stacks using advanced interferometric techniques known as TomoSAR-into buildings and nonbuildings with the aid of auxiliary information (i.e., either using openly available 2-D building footprints or adopting an optical image classification scheme) and later back project the extracted building points onto the SAR imaging coordinates to produce automatic large-scale benchmark labeled (buildings/nonbuildings) SAR data sets. Second, these labeled data sets (i.e., building masks) have been utilized to construct and train the state-of-the-art deep fully convolution neural networks with an additional conditional random field represented as a recurrent neural network to detect building regions in a single VHR SAR image. Such a cascaded formation has been successfully employed in computer vision and remote sensing fields for optical image classification but, to our knowledge, has not been applied to SAR images. The results of the building detection are illustrated and validated over a TerraSAR-X VHR spotlight SAR image covering approximately 39 km 2 -almost the whole city of Berlin- with the mean pixel accuracies of around 93.84%.","['Synthetic aperture radar', 'Buildings', 'Feature extraction', 'Optical distortion', 'Optical sensors', 'Optical interferometry', 'Optical imaging']","['Building detection', 'fully convolution neural networks (CNNs)', 'OpenStreetMap (OSM)', 'synthetic aperture radar (SAR)', 'SAR tomography (TomoSAR)', 'TerraSAR-X/TanDEM-X']"
"The existence of mixed pixels is a major problem in remote-sensing image classification. Although the soft classification and spectral unmixing techniques can obtain an abundance of different classes in a pixel to solve the mixed pixel problem, the subpixel spatial attribution of the pixel will still be unknown. The subpixel mapping technique can effectively solve this problem by providing a fine-resolution map of class labels from coarser spectrally unmixed fraction images. However, most traditional subpixel mapping algorithms treat all mixed pixels as an identical type, either boundary-mixed pixel or linear subpixel, leading to incomplete and inaccurate results. To improve the subpixel mapping accuracy, this paper proposes an adaptive subpixel mapping framework based on a multiagent system for remote-sensing imagery. In the proposed multiagent subpixel mapping framework, three kinds of agents, namely, feature detection agents, subpixel mapping agents and decision agents, are designed to solve the subpixel mapping problem. Experiments with artificial images and synthetic remote-sensing images were performed to evaluate the performance of the proposed subpixel mapping algorithm in comparison with the hard classification method and other subpixel mapping algorithms: subpixel mapping based on a back-propagation neural network and the spatial attraction model. The experimental results indicate that the proposed algorithm outperforms the other two subpixel mapping algorithms in reconstructing the different structures in mixed pixels.",[],[]
"The Argos service was launched in 1978 to serve environmental applications, including oceanography, wildlife tracking, fishing vessel monitoring, and maritime safety. The system allows for worldwide near-real-time positioning and data collection of platform terminal transmitters (PTTs). The positioning of the PTTs is achieved by exploiting the Doppler shift in the carrier frequency of the transmitter as recorded by satelliteborne Argos receivers. Until March 15, 2011, a classical nonlinear least squares estimation technique was systematically used to estimate Argos positions. Since then, a second positioning algorithm using a multiple-model Kalman filter was implemented in the operational Argos positioning software. This paper presents this new algorithm and analyzes its performance using a large data set obtained from over 200 mobiles carrying both an Argos transmitter and a GPS receiver used as ground truth. The results show that the new algorithm significantly improves the positioning accuracy, particularly in difficult conditions (for class-A and class-B locations, in the Argos terminology). Moreover, the new algorithm enables the retrieval of a larger number of estimated positions and the systematic estimation of the location error.","['Satellites', 'Frequency measurement', 'Noise', 'Doppler effect', 'Kalman filters', 'Vectors', 'Standards']","['Argos system', 'Doppler location', 'interacting multiple model (IMM) Kalman filter (KF)', 'least squares (LS) estimation', 'target tracking']"
"A technique for comparing spaceborne microwave radiometer brightness temperatures (Tb) is described in the context of the upcoming National Aeronautics and Space Administration Global Precipitation Measurement (GPM) mission. The GPM mission strategy is to measure precipitation globally with high temporal resolution by using a constellation of satellite radiometers logically united by the GPM core satellite, which will be in a non-sun-synchronous medium inclination orbit. The usefulness of the combined product depends on the consistency of precipitation retrievals from the various microwave radiometers. The Tb calibration requirement to achieve such consistency demands first that Tb's from the individual radiometers be free of instrument and measurement artifacts and, second, that these self-consistent Tb's will be translated to a common standard (GPM core) for the unification of the precipitation retrieval. The intersatellite radiometric calibration technique described herein serves both the purposes by comparing individual radiometer observations to radiative transfer model (RTM) simulations (for “self-consistency” check) and by using a double-difference technique (to establish a linear calibration transfer function from one radiometer to another). This double-difference technique subtracts the RTM-simulated difference from the observed difference between a pair of radiometer Tb's. To establish a linear inter-radiometer calibration transfer function, comparisons at both the cold (ocean) and the warm (land) end of the Tb's are necessary so that, using these two points, slope and offset coefficients are determined. To this end, a simplified calibration transfer technique at the warm end (over the Amazon and Congo rain forest) is introduced. Finally, an error model is described that provides an estimate of the uncertainty of the radiometric bias estimate between comparison radiometer channels.","['Microwave radiometry', 'Calibration', 'Atmospheric modeling', 'Ocean temperature', 'Instruments', 'Brightness temperature']","['Global Precipitation Measurement (GPM) Intersatellite Radiometer Calibration Working Group (XCAL)', 'intersatellite radiometric calibration', 'microwave radiometry']"
"Monitoring the kinematic behavior of enormous amounts of points and objects anywhere on Earth is now feasible on a weekly basis using radar interferometry from Earth-orbiting satellites. An increasing number of satellite missions are capable of delivering data that can be used to monitor geophysical processes, mining and construction activities, public infrastructure, or even individual buildings. The parameters estimated from these data are used to better understand various natural hazards, improve public safety, or enhance asset management activities. Yet, the mathematical estimation of kinematic parameters from interferometric data is an ill-posed problem as there is no unique solution, and small changes in the data may lead to significantly different parameter estimates. This problem results in multiple possible outcomes given the same data, hampering public acceptance, particularly in critical conditions. Here, we propose a method to address this problem in a probabilistic way, which is based on multiple hypotheses testing. We demonstrate that it is possible to systematically evaluate competing kinematic models in order to find an optimal model and to assign likelihoods to the results. Using the B-method of testing, a numerically efficient implementation is achieved, which is able to evaluate hundreds of competing models per point. Our approach will not solve the nonuniqueness problem of interferometric synthetic aperture radar (InSAR), but it will allow users to critically evaluate (conflicting) results, avoid overinterpretation, and thereby consolidate InSAR as a geodetic technique.","['Kinematics', 'Time series analysis', 'Testing', 'Covariance matrices', 'Eigenvalues and eigenfunctions', 'Noise', 'Libraries']","['B-method of testing', 'deformation modeling', 'multiple hypotheses testing (MHT)']"
"Full-waveform inversion is an important and widely used method to reconstruct subsurface velocity images. Waveform inversion is a typical nonlinear and ill-posed inverse problem. Existing physics-driven computational methods for solving waveform inversion suffer from the cycle-skipping and local-minima issues, and do not mention that solving waveform inversion is computationally expensive. In recent years, data-driven methods become a promising way to solve the waveform-inversion problem. However, most deep-learning frameworks suffer from the generalization and overfitting issue. In this article, we developed a real-time data-driven technique and we call it VelocityGAN, to reconstruct accurately the subsurface velocities. Our VelocityGAN is built on a generative adversarial network (GAN) and trained end to end to learn a mapping function from the raw seismic waveform data to the velocity image. Different from other encoder-decoder-based data-driven seismic waveform-inversion approaches, our VelocityGAN learns regularization from data and further imposes the regularization to the generator so that inversion accuracy is improved. We further develop a transfer-learning strategy based on VelocityGAN to alleviate the generalization issue. A series of experiments is conducted on the synthetic seismic reflection data to evaluate the effectiveness, efficiency, and generalization of VelocityGAN. We not only compare it with the existing physics-driven approaches and data-driven frameworks but also conduct several transfer-learning experiments. The experimental results show that VelocityGAN achieves the state-of-the-art performance among the baselines and can improve the generalization results to some extent.","['Inverse problems', 'Computational modeling', 'Mathematical model', 'Generative adversarial networks', 'Neural networks', 'Gallium nitride', 'Generators']","['Condition adversarial networks', 'data-driven method', 'full-waveform inversion (FWI)', 'transfer learning']"
"Fast precise point positioning (Fast-PPP) is a satellite-based navigation technique using an accurate real-time ionospheric modeling to achieve high accuracy quickly. In this paper, an end-to-end performance assessment of Fast-PPP is presented in near-maximum Solar Cycle conditions; from the accuracy of the Central Processing Facility corrections, to the user positioning. A planetary distribution of permanent receivers including challenging conditions at equatorial latitudes, is navigated in pure kinematic mode, located from 100 to 1300 km away from the nearest reference station used to derive the ionospheric model. It is shown that satellite orbits and clocks accurate to few centimeters and few tenths of nanoseconds, used in conjunction with an ionosphere with an accuracy better than 1 Total Electron Content Unit (16 cm in L1) reduce the convergence time of dual-frequency Precise Point Positioning, to decimeter-level (3-D) solutions. Horizontal convergence times are shortened 40% to 90%, whereas the vertical components are reduced by 20% to 60%. A metric to evaluate the quality of any ionospheric model for Global Navigation Satellite System is also proposed. The ionospheric modeling accuracy is directly translated to mass-market single-frequency users. The 95th percentile of horizontal and vertical accuracies is shown to be 40 and 60 cm for single-frequency users and 9 and 16 cm for dual-frequency users. The tradeoff between the formal and actual positioning errors has been carefully studied to set realistic confidence levels to the corrections.","['Satellites', 'Accuracy', 'Orbits', 'Receivers', 'Clocks', 'Global Positioning System']","['Global Navigation Satellite System (GNSS)', 'precise point positioning (PPP)', 'real-time ionospheric corrections', 'undifferenced ambiguity fixing']"
"The intensity of a Sentinel-1 Terrain Observation with Progressive Scans synthetic aperture radar image is disturbed by additive thermal noise, particularly in the cross-polarization channel. Although the European Space Agency provides calibrated noise vectors for noise power subtraction, residual noise contributions are significant when considering the relatively narrow backscattering distribution of the cross-polarization channel. In this paper, we investigate the characteristics of noise and propose an efficient method for noise reduction based on a three-step correction process comprised of azimuth descalloping, noise scaling and interswath power balancing, and local residual noise power compensation. The core idea is to find the optimal correction coefficients resulting in the most noise-uncorrelated gentle backscatter profile over a homogeneous region and to combine them with the scalloping gain for a reconstruction of the complete 2-D noise field. Denoising is accomplished by subtracting the reconstructed noise field from the original image. The performance improvement in some applications by adopting the denoising procedure shows the effectiveness of the proposed method.","['Thermal noise', 'Azimuth', 'Synthetic aperture radar', 'Noise reduction', 'Antennas', 'Backscatter', 'Additives']","['Cross polarization', 'denoising', 'scalloping', 'Sentinel-1', 'Terrain Observation with Progressive Scans synthetic aperture radar (TOPSAR)', 'thermal noise']"
"Recently, the field of hyperspectral image (HSI) classification is dominated by deep learning-based methods. However, training deep learning models usually needs a large number of labeled samples to optimize thousands of parameters. In this article, a deep multiview learning method is proposed to deal with the small sample problem of HSI. First, two views of an HSI scene are constructed by applying principal component analysis to different bands. Second, a deep residual network is designed to embed the different views of a sample to a latent space. The designed deep residual network is trained by maximizing agreement between differently augmented views of the same data sample via a contrastive loss in the latent space. Note that the training procedure of the designed deep residual network does not use labeled information. Therefore, the proposed method belongs to the category of unsupervised learning, which could alleviate the lack of labeled training samples. Finally, a conventional machine learning method (e.g., support vector machine) is used to complete the classification task in the learned latent space. To demonstrate the effectiveness of the proposed method, extensive experiments are carried on four widely used hyperspectral data sets. The experimental results demonstrate that the proposed method could improve the classification accuracy with small samples.","['Training', 'Support vector machines', 'Radio frequency', 'Deep learning', 'Task analysis', 'Unsupervised learning', 'Residual neural networks']","['Deep learning', 'hyperspectral image (HSI) classification', 'multiview learning', 'small samples']"
"Accurately classifying 3-D point clouds into woody and leafy components has been an interest for applications in forestry and ecology including the better understanding of radiation transfer between canopy and atmosphere. The past decade has seen an increase in the methods attempting to classify leaves and wood in point clouds based on radiometric or geometric features. However, classification purely based on radiometric features is sensor-specific, and the method by which the local neighborhood of a point is defined affects the accuracy of classification based on geometric features. Here, we present a leaf-wood classification method combining geometrical features defined by radially bounded nearest neighbors at multiple spatial scales in a machine learning model. We compared the performance of three different machine learning models generated by the random forest (RF), XGBoost, and lightGBM algorithms. Using multiple spatial scales eliminates the need for an optimal neighborhood size selection and defining the local neighborhood by radially bounded nearest neighbors makes the method broadly applicable for point clouds of varying quality. We assessed the model performance at the individual tree- and plot-level on field data from tropical and deciduous forests, as well as on simulated point clouds. The method has an overall average accuracy of 94.2% on our data sets. For other data sets, the presented method outperformed the methods in literature in most cases without the need for additional postprocessing steps that are needed in most of the existing methods. We provide the entire framework as an open-source python package.","['Forestry', 'Three-dimensional displays', 'Vegetation', 'Measurement by laser beam', 'Radiometry', 'Data models', 'Machine learning']","['Leaf versus wood separation', 'LiDAR', 'machine learning', 'python package', 'tropical forests']"
"Automatic optical-to-SAR image registration is considered as a challenging problem because of the inconsistency of radiometric and geometric properties. Feature-based methods have proven to be effective; however, common features are difficult to extract and match, and the robustness of those methods strongly depends on feature extraction results. In this paper, a new method based on iterative line extraction and Voronoi integrated spectral point matching is developed. The core idea consists of three aspects: 1) An iterative procedure that combines line segment extraction and line intersections matching is proposed to avoid registration failure caused by poor feature extraction. 2) A multilevel strategy of coarse-to-fine registration is presented. The coarse registration aims to preserve main linear structures while reducing data redundancy, thus providing robust feature matching results for fine registration. 3) Voronoi diagram is introduced into spectral point matching to further enhance the matching accuracy between two sets of line intersection. Experimental results show that the proposed method improves the matching performance. Compared with previous methods, the proposed algorithm can effectively and robustly generate sufficient reliable point pairs and provide accurate registration.","['Feature extraction', 'Optical imaging', 'Optical sensors', 'Adaptive optics', 'Image segmentation', 'Synthetic aperture radar', 'Image edge detection']","['Iterative line extraction', 'multiscale', 'optical-to-SAR image registration', 'Voronoi integrated spectral point matching (VSPM)', 'Voronoi polygons']"
"In this paper, we propose a framework referred to as “geodetic synthetic aperture radar (SAR) tomography” that fuses the SAR imaging geodesy and tomographic SAR inversion (TomoSAR) approaches to obtain absolute 3-D positions of a large amount of natural scatterers. The methodology is applied on four very high resolution TerraSAR-X spotlight image stacks acquired over the city of Berlin. Since all the TomoSAR estimates are relative to the same reference point object whose absolute 3-D positions are retrieved by means of stereo SAR, the point clouds reconstructed using data acquired from different viewing angles can be geodetically fused. To assess the accuracy of the position estimates, the resulting absolute shadow-free 3-D TomoSAR point clouds are compared with a digital surface model obtained by airborne LiDAR. It is demonstrated that an absolute positioning accuracy of around 20 cm and a meter-order relative positioning accuracy can be achieved by the proposed framework using TerraSAR-X data.","['Synthetic aperture radar', 'Tomography', 'Orbits', 'Three-dimensional displays', 'Atmospheric modeling', 'Azimuth', 'Geometry']","['Absolute positioning', 'geodetic SAR tomography', 'geodetical fusion', 'SAR geodesy', 'SAR tomography', 'stereo SAR', 'synthetic aperture radar (SAR)', 'TerraSAR-X']"
"The National Aeronautics and Space Administration (NASA) initiated a program called Operation IceBridge for monitoring critical parts of Greenland and Antarctica with airborne LIDARs until ICESat-II is launched in 2016. We have been operating radar instrumentation on the NASA DC-8 and P-3 aircraft used for LIDAR measurements over Antarctica and Greenland, respectively. The radar package on both aircraft includes a radar depth sounder/imager operating at the center frequency of 195 MHz. During high-altitude missions flown to perform surface-elevation measurements, we also collected radar depth sounder data. We obtained good ice thickness information and mapped internal layers for both thicker and thinner ice. We successfully sounded 3.2-km-thick low-loss ice with a smooth surface and also sounded about 1-km or less thick shallow ice with a moderately rough surface. The successful sounding required processing of data with an algorithm to obtain 56-dB or lower range sidelobes and array processing with a minimum variance distortionless response algorithm to reduce cross-track surface clutter. In this paper, we provide a brief description of the radar system, discuss range-sidelobe reduction and array processing algorithms, and provide sample results to demonstrate the successful sounding of the ice bottom interface from high altitudes over the Antarctic and Greenland ice sheets.","['Ice', 'Arrays', 'Aircraft', 'Radar imaging', 'Clutter', 'Antarctica']","['High-altitude ice sheet measurements', 'ice surface clutter reduction', 'multichannel airborne radar']"
"Although many efforts have been made on the fusion of Light Detection and Ranging (LiDAR) and aerial imagery for the extraction of houses, little research on taking advantage of a building's geometric features, properties, and structures for assisting the further fusion of the two types of data has been made. For this reason, this paper develops a seamless fusion between LiDAR and aerial imagery on the basis of aspect graphs, which utilize the features of houses, such as geometry, structures, and shapes. First, 3-D primitives, standing for houses, are chosen, and their projections are represented by the aspects. A hierarchical aspect graph is then constructed using aerial image processing in combination with the results of LiDAR data processing. In the aspect graph, the note represents the face aspect and the arc is described by attributes obtained by the formulated coding regulations, and the coregistration between the aspect and LiDAR data is implemented. As a consequence, the aspects and/or the aspect graph are interpreted for the extraction of houses, and then the houses are fitted using a planar equation for creating a digital building model (DBM). The experimental field, which is located in Wytheville, VA, is used to evaluate the proposed method. The experimental results demonstrated that the proposed method is capable of effectively extracting houses at a successful rate of 93%, as compared with another method, which is 82% effective when LiDAR spacing is approximately 7.3 by 7.3 ft 2 . The accuracy of 3-D DBM is higher than the method using only single LiDAR data.","['Laser radar', 'Face', 'Buildings', 'Encoding', 'Optical imaging', 'Feature extraction', 'Merging']","['Aerial image', 'extraction', 'house', 'image processing', 'light detection and ranging (LiDAR)', 'urban']"
"In modern society, the anthropogenic influences on ecosystems are central points to understand the evolution of our planet. A polarimetric synthetic aperture radar may have a significant contribution in tackling problems concerning land use change, since such data are available with any-weather conditions. Additionally, the discrimination capability can be enhanced by the polarimetric analysis. Recently, an algorithm able to identify targets scattering an electromagnetic wave with any degree of polarization has been developed, which makes use of a vector rearrangement of the elements of the coherency matrix. In the present work, this target detector is modified to perform change detection between two polarimetric acquisitions, for land use monitoring purposes. Regarding the selection of the detector parameters, a physical rationale is followed, developing a new parameterization of the algebraic space where the detector is defined. As it will be illustrated in the following, this space is 6-D complex with restrictions due to the physical feasibility of the vectors. Specifically, a link between the detector parameters and the angle differences of the eigenvector model is obtained. Moreover, a dual polarimetric version of the change detector is developed, in case quad-polarimetric data are not available. With the purpose of testing the methodology, a variety of data sets were exploited: quad-polarimetric airborne data at L-band (E-SAR), quad-polarimetric satellite data at C-band (Radarsat-2), and dual-polarimetric satellite data at X-band (TerraSAR-X). The algorithm results show agreement with the available information about land changes. Moreover, a comparison with a known change detector based on the maximum likelihood ratio is presented, providing improvements in some conditions. The two methodologies differ in the analysis of the total amplitude of the backscattering, where the proposed algorithm does not take this into consideration.","['Algorithm design and analysis', 'Synthetic aperture radar', 'Scattering', 'Thyristors', 'Covariance matrix', 'Radar polarimetry', 'Change detection algorithms']","['Change detection', 'polarimetry', 'synthetic aperture radar (SAR)']"
"Deep learning-based coastline detection algorithms have begun to outshine traditional statistical methods in recent years. However, they are usually trained only as single-purpose models to either segment land and water or delineate the coastline. In contrast to this, a human annotator will usually keep a mental map of both segmentation and delineation when performing manual coastline detection. To take into account this task duality, we, therefore, devise a new model to unite these two approaches in a deep learning model. By taking inspiration from the main building blocks of a semantic segmentation framework (UNet) and an edge detection framework (HED), both tasks are combined in a natural way. Training is made efficient by employing deep supervision on side predictions at multiple resolutions. Finally, a hierarchical attention mechanism is introduced to adaptively merge these multiscale predictions into the final model output. The advantages of this approach over other traditional and deep learning-based methods for coastline detection are demonstrated on a data set of Sentinel-1 imagery covering parts of the Antarctic coast, where coastline detection is notoriously difficult. An implementation of our method is available at https://github.com/khdlr/HED-UNet .","['Image edge detection', 'Image segmentation', 'Antarctica', 'Task analysis', 'Semantics', 'Predictive models', 'Ice']","['Antarctica', 'edge detection', 'glacier front', 'semantic segmentation']"
"In this paper, we introduce the concept, develop the theory, and demonstrate the advantages of fully focused coherent processing of pulse echoes from a nadir-looking pulse-limited radar altimeter. This process, similar to synthetic aperture radar (SAR) imaging systems, reduces the along-track resolution down to the theoretical limit equal to half the antenna length. We call this the fully focused SAR (FF-SAR) altimetry processing. The technique is directly applicable to SAR altimetry missions such as CryoSat-2, Sentinel-3, or Sentinel-6/Jason-CS. The footprint of an FF-SAR altimeter measurement is a narrow strip on the surface, which is pulse limited across track and SAR focused along track. Despite the asymmetry of the altimeter footprint, the fully focused technique may be useful for applications in which one needs to separate specific targets within highly heterogeneous scenes, such as in the case of sea ice lead detection, hydrology, and coastal altimetry applications. In addition, over rough homogeneous surfaces, such as the ocean or ice sheets, the improved multilooking capability of FF-SAR leads to a significant increase in the effective number of looks with respect to the delay/Doppler processing, resulting in better geophysical parameter estimation. The proposed processing technique was verified by processing full bit rate CryoSat-2 SAR mode data over the Svalbard transponder. Hydrology, sea ice, and open ocean applications are also demonstrated in this paper, showing the improvement of this technique with respect to conventional and delay/Doppler altimetry.","['Synthetic aperture radar', 'Altimetry', 'Radar tracking', 'Spaceborne radar', 'Satellites', 'Radar imaging']","['Altimetry', 'delay/Doppler altimetry', 'SAR altimetry', 'synthetic aperture radar (SAR)']"
"The Edition 2 (Ed2) cloud property retrieval algorithm system was upgraded and applied to the MODerate-resolution Imaging Spectroradiometer (MODIS) data for the Clouds and the Earth's Radiant Energy System (CERES) Edition 4 (Ed4) products. New calibrations for solar channels and the use of the 1.24-μm channel for cloud optical depth (COD) over snow improve the daytime consistency between Terra and Aqua MODIS retrievals. Use of additional spectral channels and revised logic enhanced the cloud-top phase retrieval accuracy. A new ice crystal reflectance model and a CO 2 -channel algorithm retrieved higher ice clouds, while a new regional lapse rate technique produced more accurate water cloud heights than in Ed2. Ice cloud base heights are more accurate due to a new cloud thickness parameterization. Overall, CODs increased, especially over the polar (PO) regions. The mean particle sizes increased slightly for water clouds, but more so for ice clouds in the PO areas. New experimental parameters introduced in Ed4 are limited in utility, but will be revised for the next CERES edition. As part of the Ed4 retrieval evaluation, the average properties are compared with those from other algorithms and the differences between individual reference data and matched Ed4 retrievals are explored. Part II of this article provides a comprehensive, objective evaluation of selected parameters. More accurate interpretation of the CERES radiation measurements has resulted from the use of the Ed4 cloud properties.","['Clouds', 'MODIS', 'Integrated optics', 'Ice', 'Cloud computing', 'Meteorology', 'Optical imaging']","['Climate', 'cloud', 'cloud height', 'cloud particle size', 'cloud phase', 'cloud optical depth (COD)', 'cloud remote sensing MODerate-resolution Imaging Spectroradiometer (MODIS)', 'clouds and the Earth’s radiant energy system (CERES)', 'validation']"
"Soil moisture retrievals, delivered as a CATDS (Centre Aval de Traitement des Données SMOS) Level-3 product of the Soil Moisture and Ocean Salinity (SMOS) mission, form an important information source, particularly for updating land surface models. However, the coarse resolution of the SMOS product requires additional treatment if it is to be used in applications at higher resolutions. Furthermore, the remotely sensed soil moisture often does not reflect the climatology of the soil moisture predictions, and the bias between model predictions and observations needs to be removed. In this paper, a statistical framework is presented that allows for the downscaling of the coarse-scale SMOS soil moisture product to a finer resolution. This framework describes the interscale relationship between SMOS observations and model-predicted soil moisture values, in this case, using the variable infiltration capacity (VIC) model, using a copula. Through conditioning, the copula to a SMOS observation, a probability distribution function is obtained that reflects the expected distribution function of VIC soil moisture for the given SMOS observation. This distribution function is then used in a cumulative distribution function matching procedure to obtain an unbiased fine-scale soil moisture map that can be assimilated into VIC. The methodology is applied to SMOS observations over the Upper Mississippi River basin. Although the focus in this paper is on data assimilation applications, the framework developed could also be used for other purposes where downscaling of coarse-scale observations is required.","['Soil moisture', 'Distribution functions', 'Remote sensing', 'Data models', 'Predictive models', 'Data assimilation']","['Hydrology', 'microwave radiometry', 'soil moisture']"
"Multilabel classification plays a momentous role in perceiving intricate contents of an aerial image and triggers several related studies over the last years. However, most of them deploy few efforts in exploiting label relations, while such dependencies are crucial for making accurate predictions. Although an long short term memory (LSTM) layer can be introduced to modeling such label dependencies in a chain propagation manner, the efficiency might be questioned when certain labels are improperly inferred. To address this, we propose a novel aerial image multilabel classification network, attention-aware label relational reasoning network. Particularly, our network consists of three elemental modules: 1) a label-wise feature parcel learning module; 2) an attentional region extraction module; and 3) a label relational inference module. To be more specific, the label-wise feature parcel learning module is designed for extracting high-level label-specific features. The attentional region extraction module aims at localizing discriminative regions in these features without region proposal generation, yielding attentional label-specific features. The label relational inference module finally predicts label existences using label relations reasoned from outputs of the previous module. The proposed network is characterized by its capacities of extracting discriminative label-wise features and reasoning about label relations naturally and interpretably. In our experiments, we evaluate the proposed model on two multilabel aerial image data sets, of which one is newly produced. Quantitative and qualitative results on these two data sets demonstrate the effectiveness of our model. To facilitate progress in the multilabel aerial image classification, our produced data set will be made publicly available.","['Feature extraction', 'Semantics', 'Cognition', 'Correlation', 'Remote sensing', 'Task analysis', 'Soil']","['Attentional region extraction', 'convolutional neural network (CNN)', 'high-resolution aerial image', 'label relational reasoning', 'multilabel classification']"
"Miniaturized hyperspectral imaging sensors are becoming available to small unmanned airborne vehicle (UAV) platforms. Imaging concepts based on frame format offer an attractive alternative to conventional hyperspectral pushbroom scanners because they enable enhanced processing and interpretation potential by allowing for acquisition of the 3-D geometry of the object and multiple object views together with the hyperspectral reflectance signatures. The objective of this investigation was to study the performance of novel visible and near-infrared (VNIR) and short-wave infrared (SWIR) hyperspectral frame cameras based on a tunable Fabry-Pérot interferometer (FPI) in measuring a 3-D digital surface model and the surface moisture of a peat production area. UAV image blocks were captured with ground sample distances (GSDs) of 15, 9.5, and 2.5 cm with the SWIR, VNIR, and consumer RGB cameras, respectively. Georeferencing showed consistent behavior, with accuracy levels better than GSD for the FPI cameras. The best accuracy in moisture estimation was obtained when using the reflectance difference of the SWIR band at 1246 nm and of the VNIR band at 859 nm, which gave a root mean square error (rmse) of 5.21 pp (pp is the mass fraction in percentage points) and a normalized rmse of 7.61%. The results are encouraging, indicating that UAV-based remote sensing could significantly improve the efficiency and environmental safety aspects of peat production.","['Cameras', 'Sensors', 'Hyperspectral imaging', 'Moisture', 'Production']","['Calibration', 'geographic information system', 'geometry', 'image classification', 'radiometry', 'remote sensing', 'remotely piloted aircraft', 'spectroscopy', 'stereo vision']"
"The objectives of this research are to develop robust methods for segmentation of multitemporal synthetic aperture radar (SAR) and optical data and to investigate the fusion of multitemporal ENVISAT advanced synthetic aperture radar (ASAR) and Chinese HJ-1B multispectral data for detailed urban land-cover mapping. Eight-date multiangle ENVISAT ASAR images and one-date HJ-1B charge-coupled device image acquired over Beijing in 2009 are selected for this research. The edge-aware region growing and merging (EARGM) algorithm is developed for segmentation of SAR and optical data. Edge detection using a Sobel filter is applied on SAR and optical data individually, and a majority voting approach is used to integrate all edge images. The edges are then used in a segmentation process to ensure that segments do not grow over edges. The segmentation is influenced by minimum and maximum segment sizes as well as the two homogeneity criteria, namely, a measure of color and a measure of texture. The classification is performed using support vector machines. The results show that our EARGM algorithm produces better segmentation than eCognition, particularly for built-up classes and linear features. The best classification result (80%) is achieved using the fusion of eight-date ENVISAT ASAR and HJ-1B data. This represents 5%, 11%, and 14% improvements over eCognition, HJ-1B, and ASAR classifications, respectively. The second best classification is achieved using fusion of four-date ENVISAT ASAR and HJ-1B data (78%). The result indicates that fewer multitemporal SAR images can achieve similar classification accuracy if multitemporal multiangle dual-look-direction SAR data are carefully selected.","['Synthetic aperture radar', 'Image segmentation', 'Image edge detection', 'Support vector machines', 'Optical imaging', 'Merging', 'Optical sensors']","['Edge-aware region growing and merging (EARGM)', 'ENVISAT advanced synthetic aperture radar (SAR) (ASAR)', 'fusion', 'HJ-1B', 'multitemporal', 'segmentation', 'urban land cover']"
"Remotely sensed images may contain some missing areas because of poor weather conditions and sensor failure. Information of those areas may play an important role in the interpretation of multitemporal remotely sensed data. This paper aims at reconstructing the missing information by a nonlocal low-rank tensor completion method. First, nonlocal correlations in the spatial domain are taken into account by searching and grouping similar image patches in a large search window. Then, low rankness of the identified fourth-order tensor groups is promoted to consider their correlations in spatial, spectral, and temporal domains, while reconstructing the underlying patterns. Experimental results on simulated and real data demonstrate that the proposed method is effective both qualitatively and quantitatively. In addition, the proposed method is computationally efficient compared with other patch-based methods such as the recently proposed patch matching-based multitemporal group sparse representation method.","['Tensile stress', 'Image reconstruction', 'Remote sensing', 'Correlation', 'Earth', 'Indexes', 'Europe']","['Missing information reconstruction', 'multitemporal remotely sensed images', 'tensor completion']"
"We present the implementation of a facet-based simulator to investigate the forward scattering of L-band signals from realistic sea surfaces and its application to spaceborne ocean Global Navigation Satellite System (GNSS) Reflectometry. This approach provides a new flexible tool to assess the influence of the ocean surface roughness on scattered GNSS signals. The motivation stems from the study by Clarizia , which revealed significant differences between delay–Doppler maps (DDMs) obtained from UK-DMC satellite data and DDMs simulated with the Zavorotny–Voronovich (Z-V) model. Here, the scattered power and polarization ratio (PR) are computed for explicit 3-D ocean wave fields, using a novel implementation of the Kirchhoff approximation (KA), which we call the Facet Approach (FA). We find that the FA is consistent with the full KA and the Geometrical Optics (GO) used in the Z-V model, while being less computationally expensive than the KA and able to represent polarization effects not captured by the GO. Instantaneous maps of the bistatic normalized radar cross section computed with the FA show clear patterns associated with the underlying waves. The wave field is particularly visible in the PR, indicating that the scattering is generally dominated by the HH component, particularly from ocean wave troughs. Polarization effects show, for the first time, a strong correlation to the explicit sea surface from which the scattering originated. DDMs of the scattered power computed with the FA reveal patchy patterns and power distributions that differ from those obtained with Z-V and show closer similarities with observed DDMs from UK-DMC.","['Sea surface', 'Scattering', 'Surface waves', 'Surface roughness', 'Rough surfaces', 'Optical surface waves']","['Facet approach (FA)', 'Global Navigation Satellite System Reflectometry (GNSS-R)', 'Kirchhoff approximation (KA)', 'ocean waves', 'polarization', 'scattering']"
"Detection of changes caused by major events-such as earthquakes, volcanic eruptions, and floods-from interferometric synthetic aperture radar (SAR) data is challenging because of the coupled effects with temporal decorrelation caused by natural phenomena, including rain, snow, wind, and seasonal changes. The coupled effect of major events and natural phenomena sometimes leads to misinterpretation of interferometric coherence maps and often degrades the performance of change detection algorithms. To differentiate decorrelation sources caused by natural changes from those caused by an event of interest, we formulated a temporal decorrelation model that accounts for the random motion of canopy elements, temporally correlated dielectric changes, and temporally uncorrelated dielectric changes of canopy and ground. The model parameters are extracted from the interferometric pairs associated with natural changes in canopy and ground using the proposed temporal decorrelation model. In addition, the cumulative distribution functions of the temporally uncorrelated model parameters, which are associated with natural changes in canopy and ground, are estimated from interferometric pairs acquired before the event. Model parameters are also extracted from interferometric SAR data acquired across the event and compared with the cumulative probabilities of natural changes in order to calculate the probability of a major event. Subsequently, pixels with cumulative probabilities greater than 75% are marked as changed due to the event. A case study for detecting volcanic ash during the eruption of the Shinmoedake volcano in January 2011 was carried out using L-band Advanced Land Observation Satellite PALSAR data.","['Decorrelation', 'Coherence', 'Synthetic aperture radar', 'Spaceborne radar', 'Interferometry', 'Data models']","['Coherence change detection', 'synthetic aperture radar (SAR) interferometry', 'temporal decorrelation model', 'volcanic ash']"
"Object detection in very-high-resolution (VHR) remote sensing imagery remains a challenge. Environmental factors, such as illumination intensity and weather, reduce image quality, resulting in poor feature representation and limited detection accuracy. To enrich the feature representation and mine the underlying context information among objects, this article proposes a context-aware convolutional neural network (CA-CNN) model for object detection that includes proposal generation, context feature extraction, feature fusion, and classification. During feature extraction, we propose integrating a context-regions-of-interests (Context-RoIs) mining layer into the CNN model and extracting context features by mapping Context-RoIs mined from the foreground proposals to multilevel feature maps. Finally, the context features extracted from multilevel layers are fused into a single layer, and the proposals represented by the fused features are classified by a softmax classifier. In this article, through numerous experiments, we thoroughly explore the influence of key factors, such as Context-RoIs, different feature scales, and different spatial context window sizes. Because of the end-to-end network design approach, our proposed model simultaneously maintains high efficiency and effectiveness. We conducted all model testing on the public NWPU VHR-10 data set. The experimental results demonstrate that our proposed CA-CNN model achieves significantly improved model performance and better detection results compared with the state-of-the-art methods.","['Feature extraction', 'Object detection', 'Proposals', 'Microsoft Windows', 'Semantics', 'Context modeling', 'Convolutional codes']","['Contextual information mining', 'convolutional neural network (CNN)', 'object detection']"
"Using synthetic aperture radar (SAR) interferometry to monitor long-term millimeter-level deformation of urban infrastructures, such as individual buildings and bridges, is an emerging and important field in remote sensing. In the state-of-the-art methods, deformation parameters are retrieved and monitored on a pixel basis solely in the SAR image domain. However, the inevitable side-looking imaging geometry of SAR results in undesired occlusion and layover in urban area, rendering the current method less competent for a semantic-level monitoring of different urban infrastructures. This paper presents a framework of a semantic-level deformation monitoring by linking the precise deformation estimates of SAR interferometry and the semantic classification labels of optical images via a 3-D geometric fusion and semantic texturing. The proposed approach provides the first “SARptical” point cloud of an urban area, which is the SAR tomography point cloud textured with attributes from optical images. This opens a new perspective of InSAR deformation monitoring. Interesting examples on bridge and railway monitoring are demonstrated.","['Optical imaging', 'Optical scattering', 'Synthetic aperture radar', 'Three-dimensional displays', 'Optical sensors', 'Adaptive optics', 'Optical interferometry']","['Bridge monitoring', 'interferometric synthetic aperture radar (InSAR)', 'optical InSAR fusion', 'railway monitoring', 'SAR', 'semantic classification']"
"Microwave backscatter from vegetated surfaces is influenced by vegetation structure and vegetation water content (VWC), which varies with meteorological conditions and moisture in the root zone. Radar backscatter observations are used for many vegetation and soil moisture monitoring applications under the assumption that VWC is constant on short timescales. This research aims to understand how backscatter over agricultural canopies changes in response to diurnal differences in VWC due to water stress. A standard water-cloud model and a two-layer water-cloud model for maize were used to simulate the influence of the observed variations in bulk/leaf/stalk VWC and soil moisture on the various contributions to total backscatter at a range of frequencies, polarizations, and incidence angles. The bulk VWC and leaf VWC were found to change up to 30% and 40%, respectively, on a diurnal basis during water stress and may have a significant effect on radar backscatter. Total backscatter time series are presented to illustrate the simulated diurnal difference in backscatter for different radar frequencies, polarizations, and incidence angles. Results show that backscatter is very sensitive to variations in VWC during water stress, particularly at large incidence angles and higher frequencies. The diurnal variation in total backscatter was dominated by variations in leaf water content, with simulated diurnal differences of up to 4 dB in X- through K u -bands (8.6-35 GHz) . This study highlights a potential source of error in current vegetation and soil monitoring applications and provides insights into the potential use for radar to detect variations in VWC due to water stress.","['Backscatter', 'Stress', 'Radar', 'Soil moisture', 'Vegetation mapping', 'Soil measurements']","['Agriculture', 'diurnal differences', 'hydrology', 'microwaves', 'radar', 'vegetation', 'vegetation water content (VWC)', 'water stress']"
"Multi-modal data fusion has recently been shown promise in classification tasks in remote sensing. Optical data and radar data, two important yet intrinsically different data sources, are attracting more and more attention for potential data fusion. It is already widely known that a machine learning-based methodology often yields excellent performance. However, the methodology relies on a large training set, which is very expensive to achieve in remote sensing. The semi-supervised manifold alignment (SSMA), a multi-modal data fusion algorithm, has been designed to amplify the impact of an existing training set by linking labeled data to unlabeled data via unsupervised techniques. In this paper, we explore the potential of SSMA in fusing optical data and polarimetric synthetic aperture radar (SAR) data, which are multi-sensory data sources. Furthermore, we propose a MAPPER-induced manifold alignment (MIMA) for the semi-supervised fusion of multi-sensory data sources. Our proposed method unites SSMA with MAPPER, which is developed from the emerging topological data analysis (TDA) field. To the best of our knowledge, this is the first time that SSMA has been applied on fusing optical data and SAR data, and also the first time that TDA has been applied in remote sensing. The conventional SSMA derives a topological structure using k-nearest neighbor (kNN), while MIMA employs MAPPER, which considers the field knowledge and derives a novel topological structure through the spectral clustering in a data-driven fashion. The experimental results on data fusion with respect to land cover land use classification and local climate zone classification suggest superior performance of MIMA.","['Optical imaging', 'Optical sensors', 'Synthetic aperture radar', 'Remote sensing', 'Optical distortion', 'Manifolds', 'Optical scattering']","['Hyperspectral image', 'MAPPER', 'multi-modal data fusion', 'multi-sensory data fusion', 'multispectral image', 'polarimetric synthetic aperture radar (PolSAR)', 'semi-supervised manifold alignment (SSMA)', 'topological data analysis (TDA)']"
"The majority of optical observations acquired via spaceborne Earth imagery are affected by clouds. While there is numerous prior work on reconstructing cloud-covered information, previous studies are, oftentimes, confined to narrowly defined regions of interest, raising the question of whether an approach can generalize to a diverse set of observations acquired at variable cloud coverage or in different regions and seasons. We target the challenge of generalization by curating a large novel data set for training new cloud removal approaches and evaluate two recently proposed performance metrics of image quality and diversity. Our data set is the first publically available to contain a global sample of coregistered radar and optical observations, cloudy and cloud-free. Based on the observation that cloud coverage varies widely between clear skies and absolute coverage, we propose a novel model that can deal with either extreme and evaluate its performance on our proposed data set. Finally, we demonstrate the superiority of training models on real over synthetic data, underlining the need for a carefully curated data set of real observations. To facilitate future research, our data set is made available online.","['Clouds', 'Optical imaging', 'Cloud computing', 'Earth', 'Optical sensors', 'Data integration', 'Image reconstruction']","['Cloud removal', 'data fusion', 'deep learning', 'generative adversarial network (GAN)', 'optical imagery', 'synthetic aperture radar (SAR)-optical']"
"An algorithm for surface soil moisture estimation using L-band radar observations is introduced. The formulation envelops a wide range of land surface conditions based on three limiting cases defined in terms of end-members: smooth bare soil, rough bare soil, and a maximum vegetation covered soil. Parameterizations for these end-members are obtained using forward electromagnetic scattering models. Modulation due to soil surface roughness and overlying vegetation scattering effects between end-members are accounted using the radar vegetation index and the newly introduced radar roughness index. Hence, the retrieval algorithm developed here does not depend on ancillary vegetation or roughness information. The algorithm is tested with ground-based truck-mounted bare soil observations and observations from several airborne field campaigns that represent a wide range of surface conditions.","['Vegetation mapping', 'Soil moisture', 'Sensitivity', 'Rough surfaces', 'Surface roughness', 'Backscatter']","['Radar roughness (RR)', 'radar soil moisture', 'radar vegetation']"
"Building footprint maps are vital to many remote sensing (RS) applications, such as 3-D building modeling, urban planning, and disaster management. Due to the complexity of buildings, the accurate and reliable generation of the building footprint from RS imagery is still a challenging task. In this article, an end-to-end building footprint generation approach that integrates convolution neural network (CNN) and graph model is proposed. CNN serves as the feature extractor, while the graph model can take spatial correlation into consideration. Moreover, we propose to implement the feature pairwise conditional random field (FPCRF) as a graph model to preserve sharp boundaries and fine-grained segmentation. Experiments are conducted on four different data sets: 1) Planetscope satellite imagery of the cities of Munich, Paris, Rome, and Zurich; 2) ISPRS Benchmark data from the city of Potsdam; 3) Dstl Kaggle data set; and 4) Inria Aerial Image Labeling data of Austin, Chicago, Kitsap County, Western Tyrol, and Vienna. It is found that the proposed end-to-end building footprint generation framework with the FPCRF as the graph model can further improve the accuracy of building footprint generation by using only CNN, which is the current state of the art.","['Buildings', 'Semantics', 'Feature extraction', 'Image segmentation', 'Task analysis', 'Remote sensing', 'Computer architecture']","['Building footprint', 'conditional random field (CRF)', 'convolution neural network (CNN)', 'graph model', 'semantic segmentation']"
"Polarimetric technology has been one of the most important advances in microwave remote sensing during recent decades. H-alpha decomposition, which is a type of polarimetric analysis technique, has been common for terrain and land-use classification in polarimetric synthetic aperture radar. However, the technique has been less common in the ground penetrating radar (GPR) community. In this paper, we apply the H-alpha decomposition to analyze the surface GPR data to obtain polarimetric attributes for subsurface target classification. Also, by combining H-alpha decomposition and migration, we can obtain a subsurface H-alpha color-coded reconstructed target image, from which we can use both the polarimetric attributes and the geometrical features of the subsurface targets to enhance the ability of subsurface target classification of surface GPR. A 3-D full polarimetric GPR data set was acquired in a laboratory experiment, in which four targets, a scatterer with many branches, a ball, a plate, and a dihedral scatter, were buried in dry sand under flat ground surface, and used to test these techniques. As results, we obtained the subsurface H-alpha distribution and classified the subsurface targets. Also, we derived a subsurface H-alpha color-coded reconstructed target image and identified all four targets in the laboratory experiment.","['Ground penetrating radar', 'Scattering', 'Image reconstruction', 'Transmitting antennas', 'Antenna measurements', 'Arrays', 'Antenna arrays']","['Classification', 'ground penetrating radar (GPR)', 'H-alpha decomposition', 'imaging', 'migration', 'subsurface targets']"
"Several environmental and sensor effects make the determination of the wavelength position of absorption features in the visible near infrared (VNIR) (400-1200 nm) from hyperspectral imagery more difficult than from nonimaging spectrometers. To evaluate this, we focus on the ferric iron crystal field absorption, located at about 900 nm (F900), because it is impacted by both environmental and sensor effects. The consistency with which the wavelength position of F900 can be determined from imagery acquired in laboratory and field settings is evaluated under artificial and natural illumination, respectively. The wavelength position of F900, determined from laboratory imagery, is also evaluated as an indicator of the proportion of goethite in mixtures of crushed rock. Results are compared with those from a high-resolution field spectrometer. Images describing the wavelength position of F900 showed large amounts of spatial variability and contained an artifact-a consistent shift in the wavelength position of F900 to longer wavelengths. These effects were greatly reduced or removed when wavelength position was determined from a polynomial fit to the data, enabling wavelength position to be used to map hematite and goethite in samples of ore and on a vertical surface (a mine face). The wavelength position of F900 from a polynomial fit was strongly positively correlated with the proportion of goethite (R 2 =0.97). Taken together, these findings indicate that the wavelength position of absorption features from VNIR imagery should be determined from a polynomial (or equivalent) fit to the original data and not from the original data themselves.","['Calibration', 'Absorption', 'Hyperspectral imaging', 'Iron', 'Imaging', 'Crystals']","['Geology', 'hyperspectral sensors', 'image classification', 'infrared spectroscopy', 'minerals', 'mining industry', 'polynomials', 'remote sensing', 'signal processing', 'spectral analysis', 'terrain mapping']"
"This paper introduces a framework for robust parameter estimation in multipass interferometric synthetic aperture radar (InSAR), such as persistent scatterer interferometry, SAR tomography, small baseline subset, and SqueeSAR. These techniques involve estimation of phase history parameters with or without covariance matrix estimation. Typically, their optimal estimators are derived on the assumption of stationary complex Gaussian-distributed observations. However, their statistical robustness has not been addressed with respect to observations with nonergodic and non-Gaussian multivariate distributions. The proposed robust InSAR optimization (RIO) framework answers two fundamental questions in multipass InSAR: 1) how to optimally treat images with a large phase error, e.g., due to unmolded motion phase, uncompensated atmospheric phase, etc.; and 2) how to estimate the covariance matrix of a non-Gaussian complex InSAR multivariate, particularly those with nonstationary phase signals. For the former question, RIO employs a robust M-estimator to effectively downweight these images; and for the latter, we propose a new method, i.e., the rank M-estimator, which is robust against non-Gaussian distribution. Furthermore, it can work without the assumption of sample stationarity, which is a topic that has not previously been addressed. We demonstrate the advantages of the proposed framework for data with large phase error and heavily tailed distribution, by comparing it with state-of-the-art estimators for persistent and distributed scatterers. Substantial improvement can be achieved in terms of the variance of estimates. The proposed framework can be easily extended to other multipass InSAR techniques, particularly to those where covariance matrix estimation is vital.","['Robustness', 'Covariance matrices', 'Atmospheric modeling', 'Maximum likelihood estimation', 'Synthetic aperture radar', 'History']","['Differential interferometric synthetic aperture radar (D-InSAR)', 'SAR interferometry (InSAR)', '$M$-estimator', 'rank covariance matrix', 'robust InSAR optimization (RIO)', 'robust estimation']"
"Artificial intelligence (AI) is paving the way for a new era of algorithms focusing directly on the information contained in the data, autonomously extracting relevant features for a given application. While the initial paradigm was to have these applications run by a server hosted processor, recent advances in microelectronics provide hardware accelerators with an efficient ratio between computation and energy consumption, enabling the implementation of AI algorithms “at the edge.” In this way only the meaningful and useful data are transmitted to the end-user, minimizing the required data bandwidth, and reducing the latency with respect to the cloud computing model. In recent years, European Space Agency (ESA) is promoting the development of disruptive innovative technologies on-board earth observation (EO) missions. In this field, the most advanced experiment to date is theΦ-sat-1, which has demonstrated the potential of artificial intelligence (AI) as a reliable and accurate tool for cloud detection on-board a hyperspectral imaging mission. The activities involved included demonstrating the robustness of the Intel Movidius Myriad 2 hardware accelerator against ionizing radiation, developing a Cloudscout segmentation neural network (NN), run on Myriad 2, to identify, classify, and eventually discard on-board the cloudy images, and assessing the innovative Hyperscout-2 hyperspectral sensor. This mission represents the first official attempt to successfully run an AI deep convolutional NN (CNN) directly inferencing on a dedicated accelerator on-board a satellite, opening the way for a new era of discovery and commercial applications driven by the deployment of on-board AI.","['Artificial intelligence', 'Satellites', 'Cloud computing', 'Space vehicles', 'Hyperspectral imaging', 'Orbits', 'Earth']","['Φ-Sat-1', 'artificial intelligence (AI)', 'earth observation (EO)', 'hyperspectral', 'microsatellite', 'nanosatellite', 'on-the edge', 'satellite camera', 'segmentation network', 'synthetic dataset']"
"We have studied the incidence angle (θ0) dependence of the sea ice backscattering coefficient (0.°) for Sentinel-1 (S-1) extra wide (EW) mode dualpolarization (HH/HV) synthetic aperture radar (SAR) imagery acquired over the Kara Sea under winter and summer melting conditions. The determination of the 0.° versus θ 0 dependence was based on SAR image pairs acquired on ascending and descending orbits over the same sea ice area with a short time difference. The SAR noise floor was subtracted from the HV images. From the image pairs 1.1 by 1.1 km windows representing level first-year ice (LFYI) and deformed first-year ice (DFYI) were manually selected, and a linear regression was fit between the resulting 0.° and θ0 differences of the windows to estimate the slope b 1 (dB/1°) between 0.° and θ0. For example, under winter condition b 1 for DFYI at HHand HV-polarizations was found to be -0.24 and -0.16 dB/1°, respectively, and b 1 for LFYI at HH-polarization was -0.25 dB/1°. It was not possible to determine a reliable b 1 for LFYI at HV due to a contamination effect of the S-1 noise floor. The b 1 values at HH compared well with previous studies. They can be used to compensate the 0.° incidence angle variation in the S-1 EW SAR images with good accuracy. The HH b 1 values are applicable to other S-1 imaging modes and other C-band SAR sensors like RADARSAT-2. Unfortunately, the HV b 1 values are specific to the S-1 EW mode due to the noise floor problem.","['Sea ice', 'Synthetic aperture radar', 'Snow', 'Backscatter', 'Arctic', 'Sea surface']","['Arctic regions', 'radar scattering', 'sea ice', 'synthetic aperture radar (SAR)']"
"Significant wave height is one of the most important parameters for characterizing ocean waves and essential for coastal protection, shipping, as well as off shore industry operations. Within this paper, a robust method is introduced for retrieving significant wave heights from Doppler speed measurements acquired with a coherent-on-receive marine radar. The Doppler velocity is caused by the surface scattering in the line of site of the radar. To a huge extent its periodic component is induced by the orbital motions associated with surface waves. The proposed methodology is based on linear wave theory, accounts for projection effects caused by the fixed antenna look direction, and was applied to a coherent-on-receive radar operating at X-band with vertical polarization in transmit and receive. To show the overall performance of the method, a data set consisting of approximately 100 days of radar measurements was analyzed and used to retrieve significant wave heights. Comparisons to wave measurements collected by a wave rider buoy resulted in a root-mean-square (rms) error of 0.21 m and a bias of 0 m without any calibration parameters needed. To further improve the accuracy of significant wave height, a calibration factor needs to be accounted for, which improves the rms error to 0.15 m with a negligible bias of -0.01 m.","['Surface waves', 'Sea surface', 'Extraterrestrial measurements', 'Sea measurements', 'Doppler radar', 'Doppler effect']","['Doppler radar', 'marine radar', 'remote sensing', 'sea surface electromagnetic scattering', 'surface waves']"
"Infrared (IR) remote sensing is increasingly used in studies of vegetation fire behavior, and high spatiotemporal resolution investigations often require data to be collected from airborne platforms, for example, standard helicopters. This paper aims to extend the range of conditions under which low-cost “handheld” thermal imaging cameras can be employed in such studies, particularly by enabling the effective and efficient geometric correction of thermal imagery collected from such devices, even when viewing far off-nadir (e.g., out of a side door or window). The approach is based on the automated detection of a set of fixed thermal “ground control points,” coupled with the use of a linear transformation matrix for warping the raw IR imagery to a fixed coordinate system. The output set of geometrically corrected brightness temperature and radiance images can be used to derive fire radiative power (FRP) and flame front rate of spread (ROS). We demonstrate and test our IR image processing methods on a series of case study fires, ranging from a small-scale laboratory to a 945-m 2 outdoor experimental burn. We compare mapped information on FRP obtained from simultaneous nadir and off-nadir views, where we find differences that are in part controlled by flame structure and/or view angle. In the large open fire case, we compare the mapped fire radiative energy and ROS to simultaneously acquired aerial photography that provides the position of fuel and flames in high detail, and we demonstrate how these data sets can be used to explore various aspects of fire behavior.","['Cameras', 'Helicopters', 'Temperature measurement', 'Fuels', 'Fires', 'Geometry']","['Fire radiative energy (FRE)', 'fire radiative power (FRP)', 'georeferencing algorithm', 'rate of spread (ROS)', 'thermal imagery']"
"The Advanced Scatterometer (ASCAT) on the MetOp-A satellite is a radar instrument designed specifically to retrieve the ocean surface wind speed and direction. The ASCAT wind vector products are produced and utilized operationally in support of the National Oceanic and Atmospheric Administration (NOAA)'s weather forecasting and warning mission. The standard ASCAT winds at NOAA are produced using the ASCAT wind data processor developed at the Royal Netherlands Meteorological Institute (KNMI) utilizing the CMOD5.n geophysical model function (GMF). Recent validation of the ASCAT wind retrievals revealed a low bias at high wind speeds when compared to both the QuikSCAT winds and the National Centers for Environmental Prediction numerical weather prediction (NWP) model winds. The goal of this paper is to investigate the ASCAT high-wind-speed performance and to modify, as appropriate, the high-wind-speed portion of CMOD5.n GMF. This effort would potentially improve the utility of ASCAT wind retrievals in supporting wind warning and analysis and thus better mitigate the loss of QuikSCAT data products. Traditionally, the GMF is developed empirically by collocating scatterometer measurements and other truth data such as buoy and NWP model winds. However, NWP models are known to underestimate the intensity of higher wind speeds, and data sources such as ship-based or buoy-based observations provide an inadequate quantity of measurements for empirical GMF development. In this paper, a method utilizing aircraft-based scatterometer measurements in the high-wind-speed regimes is used in conjunction with satellite scatterometer measurements to refine the satellite GMF. As a result of this paper, a high wind C-band satellite GMF, CMOD5.h, was developed and implemented in NOAA's ASCAT processor. The validation comparison of the high wind and standard ASCAT wind products revealed 0.6-m/s reduction in the wind speed bias for winds greater than 15 m/s with respect to QuikSCAT, WindSat, and Step Frequency Microwave Radiometer high wind measurements.","['Wind speed', 'Wind forecasting', 'Sea measurements', 'US Government agencies', 'Satellites', 'Spatial resolution']","['Advanced Scatterometer (ASCAT)', 'geophysical model function (GMF)', 'high winds', 'ocean vector winds', 'QuikSCAT']"
"Radiances measured by satellite radiometers are often subject to biases due to limitations in their radiometric calibration. In support of the Global Space-based Inter-Calibration System project, to improve the quality of calibrated radiances from atmospheric sounders and imaging radiometers, an activity is underway to compare routinely measured radiances with those simulated from operational global numerical weather prediction (NWP) fields. This paper describes the results obtained from the first three years of these comparisons. Data from the High-resolution Infrared Radiation Sounder, Spinning Enhanced Visible and Infrared Imager, Advanced Along-Track Scanning Radiometer, Advanced Microwave Sounding Unit, and Microwave Humidity Sounder radiometers, together with the Atmospheric Infrared Sounder, a spectrometer, and the Infrared Atmospheric Sounding Interferometer, an interferometer, were included in the analysis. Changes in mean biases and their standard deviations were used to investigate the temporal stability of the bias and radiometric noise of the instruments. A double difference technique can be employed to remove the effect of changes or deficiencies in the NWP model which can contribute to the biases. The variation of the biases with other variables is also investigated, such as scene temperature, scan angle, location, and time of day. Many of the instruments were shown to be stable in time, with a few exceptions, but measurements from the same instrument on different platforms are often biased with respect to each other. The limitations of the polar simultaneous nadir overpasses often used to monitor biases between polar-orbiting sensors are shown with these results due to the apparent strong dependence of some radiance biases on scene temperature.","['Instruments', 'Computational modeling', 'Clouds', 'Satellites', 'Ocean temperature', 'Atmospheric modeling', 'Monitoring']","['Calibration', 'numerical weather prediction (NWP) models', 'remote sensing', 'satellites']"
"Terrestrial laser scanning (TLS) data provide 3-D measurements of vegetation structure and have the potential to support the calibration and validation of satellite and airborne sensors. The increasing range of different commercial and scientific TLS instruments holds challenges for data and instrument interoperability. Using data from various TLS sources will be critical to upscale study areas or compare data. In this paper, we provide a general framework to compare the interoperability of TLS instruments. We compare three TLS instruments that are the same make and model, the RIEGL VZ-400. We compare the range accuracy and evaluate the manufacturer's radiometric calibration for the uncalibrated return intensities. Our results show that the range accuracy between instruments is comparable and within the manufacturer's specifications. This means that the spatial XYZ data of different instruments can be combined into a single data set. Our findings demonstrate that radiometric calibration is instrument specific and needs to be carried out for each instrument individually before including reflectance information in TLS analysis. We show that the residuals between the calibrated reflectance panels and the apparent reflectance measured by the instrument are greatest for highest reflectance panels (residuals ranging from 0.058 to 0.312).","['Instruments', 'Calibration', 'Radiometry', 'Measurement by laser beam', 'Interoperability', 'Earth', 'Australia']","['Data interoperability', 'radiometric calibration', 'RIEGL VZ-400', 'terrestrial light detection and ranging (LiDAR)']"
"Geomechanical logs are of ultimate importance for subsurface description and evaluation, as well as for the exploration of underground resources, such as oil and gas, groundwater, minerals, and geothermal energy. Together with geological and hydrological properties, low-cost and high-accuracy models can be generated based on geomechanical parameters. However, it is challenging to directly measure geomechanical parameters, and they are usually estimated based on other measured quantities. For example, geomechanical logs may be obtained with certain empirical models from sonic logs together with prior information such as rock types, which are not readily available. Finding a way to directly estimate geomechanical logs based on easily available conventional well logs can result in significant cost savings and increased efficiency. In this article, we showed that deep learning via the long short-term memory network (LSTM) is effective in constructing an end-to-end model that takes the spatial dependence in well logs into consideration. We further proposed a physics-constrained LSTM, in which the physical mechanism behind the geomechanical parameters is utilized as a priori information. This state-of-the-art model is capable to directly estimate geomechanical logs based on easily available data, and it achieves higher prediction accuracy since the domain knowledge of the problem is considered.","['Logic gates', 'Predictive models', 'Machine learning', 'Neural networks', 'Mathematical model', 'Computer architecture', 'Geology']","['Geomechanical parameters', 'long short-term memory network (LSTM)', 'physics-constrained', 'physics-informed', 'well logs']"
"Long-term trends in Arctic sea ice are of particular interest in studies of global temperature, climate change, and industrial application. This paper analyzes intra-annual and interannual trends in Ku-band backscatter over first-year (FY) and multiyear (MY) sea ice to develop a new sea-ice-type classification method. Histograms of backscatter are derived from high-resolution backscatter images created using the scatterometer image reconstruction (SIR) algorithm applied to measurements obtained by the SeaWinds instrument aboard QuikSCAT. The backscatter of FY and MY sea ice are clearly identifiable and are observed to vary seasonally. Using an average of the annual backscatter trends obtained from QuikSCAT, a classification of MY ice is obtained, which uses a time-dependent threshold value. Validation of the classification method is done using regional ice charts from the Canadian Ice Service. Differences in ice classification are found to be less than 6%for the winters of 2006–2007 and 2007–2008, and the end of 2008. Anomalies in the distribution of sea ice backscatter from year to year suggest a reduction in MY ice cover between 2003 and 2009 and an approximately equivalent increase in FY ice cover.","['Backscatter', 'Sea ice', 'Sea measurements', 'Arctic', 'Histograms', 'Snow', 'Climate change']","['Ice classification', 'microwave remote sensing', 'QuikSCAT', 'sea ice']"
"Near-space is recognized as the atmospheric region from 20 to 100 km above the Earth's surface. Near-space vehicles offer several advantages compared to low earth orbit satellites and airplanes because near-space vehicles are not constrained by orbital mechanics and fuel consumption. These advantages provide potential for future remote sensing applications, but little related work has been published. This paper explains what near-space is and how it should be exploited for remote sensing applications. A near-space vehicle-borne synthetic aperture radar (SAR) with reflector antenna and digital beamforming on receive is proposed for high-resolution and wide-swath (HRWS) remote sensing. The system configuration, signal model, imaging scheme, system performance, and nadir echo suppression are investigated. An example system is conceptually designed, along with its system performance analysis. It is shown that the near-space vehicle-borne SAR with reflector antenna can operate with high flexibility and reconfigurability, thus enabling a satisfactory HRWS remote sensing performance.","['Azimuth', 'Remote sensing', 'Image resolution', 'Imaging', 'Vehicles', 'Antennas', 'Doppler effect']","['Digital beamforming(DBF)', 'high resolution and wide swath (HRWS)', 'near-space', 'near-space vehicle', 'reflector antenna', 'scan-on-receive (SCORE)', 'synthetic aperture radar (SAR)']"
"In recent years, spaceborne synthetic aperture radar (SAR) polarimetry has become a valuable tool for sea ice analysis. Here, we employ an automatic sea ice classification algorithm on two sets of spatially and temporally near coincident fully polarimetric acquisitions from the ALOS-2, Radarsat-2, and TerraSAR-X/TanDEM-X satellites. Overlapping coincident sea ice freeboard measurements from airborne laser scanner data are used to validate the classification results. The automated sea ice classification algorithm consists of two steps. In the first step, we perform a polarimetric feature extraction procedure. Next, the resulting feature vectors are ingested into a trained neural network classifier to arrive at a pixelwise supervised classification. Coherency matrix-based features that require an eigendecomposition are found to be either of low relevance or redundant to other covariance matrix-based features, which makes coherency matrix-based features dispensable for the purpose of sea ice classification. Among the most useful features for classification are matrix invariant-based features (geometric intensity, scattering diversity, and surface scattering fraction). Classification results show that 100% of the open water is separated from the surrounding sea ice and that the sea ice classes have at least 96.9% accuracy. This analysis reveals analogous results for both X-band and C-band frequencies and slightly different for the L-band. The subsequent classification produces similarly promising results for all four acquisitions. In particular, the overlapping image portions exhibit a reasonable congruence of detected sea ice when compared with high-resolution airborne measurements.","['Sea ice', 'Synthetic aperture radar', 'Temperature measurement', 'Sea measurements', 'Neural networks', 'Arctic']","['Airborne laser scanner (ALS)', 'artificial neural network (ANN)', 'multifrequency synthetic aperture radar (SAR)', 'near real time (NRT) processing', 'polarimetry', 'sea ice']"
"Deep learning (DL) in remote sensing has nowadays become an effective operative tool: it is largely used in applications, such as change detection, image restoration, segmentation, detection, and classification. With reference to the synthetic aperture radar (SAR) domain, the application of DL techniques is not straightforward due to the nontrivial interpretation of SAR images, especially caused by the presence of speckle. Several DL solutions for SAR despeckling have been proposed in the last few years. Most of these solutions focus on the definition of different network architectures with similar cost functions, not involving SAR image properties. In this article, a convolutional neural network (CNN) with a multi-objective cost function taking care of spatial and statistical properties of the SAR image is proposed. This is achieved by the definition of a peculiar loss function obtained by the weighted combination of three different terms. Each of these terms is dedicated mainly to one of the following SAR image characteristics: spatial details, speckle statistical properties, and strong scatterers identification. Their combination allows balancing these effects. Moreover, a specifically designed architecture is proposed to effectively extract distinctive features within the considered framework. Experiments on simulated and real SAR images show the accuracy of the proposed method compared with the state-of-art despeckling algorithms, both from a quantitative and qualitative point of view. The importance of considering such SAR properties in the cost function is crucial for correct noise rejection and details preservation in different underlined scenarios, such as homogeneous, heterogeneous, and extremely heterogeneous.","['Speckle', 'Radar polarimetry', 'Synthetic aperture radar', 'Cost function', 'Backscatter', 'Training', 'Noise measurement']","['Convolutional neural network (CNN)', 'deep learning (DL)', 'despeckling', 'image restoration', 'statistical distribution', 'synthetic aperture radar (SAR)']"
"This paper examines the capability assessment of fully polarimetric L-band data for the snow and nonsnow-area classifications. The data sets used are the fully polarimetric Advanced Land Observation Satellite–Phased Array-Type L-Band Synthetic Aperture Radar data, optical Advanced Land Observing Satellite (ALOS)-advanced visible and near-infrared radiometer-2 data close to the radar acquisition, and environmental satellite–advanced synthetic aperture radar data. Several parameters are used to discriminate the snow-covered areas from nonsnow-covered areas in the Indian Himalayan region, including backscattering coefficients, the ratio of cross/copolarized backscattering power and polarization fraction (PF) value. Supervised classification schemes are employed using polarimetric decomposition methods based on the complex Wishart classifier. The accuracy of the classification was found to be 97.95% for the Wishart-supervised classification. Among various parameters and methods, it was found that the alternative newly proposed PF scheme, based on the implementation of fully polarimetric synthetic aperture radar data, yielded the best classification result in the absence of the training samples. The PF value has been effective for discrimination of the snow-covered areas from nonsnow-covered areas, debris-covered glacier, and vegetation. The results of this investigation show that L-band fully polarimetric SAR data provide considerable improvement but may not possess the optimal capability to discriminate snow from other inherent natural and man-made scatterers in heavy snow-laden mountainous scenarios, which may require fully polarimetric S-band or C-band PolSAR measurements.",[],[]
"An unmanned aerial vehicle was used as a mobile sensor platform to collect sea-ice features at Ny-Ålesund in early May 2011, and several image processing algorithms have been applied to samples of sea-ice images to extract useful information about sea ice. The sea-ice statistics given by the floe size distribution, being an important parameter for climate and wave- and structure-ice analysis, is challenging to calculate due to difficulties in ice floe identification, particularly the separation of seemingly connected ice floes. In this paper, the gradient vector flow (GVF) snake algorithm is applied to solve this problem. To evolve the GVF snake algorithm automatically, an initialization based on the distance transform is proposed to detect individual ice floes, and the morphological cleaning is afterward applied to smoothen the shape of each identified ice floe. Based on the identification result, the image is separated into four different layers: ice floes, brash pieces, slush, and water. This makes it further possible to present a color map of the ice floes and brash pieces based on sizes, and the corresponding ice floe size distribution histogram. The proposed algorithm yields an acceptable identification result, and its effectiveness is demonstrated in a case study. A discussion on the methods and results concludes the paper.","['Sea ice', 'Image segmentation', 'Shape', 'Arctic', 'Algorithm design and analysis']","['Floe size distribution', 'ice floe identification', 'image processing', 'marginal ice zone', 'remote sensing']"
"This paper analyzes the spatial decorrelation between repeat-pass bistatic synthetic aperture radar (BSAR) images with Global Navigation Satellite Systems as transmitters and a fixed receiver. This study is needed in the development of such a system to monitor temporal changes in a scene. The main challenge is that, in this bistatic configuration, spatial coherence heavily depends on the data acquisition geometry. The appropriate theoretical framework to describe spatial coherence for this case is developed by extending well-established monostatic models and, in principle, can be applied to any fixed-receiver BSAR with a spaceborne transmitter. Theoretical results are initially supported by Monte Carlo simulations. Finally, the validity of the model is confirmed by comparing real images.","['Decorrelation', 'Satellites', 'Azimuth', 'Synthetic aperture radar', 'Receivers', 'Geometry', 'Spatial resolution']","['Bistatic synthetic aperture radar (BSAR)', 'coherent change detection (CCD)', 'GNSS-based SAR', 'spatial decorrelation']"
"The Clouds and Earth's Radiant Energy System (CERES) has been monitoring clouds and radiation since 2000 using algorithms developed before 2002 for CERES Edition 2 (Ed2) products. To improve cloud amount accuracy, CERES Edition 4 (Ed4) applies revised algorithms and input data to Terra and Aqua MODerate-resolution Imaging Spectroradiometer (MODIS) radiances. The Ed4 cloud mask uses 5-7 additional channels, new models for clear-sky ocean and snow/ice-surface radiances, and revised Terra MODIS calibrations. Mean Ed4 daytime and nighttime cloud amounts exceed their Ed2 counterparts by 0.035 and 0.068. Excellent consistency between average Aqua and Terra cloud fraction is found over nonpolar regions. Differences over polar regions are likely due to unresolved calibration discrepancies. Relative to Ed2, Ed4 cloud amounts agree better with those from the Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observations (CALIPSO). CALIPSO comparisons indicate that Ed4 cloud amounts are more than or as accurate as other available cloud mask systems. The Ed4 mask correctly identifies cloudy or clear areas 90%-96% of the time during daytime over nonpolar areas depending on the CALIPSO-MODIS averaging criteria. At night, the range is 88%-95%. Accuracy decreases over land. The polar day and night accuracy ranges are 90%-91% and 80%-81%, respectively. The mean Ed4 cloud fractions slightly exceed the average for seven other imager cloud masks. Remaining biases and uncertainties are mainly attributed to errors in Ed4 predicted clear-sky radiances. The resulting cloud fractions should help CERES produce a more accurate radiation budget and serve as part of a cloud property climate data record.","['Clouds', 'MODIS', 'Cloud computing', 'Calibration', 'Meteorology', 'Satellites', 'Broadband communication']","['Climate', 'cloud', 'Clouds and the Earth’s Radiant Energy System (CERES)', 'cloud mask', 'cloud remote sensing', 'MODerate-resolution Imaging Spectroradiometer (MODIS)']"
"Tomographic synthetic aperture radar (TomoSAR) inversion of urban areas is an inherently sparse reconstruction problem and, hence, can be solved using compressive sensing (CS) algorithms. This paper proposes solutions for two notorious problems in this field. First, TomoSAR requires a high number of data sets, which makes the technique expensive. However, it can be shown that the number of acquisitions and the signal-to-noise ratio (SNR) can be traded off against each other, because it is asymptotically only the product of the number of acquisitions and SNR that determines the reconstruction quality. We propose to increase SNR by integrating nonlocal (NL) estimation into the inversion and show that a reasonable reconstruction of buildings from only seven interferograms is feasible. Second, CS-based inversion is computationally expensive and therefore, barely suitable for large-scale applications. We introduce a new fast and accurate algorithm for solving the NL L1-L2-minimization problem, central to CS-based reconstruction algorithms. The applicability of the algorithm is demonstrated using simulated data and TerraSAR-X high-resolution spotlight images over an area in Munich, Germany.","['Signal to noise ratio', 'Image reconstruction', 'Synthetic aperture radar', 'Estimation', 'Tomography', 'Signal processing algorithms', 'Compressed sensing']","['Compressive sensing (CS)', 'interferometric synthetic aperture radar (InSAR)', 'nonlocal (NL) filtering', 'tomographic SAR (TomoSAR)']"
"High-quality and large-scale image composites are increasingly important for a variety of applications. Yet a number of challenges still exist in the generation of composites with certain desirable qualities such as maintaining the spectral relationship between bands, reduced spatial noise, and consistency across scene boundaries so that large mosaics can be generated. We present a new method for generating pixel-based composite mosaics that achieves these goals. The method, based on a high-dimensional statistic called the `geometric median,' effectively trades a temporal stack of poor quality observations for a single high-quality pixel composite with reduced spatial noise. The method requires no parameters or expert-defined rules. We quantitatively assess its strengths by benchmarking it against two other pixel-based compositing approaches over Tasmania, which is one of the most challenging locations in Australia for obtaining cloud-free imagery.","['Earth', 'Satellites', 'Time series analysis', 'Remote sensing', 'Australia', 'Electric breakdown', 'Clouds']","['Big data applications', 'image analysis', 'remote sensing', 'time series analysis']"
"In order to analyze synthetic aperture radar (SAR) images of the sea surface, ship wake detection is essential for extracting information on the wake generating vessels. One possibility is to assume a linear model for wakes, in which case detection approaches are based on transforms such as Radon and Hough. These express the bright (dark) lines as peak (trough) points in the transform domain. In this article, ship wake detection is posed as an inverse problem, with the associated cost function including a sparsity enforcing penalty, i.e., the generalized minimax concave (GMC) function. Despite being a nonconvex regularizer, the GMC penalty enforces the overall cost function to be convex. The proposed solution is based on a Bayesian formulation, whereby the point estimates are recovered using a maximum a posteriori (MAP) estimation. To quantify the performance of the proposed method, various types of SAR images are used, corresponding to TerraSAR-X, COSMO-SkyMed, Sentinel-1, and Advanced Land Observing Satellite 2 (ALOS2). The performance of various priors in solving the proposed inverse problem is first studied by investigating the GMC along with the L 1 , L p , nuclear, and total variation (TV) norms. We show that the GMC achieves the best results and we subsequently study the merits of the corresponding method in comparison to two state-of-the-art approaches for ship wake detection. The results show that our proposed technique offers the best performance by achieving 80% success rate.","['Marine vehicles', 'Radon', 'Radar polarimetry', 'Transforms', 'Sea surface', 'Kelvin', 'Inverse problems']","['Generalized minimax concave (GMC) regularization', 'inverse problem', 'maximum a posteriori (MAP) estimation', 'ship wake detection', 'synthetic aperture radar (SAR) imagery']"
"One of the main limitations in measuring ground deformation using synthetic aperture radar interferometry (InSAR) is atmospheric phase delay effects. In volcanic regions, the atmospheric phase delay effects can cause serious problems in detecting volcanic unrest because atmospheric thickness is inversely related with the elevation of a volcanic mountain. It is commonly known that the atmospheric phase screen (APS) can be decomposed spatially into stratified and turbulent components. In this paper, the stratified and turbulent atmospheric conditions of a volcanic area were simulated using weather research and forecasting (WRF) model, and the simulated atmospheric conditions were compared with in situ radiosonde data. The comparison results proved that the stratified APS from the WRF model could reflect the reasonable patterns of seasonal changes and vertical profiles with dependable quality. We also found that the stratified APS was significantly correlated with time and sometimes severely contaminated the quality of volcanic deformation estimation. These results indicate that the temporal high-pass (HP) filtering, which has been usually applied in time series InSAR analysis for extracting and removing APS, cannot work properly in volcanic area. Thus, we propose a new method that employs the stratified APS obtained from the WRF model by correlating with topography and the (residual) turbulent APS that can be effectively eliminated by temporal HP filtering in persistent scatterer interferometry (PSI). We applied the proposed method (atmosphere-corrected PSI) to Advanced Land Observing Satellite Phased Array type L-band SAR data that cover Shinmoedake volcano, Japan, and found that the estimated surface deformation and APS agreed well with those measured from GPS and Moderate-Resolution Imaging Spectroradiometer data, respectively.","['Atmospheric modeling', 'Delays', 'Atmospheric measurements', 'Strain', 'Synthetic aperture radar', 'Data models', 'Predictive models']","['Atmospheric phase screen (APS)', 'stratified APS', 'time series interferometric synthetic aperture radar (InSAR)', 'volcano', 'weather research and forecasting (WRF)']"
"In past research, two-pass repeat-geometry synthetic aperture radar (SAR) coherent change detection (CCD) predominantly utilized the sample degree of coherence as a measure of the temporal change occurring between two complex-valued image collects. Previous coherence-based CCD approaches tend to show temporal change when there is none in areas of the image that have a low clutter-to-noise power ratio. Instead of employing the sample coherence magnitude as a change metric, in this paper, we derive a new maximum-likelihood (ML) temporal change estimate-the complex reflectance change detection (CRCD) metric to be used for SAR coherent temporal change detection. The new CRCD estimator is a surprisingly simple expression, easy to implement, and optimal in the ML sense. This new estimate produces improved results in the coherent pair collects that we have tested.","['Synthetic aperture radar', 'Maximum likelihood estimation', 'Coherence', 'Charge coupled devices', 'Maximum likelihood detection', 'Clutter', 'Measurement']","['Coherent change detection', 'maximum likelihood estimator', 'radar interferometry', 'synthetic aperture radar']"
"The Kilpisjärvi Atmospheric Imaging Receiver Array (KAIRA) is a dual array of omnidirectional VHF radio antennas located near Kilpisjärvi, Finland. It is operated by the Sodankylä Geophysical Observatory. It makes extensive use of the proven LOFAR antenna and digital signal-processing hardware, and can act as a stand-alone passive receiver, as a receiver for the European Incoherent Scatter (EISCAT) very high frequency (VHF) incoherent scatter radar in Tromsø, or for use in conjunction with other Fenno-Scandinavian VHF experiments. In addition to being a powerful observing instrument in its own right, KAIRA will act as a pathfinder for technologies to be used in the planned EISCAT_3-D phased-array incoherent scatter radar system and participate in very long baseline interferometry experiments. This paper gives an overview of KAIRA, its principal hardware and software components, and its main science objectives. We demonstrate the applicability of the radio astronomy technology to our geoscience application. Furthermore, we present a selection of results from the commissioning phase of this new radio observatory.","['Arrays', 'Radar antennas', 'Antenna arrays', 'Radar scattering', 'Radar imaging']","['Antenna arrays', 'phased arrays', 'radar antennas', 'radio astronomy', 'receiving antennas']"
"Spatial–temporal variability and changes of Greenland ice sheet elevation from 1992 to 2008 are analyzed from merged ERS-1, ERS-2, and Envisat satellite radar altimeter data. A methodology for determining intersatellite biases was developed and applied in order to merge measurements from these different satellites and to create continuous and consistent time series. Intersatellite biases of elevation and backscatter coefficient have shown to be significantly affected by the bias between measurements in ascending and descending orbits. Adjustment of elevation time series for its dependence on backscatter coefficient and other waveform parameters performed in this paper substantially reduced the amplitude of elevation seasonal variations and locally corrected elevation change-rate estimates by up to several centimeters per year. It was found that the correction depends not only on the variations in the waveform parameters but also on the temporal variations of the correlation gradients, which represent the sensitivity of the elevation change to the change in the waveform parameters. An elevation change rate of+2.8±0.2 cm/yearfrom 1992 to 2008 over 76% of the Greenland ice sheet area was found. Increases in surface elevation from 1995 observed over the high-elevation regions of Greenland were followed by an elevation decrease from 2006. For the whole period of 1992–2008, the elevation increase is 4.0±0.2 cm/year over 87% of the area above 1500 m. In contrast, over 38% of the low-elevation areas below 1500 m, the rate of elevation change is−7.0±1.0 cm/year, and the surface elevation decrease that started from 2000 has continued.",[],[]
"The MicroWave Radiation Imager (MWRI) onboard the FengYun (FY)-3B satellite has five frequencies at 10.65, 18.7, 23.8, 36.5, and 89.0 GHz, each having dual channels at vertical and horizontal polarization states, respectively. It is found that radio-frequency interference (RFI) is present in MWRI data over land. The RFI signals are, in general, detectable from a spectral difference method and a principal component analysis (PCA) method. In particular, the PCA method is applied to derive RFI signals from natural radiations by using the characteristics of natural radiation measurements having all-channel correlations. In the area where data have a higher projection onto the first principle component (PC) mode, RFI is, in general, present. However, both the spectral and PCA methods cannot detect RFI reliably over frozen grounds and scattering surfaces, where the brightness temperature difference between 10.65 and 18.7 GHz is large. Thus, detection is improved through the use of normalized PCA. The new RFI detection algorithm is now working reliably for MWRI applications. It is found that RFI at 10.65 GHz distributes widely over Europe and Japan, and is less popular over the United States and China.","['Brightness temperature', 'Principal component analysis', 'Satellites', 'Microwave radiometry', 'Radiofrequency interference', 'Calibration', 'Snow']","['Microwave remote sensing', 'MicroWave Radiation Imager (MWRI)', 'radio-frequency interference (RFI)']"
"A low-cost multicamera Unmanned Aircraft System (UAS) is used to simultaneously estimate open-coast topography and bathymetry from a single longitudinal coastal flight. The UAS combines nadir and oblique imagery to create a wide field of view (FOV), which enables collection of mobile, long dwell timeseries of the littoral zone suitable for structure-from-motion (SfM), and wave speed inversion algorithms. Resultant digital surface models (DSMs) compare well with terrestrial topographic lidar and bathymetric survey data at Duck, NC, USA, with roor-mean-square error (RMSE)/bias of 0.26/-0.05 and 0.34/-0.05 m, respectively. Bathymetric data from another flight at Virginia Beach, VA, USA, demonstrates successful comparison (RMSE/bias of 0.17/0.06 m) in a secondary environment. UAS-derived engineering data products, total volume profiles and shoreline position, were congruent with those calculated from traditional topo-bathymetric surveys at Duck. Capturing both topography and bathymetry within a single flight, the presented multicamera system is more efficient than data acquisition with a single camera UAS; this advantage grows for longer stretches of coastline (10 km). Efficiency increases further with an on-board Global Navigation Satellite System-Inertial Navigation System (GNSS-INS) to eliminate ground control point (GCP) placement. The Appendix reprocesses the Virginia Beach flight with the GNSS-INS input and no GCPs. The resultant DSM products are comparable [root-mean-squared difference (RMSD)/bias of 0.62/-0.09 m, and processing time is significantly reduced.","['Cameras', 'Sea measurements', 'Surfaces', 'Laser radar', 'Global Positioning System', 'Payloads', 'Aircraft']","['Coastal mapping', 'multiview stereo (MVS)', 'nearshore morphology', 'remote sensing', 'structure from motion (SfM)', 'Unmanned Aircraft Systems (UAS)']"
"Bright curvilinear features arising from the geometry of man-made structures are characteristic of synthetic aperture radar (SAR) images of urban areas, particularly due to double-reflection mechanisms. An approach to urban earthquake damage detection using double-reflection line amplitude change in single-look images has been established in previous literature. Based on this method, this paper introduces an automated tool for fast, unsupervised damage detection in urban areas. Ridge-based curvilinear features are extracted from a preevent SAR image, and double-reflection candidates are selected using prior probability distributions derived from a simple geometrical building model. The candidate features are then used with the ratio of a pair of single preevent and postevent SAR single-look amplitude images to estimate damage levels. The algorithm is very efficient, with overall computational complexity of O(Nlogk) for an N-pixel image containing features of mean length k. The technique is demonstrated using COSMO-SkyMed data covering L'Aquila, Italy, and Port-au-Prince, Haiti.","['Feature extraction', 'Buildings', 'Synthetic aperture radar', 'Earthquakes', 'Urban areas', 'Image segmentation', 'Estimation']","['Earthquake damage detection', 'feature extraction', 'multitemporal synthetic aperture radar (SAR)', 'urban areas']"
"Ocean water albedo (OWA) plays an important role in the global climate variation. Compared with the achievements in land surface albedo studies, the global distributions of ocean water and sea ice albedo are seldom addressed. This study designed an operational global OWA algorithm based on the three-component reflectance model of the ocean water: sun glint, whitecaps, and water-leaving reflectance. The related achievements in these three areas are reviewed and integrated into the operational algorithm. After the sensitive analysis, the algorithm is compared with previous studies and validated with ground observations at COVE site located 25 km east of Virginia Beach (36.91° N, 75.71° W), and the results indicate that the proposed algorithm is generally consistent with previous parameterization scheme. As an example, the global OWAs in summer and winter 2011 are generated using the remote sensing reflectance data sets via the Moderate Resolution Imaging Spectroradiometer and Modern-Era Retrospective analysis for Research and Applications meteorological reanalysis data set. The generated product includes instantaneous (e.g., local noon) and daily mean OWAs under both clear-sky and white-sky conditions. Upon the examples, the local noon clear-sky OWA shows a significant latitude variation due to the dominance of the solar angle, whereas the white-sky OWA is sensitive to wind speeds and optical constituents. The global distribution of the daily mean OWA exhibits a similar trend to the local noon OWA. However, the daily mean clear-sky OWA is significantly larger than the local noon OWA; this finding should be noted when using OWA products for energy balance research. Additionally, all forms of OWA products exhibit increase in coastal areas with high input of terrestrial matters.","['Open wireless architecture', 'Sea surface', 'Wind speed', 'Remote sensing', 'Sea measurements', 'Biological system modeling']","['Ocean water albedo (OWA)', 'sun glint', 'water-leaving reflectance', 'whitecaps']"
"Hidden Markov models (HMMs) have previously been successfully applied to subsurface threat detection using ground penetrating radar (GPR) data. However, parameter estimation in most HMM-based landmine detection approaches is difficult since object locations are typically well known for the 2-D coordinates on the Earth's surface but are not well known for object depths underneath the ground/time of arrival in a GPR A-scan. As a result, in a standard expectation maximization HMM (EM-HMM), all depths corresponding to a particular alarm location may be labeled as target sequences although the characteristics of data from different depths are substantially different. In this paper, an alternate HMM approach is developed using a multiple-instance learning (MIL) framework that considers an unordered set of HMM sequences at a particular alarm location, where the set of sequences is defined as positive if at least one of the sequences is a target sequence; otherwise, the set is defined as negative. Using the MIL framework, a collection of these sets (bags), along with their labels is used to train the target and nontarget HMMs simultaneously. The model parameters are inferred using variational Bayes, making the model tractable and computationally efficient. Experimental results on two synthetic and two landmine data sets show that the proposed approach performs better than a standard EM-HMM.","['Hidden Markov models', 'Landmine detection', 'Ground penetrating radar', 'Data models', 'Standards', 'Manganese', 'Computational modeling']","['Ground penetrating radar (GPR)', 'hidden Markov model (HMM)', 'landmine detection', 'multiple-instance learning (MIL)', 'variational Bayes (VB)']"
"The uncertainty of differential code bias (DCB) is one of the main error sources in the low Earth orbit (LEO) based total electron content (TEC) retrieval, whereas the derivation of the LEO DCB is not systematically studied. In this paper, we propose an improved DCB estimation method (ZERO method) based on the assumption that the LEO-based TEC can reach zero and also optimize the parameter configuration in the commonly used least square method (LSQ method). In the improved ZERO method, the combination of the lower quartile minimum relative TEC during each orbital revolution with the daily minimum relative TEC gives a stable and reliable DCB estimation. For the LSQ method, the 3-TECU cutoff vertical TEC with 10° cutoff elevation is considered to offer a reasonable DCB estimation. Subsequently, Global Positioning System (GPS) observations from multiple LEO satellites at different altitudes are used to study the variability of the LEO DCBs. Our results revealed that the LEO DCBs underwent obvious long-term variation and periodic oscillations of months. Moreover, the CHAMP data illustrated that the long-term variation of LEO DCBs is partly associated with the GPS satellite replacement, and the periodic variation can be attributed to the variation of the hardware thermal status, represented by the receiver CPU temperature in this study.","['Low earth orbit satellites', 'Receivers', 'Global Positioning System', 'Estimation', 'Orbits', 'Satellite broadcasting']","['Differential code bias (DCB)', 'Global Navigation Satellite System (GNSS)', 'low Earth orbit (LEO) satellite', 'total electron content (TEC)']"
"The Remote Sensing Technology Institute (Institut fur Methodik der Fernerkundung) of the German Aerospace Agency (DLR) operates two sensors for airborne hyperspectral imaging, i.e., a Norsk Elektro Optikk A/S (NEO) HySpex VNIR-1600 and a NEO HySpex SWIR-320m-e. Since these sensors are used for the development of physically based inversion algorithms, atmospheric correction algorithms and for calibration/ validation activities, their properties need to be characterized in detail, and an accurate calibration is mandatory. The characterization is performed at the calibration laboratory of DLR for imaging spectrometers in Oberpfaffenhofen. Key results of the characterization are assessments of the radiometric, spectral, and geometric performances, including the typical optical distortions prevalent in pushbroom imaging spectrometers, keystone and smile, and the associated measurement uncertainties. Potential sources of systematic error, the detector nonlinearity and the polarization sensitivity are discussed. The radiometric calibration is traceably performed to the German national metrology institute Physikalisch-Technische Bundesanstalt, whereas the spectral measurements can be traced back to the spectral properties of atomic line lamps. The implemented level 0 to level 1 calibration procedure is presented as well.","['Detectors', 'Calibration', 'Radiometry', 'Arrays', 'Noise', 'Imaging']","['Calibration', 'characterization', 'hyperspectral', 'HySpex']"
"Azimuth ambiguity may introduce false targets into synthetic aperture radar images, particularly likely in inshore and oceanic observation. To suppress the azimuth ambiguities for any acquisition mode, a new model is developed to describe the impact of spatially variant azimuth antenna pattern weighting on azimuth ambiguities. By accurately estimating the ratio of ambiguous to main zone energy based on the model, the proposed algorithm selects the subspectra with less ambiguous disturbance, and adopts extrapolation with weighted energy measure to obtain a full spectrum. Due to spectral selection and extrapolation, the novel algorithm achieves superior performance in azimuth ambiguity suppression and resolution preservation, which is compared with the classical algorithm, and validated by applying TerraSAR-X and RADARSAT-2 images.","['Azimuth', 'Doppler effect', 'Synthetic aperture radar', 'Bandwidth', 'Spaceborne radar', 'Antennas', 'Extrapolation']","['Azimuth ambiguity suppression', 'signal processing', 'synthetic aperture radar (SAR)']"
"The full deramp pulse compression scheme employed by satellite radar altimeters digitizes each radar echo at a sampling rate matched to the chirp bandwidth. Echo power is undersampled by a factor of 2 when the power samples are simply obtained by squaring the magnitude of the echo samples, without first resampling, as is done in all altimeters to date. This results in inadequate sampling of the leading edge of the waveform if the significant wave height (SWH) is low. For a typical Ku-band altimeter with a chirp bandwidth of 320 MHz, simple squaring should be inadequate over ocean surfaces with SWH less than 2 m, that is, half of the ocean. Simply zero-padding the digital samples prior to range Fourier transform alleviates the problem introduced by magnitude squaring. Data from the CryoSat altimeter are used to demonstrate this remedy, and it is found that this reduces the variance in estimated range by 10% and in SWH by 20%. These improvements are confined to a range of SWH values between 1 and 4 m. Zero-padding also seems to have some small impact on values estimated over very flat surfaces (SWH ≪ 1 m), although theory suggests that better resolution of such surfaces would require a bandwidth exceeding 320 MHz. The 500-MHz bandwidth of the Ka-band altimeter on the Satellite with ARGOS and AltiKa mission should encounter these difficulties at smaller SWH values. Onboard range tracking and automatic gain control loops in future altimeters might be improved if zero-padding were employed during onboard waveform processing.","['Synthetic aperture radar', 'Instruments', 'Radar tracking', 'Spaceborne radar', 'Sea surface', 'Chirp']","['Geophysical sea measurements', 'ocean surface', 'pulse compression methods', 'pulse measurements', 'pulse modulation', 'radar altimetry', 'radar remote sensing', 'signal sampling', 'spaceborne radar', 'synthetic aperture radar (SAR)']"
"Airborne co-polarization and cross-polarization observations of ocean surface normalized radar cross section (NRCS) were conducted over the North Atlantic during January and February 2015. Observations were made using the University of Massachusetts' Imaging Wind and Rain Airborne Profiler (IWRAP) radar system and a prototype antenna for the next-generation European scatterometer aboard MetOp-SG. Both were installed on a National Oceanic and Atmospheric Administration (NOAA) WP-3D research aircraft to characterize the wind response of the ocean-surface cross-polarization NRCS. During the flights, numerous constant-roll-angle circle maneuvers were performed at several different angles to collect NRCS measurements over a range of incidence angles. Surface winds at speeds between 8 and 34 ms -1 were observed at incidence angles from 20° to 60° at all polarization combinations. The majority of measurements fell between 8 and 20 ms -1 . Wind-direction dependence similar to copolarized NRCS was observed in the cross-polarized (VH) NRCS. The amplitude of the VH NRCS with respect to direction is less than that of copolarized NRCS at all wind speeds. Incidence angle dependence was also observed in the VH NRCS at all wind speeds. As a function of wind speed, the mean VH NRCS (A 0 ) has a similar shape to the VV NRCS. The VH NRCS appears to not saturate at most incidence angles, unlike the VV and HH NRCS. VH and HH geophysical model functions (GMFs) were developed as functions of wind speed, incidence angle, and wind-relative azimuth for the wind speeds and incidence angles observed.","['Orbits', 'Wind speed', 'Radar', 'Sea surface', 'Aircraft', 'Sea measurements']","['Airborne radar', 'C-band', 'cross-polarization', 'ocean vector winds', 'radar cross section', 'scatterometry']"
"Most change detection (CD) methods assume that prechange and postchange images are acquired by the same sensor. However, in many real-life scenarios, e.g., natural disasters, it is more practical to use the latest available images before and after the occurrence of incidence, which may be acquired using different sensors. In particular, we are interested in the combination of the images acquired by optical and synthetic aperture radar (SAR) sensors. SAR images appear vastly different from the optical images even when capturing the same scene. Adding to this, CD methods are often constrained to use only target image-pair, no labeled data, and no additional unlabeled data. Such constraints limit the scope of traditional supervised machine learning and unsupervised generative approaches for multisensor CD. The recent rapid development of self-supervised learning methods has shown that some of them can even work with only few images. Motivated by this, in this work, we propose a method for multisensor CD using only the unlabeled target bitemporal images that are used for training a network in a self-supervised fashion by using deep clustering and contrastive learning. The proposed method is evaluated on four multimodal bitemporal scenes showing change, and the benefits of our self-supervised approach are demonstrated. Code is available at https://gitlab.lrz.de/ai4eo/cd/-/tree/main/sarOpticalMultisensorTgrs2021 .","['Optical sensors', 'Optical imaging', 'Training', 'Earth', 'Synthetic aperture radar', 'Deep learning', 'Spatial resolution']","['Change detection (CD)', 'deep learning', 'multisensor analysis', 'self-supervised learning']"
"We propose a new deterministic approach for remote sensing retrieval, called modified total least squares (MTLS), built upon the total least squares (TLS) technique. MTLS implicitly determines the optimal regularization strength to be applied to the normal equation first-order Newtonian retrieval using all of the noise terms embedded in the residual vector. The TLS technique does not include any constraint to prevent noise enhancement in the state space parameters from the existing noise in measurement space for an inversion with an ill-conditioned Jacobian. To stabilize the noise propagation into parameter space, we introduce an additional empirically derived regularization proportional to the logarithm of the condition number of the Jacobian and inversely proportional to the L2-norm of the residual vector. The derivation, operational advantages and use of the MTLS method are demonstrated by retrieving sea surface temperature from GOES-13 satellite measurements. An analytic equation is derived for the total retrieval error, and is shown to agree well with the observed error. This can also serve as a quality indicator for pixel-level retrievals. We also introduce additional tests from the MTLS solutions to identify contaminated pixels due to residual clouds, error in the water vapor profile and aerosols. Comparison of the performances of our new and other methods, namely, optimal estimation and regression-based retrieval, is performed to understand the relative prospects and problems associated with these methods. This was done using operational match-ups for 42 months of data, and demonstrates a relatively superior temporally consistent performance of the MTLS technique.","['Satellites', 'Jacobian matrices', 'Inverse problems', 'Sea measurements', 'Remote sensing', 'Ocean temperature', 'Measurement uncertainty']","['Condition number of matrix', 'ill-conditioned inverse methods', 'regularization', 'satellite remote sensing', 'sea surface temperature (SST)', 'total error', 'total least squares (TLS)']"
"Processing large very high-resolution remote sensing images on resource-constrained devices is a challenging task because of the large size of these data sets. For applications such as environmental monitoring or natural resources management, complex algorithms have to be used to extract information from the images. The memory required to store the images and the data structures of such algorithms may be very high (hundreds of gigabytes) and therefore leads to unfeasibility on commonly available computers. Segmentation algorithms constitute an essential step for the extraction of objects of interest in a scene and will be the topic of the investigation in this paper. The objective of the present work is to adapt image segmentation algorithms for large amounts of data. To overcome the memory issue, large images are usually divided into smaller image tiles, which are processed independently. Region-merging algorithms do not cope well with image tiling since artifacts are present on the tile edges in the final result due to the incoherencies of the regions across the tiles. In this paper, we propose a scalable tile-based framework for region-merging algorithms to segment large images, while ensuring identical results, with respect to processing the whole image at once. We introduce the original concept of the stability margin for a tile. It allows ensuring identical results to those obtained if the whole image had been segmented without tiling. Finally, we discuss the benefits of this framework and demonstrate the scalability of this approach by applying it to real large images.","['Image segmentation', 'Merging', 'Image edge detection', 'Stability criteria', 'Partitioning algorithms', 'Measurement']","['Image processing', 'image segmentation', 'image tiling', 'region merging', 'scalability']"
"Global Navigation Satellite System (GNSS) and satellite synthetic aperture radar (SAR) interferometry represent the most important space geodetic techniques usually exploited to measure millimetric ground deformation on earth surface at both local and wide-area scale. SAR images processed with persistent scatterers interferometry (PSI) multitemporal approach lack of an absolute reference datum. In this paper, SAR images are calibrated with data derived from permanent GNSS stations, in order to obtain absolute and more accurate displacement values. The method used to correct PSI with GNSS is based on existing methodologies commonly applied in the geodetic practice of combining crustal and local deformation studies with geospatial statistical analysis. The spatial distribution of vertical terrain deformations and their temporal changes are coherently measured, leading to a fine-scale surface velocity map in the central-eastern Po Plain, Northern Apennines, and Southern Alps in Italy. The results reveal significant subsidence rates on the north-western Adriatic coast including the Po Delta and the lagoon of Venice, as well as on Bologna and Ferrara cities in agreement with long-term displacement motion values provided by geological data and other previous works performed at local scale. This paper demonstrates the importance and effectiveness in creating a single, unique surface motion map by merging different data sets in which geodesy plays a relevant role in the datum alignment of PSI products before the stacking of the SAR maps.","['Global navigation satellite system', 'Satellites', 'Synthetic aperture radar', 'Strain', 'Spaceborne radar', 'Benchmark testing']","['Calibration', 'Global Navigation Satellite System (GNSS)', 'integration', 'persistent scatterer interferometry (PSI)', 'synthetic aperture radar (SAR)']"
"This paper proposes a different thermal channel combination split-window (DTCC-SW) method to estimate the land surface temperature (LST) and sea ST (SST) from the Chinese Gaofen-5 (GF-5) satellite thermal infrared (TIR) data. A nonlinear combination of two adjacent channels CH 8.20 (centered at 8.20 μm) and CH 8.63 (centered at 8.63 μm) was proposed to estimate LST for low-emissivity surfaces. A nonlinear combination of two adjacent channels, CH 10.80 (centered at 10.80 μm) and CH 11.95 (centered at 11.92 μm), was developed to estimate LST and SST for high-emissivity surfaces under dry atmospheric conditions, and a nonlinear combination of two channels, CH 8.63 and CH 11.95 , was used to estimate LST and SST for high-emissivity surfaces under wet atmospheric conditions. The numerical values of the DTCC-SW coefficients were obtained using a statistical regression method from synthetic data simulated with an accurate atmospheric radiative transfer model moderate spectral resolution atmospheric transmittance mode 5 over a wide range of atmospheric and surface conditions. The LST (SST), mean emissivity, and atmospheric water vapor content were divided into several tractable subranges to improve the fitting accuracy. The experimental results and the preliminary evaluation results showed that the root-mean-square error between the actual and estimated LSTs (SSTs) is less than 0.7 K (0.3 K), provided that the land surface emissivities are known, which indicates that the proposed DTCC-SW method can accurately estimate the LST and SST from the GF-5 TIR data.","['Atmospheric modeling', 'Ocean temperature', 'Land surface temperature', 'Sea surface', 'Land surface', 'Satellites', 'Channel estimation']","['Different thermal channel combination split-window (DTCC-SW)', 'Gaofen-5 (GF-5)', 'land surface temperature (LST)', 'sea surface temperature (SST)', 'thermal infrared (TIR)']"
"The availability of high-resolution along-track interferometric synthetic aperture radar (ATI-SAR) data with large coverage, such as TerraSAR-X (TSX) data, motivates spaceborne ground moving target detection as an attractive alternative to conventional traffic data acquisition. In this paper, a performance analysis of ground moving targets detection by means of ATI-SAR systems and using a statistical approach is carried out on both simulated and real data. A Gaussian clutter model and a deterministic target response have been assumed. The receiver operating characteristic for the likelihood ratio test (LRT), which can be assumed as a reference best performance case, has been expressed in closed form and has been related to the deflection values, which can be exploited for assessing the improvements in the detection probability with a constant false-alarm rate. For practical applications, the performance of a generalized LRT (GLRT) has been investigated. The analysis carried out on simulated data revealed that the detection results achieved using a GLRT based on a deterministic target model are comparable with those obtained using a GLRT based on a Gaussian target model and are not significantly worse than the theoretical performance of the LRT. Finally, ground moving target detection results on TSX real data are showed.","['Thyristors', 'Clutter', 'Detectors', 'Vectors', 'Synthetic aperture radar', 'Object detection', 'Azimuth']","['Along-track interferometry (ATI)', 'detection', 'generalized likelihood ratio test (GLRT)', 'ground moving target', 'likelihood ratio test (LRT)', 'synthetic aperture radar (SAR)']"
"The Advanced Himawari Imager (AHI) onboard Japanese geostationary satellite Himawari-8 provides two more visible, three more near-infrared, and six more infrared channels than the only one visible and four infrared channels available from the previous geostationary imager instruments. By taking advantage of AHI's newly added channels 1, 3, and 4 with wavelengths centered at 0.46, 0.64, and $0.86 μm, respectively, a fast cloud detection algorithm is developed. Since the spectral differences of the reflectance between any two of AHI's channels 1, 3, and 4 over clouds are smaller than those over land and ocean, a visible-based cloud index (VCI) for daytime cloud detection can thus be defined by the root mean square of the three differences between any two of these three channels. An AHI pixel is identified as cloudy if the VCI is smaller than a threshold, which has different values over ocean and land. Cloud detection is further adjusted by a bias correction using AHI channels 7 and 13. The average accuracy of the proposed simple cloud detection is comparable with those obtained from a more complicated cloud mask algorithm involving not only more channels but also model simulations. It is also found that the bias correction is needed mostly over cirrus clouds and Gobi.","['Clouds', 'Sea surface', 'Ocean temperature', 'Detection algorithms', 'Surface waves', 'Satellites', 'Atmospheric modeling']","['Clouds', 'meteorology', 'remote sensing', 'satellite application']"
"The snake algorithm has been proposed to solve many remote sensing and computer vision problems such as object segmentation, surface reconstruction, and object tracking. This paper introduces a framework for 3-D building model construction from LIDAR data based on the snake algorithm. It consists of nonterrain object identification, building and tree separation, building topology extraction, and adjustment by the snake algorithm. The challenging task in applying the snake algorithm to building topology adjustment is to find the global minima of energy functions derived for 2-D building topology. The traditional snake algorithm uses dynamic programming for computing the global minima of energy functions which is limited to snake problems with 1-D topology (i.e., a contour) and cannot handle problems with 2-D topology. In this paper, we have extended the dynamic programming method to address the snake problems with a 2-D planar topology using a novel graph reduction technique. Given a planar snake, a set of reduction operations is defined and used to simplify the graph of the planar snake into a set of isolated vertices while retaining the minimal energy of the graph. Another challenging task for 3-D building model reconstruction is how to enforce different kinds of geometric constraints during building topology refinement. This framework proposed two energy functions, deviation and direction energy functions, to enforce multiple geometric constraints on 2-D topology refinement naturally and efficiently. To examine the effectiveness of the framework, the framework has been applied on different data sets to construct 3-D building models from airborne LIDAR data. The results demonstrate that the proposed snake algorithm successfully found the global optima in polynomial time for all of the building topologies and generated satisfactory 3-D models for most of the buildings in the study areas.","['Buildings', 'Topology', 'Laser radar', 'Solid modeling', 'Atmospheric modeling', 'Data models', 'Heuristic algorithms']","['Light detection and ranging (LIDAR)', 'snake algorithm', 'topology']"
"This paper presents a completely automatic processing chain for orthorectification of optical pushbroom sensors. The procedure is robust and works without manual intervention from raw satellite image to orthoimage. It is modularly divided in four main steps: metadata extraction, automatic ground control point (GCP) extraction, geometric modeling, and orthorectification. The GCP extraction step uses georeferenced vector roads as a reference and produces a file with a list of points and their accuracy estimation. The physical geometric model is based on collinearity equations and works with sensor-corrected (level 1) optical satellite images. It models the sensor position and attitude with second-order piecewise polynomials depending on the acquisition time. The exterior orientation parameters are estimated in a least squares adjustment, employing random sample consensus and robust estimation algorithms for the removal of erroneous points and fine-tuning of the results. The images are finally orthorectified using a digital elevation model and positioned in a national coordinate system. The usability of the method is presented by testing three RapidEye images of regions with different terrain configurations. Several tests were carried out to verify the efficiency of the procedure and to make it more robust. Using the geometric model, subpixel accuracy on independent check points was achieved, and positional accuracy of orthoimages was around one pixel. The proposed procedure is general and can be easily adapted to various sensors.","['Roads', 'Satellites', 'Mathematical model', 'Accuracy', 'Data mining', 'Optical sensors']","['Automatic orthorectification', 'general physical geometric model', 'ground control point (GCP) extraction', 'optical imagery', 'random sample consensus (RANSAC)', 'RapidEye', 'robust estimation']"
"The Global Precipitation Measuring Mission requires the ability to compare the calibrations of similar, but not identical, orbiting microwave radiometers. A fitting algorithm has been developed which adjusts a set of geophysical parameters to match the radiances of a source sensor. The adjusted parameters are then used to compute the radiances for a target sensor. For comparison purposes, a simple (weather forecast) analysis-based algorithm has also been implemented. The algorithms have been tested on two pairs of sensors, TMI/Windsat and TMI/AMSR-E. The differences in the results between the two algorithms are generally small. The contribution of various error sources has also been evaluated. The error analysis suggests similar quality for both algorithms. A comparison of the observed variability in the differences between the two sensors in each pair shows very similar variability for the TMI/AMSR-E pair as for the TMI/Windsat pair. It is also shown that the fitting algorithm partially compensates for shortcomings in the radiative transfer models by introducing spurious correlations among the retrieved parameters.","['Calibration', 'Radiometers', 'Uncertainty', 'Algorithm design and analysis', 'Ocean temperature', 'Clouds', 'Educational institutions']","['Calibration', 'microwave radiometry', 'oceans', 'satellites', 'terrestrial atmosphere']"
"The Suomi-NPP Visible Infrared Imager Radiometer Suite (VIIRS) instrument provides the next generation of visible/infrared imaging including the day/night band (DNB) with nominal bandwidth from 500 to 900 nm. Previous to VIIRS, the Defense Meteorological Satellite Program Operational Linescan System (OLS) measured radiances that spanned over seven orders of magnitude, using an onboard gain adjustment to provide the capability to image atmospheric features across the solar terminator, to observe nighttime light emissions over the globe, and to monitor the global distribution of clouds. The VIIRS DNB detects radiances that span over eight orders of magnitude, and because it has 13-14-b quantization (compared with 6 b for OLS) with three gain stages, the DNB has its full dynamic range at every part of the scan. One process that is applied to the VIIRS DNB radiances is a solar/lunar zenith angle dependent gain adjustment to create near-constant contrast (NCC) imagery. The at-launch NCC algorithm was designed to reproduce the OLS capability and, thus, was constrained to solar and lunar angles from 0° to 105°. This limitation has, in part, lead to suboptimal imagery due to the assumption that DNB radiances fall off exponentially beyond twilight. The VIIRS DNB ultrasensitivity in low-light conditions enables it to detect faint emissions from a phenomenon called airglow, thus invalidating the exponential fall-off assumption. Another complication to the NCC imagery algorithm is the stray light contamination that contaminates the DNB radiances in the astronomical twilight region. We address these issues and develop a solution that leads to high-quality imagery for all solar and lunar conditions.","['Moon', 'Table lookup', 'Stray light', 'Lighting', 'Clouds', 'Dynamic range', 'Signal processing algorithms']","['Day/night band (DNB)', 'near-constant contrast (NCC)', 'Suomi National Polar-orbiting Partnership (S-NPP)', 'terminator visible imaging', 'Visible Infrared Imager Radiometer Suite (VIIRS)']"
"This article shows how the array of corner reflectors (CRs) in Queensland, Australia, together with highly accurate geodetic synthetic aperture radar (SAR) techniques-also called imaging geodesy-can be used to measure the absolute and relative geometric fidelity of SAR missions. We describe, in detail, the end-to-end methodology and apply it to TerraSAR-X Stripmap (SM) and ScanSAR (SC) data and to Sentinel-1 interferometric wide swath (IW) data. Geometric distortions within images that are caused by commonly used SAR processor approximations are explained, and we show how to correct them during postprocessing. Our results, supported by the analysis of 140 images across the different SAR modes and using the 40 reflectors of the array, confirm our methodology and achieve the limits predicted by theory for both Sentinel-1 and TerraSAR-X. After our corrections, the Sentinel-1 residual errors are 6 cm in range and 26 cm in azimuth, including all error sources. The findings are confirmed by the mutual independent processing carried out at University of Zurich (UZH) and German Aerospace Center (DLR). This represents an improvement of the geolocation accuracy by approximately a factor of four in range and a factor of two in azimuth compared with the standard Sentinel-1 products. The TerraSAR-X results are even better. The achieved geolocation accuracy now approaches that of the global navigation satellite system (GNSS)-based survey of the CRs positions, which highlights the potential of the end-to-end SAR methodology for imaging geodesy.","['Synthetic aperture radar', 'Geology', 'Azimuth', 'Orbits', 'Radar polarimetry', 'Australia', 'Arrays']","['Geodesy', 'radar remote sensing', 'spaceborne radar', 'synthetic aperture radar']"
"Nowadays, deep learning (DL) finds application in a large number of scientific fields, among which the estimation and the enhancement of signals disrupted by the noise of different natures. In this article, we address the problem of the estimation of the interferometric parameters from synthetic aperture radar (SAR) data. In particular, we combine convolutional neural networks together with the concept of residual learning to define a novel architecture, named Φ-Net, for the joint estimation of the interferometric phase and coherence. Φ-Net is trained using synthetic data obtained by an innovative strategy based on the theoretical modeling of the physics behind the SAR acquisition principle. This strategy allows the network to generalize the estimation problem with respect to: 1) different noise levels; 2) the nature of the imaged target on the ground; and 3) the acquisition geometry. We then analyze the Φ-Net performance on an independent data set of synthesized interferometric data, as well as on real InSAR data from the TanDEM-X and Sentinel-1 missions. The proposed architecture provides better results with respect to state-of-the-art InSAR algorithms on both synthetic and real test data. Finally, we perform an application-oriented study on the retrieval of the topographic information, which shows that Φ-Net is a strong candidate for the generation of high-quality digital elevation models at a resolution close to the one of the original single-look complex data.","['Estimation', 'Synthetic aperture radar', 'Coherence', 'Noise reduction', 'Convolution', 'Measurement', 'Wavelet transforms']","['Coherence', 'convolutional neural network (CNN)', 'deep learning (DL)', 'denoising', 'interferometric phase', 'residual learning', 'synthetic aperture radar (SAR) interferometry']"
"The application of empirical mode decomposition (EMD) in the analysis and processing of lightning electric field waveforms acquired by the low-frequency e-field detection array (LFEDA) in China has significantly improved the capabilities of the low-frequency/very-low-frequency (LF/VLF) time-of-arrival technique for studying the lightning discharge processes. However, the inherent mode mixing and the endpoint effect of EMD lead to certain problems, such as an inadequate noise reduction capability, the incorrect matching of multistation waveforms, and the inaccurate extraction of pulse information, which limit the further development of the LFEDA's positioning ability. To solve these problems, the advanced ensemble EMD (EEMD) technique is introduced into the analysis of LF/VLF lightning measurements, and a double-sided bidirectional mirror (DBM) extension method is proposed to overcome the endpoint effect of EMD. EEMD can effectively suppress mode mixing, and the DBM extension method proposed in this article can effectively suppress the endpoint effect, thus greatly improving the accuracy of a simulated signal after a 25-500-kHz bandpass filter. The resulting DBM_EEMD algorithm can be used in the LFEDA system to process and analyze the detected electric field signals to improve the system's lightning location capabilities, especially in terms of accurate extraction and location of weak signals from lightning discharges. In this article, a 3-D image of artificially triggered lightning obtained from an LF/VLF location system is reported for the first time, and methods for further improving the location capabilities of the LF/VLF lightning detection systems are discussed.","['Lightning', 'Discharges (electric)', 'Fault location', 'Signal analysis', 'Empirical mode decomposition']","['Double-sided bidirectional mirror (DBM) extension', 'ensemble empirical mode decomposition (EEMD)', 'lightning location', 'low-frequency/very-low-frequency (LF/VLF) electric field']"
"After 15 years, the Système Pour l'Observation de la Terre (SPOT)-VEGETATION (VGT) program reached the end of its life in May 2014 and was replaced by the Project for On-Board Autonomy-Vegetation (PROBA-V) mission. Exploiting the period of overlap between instruments, this study compares the normalized difference vegetation index (NDVI) of two instruments from the point of view of the user interested in operational crop monitoring. The comparison is performed for Morocco, Algeria, and Tunisia, where NDVI is used to derive anomaly maps, temporal profiles, and cereal yield forecasts. A relevant scatter due to unexplained unsystematic variability exists between anomaly values. A mismatch between anomaly classes is observed for 20%-30% of the crop area. However, when the NDVI is averaged over cropland and administrative units to derive temporal profiles, the two data sources show a high agreement. Results for yield estimation comparison indicate an overall high agreement, and both the (null) hypotheses that the model predictions and the root mean square error (RMSE) in yield estimation are not different, when using PROBA-V instead of SPOT-VGT, cannot be rejected in all cases for Morocco and Algeria. On the contrary, in Tunisia, where RMSE is lower using PROBA-V, the hypothesis of no difference in RMSE is rejected. These findings therefore indicate that yield estimation performances are not affected (Morocco and Algeria) or improved (Tunisia) by the source transition. Finally, despite the same nominal spatial resolution, the different spatial quality of the sensors was found to have an effect on yield estimation in areas characterized by sharp transitions between cropland and desert.","['Agriculture', 'Instruments', 'Predictive models', 'Monitoring', 'Forecasting', 'Vegetation mapping', 'Analytical models']","['Crop monitoring', 'crop yield forecasting', 'normalized difference vegetation index (NVDI)', 'Project for On-Board Autonomy-Vegetation (PROBA-V)', ""Syst?me Pour l'Observation de la Terre (SPOT)-VEGETATION (VGT)""]"
"The TanDEM-X mission (TDM) is a spaceborne radar interferometer which delivers a global digital surface model (DSM) with an unprecedented spatial resolution. This allows resolving objects above ground such as buildings. Extracting and characterizing those objects in an automated manner represents a challenging problem but opens simultaneously a broad range of large-area applications. In this paper, we discuss and evaluate the suitability of morphological filters (MFs) for the derivation of normalized DSMs from the TDM in complex urban environments and introduce a novel region-growing-based progressive MF procedure. This approach is jointly proposed and can be combined with a postclassification processing scheme to specifically allow for a viable reconstruction of urban morphology in a challenging terrain. The filter approach comprises a multistep procedure using concepts of morphological image filtering, region growing, and interpolation techniques. Therefore, it extends the idea of progressive MFs. The latter aim to identify nonground pixels in the DSM by gradually increasing the size of a structuring element and applying iteratively an elevation difference threshold. After the identification of initial nonground pixels, here, potential nonground pixels are identified within each iteration, and their similarity with respect to neighboring nonground pixels is assessed. Pixels are finally labeled as nonground if a constraint is fulfilled. The postclassification processing scheme adapts techniques of object-based image analyses to further refine regions of classified nonground pixels. Digital terrain models are subsequently generated by interpolating between identified ground pixels. Experimental results are obtained for settlement areas that cover large parts of the cities of Izmir (Turkey) and Wuppertal (Germany). They confirm the capability of the proposed approaches for a reduction of omission errors compared to basic MF-based methods when classifying ground pixels, which is favorable in a mountainous terrain with steep slopes.","['Buildings', 'Time division multiplexing', 'Image segmentation', 'Surface morphology', 'Spatial resolution', 'Urban areas', 'Interpolation']","['Digital surface models (DSM)', 'digital terain models (DTM)', 'morphological filters', 'TanDEM-X']"
"The most unique advantage of multipass synthetic aperture radar interferometry (InSAR) is the retrieval of long-term geophysical parameters, e.g., linear deformation rates, over large areas. Recently, an object-based multipass InSAR framework has been proposed by Kang, as an alternative to the typical single-pixel methods, e.g., persistent scatterer interferometry (PSI), or pixel-cluster-based methods, e.g., SqueeSAR. This enables the exploitation of inherent properties of InSAR phase stacks on an object level. As a follow-on, this paper investigates the inherent low rank property of such phase tensors and proposes a Robust Multipass InSAR technique via Object-based low rank tensor decomposition. We demonstrate that the filtered InSAR phase stacks can improve the accuracy of geophysical parameters estimated via conventional multipass InSAR techniques, e.g., PSI, by a factor of 10–30 in typical settings. The proposed method is particularly effective against outliers, such as pixels with unmodeled phases. These merits, in turn, can effectively reduce the number of images required for a reliable estimation. The promising performance of the proposed method is demonstrated using high-resolution TerraSAR-X image stacks.","['Tensile stress', 'Strain', 'Matrix decomposition', 'Robustness', 'Synthetic aperture radar', 'Covariance matrices', 'Principal component analysis']","['Iterative reweight', 'low rank', 'object-based', 'synthetic aperture radar (SAR)', 'SAR interferometry (InSAR)', 'tensor decomposition']"
"The thermal noise-induced distortions in a Sentinel-1 terrain observation with progressive scans synthetic aperture radar image cannot be corrected by the noise equivalent sigma nought (NESZ) subtraction only. Since the thermal noise is scaled during synthetic aperture radar processing, it resides not only as an additive noise in each pixel but also as a multiplicative noise in the interpixel contrast. In this paper, we investigate the noise characteristics and propose an efficient method for the multiplicative textural noise correction. The core ideas are to find the optimal coefficient of the noise-induced standard deviation (SD) and model the noise contribution to the local SD as a function of the NESZ and the signal-to-noise ratio. Denoising is accomplished by a subwindow-wise adaptive rescaling of the pixel values. The improvements in the first- and second-order statistical textural features demonstrate the effectiveness of the proposed method.","['Thermal noise', 'Synthetic aperture radar', 'Signal to noise ratio', 'Additives', 'Noise reduction', 'Image reconstruction', 'Distortion']","['Cross polarization', 'image texture', 'Sentinel-1', 'synthetic aperture radar (SAR)', 'terrain observation with progressive scans SAR (TOPSAR)', 'thermal noise']"
"Over the Arctic regions, current conventional altimetry products suffer from a lack of coverage or from degraded performance due to the inadequacy of the standard processing applied in the ground segments. This paper presents a set of dedicated algorithms able to process consistently returns from open ocean and from sea-ice leads in the Arctic Ocean (detection of water surfaces and derivation of water levels using returns from these surfaces). This processing extends the area over which a precise sea level can be computed. In the frame of the European Space Agency Sea Level Climate Change Initiative ( http://cci.esa.int ), we have first developed a new surface identification method combining two complementary solutions, one using a multiple-criteria approach (in particular the backscattering coefficient and the peakiness coefficient of the waveforms) and one based on a supervised neural network approach. Then, a new physical model has been developed (modified from the Brown model to include anisotropy in the scattering from calm protected water surfaces) and has been implemented in a maximum likelihood estimation retracker. This allows us to process both sea-ice lead waveforms (characterized by their peaky shapes) and ocean waveforms (more diffuse returns), guaranteeing, by construction, continuity between open ocean and ice-covered regions. This new processing has been used to produce maps of Arctic sea level anomaly from 18-Hz ENVIronment SATellite/RA-2 data.","['Arctic', 'Sea surface', 'Sea ice', 'Sea level', 'Surface waves', 'Climate change']","['Arctic Ocean', 'oceans sea level', 'radar remote sensing', 'satellite altimetry', 'sea ice']"
"This paper deals with coherent (in the sense that both amplitudes and relative phases of the polarimetric returns are used to construct the decision statistic) multipolarization synthetic aperture radar (SAR) change detection assuming the availability of reference and test images collected from N multiple polarimetric channels. At the design stage, the change detection problem is formulated as a binary hypothesis testing problem, and the principle of invariance is used to come up with decision rules sharing the constant false alarm rate property. The maximal invariant statistic and the maximal invariant in the parameter space are obtained. Hence, the optimum invariant test is devised proving that a uniformly most powerful invariant detector does not exist. Based on this, the class of suboptimum invariant receivers, which also includes the generalized likelihood ratio test, is considered. At the analysis stage, the performance of some tests, belonging to the aforementioned class, is assessed and compared with the optimum clairvoyant invariant detector. Finally, detection maps on real high-resolution SAR data are computed showing the effectiveness of the considered invariant decision structures.","['Detectors', 'Synthetic aperture radar', 'Receivers', 'Vectors', 'Linear matrix inequalities', 'Space vehicles', 'Orbits']","['Coherent change detection', 'invariant rules', 'maximal invariant', 'multipolarization']"
"This paper presents a nonlocal interferometric synthetic aperture radar (InSAR) filter with the goal of generating digital elevation models (DEMs) of higher resolution and accuracy from bistatic TanDEM-X strip map interferograms than with the processing chain used in production. The currently employed boxcar multilooking filter naturally decreases the resolution and has inherent limitations on what level of noise reduction can be achieved. The proposed filter is specifically designed to account for the inherent diversity of natural terrain by setting several filtering parameters adaptively. In particular, it considers the local fringe frequency and scene heterogeneity, ensuring proper denoising of interferograms with considerable underlying topography as well as urban areas. A comparison using synthetic and TanDEM-X bistatic strip map data sets with existing InSAR filters shows the effectiveness of the proposed techniques, most of which could readily be integrated into existing nonlocal filters. The resulting DEMs outclass the ones produced with the existing global TanDEM-X DEM processing chain by effectively increasing the resolution from 12 to 6 m and lowering the noise level by roughly a factor of two.","['Synthetic aperture radar', 'Noise reduction', 'Spatial resolution', 'Microsoft Windows', 'Remote sensing', 'Digital elevation models', 'Strips']","['Digital elevation model (DEM)', 'interferometric synthetic aperture radar (InSAR)', 'nonlocal filtering']"
"We evaluate the potential of combined Advanced Land Observing Satellite Advanced Visible and Near-Infrared (AVNIR-2) and fully polarimetric Phased-Array-type L-band Synthetic Aperture Radar (PALSAR) data for land cover classification. Optical AVNIR-2 and fully polarimetric PALSAR can provide both surface spectral information and scattering information of the ground surface. The fully polarimetric PALSAR is particularly important for land cover classification because quad-polarization PALSAR data and its polarimetric parameters contain additional surface information. As a consequence, by combining optical AVNIR-2, PALSAR, and polarimetric parameters into a single data set, land cover classification accuracy may be further improved. For efficient and convenient handling of the combined multisource data, we used a subspace method for the classification and estimated its classification capability for various combinations of optical, PALSAR, and polarimetric parameter data sets in the Lake Kasumigaura region of Japan. We also compared the results obtained using the subspace method with those obtained by the support vector machine (SVM) and maximum-likelihood classification (MLC) methods. The classification results confirm that, when the combined optical AVNIR-2, PALSAR, and polarimetric coherency matrix data were used, the classification accuracy of the subspace method was better than that when other data combinations were used. The subspace method also performed better than the SVM or MLC method in high-dimensional data set classification. Moreover, the experimental results demonstrated that the proposed subspace method is robust for data classification when there is data redundancy and thus allows optimal feature selection procedures to be avoided.","['Accuracy', 'Training', 'Correlation', 'Optical scattering', 'Optical sensors', 'Remote sensing']","['Coherency matrix', 'Phased-Array-type L-band Synthetic Aperture Radar (PALSAR)', 'polarimetry', 'subspace method', 'supervised classification']"
"Planar or cylindrical phased arrays are two candidate antennas for future polarimetric weather radar. These two candidate antennas have distinctly different attributes when used to make quantitative measurements of the polarimetric properties of precipitation. Of critical concern is meeting the required polarimetric performance for all directions of the electronically steered beam. The copolar and cross-polar radiation patterns and polarimetric parameter estimation performances of these two phased array antennas are studied and compared with that obtained using a dual-polarized parabolic reflector antenna. Results obtained from simulation show that the planar polarimetric phased array radar has unacceptable polarimetric parameter biases that require beam to beam correction, whereas biases obtained with the cylindrical polarimetric phased array radar are much lower and comparable to that obtained using the parabolic reflector antenna.","['Antenna radiation patterns', 'Meteorology', 'Radar antennas', 'Apertures', 'Radar polarimetry', 'Phased arrays']","['Antenna', 'cylindrical array', 'phased array radar', 'planar array', 'polarimetry', 'radiation patterns', 'remote sensing', 'weather measurement']"
"Statistical measures of patterns (textures) in surface roughness are used to quantitatively differentiate volcanic deposit facies on the Pumice Plain, on the northern flank of Mount St. Helens (MSH). Surface roughness values are derived from a Light Detection and Ranging (LiDAR) point cloud collected in 2004 from a fixed-wing airborne platform. Patterns in surface roughness are characterized using co-occurrence texture statistics. Pristine-pyroclastic, reworked-pyroclastic, mudflow, boulder beds, eroded lava flows, braided streams, and other units within the Pumice Plain are all found to have significantly distinct roughness textures. The MSH deposits are reasonably accessible, and the textural variations have been verified in the field. Results of this work indicate that by affecting the distribution of large clasts and tens-of-meter scale landforms, modification of pyroclastic deposits by lahars alters the morphology of the surface in detectable quantifiable ways. When a lahar erodes a pyroclastic deposit, surface roughness increases, as does the randomness in the deposit surface. Conversely, when a lahar deposits material, the resulting landforms are less rough but more random than pristine pumice-rich pyroclastic deposits. By mapping these relationships and others, volcanic deposit facies can be differentiated. This new method of mapping, based on roughness texture, has the potential to aid mapping efforts in more remote regions, both on this planet and elsewhere in the solar system.","['Rough surfaces', 'Surface roughness', 'Surface treatment', 'Surface morphology', 'Surface texture', 'Surface topography', 'Laser radar']","['Light Detection and Ranging (LiDAR)', 'Mount St. Helens (MSH)', 'surface roughness', 'texture', 'volcanology']"
"Spectral mixture analysis has a history in mapping snow, especially where mixed pixels prevail. Using multiple spectral bands rather than band ratios or band indices, retrievals of snow properties that affect its albedo lead to more accurate estimates than widely used age-based models of albedo evolution. Nevertheless, there is substantial room for improvement. We present the Snow Property Inversion from Remote Sensing (SPIReS) approach, offering the following improvements: 1) Solutions for grain size and concentrations of light absorbing particles are computed simultaneously; 2) Only snow and snow-free endmembers are employed; 3) Cloud-masking and smoothing are integrated; 4) Similar spectra are grouped together and interpolants are used to reduce computation time. The source codes are available in an open repository. Computation is fast enough that users can process imagery on demand. Validation of retrievals from Landsat 8 operational land imager (OLI) and moderate-resolution imaging spectroradiometer (MODIS) against WorldView-2/3 and the Airborne Snow Observatory shows accurate detection of snow and estimates of fractional snow cover. Validation of albedo shows low errors using terrain-corrected in situ measurements. We conclude by discussing the applicability of this approach to any airborne or spaceborne multispectral sensor and options to further improve retrievals.","['Snow', 'MODIS', 'Remote sensing', 'Earth', 'Cloud computing', 'Artificial satellites', 'Grain size']","['Albedo', 'Landsat', 'moderate-resolution imaging spectroradiometer (MODIS)', 'multispectral imaging', 'snow', 'spectral mixing']"
"The availability of multiple images of the same scene acquired with the same radar but with different polarizations, both in transmission and reception, has the potential to enhance the classification, detection, and/or recognition capabilities of a remote sensing system. A way to take advantage of the full-polarimetric data is to extract, for each pixel of the considered scene, the polarimetric covariance matrix, the coherence matrix, and the Muller matrix and to exploit them in order to achieve a specific objective. A framework for detecting covariance symmetries within polarimetric synthetic aperture radar (SAR) images is here proposed. The considered algorithm is based on the exploitation of special structures assumed by the polarimetric coherence matrix under symmetrical properties of the returns associated with the pixels under test. The performance analysis of the technique is evaluated on both simulated and real L-band SAR data, showing a good classification level of the different areas within the image.","['Covariance matrices', 'Synthetic aperture radar', 'Symmetric matrices', 'Coherence', 'Radar imaging']","['Coherence and covariance scattering matrix', 'polarimetric SAR image', 'radar image classification', 'synthetic aperture radar (SAR)']"
"Climate change and anthropogenic pressure are causing an indisputable decline in biodiversity; therefore, the need of environmental knowledge is important to develop the appropriate management plans. In this context, remote sensing and, specifically, hyperspectral imagery (HSI) can contribute to the generation of vegetation maps for ecosystem monitoring. To properly obtain such information and to address the mixed pixels inconvenience, the richness of the hyperspectral data allows the application of unmixing techniques. In this sense, a problem found by the traditional linear mixing model (LMM), a fully constrained least squared unmixing (FCLSU), is the lack of ability to account for spectral variability. This paper focuses on assessing the performance of different spectral unmixing models depending on the quality and quantity of endmembers. A complex mountainous ecosystem with high spectral changes was selected. Specifically, FCLSU and 3 approaches, which consider the spectral variability, were studied: scaled constrained least squares unmixing (SCLSU), Extended LMM (ELMM) and Robust ELMM (RELMM). The analysis includes two study cases: 1) robust endmembers and 2) nonrobust endmembers. Performances were computed using the reconstructed root-mean-square error (RMSE) and classification maps taking the abundances maps as inputs. It was demonstrated that advanced unmixing techniques are needed to address the spectral variability to get accurate abundances estimations. RELMM obtained excellent RMSE values and accurate classification maps with very little knowledge of the scene and minimum effort in the selection of endmembers, avoiding the curse of dimensionality problem found in HSI.","['Ecosystems', 'Hyperspectral imaging', 'Vegetation mapping', 'Biodiversity', 'Climate change', 'Monitoring']","['CASI', 'hyperspectral image classification', 'spectral unmixing', 'Hughes phenomenon', 'endmembers', 'spectral variability']"
"Ground-penetrating radar (GPR) has been studied for landmine detection and identification. Since this application employs higher frequencies as compared to conventional largescale GPR measurements, the GPR performance is greatly influenced by soil properties and their spatial heterogeneity. In order to study the influence of soil heterogeneity on GPR performance, three types of soil were investigated. From the soil heterogeneity, GPR clutter was modeled with the aim of assessing the difficulty encountered in successful GPR performance. A handheld dual-sensor system that combines a metal detector and GPR was tested in these three test soils, and its performance for identifying buried objects was evaluated. The GPR performance obtained from the test showed a clear correlation with the modeled GPR clutter. Hence, the present study illustrates that clutter plays a major role in the detection of small objects in heterogeneous soil by GPR.","['Soil', 'Ground penetrating radar', 'Permittivity', 'Dielectrics', 'Clutter', 'Metals', 'Detectors']","['Clutter', 'dielectric permittivity', 'geostatistics', 'ground-penetrating radar (GPR)', 'landmine detection', 'scattering', 'soil heterogeneity']"
"Deformation monitoring by multipass synthetic aperture radar (SAR) interferometry (InSAR) is, so far, the only imaging-based method to assess millimeter-level deformation over large areas from space. Past research mostly focused on the optimal retrieval of deformation parameters on the basis of a single pixel or a pixel cluster. Only until recently, the first demonstration of object-based urban infrastructure monitoring by fusing InSAR and the semantic classification labels derived from optical images was presented by Wang et al. Given such classification labels in the SAR image, we propose a general framework for object-based InSAR parameter retrieval, where the parameters of the whole object are jointly estimated by the inversion of a regularized tensor model instead of pixelwise. Our approach does not assume the stationarity of each sample in the object, which is usually assumed in other pixel cluster-based methods, such as SqueeSAR. In addition, to handle outliers in real data, a robust phase recovery step prior to parameter retrieval is also introduced. In typical settings, the proposed method outperforms the current pixelwise estimators, e.g., periodogram, by a factor of several tens in the accuracy of the linear deformation estimates. Last but not least, for a practical demonstration on bridge monitoring, we present a full workflow of long-term bridge monitoring using the proposed approach.","['Tensile stress', 'Synthetic aperture radar', 'Image reconstruction', 'Robustness', 'Bridges', 'Optimization', 'Deformable models']","['Bridge detection', 'joint deformation reconstruction', 'object-based', 'synthetic aperture radar (SAR)', 'SAR interferometry (InSAR)']"
"About half of all optical observations collected via spaceborne satellites are affected by haze or clouds. Consequently, cloud coverage affects the remote-sensing practitioner’s capabilities of a continuous and seamless monitoring of our planet. This work addresses the challenge of optical satellite image reconstruction and cloud removal by proposing a novel multimodal and multitemporal data set called SEN12MS-CR-TS. We propose two models highlighting the benefits and use cases of SEN12MS-CR-TS: First, a multimodal multitemporal 3-D convolution neural network that predicts a cloud-free image from a sequence of cloudy optical and radar images. Second, a sequence-to-sequence translation model that predicts a cloud-free time series from a cloud-covered time series. Both approaches are evaluated experimentally, with their respective models trained and tested on SEN12MS-CR-TS. The conducted experiments highlight the contribution of our data set to the remote-sensing community as well as the benefits of multimodal and multitemporal information to reconstruct noisy information. Our data set is available at https://patrickTUM.github.io/cloud_removal .","['Clouds', 'Optical imaging', 'Optical sensors', 'Satellites', 'Remote sensing', 'Image reconstruction', 'Earth']","['Cloud removal', 'data fusion', 'image reconstruction', 'sequence-to-sequence', 'synthetic aperture radar (SAR)-optical', 'time series']"
"Ocean microplastic concentrations are known to vary significantly by location, with especially high levels in the North Atlantic and North Pacific gyres. Most direct measurements come from plankton net trawling made in these regions; concentrations in other regions have been estimated by microplastic transport models that depend on large-scale ocean circulation patterns. However, global measurements of microplastic distribution and its temporal variability are lacking. A new method is presented for detecting and imaging the global distribution of ocean microplastics from space. The method uses spaceborne bistatic radar measurements of ocean surface roughness and relies on an assumed reduction in responsiveness to wind-driven roughening caused by surfactants that act as tracers for microplastics near the surface. Annual mean microplastic distributions estimated by the radars are generally consistent with model predictions. The spaceborne observations are also able to detect temporal changes that are not resolved by the models. For example, seasonal dependencies are observed at mid-latitudes in both Northern and Southern Hemispheres, with lower concentrations noted in the winter months. Time lapse images at finer spatial and temporal scales reveal episodic bursts of microplastic tracers in the outflow from major river discharges into the sea. This new method will provide better monitoring of ocean microplastics and will support future model development and validation.","['Oceans', 'Plastics', 'Sea surface', 'Surface roughness', 'Rough surfaces', 'Wind speed', 'Sea measurements']","['GNSS data', 'miscellaneous applications', 'oceans and water']"
"Global Space-based Inter-Calibration System (GSICS) products to correct the calibration of the infrared channels of the Meteosat/SEVIRI (Spinning Enhanced Visible and Infrared Imager) geostationary imagers are based on comparisons of collocated observations with Metop/IASI (Infrared Atmospheric Sounding Interferometer) as a reference instrument. Each step of the cross-calibration algorithm is analyzed to produce a comprehensive error budget, following the Guide to the Expression of Uncertainty in Measurement. This paper aims to validate the quality indicators provided as uncertainty estimates with the GSICS correction. The methodology presented provides a framework to allow quantitative tradeoffs between the collocation criteria and the number of collocations generated to recommend further algorithm improvements. It is shown that random errors dominate systematic ones and that combined standard uncertainties (with coverage factor k = 1) in the corrected brightness temperatures are ~ 0.01 K for typical clear sky conditions but increase rapidly for low radiances - by more than one order of magnitude for 210 K scenes, corresponding to cold cloud tops.","['Uncertainty', 'Systematics', 'Instruments', 'Standards', 'Sensitivity', 'Calibration', 'Clouds']","['Calibration', 'Earth Observing System', 'infrared (IR) image sensors', 'measurement uncertainty', 'meteorology']"
"Band selection refers to the process of choosing the most relevant bands in a hyperspectral image. By selecting a limited number of optimal bands, we aim at speeding up model training, improving accuracy, or both. It reduces redundancy among spectral bands while trying to preserve the original information of the image. By now, many efforts have been made to develop unsupervised band selection approaches, of which the majorities are heuristic algorithms devised by trial and error. In this article, we are interested in training an intelligent agent that, given a hyperspectral image, is capable of automatically learning policy to select an optimal band subset without any hand-engineered reasoning. To this end, we frame the problem of unsupervised band selection as a Markov decision process, propose an effective method to parameterize it, and finally solve the problem by deep reinforcement learning. Once the agent is trained, it learns a band-selection policy that guides the agent to sequentially select bands by fully exploiting the hyperspectral image and previously picked bands. Furthermore, we propose two different reward schemes for the environment simulation of deep reinforcement learning and compare them in experiments. This, to the best of our knowledge, is the first study that explores a deep reinforcement learning model for hyperspectral image analysis, thus opening a new door for future research and showcasing the great potential of deep reinforcement learning in remote sensing applications. Extensive experiments are carried out on four hyperspectral data sets, and experimental results demonstrate the effectiveness of the proposed method. The code is publicly available.","['Hyperspectral imaging', 'Task analysis', 'Reinforcement learning', 'Markov processes', 'Earth', 'Correlation', 'Feature extraction']","['Deep Q-network', 'deep reinforcement learning', 'hyperspectral band selection', 'hyperspectral image classification', 'neural network', 'unsupervised learning']"
"Current and future satellite sensors provide measurements in and around the oxygen A-band on a global basis. These data are commonly used for the determination of cloud and aerosol properties. In this paper, we assess the information content in the oxygen A-band for the retrieval of macrophysical cloud parameters using precise radiative transfer simulations covering a wide range of geophysical conditions in conjunction with advance inversion techniques. The information content of the signal with respect to the retrieved parameters is analyzed in a stochastic framework using two common criteria: the degrees of freedom for a signal and the Shannon information content. It is found that oxygen A-band measurements with moderate spectral resolution (0.2 nm) provide two pieces of independent information that allow the accurate retrieval of cloud-top height together with either cloud optical thickness or cloud fraction. Additionally, our results confirm previous studies indicating that the retrieval of cloud geometrical thickness (CGT) from single-angle measurements is not reliable in this spectral region. Finally, a sensitivity study shows that the retrieval of macrophysical cloud parameters is slightly sensitive to the uncertainty in the CGT and very sensitive to the uncertainty in the surface albedo.","['Clouds', 'Atmospheric modeling', 'Satellites', 'Vectors', 'Optical scattering', 'Optical sensors', 'Gases']","['Information content of hyperspectral measurements', 'oxygen A-band', 'retrieval of macrophysical cloud parameters']"
"The Environmental trace gas Monitoring Instrument (EMI) onboard the Chinese high-resolution remote sensing satellite GaoFen-5 is an ultraviolet–visible imaging spectrometer, aiming to quantify the global distribution of tropospheric and stratospheric trace gases and planned to be launched in spring 2018. The preflight calibration phase is essential to characterize the properties and performance of the EMI in order to provide information for data processing and trace gas retrievals. In this paper, we present the first EMI measurement of nitrogen dioxide (NO 2 ) from a gas absorption cell using scattered sunlight as the light source by the differential optical absorption spectroscopy technique. The retrieved NO 2 column densities in the UV and Vis wavelength ranges are consistent with the column density in the gas cell calculated from the NO 2 mixing ratio and the length of the gas cell. Furthermore, the differences of the retrieved NO 2 column densities among the adjoining spatial rows of the detector are less than 3%. This variation is similar to the well-known “stripes-pattern” of the Ozone Monitoring Instrument and is probably caused by remaining systematic effects like a nonperfect description of the individual instrument functions. Finally, the signal-to-noise ratios of EMI in-orbit measurements of NO 2 are estimated on the basis of on-ground scattered sunlight measurements and radiative transfer model simulations. Based on our results, we conclude that the EMI is capable of measuring the global distribution of the NO 2 column with the retrieval precision and accuracy better than 3% for the tested wavelength ranges and viewing angles.","['Electromagnetic interference', 'Atmospheric measurements', 'Calibration', 'Instruments', 'Extraterrestrial measurements', 'Gases', 'Absorption']","['Differential optical absorption spectroscopy (DOAS)', 'NO₂', 'remote sensing', 'satellite', 'spectral analysis', 'trace gases']"
"Semantic segmentation for high-resolution remote sensing images is one of the most significant tasks in the field of remote sensing applications. Remote sensing images contain substantial detailed information of ground objects, such as shape, location, and texture. Therefore, these objects make the images exhibit large intraclass variance and small interclass variance, which makes it very difficult to be recognized. In this study, an end-to-end attention-based semantic segmentation network (SSAtNet) is proposed. A pyramid attention pooling module is proposed to introduce the attention mechanism into the multiscale module for adaptive features refinement. To correct the detailed information, the pooling index correction module integrates pooling index maps from the encoder with high-level feature maps, which can help recover the fine-grained features. In the encoder phase, a more effective ResNet-101 backbone is designed to capture detailed features. What is more, a series of data augmentation methods are proposed to enhance the model’s robustness. The proposed model is compared with several previous advanced networks and achieves the state of the art on the ISPRS Vaihingen dataset. The experiment results prove the effectiveness of the SSAtNet.","['Image segmentation', 'Task analysis', 'Semantics', 'Remote sensing', 'Convolution', 'Indexes', 'Feature extraction']","['Attention mechanism', 'convolutional neural networks (CNNs)', 'multiscale module', 'remote sensing', 'semantic segmentation']"
"Huynen decomposition (HD) as the first formalized target decomposition has not been widely accepted. The preference for symmetry and regularity restricts not only its application but also its unification with other target dichotomies. The nonuniqueness issue then arises because we may have different dichotomies of radar targets, but we have no idea on how to select them. In this paper, a unified Huynen dichotomy is developed by extending HD for a full preference for symmetry and regularity, nonsymmetry, irregularity, and their couplings. It covers all of the existing dichotomies and provides a unified selection mechanism for them. Scattering preference is identified as a main feature of target dichotomy, and its concise description is devised by relating each dichotomy to a canonical scattering. A scattering degree of preference (SDoP) parameter is defined to measure the preference of each dichotomy. In virtue of an adaptive combination and permutation of SDoPs, a scattering pyramid description of the mixed scattering is developed, which has better discrimination of target than entropy/alpha. An SDoP/alpha classification is further proposed by statistical modeling of the unified dichotomy, which is a competent alternative to entropy/alpha. The excellent performance of unified dichotomy makes us believe that the existing concerns on HD are well treated and the Huynen-Cloude controversy, in a sense, may be ended.","['High definition video', 'Scattering', 'Radar', 'Transforms', 'Couplings', 'Entropy', 'Matrix decomposition']","['Huynen decomposition (HD)', 'radar polarimetry', 'target decomposition', 'target extraction', 'unsupervised classification']"
"This article describes four-year calibration results of the dual-frequency precipitation radar (DPR) onboard the Global Precipitation Measurement (GPM) Core Observatory. The calibration method basically follows the method that was used to calibrate the precipitation radar (PR) onboard the Tropical Rainfall Measuring Mission (TRMM) satellite. However, both the hardware and data processing method for calibration are improved by taking advantage of the lessons learned from the PR’s calibration. Since the response of the radar receivers was found to depend on the waveform, the active calibrator was improved in such a way that the external calibration can be performed with both continuous and pulse waves. The methods for evaluating the calibration data were also improved. Instead of assuming a Gaussian antenna pattern, the effective beamwidths were determined by assuming an antenna pattern created by the Taylor distribution that was used to design the antennas. The results of the calibration including these improvements provide the new precise parameters of DPR’s calibration. The new parameters increased the Ku-band precipitation radar’s (KuPR’s) radar reflectivity factor (Z) by about 1.3 dB and that of the Ka-band precipitation radar (KaPR) by about 1.2 dB from the precalibratedZvalues, and the minimum detectable radar reflectivities were 15.46, 19.18, and 13.71 dBZ for KuPR, matched beam of KaPR, and high-sensitivity beam of KaPR, respectively. After applying the new calibration methods to both DPR and PR, normalized radar cross sections (σ0) from the DPR and PR agree with each other.","['Calibration', 'Spaceborne radar', 'Radar', 'Radar antennas', 'Antennas', 'Gain', 'Satellites']","['Calibration', 'Global Precipitation Measurement (GPM)', 'spaceborne precipitation radar (PR)', 'Tropical Rainfall Measuring Mission (TRMM)']"
"Scale effect, which is caused by a combination of model nonlinearity and surface heterogeneity, has been of interest to the remote sensing community for decades. However, there is no current analysis of scale effect in the ground-based indirect measurement of leaf area index (LAI), where model nonlinearity and surface heterogeneity also exist. This paper examines the scale effect on the indirect measurement of LAI. We built multiscale data sets based on realistic scenes and field measurements. We then implemented five representative methods of indirect LAI measurement at scales (segment lengths) that range from meters to hundreds of meters. The results show varying degrees of deviation and fluctuation that exist in all five methods when the segment length is shorter than 20 m. The retrieved LAI from either Beer's law or the gap-size distribution method shows a decreasing trend with increasing segment lengths. The length at which the LAI values begin to stabilize is about a full period of row in row crops and 100 m in broadleaf or coniferous forests. The impacts of segment length on the finite-length averaging method, the combination of gap-size distribution and finite-length methods, and the path-length distribution method are relatively small. These three methods stabilize at the segment scale longer than 20 m in all scenes. We also find that computing the average LAI of all of the short segment lengths, which is commonly done, is not as good as merging these short segments into a longer one and computing the LAI value of the merged one.","['Indexes', 'Remote sensing', 'Instruments', 'Area measurement', 'Length measurement', 'Optical variables measurement', 'Current measurement']","[""Beer's law"", 'gap probability', 'indirect measurement', 'leaf area index (LAI)', 'scale effect', 'segment length']"
"This paper presents an electromagnetic (EM) technique for the reconstruction of the physical and geometrical properties (permittivity and thickness) of stratified media. The key points of the approach, belonging to the so-called layer stripping algorithms, are the introduction of an equalization step that takes into account propagation effects, and the design of a procedure devoted to multiple reflections' removal. Furthermore, the proposed main processing block is an energy-based method able to accurately estimate amplitudes and time of delays of backscattered echoes in the time domain. A numerical analysis of the algorithm's potentialities will show that it can be successfully employed under different working conditions and in the presence of noisy data.","['Permittivity', 'Image reconstruction', 'Media', 'Dielectrics', 'Receivers', 'Algorithm design and analysis', 'Time-domain analysis']","['Layer stripping (LS)', 'multilayered media', 'nondestructive testing']"
"A new generation of multiwavelength lidars offers the potential to measure the structure and biochemistry of vegetation simultaneously, using range resolved spectral indices to overcome the confounding effects in passive optical measurements. However, the reflectance of leaves depends on the angle of incidence, and if this dependence varies between wavelengths, the resulting spectral indices will also vary with the angle of incidence, complicating their use in separating structural and biochemical effects in vegetation canopies. The Salford Advanced Laser Canopy Analyser (SALCA) dual-wavelength terrestrial laser scanner was used to measure the angular dependence of reflectance for a range of leaves at the wavelengths used by the new generation of multiwavelength lidars, 1063 and 1545 nm, as used by SALCA, DWEL, and the Optech Titan. The influence of the angle of incidence on the normalized difference index (NDI) of these wavelengths was also assessed. The reflectance at both wavelengths depended on the angle of incidence and could be well modelled as a cosine. The change in the NDI with the leaf angle of incidence was small compared with the observed difference in the NDI between fresh and dry leaves and between leaf and bark. Therefore, it is concluded that angular effects will not significantly impact leaf moisture retrievals or prevent leaf/bark separation for the wavelengths used in the new generation of 1063- and 1545-nm multiwavelength lidars.","['Laser radar', 'Vegetation mapping', 'Wavelength measurement', 'Measurement by laser beam', 'Scattering', 'Surface emitting lasers']","['Laser radar', 'remote sensing', 'technology assessment', 'vegetation']"
"A cold bias of ~-2 K was found for Channel 6 (13.3 μm) of the Imager instrument on the 13th of Geostationary Operational Environmental Satellite (GOES-13) during its postlaunch tests. Similar bias was found previously for GOES-12 and for other instruments (the High Resolution Infrared Radiation Sounder, the Moderate Resolution Imaging Spectroradiometer, and the Spinning Enhanced Visible and Infrared Imager) in the similar spectral region. It was often suspected that the spectral response function (SRF) of these instruments may be in error; in some cases, it had been demonstrated that an altered SRF can eliminate most of the differences between the measured and the expected values. Using products recently developed for the Global Space-based Inter-Calibration System, this paper concluded that an SRF error is the root cause for the GOES Imager Channel 6 bias. Based on this theory, an algorithm was developed to correct for the bias. Application of this correction to GOES-13 Imager Channel 6 resulted in an SRF shift of -2.1 cm -1 . The remaining biases have mean of nearly zero and much reduced standard deviation and are independent of the thermal structure of the interlaying atmosphere. This correction has also been successfully applied of other channels and of other GOES, which was described in a companion paper.","['Calibration', 'Instruments', 'Satellites', 'Atmospheric measurements', 'Sea measurements', 'Atmosphere', 'Water pollution']","['Calibration', 'Geostationary Operational Environmental Satellite (GOES)', 'Global Space-based Inter-Calibration System (GSICS)', 'spectral response']"
"The intercalibration of the infrared channels of the geostationary Meteosat/Spinning Enhanced Visible and InfraRed Imager (SEVIRI) satellite instruments shows that most channels are radiometrically consistent with those of Metop/IASI (Infrared Atmospheric Sounding Interferometer), which is used as a reference instrument. However, the 13.4-μm channel shows a cold bias of ~1 K in warm scenes, which changes with time. This is shown to be consistent with the contamination of SEVIRI by a layer of ice ~1 μm thick building up on the optics, which is believed to have condensed from water outgassed from the spacecraft. This ice modifies the spectral response functions and, hence, the weighting functions of the channels in stronger atmospheric absorption bands, thus introducing an apparent calibration error. Analysis of the radiometer's gain using an onboard black body source and a view of cold space confirms a loss consistent with transmission through a layer of comparable thickness, which also increases the radiometric noise-particularly for channels near the 12-μm libration band of water ice. Intercalibration, such as the Global Space-based Inter-Calibration System Correction, offers an empirical method to correct this bias.","['Ice', 'Decontamination', 'Instruments', 'Contamination', 'Calibration', 'Atmospheric modeling', 'Satellites']","['Calibration', 'decontamination', 'Earth-observing system', 'infrared (IR) image sensors', 'radiometers', 'satellites']"
"North Korea announced a second nuclear test on 25 May 2009, the first having taken place on October 9, 2006. Both tests were detected by the global seismic network of the Comprehensive nuclear Test-Ban-Treaty Organisation. We apply a correlation detector using a 10-s signal template from the 2006 test on the MJAR array in Japan to: 1) assess the potential for automatically detecting subsequent explosions at or near the test site; and 2) monitor the associated false alarm rate. The 2009 signal is detected clearly with no false alarms in a three-year period. By detecting scaled-down copies of the explosion signals submerged into background noise, we argue that a significantly smaller explosion at the site would have been detected automatically, with a low false alarm rate. The performance of the correlator on MJAR is not diminished by the signal incoherence that makes conventional array processing problematic at this array. We demonstrate that false alarm elimination by f-k analysis of single channel detection statistic traces is crucial for maintaining a low detection threshold. Correlation detectors are to be advocated as a routine complement to the existing pipeline detectors, both for reducing the detection threshold for sites of interest and providing automatic classification of signals from repeating sources.","['Arrays', 'Detectors', 'Monitoring', 'Correlation', 'Signal to noise ratio', 'Explosions']","['Array signal processing', 'arrays', 'correlation', 'detectors', 'matched filters', 'nuclear explosions', 'seismic waves', 'seismology']"
"With a growing number of different satellite sensors, data fusion offers great potential in many applications. In this work, a convolutional neural network (CNN) architecture is presented for fusing Sentinel-1 synthetic aperture radar (SAR) imagery and the Advanced Microwave Scanning Radiometer 2 (AMSR2) data. The CNN is applied to the prediction of Arctic sea ice for marine navigation and as input to sea ice forecast models. This generic model is specifically well suited for fusing data sources where the ground resolutions of the sensors differ with orders of magnitude, here 35 km × 62 km (for AMSR2, 6.9 GHz) compared with the 93 m × 87 m (for sentinel-1 IW mode). In this work, two optimization approaches are compared using the categorical cross-entropy error function in the specific application of CNN training on sea ice charts. In the first approach, concentrations are thresholded to be encoded in a standard binary fashion, and in the second approach, concentrations are used as the target probability directly. The second method leads to a significant improvement in R 2 measured on the prediction of ice concentrations evaluated over the test set. The performance improves both in terms of robustness to noise and alignment with mean concentrations from ice analysts in the validation data, and an R2 value of 0.89 is achieved over the independent test set. It can be concluded that CNNs are suitable for multisensor fusion even with sensors that differ in resolutions by large factors, such as in the case of Sentinel-1 SAR and AMSR2.","['Sea ice', 'Synthetic aperture radar', 'Microwave radiometry', 'Satellite broadcasting', 'Sensor fusion']","['Cryosphere', 'microwave radiometry', 'synthetic aperture radar (SAR) data']"
"L1regularization is used for finding sparse solutions to an underdetermined linear system. As sparse signals are widely expected in remote sensing, this type of regularization scheme and its extensions have been widely employed in many remote sensing problems, such as image fusion, target detection, image super-resolution, and others, and have led to promising results. However, solving such sparse reconstruction problems is computationally expensive and has limitations in its practical use. In this paper, we proposed a novel efficient algorithm for solving the complex-valuedL1regularized least squares problem. Taking the high-dimensional tomographic synthetic aperture radar (TomoSAR) as a practical example, we carried out extensive experiments, both with the simulation data and the real data, to demonstrate that the proposed approach can retain the accuracy of the second-order methods while dramatically speeding up the processing by one or two orders. Although we have chosen TomoSAR as the example, the proposed method can be generally applied to any spectral estimation problems.","['Synthetic aperture radar', 'Image reconstruction', 'Image resolution', 'Signal to noise ratio', 'Estimation', 'Remote sensing', 'Sensors']","['Basis pursuit denoising (BPDN)', 'L₁ regularization', 'proximal gradient (PG)', 'second-order cone programming (SOCP)', 'TomoSAR']"
"In this paper, a new framework of mixed sparse representations (MSRs) is proposed for solving ill-conditioned problems with remote sensing images. In general, it is very difficult to find a common sparse representation for remote sensing images because of complicated ground features. Here we regard a remote sensing image as a combination of subimage of smooth, edges, and point-like components, respectively. Since each domain transformation method is capable of representing only a particular kind of ground object or texture, a group of domain transformations are used to sparsely represent each subimage. To demonstrate the effect of the framework of MSR for remote sensing images, MSR is regarded as a prior for maximum a posteriori when solving ill-conditioned problems such as classification and super resolution (SR), respectively. The experimental results show that not only the new framework of MSR can improve classification accuracy but also it can construct a much better high-resolution image than other common SR methods. The proposed framework MSR is a competitive candidate for solving other remote sensing images-related ill-conditioned problems.","['Remote sensing', 'Spatial resolution', 'Hidden Markov models', 'Lenses', 'Wavelet transforms']","['Classification', 'compressive sensing (CS)', 'mixed sparse representations (MSRs)', 'super-resolution (SR)']"
"Considering that the statistics of the phase and the power of weather signals in the spectral domain are different from those statistics for echoes from stationary objects, a spectrum clutter identification (SCI) algorithm has been developed to detect ground clutter using single polarization radars, but SCI can be extended for dual-pol radars. SCI examines both the power and phase in the spectral domain and uses a simple Bayesian classifier to combine four discriminants: spectral power distribution, spectral phase fluctuations, spatial texture of echo power, and spatial texture of spectrum width to make decisions as to the presence of clutter that can corrupt meteorological measurements. This work is focused on detecting ground clutter mixed with weather signals, even if the clutter power to signal power ratio is low. The performance of the SCI algorithm is shown by applying it to radar data collected by University of Oklahoma-Polarimetric Radar for Innovation in Meteorology and Engineering.","['Clutter', 'Meteorology', 'Meteorological radar', 'Radar clutter', 'Educational institutions', 'Doppler effect']","['Bayesian methods', 'meteorological radar', 'radar clutter', 'radar detection', 'spectral analysis']"
"High-resolution inputs of rainfall are important in hydrological sciences, especially for urban hydrology. This is mainly because heavy rainfall-induced events such as flash floods can have a tremendous impact on society given their destructive nature and the short time scales in which they develop. With the development of technologies such as radars, satellites and (commercial) microwave links (CMLs), the spatiotemporal resolutions at which rainfall can be retrieved are becoming higher and higher. For the land surface of The Netherlands, we evaluate here four rainfall products, i.e., link-derived rainfall maps, Integrated Multisatellite Retrievals for Global Precipitation Measurement (IMERG) Final Run (IMERG-Global Precipitation Measurement mission), Meteosat Second Generation Cloud Physical Properties (CPP), and Nighttime Infrared Precipitation Estimation (NIPE). All rainfall products are compared against gauge-adjusted radar data, considered as the ground truth given its high quality, resolution, and availability. The evaluation is done for seven months at 30 min and 24h. Overall, we found that link-derived rainfall maps outperform the satellite products and that IMERG outperforms CPP and NIPE. We also explore the potential of a CML network to validate satellite rainfall products. Usually, satellite derived products are validated against radar or rain gauge networks. If data from CMLs would be available, this would be highly relevant for ground validation in areas with scarce rainfall observations, since link-derived rainfall is truly independent of satellite-derived rainfall. The large worldwide coverage of CMLs potentially offers a more extensive platform for the ground validation of satellite estimates over the land surface of the Earth.","['Spaceborne radar', 'Satellites', 'Microwave measurement', 'Meteorological radar', 'Spatial resolution', 'Rain']","['Cloud physical properties (CPP)', 'commercial microwave link (CML)', 'global precipitation measurement mission (GPM)', 'Integrated Multi-satellitE Retrievals for GPM (IMERG)', 'Meteosat Second Generation (MSG)', 'nighttime infrared (IR) precipitation estimation (NIPE)', 'radar', 'rain', 'satellites']"
"This paper considers the problem of coherent (in the sense that both amplitudes and relative phases of the polarimetric returns are used to construct the decision statistic) multipolarization synthetic aperture radar change detection starting from the availability of image pairs exhibiting possible power mismatches/miscalibrations. The principle of invariance is used to characterize the class of scale-invariant decision rules which are insensitive to power mismatches and ensure the constant false alarm rate property. A maximal invariant statistic is derived together with the induced maximal invariant in the parameter space which significantly compresses the data/parameter domain. A generalized likelihood ratio test is synthesized both for the cases of two- and three-polarimetric channels. Interestingly, for the two-channel case, it is based on the comparison of the condition number of a data-dependent matrix with a suitable threshold. Some additional invariant decision rules are also proposed. The performance of the considered scale-invariant structures is compared to those from two noninvariant counterparts using both simulated and real radar data. The results highlight the robustness of the proposed method and the performance tradeoff involved.","['Detectors', 'Linear matrix inequalities', 'Space vehicles', 'Orbits', 'Synthetic aperture radar', 'Testing', 'Receivers']","['Coherent change detection', 'maximal invariant', 'multipolarization', 'scale invariance']"
"This paper proposes a novel framework for fusing multi-temporal, multispectral satellite images and OpenStreetMap (OSM) data for the classification of local climate zones (LCZs). Feature stacking is the most commonly used method of data fusion but does not consider the heterogeneity of multimodal optical images and OSM data, which becomes its main drawback. The proposed framework processes two data sources separately and then combines them at the model level through two fusion models (the landuse fusion model and building fusion model) that aim to fuse optical images with landuse and buildings layers of OSM data, respectively. In addition, a new approach to detecting building incompleteness of OSM data is proposed. The proposed framework was trained and tested using the data from the 2017 IEEE GRSS Data Fusion Contest and further validated on one additional test (AT) set containing test samples that are manually labeled in Munich and New York. The experimental results have indicated that compared with the feature stacking-based baseline framework, the proposed framework is effective in fusing optical images with OSM data for the classification of LCZs with high generalization capability on a large scale. The classification accuracy of the proposed framework outperforms the baseline framework by more than 6% and 2% while testing on the test set of 2017 IEEE GRSS Data Fusion Contest and the AT set, respectively. In addition, the proposed framework is less sensitive to spectral diversities of optical satellite images and thus achieves more stable classification performance than the state-of-the-art frameworks.","['Satellites', 'Urban areas', 'Data integration', 'Meteorology', 'Artificial neural networks', 'Training', 'Radio frequency']","['Canonical correlation forest (CCF)', 'heterogeneous data fusion', 'local climate zones (LCZs)', 'OpenStreetMap (OSM)', 'satellite images']"
"With the advances of deep learning, many recent CNN-based methods have yielded promising results for image classification. In very high-resolution (VHR) remote sensing images, the contributions of different regions to image classification can vary significantly, because informative areas are generally limited and scattered throughout the whole image. Therefore, how to pay more attention to these informative areas and better incorporate them over long distances are two main challenges to be addressed. In this article, we propose a gated recurrent multiattention neural network (GRMA-Net) to address these problems. Because informative features generally occur at multiple stages in a network (i.e., local texture features at shallow layers and global profile features at deep layers), we use multilevel attention modules to focus on informative regions to extract more discriminative features. Then, these features are arranged as spatial sequences and fed into a deep-gated recurrent unit (GRU) to capture long-range dependency and contextual relationship. We evaluate our method on the UC Merced (UCM), Aerial Image dataset (AID), NWPU-RESISC (NWPU), and Optimal-31 (Optimal) datasets. Experimental results have demonstrated the superior performance of our method as compared to other state-of-the-art methods.","['Feature extraction', 'Remote sensing', 'Logic gates', 'Semantics', 'Visualization', 'Recurrent neural networks', 'Satellites']","['Gated recurrent unit (GRU)', 'multilevel attention mechanism', 'scene classification', 'very high-resolution (VHR) remote sensing']"
"Modern operational forest inventory often uses remotely sensed data that cover the whole inventory area to produce spatially explicit estimates of forest properties through statistical models. The data obtained by airborne light detection and ranging (LiDAR) correlate well with many forest inventory variables, such as the tree height, the timber volume, and the biomass. To construct an accurate model over thousands of hectares, LiDAR data must be supplemented with several hundred field sample measurements of forest inventory variables. This can be costly and time consuming. Different LiDAR-data-based and spatial-data-based sampling designs can reduce the number of field sample plots needed. However, problems arising from the features of the LiDAR data, such as a large number of predictors compared with the sample size (overfitting) or a strong correlation among predictors (multicollinearity), may decrease the accuracy and precision of the estimates and predictions. To overcome these problems, a Bayesian linear model with the singular value decomposition of predictors, combined with regularization, is proposed. The model performance in predicting different forest inventory variables is verified in ten inventory areas from two continents, where the number of field sample plots is reduced using different sampling designs. The results show that, with an appropriate field plot selection strategy and the proposed linear model, the total relative error of the predicted forest inventory variables is only 5%-15% larger using 50 field sample plots than the error of a linear model estimated with several hundred field sample plots when we sum up the error due to both the model noise variance and the model's lack of fit.","['Predictive models', 'Data models', 'Training', 'Laser radar', 'Biological system modeling', 'Matrix decomposition', 'Bayes methods']","['Bayesian linear model', 'model-based forest inventory', 'regularization', 'sampling design', 'singular value decomposition (SVD)']"
"A novel approach in addressing cyclone global navigation satellite system (CyGNSS) intersatellite and GPS-related calibration issues is proposed, based on a track-wise \sigma ^{o} bias correction method. This method makes use of both ancillary data from numerical weather prediction models and a semiempirical geophysical model function. Care is taken, so the track-wise \sigma ^{o} bias correction maintains CyGNSS signal sensitivity. Both intersatellite and GPS-related calibration issues are removed after correction. Long-term \sigma ^{o} downward trend, observed throughout the CyGNSS mission, is greatly reduced. Using the corrected \sigma ^{o} measurements, a wind retrieval method is also presented and its product thoroughly assessed for a three-year period against European Centre for Medium-Range Weather Forecasts (ECMWFs), Advanced Scatterometer (ASCAT) A/B, Advanced Microwave Scanning Radiometer (AMSR)-2, GMI, WindSat, hurricane weather research and forecasting (HWRF) model, and the stepped frequency microwave radiometer (SFMR) winds. The overall wind speed bias and standard deviation of the error (stde) against ECMWF are 0.16 and 1.19 m/s, while these are −0.11 and 1.12 m/s against ASCAT A/B, respectively. The same metrics against AMSR-2/GMI/WindSat (combined) are −0.19 and 1.11 m/s, respectively. The bias and stde against soil moisture active passive (SMAP) are −0.38 and 1.90 m/s, respectively. In the tropical cyclone environment, the bias and stde against HWRF are −0.54 and 2.90 m/s, and −4.71 and 5.88 m/s with SFMR. Finally, CyGNSS wind performance is gauged in the presence of rain. Below 10 m/s, the bias between CyGNSS and ECMWF increases as the rain rate increases. Between 10 and 15 m/s, biases are mostly absent. Above 15 m/s, results are inconclusive due to the low number of collocated rain samples. Overall, the presented CyGNSS wind speed product both exhibits consistency and reliability, showing promise of using GNSS-R derived winds for operational purposes.","['Wind speed', 'Global Positioning System', 'Sea surface', 'Wind forecasting', 'Space vehicles', 'Predictive models', 'Calibration']","['Geophysical measurements', 'global positioning system', 'microwave reflectometry', 'radar measurements', 'remote sensing', 'scattering', 'sea surface', 'wind']"
"This paper deals with detection of oil spills from multipolarization synthetic aperture radar images. The problem is cast in terms of a composite hypothesis test aimed at discriminating between the polarimetric covariance matrix (PCM) equality (absence of oil spills in the tested region) and the situation where the region under test exhibits a PCM with at least an ordered eigenvalue smaller than that of a reference covariance. This last setup reflects the physical condition where the backscattering associated with the oil spills leads to a signal, in some eigendirections, weaker than the one gathered from a reference area where the absence of any oil slicks is a priori known. A multifamily generalized likelihood ratio test approach is pursued to come up with an adaptive detector ensuring the constant false alarm rate property. At the analysis stage, the behavior of the new architecture is investigated in comparison with a benchmark (but nonimplementable) structure and some other suboptimum adaptive detectors available in the open literature. This study, which is conducted in the presence of both simulated and real data, confirms the practical effectiveness of the new approach.","['Oils', 'Covariance matrices', 'Phase change materials', 'Synthetic aperture radar', 'Linear matrix inequalities', 'Eigenvalues and eigenfunctions', 'Detectors']","['Constant false alarm rate (CFAR)', 'covariance matrix equality', 'invariance', 'multifamily generalized likelihood ratio test (MGLRT)', 'oil spill detection', 'one-sided generalized likelihood ratio test (GLRT)']"
"In hyperspectral images, some spectral bands suffer from low signal-to-noise ratio due to noisy acquisition and atmospheric effects, thus requiring robust techniques for the unmixing problem. This paper presents a robust supervised spectral unmixing approach for hyperspectral images. The robustness is achieved by writing the unmixing problem as the maximization of the correntropy criterion subject to the most commonly used constraints. Two unmixing problems are derived: the first problem considers the fully constrained unmixing, with both the nonnegativity and sum-to-one constraints, while the second one deals with the nonnegativity and the sparsity promoting of the abundances. The corresponding optimization problems are solved using an alternating direction method of multipliers (ADMM) approach. Experiments on synthetic and real hyperspectral images validate the performance of the proposed algorithms for different scenarios, demonstrating that the correntropy-based unmixing with ADMM is particularly robust against highly noisy outlier bands.","['Hyperspectral imaging', 'Optimization', 'Robustness', 'Kernel', 'Noise measurement', 'Convex functions']","['Alternating direction method of multipliers (ADMM)', 'correntropy', 'hyperspectral image', 'maximum correntropy estimation', 'unmixing problem']"
"The Advanced Technology Microwave Sounder (ATMS) on board the Suomi National Polar-orbiting Partnership (NPP) satellite is a total power radiometer and scans across the track with a range of ±52.77° from nadir. It has 22 channels and measures the microwave radiation at either quasi-vertical or quasi-horizontal polarization from the Earth's atmosphere. The ATMS scanning reflector is made of beryllium coated with gold and can have an emission due to the surface roughness. During prelaunch phase, an estimate of the reflector emissivity was not explored. In this paper, a new methodology is developed to assess the antenna emission from the ATMS pitch-over observations. It is found that the antenna emission is significant and dominates the scan-angle-dependent features in the ATMS antenna temperatures. Retrieved emissivity from K- to G-bands ranges from 0.002 to 0.006. An error model was also developed to assess the impact of antenna emissivity to calibration accuracy of antenna temperature products. Simulation results show that the calibration error is scene temperature dependent and can be as large as 2.5 K for space view.","['Instruments', 'Microwave antennas', 'Antenna measurements', 'Microwave measurement', 'Microwave radiometry', 'Satellites']","['Advanced technology microwave sounder (ATMS)', 'antenna emissivity', 'Suomi National Polar-orbiting Partnership (SNPP)']"
"NASA's Ice, Cloud, and land Elevation Satellite (ICESat), which operated between 2003 and 2009, made the first satellite-based global lidar measurement of earth's ice sheet elevations, sea-ice thickness, and vegetation canopy structure. The primary instrument on ICESat was the Geoscience Laser Altimeter System (GLAS), which measured the distance from the spacecraft to the earth's surface via the roundtrip travel time of individual laser pulses. GLAS utilized pulsed lasers and a direct detection receiver consisting of a silicon avalanche photodiode and a waveform digitizer. Early in the mission, the peak power of the received signal from snow and ice surfaces was found to span a wider dynamic range than anticipated, often exceeding the linear dynamic range of the GLAS 1064-nm detector assembly. The resulting saturation of the receiver distorted the recorded signal and resulted in range biases as large as ~50 cm for ice- and snow-covered surfaces. We developed a correction for this “saturation range bias” based on laboratory tests using a spare flight detector, and refined the correction by comparing GLAS elevation estimates with those derived from Global Positioning System surveys over the calibration site at the salar de Uyuni, Bolivia. Applying the saturation correction largely eliminated the range bias due to receiver saturation for affected ICESat measurements over Uyuni and significantly reduced the discrepancies at orbit crossovers located on flat regions of the Antarctic ice sheet.","['Optical surface waves', 'Measurement by laser beam', 'Receivers', 'Surface emitting lasers', 'Detectors', 'Ice']","['Ice sheets', 'laser ranging', 'lidar', 'remote sensing', 'satellite laser altimetry']"
"We discuss the effects of the clutter on geosynchronous SAR systems exploiting long integration times (from minutes to hours) to counteract for two-way propagation losses and increase azimuth resolution. Only stable targets will be correctly focused whereas unstable targets will spread their energy along azimuth direction. We derive here a generic model for the spreading of the clutter energy based on the power spectral density of the clutter itself. We then assume the Billingsley Intrinsic Clutter Motion model, representing the clutter power spectrum as an exponential decay, and derive the expected GEOSAR signal-to-clutter ratio. We also provide some results from a Ground Based RADAR experiment aimed at assessing the long-term clutter statistics for different scenarios to complement the Internal Clutter Motion model, mainly derived for windblown trees. Finally, we discuss the expected performances of two GEOSAR systems with different acquisition geometries.","['Clutter', 'Decorrelation', 'Synthetic aperture radar', 'Azimuth', 'Focusing', 'Thyristors']","['Focusing', 'Geosynchronous Synthetic Aperture Radar (GEOSAR)', 'scene decorrelation', 'wind-blown clutter']"
"Monitoring ground deformations arising from groundwater dynamics in dense urban coastal terrains is crucial for the sustainable development of infrastructures in these highly populated areas. The city of Montreal, which is located in the Saint-Laurent plain in eastern Canada, with its fast-growing populations, is a unique case study for other similar cities in coastal terrains. The city undergoes high-level house foundation damages with densities reaching up to 89 repairs/km 2 resulting from time-dependent ground deformations that are correlated to groundwater dynamics and evapotranspiration. Using Radarsat-2 C-Band synthetic aperture radar interferometry, we observe 3- to 5-mm ground line-of-sight displacement variations temporally outphased by few months relative to the 2-m subartesian aquifer hydraulic head variations. The deformations are observed over a 60-km 2 area located in the central part of the Montreal Island in Canada, from 2008 to 2010. We observe displacements of ~1 mm/year uplift in the areas covered by 15-m-thick clay layer. These displacements are well correlated to the number of house repairs. We also observe ~2 mm/year subsidence on elevated terrains, associated with evapotranspiration. The amplitudes of the displacements observed during this two-year study are significant when integrated over the average lifetime of urban structures. We conclude that the observed ground deformations are related to the seasonal variation of hydraulic head in most of the areas of Montreal. Moreover, wetter climate forecasts over upcoming decades for this area, will accentuate groundwater level fluctuations; thus, more ground deformations are foreseen, and have to be considered in future infrastructure design standards.","['Maintenance engineering', 'Soil', 'Sea measurements', 'Synthetic aperture radar', 'Surface topography', 'Urban areas', 'Rocks']","['Hydrogeology and remote sensing', 'interferometric synthetic aperture radar (InSAR)']"
"A technique to estimate tropical cyclone (TC) current intensity based on geostationary satellite infrared window (IRW) and water vapor (WV) imagery is explored in this paper. First, to combine the advantages of the IRW imagery and the WV minus IRW (WV-IRW) imagery, a WV-IRW-to-IRW ratio (WIRa)-based indicator is proposed. This indicator not only can display the inner-core convection's symmetrization level and vigor but also is able to screen out thin cirrus, stratospheric WV anomaly, and overshooting tops from average deep convection. It is highly correlated with the best track minimum sea-level pressure and thus used to estimate the western North Pacific TC current intensity. Detailed analyses have demonstrated that the WIRa-based indicator can further improve the estimation of TC current intensity alongside the existing algorithms. The WIRa-based indicator is designed based on the hypothesis that “overshooting top is more useful for forecasting than initial estimation,” and the satisfying results of the WIRa-based method perhaps provide indirect evidence to support this hypothesis in turn.","['Satellites', 'Estimation', 'Tropical cyclones', 'Clouds', 'Aircraft', 'Reconnaissance', 'Databases']","['Meteorology', 'remote sensing', 'satellite applications', 'tropical cyclone']"
"The longevity and dispersion of smoke and associated chemical constituents released from wildfire events are dependent on several factors, crucially including the height at which the smoke is injected into the atmosphere. The aim here is to provide improved emission data for the initialization of chemical transport models in order to better predict aerosol and trace gas dispersion following injection into the free atmosphere. A new stereo-matching algorithm, named M6, which can effectively resolve smoke plume injection heights (SPIH), is presented here. M6 is extensively validated against two alternative spaceborne earth observation SPIH data sources and demonstrates good agreement. Further, due to the spectral and dual-view configuration of the Advanced Along-Track Scanning Radiometer imaging system, it is possible to automatically differentiate smoke from other atmospheric features effectively-a feat, which currently no other algorithm can achieve. Additionally, as the M6 algorithm shares a heritage with the other M-series matchers, it is here compared against one of its predecessors, M4, which, for the determination of SPIH, M6 is shown to substantially outperform.",[],[]
"The applicability of deep learning to remote sensing is rapidly increasing in accordance with the improvement in spatiotemporal resolution of satellite images. However, unlike satellite images acquired in near-real-time over wide areas, there are limited amount of labeled data used for model training. In this article, three kinds of deep learning applications-data augmentation, semisupervised classification, and domain-adapted architecture-were tested in an effort to overcome the limitation of insufficient labeled data. Among the diverse tasks that can be used for classification, rice paddy detection in South Korea was performed for its ability to fully utilize the advantages of deep learning and high spatiotemporal image resolution. In the process of designing each application, the domain knowledge of remote sensing and rice phenology was integrated. Then, all possible combinations of the three applications were examined and evaluated with pixel-based comparisons in various environments and city-level comparisons using national statistics. The results of this article indicated that all combinations of the applications can contribute to increase classification performance, even though the uncertainty involved in imitating or utilizing unlabeled data remains. As the effectiveness of the proposed applications was experimentally confirmed, enhancement in the applicability of deep learning was expected in various remote sensing areas. In particular, the proposed applications would be significant when they are applied to a wide range of study areas and high-resolution images, as they tend to require a large amount of learning data from diverse environments, owing to high intra-class heterogeneity.","['Machine learning', 'Image resolution', 'Remote sensing', 'Satellites', 'Feature extraction', 'Synthetic aperture radar', 'Spatiotemporal phenomena']","['Data augmentation', 'data labeling', 'deep learning', 'domain adaptation', 'remote sensing', 'semisupervised classification']"
"Modeling ionospheric variability throughout a proper total electron content (TEC) parameter estimation is a demanding, however, crucial, process for achieving better accuracy and rapid convergence in precise point positioning (PPP). In particular, the single-frequency PPP (SF-PPP) method lacks accuracy due to the difficulty of dealing adequately with the ionospheric error sources. In order to apply ionosphere corrections in techniques, such as SF-PPP, external information of global ionosphere maps (GIMs) is crucial. In this article, we propose a deep learning model to efficiently predict TEC values and to replace the GIM-derived data that inherently have a global character, with equal or better in accuracy regional ones. The proposed model is suitable for predicting the ionosphere delay at different locations of receiver stations. The model is tested during different periods of time, under different solar and geomagnetic conditions and for stations in various latitudes, providing robust estimations of the ionospheric activity at the regional level. Our proposed model is a hybrid model comprising of a 1-D convolutional layer used for the optimal feature extraction and stacked recurrent layers used for temporal time series modeling. Thus, the model achieves good performance in TEC modeling compared to other state-of-the-art methods.","['Ionosphere', 'Global navigation satellite system', 'Delays', 'Time series analysis', 'Satellites', 'Receivers', 'Computational modeling']","['3-D tensor', 'global navigation satellite system', 'ionospheric variability', 'recurrent neural network (RNN)', 'total electron content (TEC)']"
"The popular Siamese convolutional neural networks (CNNs) for remote sensing (RS) image change detection (CD) often suffer from two problems. First, they either ignore the original information of bitemporal images or insufficiently utilize the difference information between bitemporal images, which leads to the low tightness of the changed objects. Second, Siamese CNNs always employ dual-branch encoders for CD, which increases computational cost. To address the above issues, this article proposes a network based on difference enhancement and spatial–spectral nonlocal (DESSN) for CD in very-high-resolution (VHR) images. This article makes threefold contributions. First, we design a difference enhancement (DE) module that can effectively learn the difference representation between foreground and background to reduce the impact of irrelevant changes on the detection results. Second, we present a spatial–spectral nonlocal (SSN) module that is different from vanilla nonlocal because multiscale spatial global features are incorporated to model the large-scale variation of objects during CD. The module can be used to strengthen the edge integrity and internal tightness of changed objects. Third, the asymmetric double convolution with Ghost (ADCG) module is exploited instead of standard convolution. The ADCG can not only refine the edge information of the changed objects, since horizontal and vertical convolutional kernels have good contour preservation advantages, but also greatly reduce the computational complexity of the proposed model. The experiments on two public VHR CD datasets demonstrate that the proposed network can provide higher detection accuracy and requires smaller memory usage than state-of-the-art networks.","['Convolution', 'Clustering algorithms', 'Feature extraction', 'Task analysis', 'Image edge detection', 'Robustness', 'Remote sensing']","['Change detection (CD)', 'difference enhancement (DE) module', 'Siamese convolutional neural networks (CNNs)', 'spatial-spectral nonlocal (SSN) module']"
"In ionospheric tomography, the atmospheric electron density is reconstructed from different electron density related measurements, most often from ground-based measurements of satellite signals. Typically, ionospheric tomography suffers from two major complications. First, the information provided by measurements is insufficient and additional information is required to obtain a unique solution. Second, with necessary spatial and temporal resolutions, the problem becomes very high dimensional, and hence, computationally infeasible. With Bayesian framework, the required additional information can be given with prior probability distributions. The approach then provides physically quantifiable probabilistic interpretation for all model variables. Here, Gaussian Markov random fields (GMRFs) are used for constructing the prior electron density distribution. The use of GMRF introduces sparsity to the linear system, making the problem computationally feasible. The method is demonstrated over Fennoscandia with measurements from global navigation satellite system (GNSS) and low Earth orbit (LEO) satellite receiver networks, GNSS occultation receivers, LEO satellite Langmuir probes, and ionosonde and incoherent scatter radar measurements.","['Extraterrestrial measurements', 'Satellites', 'Global navigation satellite system', 'Density measurement', 'Tomography', 'Geophysical measurements', 'Low earth orbit satellites']","['Bayesian', 'Gaussian Markov random fields (GMRFs)', 'ionospheric tomography', 'multi-instrument']"
"Spectral unmixing is an important task in hyperspectral image processing for separating the mixed spectral data pertaining to various materials observed aiming at analyzing the material components in observed pixels. Recently, nonlinear spectral unmixing has received particular attention in hyperspectral image processing, as there are many situations in which the linear mixture model may not be appropriate and could be advantageously replaced by a nonlinear one. Existing nonlinear unmixing approaches are often based on specific assumptions on the nonlinearity and can be less effective when used for scenes with unknown nonlinearity. This article presents an unsupervised nonlinear spectral unmixing method that addresses a general model that consists of a linear mixture part and an additive nonlinear mixture part. The structure of a deep autoencoder network, which has a clear physical interpretation, is specifically designed to achieve this purpose. Moreover, a convolutional neural network (CNN) is used to capture the spectral-spatial priors from hyperspectral data. Extensive experiments with synthetic and real data illustrate the generality and effectiveness of this scheme compared with state-of-the-art methods.","['Hyperspectral imaging', 'Mixture models', 'Photonics', 'Neural networks', 'Kernel', 'Decoding', 'Additives']","['3-D-convolutional neural network (CNN)', 'autoencoder network', 'hyperspectral imaging', 'nonlinear spectral unmixing']"
"Accurate determination of soil complex-dielectric-permittivity spectrum is important for various applications, especially for the development of soil moisture sensors that can be used, e.g., in agriculture and for environmental monitoring. Wideband measurement of soil dielectric spectrum requires the use of large-diameter coaxial transmission-line cells connected to a vector network analyzer (VNA). We present a new method for soil dielectric-spectrum characterization in the frequency range of 0.05-3 GHz. Our methodology is based on a wideband one-port VNA measurement of a soil sample inserted into a large-diameter coaxial cell. The key part of our approach is the use of a variable load terminating the coaxial cell to extract the scattering parameters of the sample, which are then fed into a dielectric permittivity extraction algorithm. The system provides quick and repeatable measurements without the use of flexible microwave cables. Also, application of a portable one-port VNA significantly lowers the cost of the system in comparison to two-port setups. We verify our methodology based on measurements of reference materials-polytetrafluoroethylene, isopropanol, and ethanol-and then apply it to characterize the soil samples with different moisture content and salinity. Experimental results confirm the validity of our approach.","['Transmission line measurements', 'Calibration', 'Dielectric measurement', 'Soil', 'Dielectrics', 'Soil measurements', 'Scattering parameters']","['Calibration', 'dielectric measurements', 'microwave measurements', 'soil measurements', 'soil moisture and salinity']"
"Underwater hyperspectral imaging is a relatively new method for characterizing seafloor composition. To date, it has been deployed from moving underwater vehicles, such as remotely operated vehicles and autonomous underwater vehicles. While moving vehicles allow relatively rapid surveying of several 10-1000 m2, they are subjected to short-term variations in vehicle attitude that often compromise image acquisition and quality. In this study, we tested a stationary platform that was landed on the seabed and used an underwater hyperspectral imager (UHI) on a vertical swinging bracket. The imaged seafloor areas have dimensions of 2.3 m × 1 m and are characterized by very stable UHI data of high spatial resolution. The study area was the Trans-Atlantic Geotraverse hydrothermal field at the Mid-Atlantic Ridge (26° N) in water depths of 3530-3660 m. UHI data were acquired a 12 stations on an active and an inactive hydrothermal sulfide mound. Based on supervised classification, 24 spectrally different seafloor materials were detected, including hydrothermal and non-hydrothermal materials, and benthic fauna. The results show that the UHI data are able to spectrally distinguish different types of surface materials and benthic fauna in hydrothermal areas, and may therefore represent a promising tool for high-resolution seafloor exploration in potential future deep-sea mining areas.","['Hyperspectral imaging', 'Minerals', 'Sea surface', 'Sea measurements', 'Data mining', 'Optical surface waves']","['Image classification', 'remote sensing', 'spectroscopy', 'terrain mapping', 'underwater object detection']"
"This paper investigates the use of an acoustic glider to perform acoustical meteorology. This discipline consists of analyzing ocean ambient noise to infer above-surface meteorological conditions. The paper focuses on wind speed estimation, in a near-shore marine environment. In such a shallow water context, the ambient noise field is complex, with site-dependent factors and a variety of nonweather concurrent acoustic sources. A conversion relationship between sound pressure level and wind speed is proposed, taking the form of an outlier-robust nonlinear regression model learned with in situ data. This method is successfully applied to experimental data collected in Massachusetts Bay (MA, USA) during four glider surveys. An average error in wind speed estimation of 1.3 m · s -1 (i.e., average relative error of 14%) over wind speed values up to 17 m · s -1 is reported with this method, which outperformed results obtained with relationships from the literature. Quantitative results are also detailed on the dependence of wind speed error estimation on the environment characteristics, and on the classification performance of observations contaminated by acoustic sources other than wind. Passive acoustic-based weather systems are a promising solution to provide long-term in situ weather data with fine time and spatial resolutions. These data are crucial for satellite calibration and assimilation in meteorological models. From a broader perspective, this paper is the first step toward an operationalization of acoustic weather systems and their on-board embedding in underwater monitoring platforms such as gliders.","['Wind speed', 'Acoustics', 'Sea measurements', 'Estimation', 'Atmospheric modeling', 'Oceans']","['Acoustical meteorology', 'coastal glider', 'outlier-robust regression model']"
"Wildfires are one of the most destructive disasters on the planet. They also significantly impact the land surface. Satellite data have been widely used to detect the outbreak and monitor the expansion of fire incidents for damage assessment and disaster management. Polar-orbiting satellite data have been used for several decades but data from geostationary satellites, which can provide observations with a high temporal resolution, have received much less attention. This paper utilizes data from FengYun-2G, a Chinese geostationary satellite, to detect wildfires in two selected research regions in January 2016. The detection algorithm systemizes image-based analysis to filter out obvious nonfire pixels and temporal analysis to confirm the true detections. Fire detection is based on comparisons between predicted and observed values. The results show that the proposed method has some advantages compared with the use of polar-orbiting satellite data, including early detection and continuous observation. The validation work is conducted based on the collection 6.1 Global Monthly Fire Location Product generated from fire detections by Moderate Resolution Imaging Spectroradiometer (MODIS) sensors. The average accuracy within the target time is 56%, while the omission error rate is over 78%. In detail, the algorithm has a lower omission error rate in Australia while it fails in detecting most of the fire pixels in India. The dominance of small fire incidents, as well as low spatial resolution greatly limit the detection ability. Many small fires were beyond the ability of Stretched Visible and Infrared Spin Scan Radiometer (S-VISSR) data when no significant fire characteristics could be captured. Future development of the algorithm will focus on improving the results by enhancing the adaption to different regions, as well as, including multisource data sets.","['Satellite broadcasting', 'Spatial resolution', 'MODIS', 'Geostationary satellites', 'Detection algorithms']","['Active fire detection algorithm', 'FengYun-2G', 'geostationary satellite data']"
"The magnetic polarizability tensor (MPT) has attracted considerable interest due to the possibility it offers for characterizing conducting objects and assisting with the identification and location of hidden targets in metal detection. An explicit formula for its calculation for arbitrary-shaped objects is missing in the electrical engineering literature. Furthermore, the circumstances for the validity of the magnetic dipole approximation of the perturbed field, induced by the presence of the object, are not fully understood. On the other hand, in the applied mathematics community, an asymptotic expansion of the perturbed magnetic field has been derived for small objects and a rigorous formula for the calculation of the MPT has been obtained. The purpose of this paper is to relate the results of the two communities, to provide a rigorous justification for the MPT, and to explain the situations in which the approximation is valid.","['Harmonic analysis', 'Metals', 'Tensile stress', 'Sea measurements', 'Magnetic moments', 'Magnetic resonance imaging']","['Buried object detection', 'Eddy currents', 'magnetic induction', 'metal detectors', 'polarizability tensors']"
"We present a new framework, called multisensor coupled spectral unmixing (MuCSUn), that solves unmixing problems involving a set of multisensor time-series spectral images in order to understand dynamic changes of the surface at a subpixel scale. The proposed methodology couples multiple unmixing problems based on regularization on graphs between the time-series data to obtain robust and stable unmixing solutions beyond data modalities due to different sensor characteristics and the effects of nonoptimal atmospheric correction. Atmospheric normalization and cross calibration of spectral response functions are integrated into the framework as a preprocessing step. The proposed methodology is quantitatively validated using a synthetic data set that includes seasonal and trend changes on the surface and the residuals of nonoptimal atmospheric correction. The experiments on the synthetic data set clearly demonstrate the efficacy of MuCSUn and the importance of the preprocessing step. We further apply our methodology to a real time-series data set composed of 11 Hyperion and 22 Landsat-8 images taken over Fukushima, Japan, from 2011 to 2015. The proposed methodology successfully obtains robust and stable unmixing results and clearly visualizes class-specific changes at a subpixel scale in the considered study area.","['Satellites', 'Robustness', 'Earth', 'Remote sensing', 'Optimization', 'Atmospheric modeling', 'Mixture models']","['Change detection', 'coupled spectral unmixing', 'multisensor data fusion', 'time-series analysis']"
"There is much current interest in using multisensor airborne remote sensing to monitor the structure and biodiversity of woodlands. This paper addresses the application of nonparametric (NP) image-registration techniques to precisely align images obtained from multisensor imaging, which is critical for the successful identification of individual trees using object recognition approaches. NP image registration, in particular, the technique of optimizing an objective function, containing similarity and regularization terms, provides a flexible approach for image registration. Here, we develop a NP registration approach, in which a normalized gradient field is used to quantify similarity, and curvature is used for regularization (NGF-Curv method). Using a survey of woodlands in southern Spain as an example, we show that NGF-Curv can be successful at fusing data sets when there is little prior knowledge about how the data sets are interrelated (i.e., in the absence of ground control points). The validity of NGF-Curv in airborne remote sensing is demonstrated by a series of experiments. We show that NGF-Curv is capable of aligning images precisely, making it a valuable component of algorithms designed to identify objects, such as trees, within multisensor data sets.","['Laser radar', 'Hyperspectral imaging', 'Image registration', 'Feature extraction', 'Sensors']","['Aerial photograph', 'hyperspectral image', 'image registration', 'light detection and ranging (LiDAR)', 'remote sensing']"
"Moonglint and sunglint satellite imagery acquired from Moderate Resolution Imaging Spectroradiometer (MODIS) and Visible Infrared Imaging Radiometer Suite (VIIRS) satellite sensors were used to study the characteristics of internal waves (IWs) and their relationship to tides and stratification in the Sulu-Celebes Sea. Tidally generated IWs propagate northwestward and southeastward into the Sulu and Celebes Seas, showing a spring-neap tidal cycle behavior with larger/smaller IWs occurring under spring/neap tides, respectively. The length of IW crests, their wavelength, and phase speed derived from the sunglint/moonglint image pairs were observed to decrease from spring to neap tides. IWs in the Sulu-Celebes Sea which are correlated with oceanic stratification display seasonal and regional fluctuations. In the Celebes Sea, IWs were less frequently observed but propagated with faster phase speeds and larger scales compared to those in the Sulu Sea. Comparisons of stratification and barotropic tidal transport among the IWs generation sites indicate that the shallowing maximum Brunt- Väisälä frequency and strongest tidal transport around Pearl Bank are major factors contributing to greater IW activity compared to other generation sites. IWs generated near Pangutaran Island and Pearl Bank further merge into one dynamical system, creating longer wave crests of ~200 km that travel at faster phase speed (~2.5 m·s -1 ) in the Sulu Sea. This paper demonstrates a new application using a combination of sunglint/moonglint imagery in tracking an important oceanic phenomenon.","['Satellite broadcasting', 'MODIS', 'Imaging', 'Tides', 'Ocean temperature', 'Sea measurements']","['Internal wave (IWs)', 'Moderate Resolution Imaging Spectroradiometer (MODIS)', 'moonglint', 'stratification', 'Sulu–Celebes Sea', 'sunglint', 'tide', 'Visible Infrared Imaging Radiometer Suite (VIIRS)']"
"We present a new high-fidelity method of calibrating a cross-track scanning microwave radiometer using Global Positioning System (GPS) radio occultation (GPSRO) measurements. The radiometer and GPSRO receiver periodically observe the same volume of atmosphere near the Earth's limb, and these overlapping measurements are used to calibrate the radiometer. Performance analyses show that absolute calibration accuracy better than 0.25 K is achievable for temperature sounding channels in the 50-60-GHz band for a total-power radiometer using a weakly coupled noise diode for frequent calibration and proximal GPSRO measurements for infrequent (approximately daily) calibration. The method requires GPSRO penetration depth only down to the stratosphere, thus permitting the use of a relatively small GPS antenna. Furthermore, only coarse spacecraft angular knowledge (approximately one degree rms) is required for the technique, as more precise angular knowledge can be retrieved directly from the combined radiometer and GPSRO data, assuming that the radiometer angular sampling is uniform. These features make the technique particularly well suited for implementation on a low-cost CubeSat hosting both radiometer and GPSRO receiver systems on the same spacecraft. We describe a validation platform for this calibration method, the Microwave Radiometer Technology Acceleration (MiRaTA) CubeSat, currently in development for the National Aeronautics and Space Administration (NASA) Earth Science Technology Office. MiRaTA will fly a multiband radiometer and the Compact TEC/Atmosphere GPS Sensor in 2015.","['Microwave radiometry', 'Calibration', 'Refractive index', 'Terrestrial atmosphere', 'Brightness temperature', 'Microwave theory and techniques', 'Atmospheric modeling']","['Advanced Microwave Sounding Unit (AMSU)', 'Advanced Technology Microwave Sounder (ATMS)', 'calibration', 'Compact Total Electron Count (TEC)/Atmosphere Global Positioning System (GPS) Sensor (CTAGS)', 'CubeSat', 'Global Navigation Satellite System (GNSS)', 'GPS', 'GPS radio occultation (RO) (GPSRO)', 'humidity', 'Micro-sized Microwave Atmospheric Satellite (MicroMAS)', 'microwave', 'Microwave Radiometer Technology Acceleration (MiRaTA)', 'nanosatellite', 'precipitation', 'radiometer', 'remote sensing', 'RO', 'RO-Cal', 'temperature']"
"We discuss the optimization of components in a single-wavelength airborne laser bathymeter that is intended for a low-power unmanned aerial vehicle platform. The theoretical minimum energy requirement to detect the submerged sea floor in shallow (<; 5 m) water using a low signal-to-noise ratio (LSNR) detection methodology is calculated. Results are presented from tests of a prototype light detection and ranging (LiDAR) instrument that was developed by the University of Florida, Gainesville. A green wavelength (532 nm), 100-beamlet, low-energy (35-nJ/beamlet), short-pulse (480 ps) laser ranging system was operated from a low-altitude (500-m) aircraft, with a multichannel sensor that is capable of single photoelectron sensitivity and multiple stops. Data that were collected during tests display vertical structure in shallow-water areas based on fixed threshold crossings at a single-photon sensitivity level. A major concern for the binary detection strategy is the reliable identification and removal of noise events. Potential causes of ranging errors related to photomultiplier tube afterpulsing, impedance mismatching, and gain block overdrive are described. Data collection/processing solutions based on local density estimation are explored. Previous studies on LSNR performance metrics showed that short (15-cm) dead time could be expected in the case of multiple scattering objects, indicating the possibility of seamless topographic/bathymetric mapping with minimal discontinuity at the waterline. LiDAR depth estimates from airborne profiles are compared to on-site measurements, and near-shore submerged feature identification is presented.","['Laser radar', 'Scattering', 'Distance measurement', 'Detectors', 'Photonics', 'Receivers']","['Airborne laser swath mapping (ALSM)', 'bathymetry', 'light detection and ranging (LiDAR)', 'photonics']"
"This paper gives a detailed description of the prelaunch and in-orbit calibrations of the Mercury Laser Altimeter (MLA) on the MErcury Surface, Space ENvironment, GEochemistry, and Ranging (MESSENGER) mission, which was launched on August 3, 2004 and has been operating in orbit about Mercury since March 2011. A brief summary of the MLA instrument is given, followed by the instrument measurement model and calibration formulas. The prelaunch tests used to determine the values of various calibration coefficients are described. The boresight alignment parameters were verified and recalibrated by special tests, with the MESSENGER spacecraft en route to Mercury. The MLA instrument model and the calibration methods were largely derived from airborne and spaceborne lidar for Earth science observation at the NASA Goddard Space Flight Center and will benefit future space lidar developments for Earth and space science.","['Space vehicles', 'Instruments', 'Receivers', 'Measurement by laser beam', 'Extraterrestrial measurements', 'Laser theory']","['Laser ranging', 'lidar']"
"Along-track multichannel synthetic aperture radar is usually used to achieve ground moving target detection and imaging. Nevertheless, there is a design dilemma between azimuth high resolution and wide swath (HRWS). To solve this problem in HRWS mode, we introduce a virtual multichannel (VMC) scheme. For each virtual channel, the low real pulse repetition frequency (PRF) improves the ability of resolving range ambiguity for wide-swath, and the high virtual PRF improves the capability of resolving Doppler ambiguity for azimuth high resolution. For multiple virtual channels, strong ground clutter is eliminated by the joint VMC processing. Furthermore, a detailed signal model of a moving target in the virtual channel is given, and the special false-peak effect in the azimuthal image is analyzed. Moreover, we propose a novel ground moving target processing method based on the VMC scheme and the clutter suppression interferometry (CSI) technique, which is called VMC-CSI. The integration of detection, location, velocity estimation, and imaging for ground moving targets can be achieved. Accounting for the unresolved main peak and false peak for a moving target, in the VMC-CSI method, we adopt a two-step scheme to estimate the radial velocity and along-track velocity, namely, rough estimation and precise estimation. Meanwhile, considering the same interferometric phases of the main peak and the false peak, we use false peaks first for the robustness of initial azimuth location estimation and remove false peaks afterward. Numerical simulations are provided for testing the effect of the false peak and the effectiveness of VMC-CSI.","['Synthetic aperture radar', 'Imaging', 'Radar imaging', 'Clutter', 'Image resolution', 'Doppler effect']","['Ground moving target', 'high resolution and wide swath (HRWS)', 'multichannel synthetic aperture radar (SAR)', 'SAR', 'virtual channel']"
"Multi-temporal interferometric synthetic aperture radar (MT-InSAR) is used for many applications in earth observation. Most MT-InSAR methods select scatterers with high coherence throughout the entire time series. However, as time series lengthen, inevitable changes in surface scattering lead to decorrelation, which systematically decreases the number of coherent scatterers. Here, we propose a novel method to detect and process temporary coherent scatterers (TCS) by subsequently analyzing the amplitude and the interferometric phase. Two hypothesis tests are developed for amplitude analysis in order to identify the moments of appearing and/or disappearing coherent scatterers. Based on the amplitude analysis, the parameters of interest are then estimated using the interferometric phase. An optimized adaptive temporal subset approach is proposed to improve the precision of the estimated parameters. If the scatterers are not evenly distributed over the area, a secondary (support) network is designed to improve the spatial point distribution. The main advantage of this method is the reliable extraction of a subset of time series without using any contextual information. Experimental results show that the TCSs significantly increase the number of observations for displacement monitoring and improve the change detection capability in urban construction areas.","['Time series analysis', 'Coherence', 'Decorrelation', 'Probability density function', 'Signal to noise ratio', 'Synthetic aperture radar']","['Change detection', 'multi-temporal InSAR', 'Rayleigh distribution', 'temporary coherent scatterer']"
"Backscattering in Ku-band (13 GHz) over continental surfaces is analyzed, with the Tropical Rainfall Measurement Mission/Precipitation Radar instruments (incidence angles from 0° to 18°), along with observations from the Topex-Poseidon nadir-looking altimeter and the QuikSCAT scatterometer (incidence angles around 50°). The signals from the three instruments are very consistent. The backscattering tends to decrease with increasing vegetation density, as expected, making it possible to classify vegetation density with active microwaves. Over the northern African desert, a very large spatial variability of the backscattering is observed, with both surface and volume scatterings contributing to the signals. The use of multiangle observations does help characterizing the desert types, but in some areas, the ambiguity of the signals is still unexplained. The French-Chinese joint mission “Chinese-French Oceanic SATellite” will carry two active microwave instruments with a large range of incidence angles, from 0° to 50°. We show that the combined use of observations at low and high incidence angles adds information, particularly over desert surfaces.","['Backscatter', 'Sea surface', 'Land surface', 'Instruments', 'Spaceborne radar', 'Vegetation mapping', 'Radar measurements']","['Altimetry', 'radar remote sensing', 'surface roughness']"
"In this paper, we focus on ground-penetrating radar (GPR) for infrastructural health monitoring, especially for the monitoring of reinforced concrete (RC) bridge slab. Due to the demand of noncontact and high-speed monitoring technique which can handle vast amounts of aging infrastructures, GPR is a promising tool. However, because radar images consist of many reflected waves, they are usually difficult to interpret. Furthermore, the spatial resolution of system is not enough considering the thickness of target damages, cracks, and segregation are millimeter-to-centimeter order while the wavelength of ordinary GPR ultrahigh-frequency band is over 10 cm. To address these problems, for the purpose of sensitive damage detection, we propose a new algorithm based on deconvolution utilizing a super high-frequency (SHF) band system. First, a distribution of reflection coefficient is inversely estimated by 1-D bridge slab model. Because concrete is found to be a lossy medium at SHF band, we consider the attenuation of signal in deconvolution. The algorithm is called “time-variant deconvolution” in this paper. After the validation by simulation, the effects of the algorithm and frequency band on damage detection accuracy are evaluated by a field experiment. Though the results show a 1-mm horizontal crack is not detected by measured waves, when it is filled with water, it is detected by time-variant deconvolution. Moreover, the 1-mm dried crack is detected only by time-variant deconvolution at SHF band, which greatly emphasizes the peaks of the reflection coefficient of the crack.","['Bridges', 'Slabs', 'Deconvolution', 'Ground penetrating radar', 'Attenuation', 'Concrete']","['Ground-penetrating radar (GPR)', 'infrastructural health monitoring', 'thin cracks and segregation detection', 'time-variant deconvolution']"
"The cold calibration count from the Advanced Technology Microwave Sounder (ATMS) space view increases when the lunar radiation intrudes its antenna field of view (FOV). This increase is referred to as lunar contamination since the cold count is not matched with the specified brightness temperature of 2.73 K. For ATMS, it is found that the elapse time of lunar intrusion (LI) and the magnitude of the cold count increase are channel dependent. If the lunar-affected calibration counts are rejected in the processing, a data gap can be shown in brightness temperature at all channels. At ATMS channels 1 and 2, which have a large FOV, the LI can result in an increase of 40 counts in cold calibration. At higher frequency channels which have a smaller FOV size, the LI intensity is much stronger and can be as large as a few hundred counts. The LI becomes significant when its radiation appears in the ATMS antenna main beam. In the current ATMS operational calibration algorithm, the cold count anomaly is detected when the intensity of LI exceeds a certain threshold. The lunar radiation can be also corrected in the ATMS calibration. In doing so, a lunar radiation term is derived as a function of antenna gain, the solid angle of the Moon, and the brightness temperature of the Moon disk. This algorithm is applied in an ATMS calibration system developed at NOAA and shows a successful removal of all the lunar contamination on the earth-scene brightness temperature.","['Moon', 'Contamination', 'Space vehicles', 'Calibration', 'Brightness temperature', 'Microwave radiometry']","['Advanced Technology Microwave Sounder (ATMS)', 'calibration', 'contamination', 'lunar intrusion (LI)']"
"This article deals with the analysis of InSAR performance for large-scale deformation measurement. The study evaluates the use of models, especially numerical weather prediction reanalysis, to mitigate disturbances in SAR interferograms. The impact of such corrections is evaluated by analyzing short-time baseline phase variograms in order to derive a lower bound for the interferometric accuracy, especially at large distances. The variance is then propagated from single interferograms to deformation rates. Finally, using GNSS measurements, the predicted error bars are validated on a large Sentinel-1 data set.","['Strain', 'Synthetic aperture radar', 'Measurement uncertainty', 'Delays', 'Deformable models', 'Atmospheric measurements', 'Time series analysis']","['Deformation', 'GNSS', 'InSAR', 'numerical weather prediction (NWP)', 'performance']"
"Pansharpening technique is used to merge the original multispectral image (MS) with a high spatial resolution panchromatic image (PAN). Due to its robustness, the multiresolution analysis (MRA) is an important part of pansharpening. The scale regression model is effective for improving MRA. However, the existing MRA based on scale regression results into single-scale regression information, thus affecting the final pansharpening result. To address this problem, in this work, we propose a dual-scale regression-based MRA for pansharpening. First, we establish a scale regression-based model. Then, this model is improved using a high-pass modulation (HPM) injection scheme. Finally, the dual-scale information is added to the scale regression to construct the dual-scale regression for obtaining the final pansharpening result. We perform experiments using five datasets. The results show that the proposed method obtains a better pansharpening result as compared to various state-of-the-art MRA methods. In addition, the quantitative and qualitative analysis of the results shows that the proposed method achieves appropriate spatial and spectral resolution fusion. Therefore, it has a great potential in pansharpening technique.","['Pansharpening', 'Spatial resolution', 'Image resolution', 'Reactive power', 'Modulation', 'Matched filters', 'Laboratories']","['Dual scale regression', 'multiresolution analysis (MRA)', 'multispectral image (MS)', 'panchromatic image (PAN)', 'pansharpening']"
"The temporal variations (diurnal and annual) in arboreal (ε Tree ) and bare soil (ε Soil ) dielectric constants and their correlation with precipitation were examined for several trees in Japan. A significant (1 σ (standard deviation) and 2 σ) ε Tree increase is observed after rainfall at 89.8% and 90.5% probability. However, rainfall does not always induce significant ε Tree increases. Rainfall of more than 5 mm/day can induce 1 σ ε Tree Tree increase at a 59.6% probability. In order to examine whether the increase in εTree affects the L-band σ 0 variation in a forest, the four-year temporal variation of the L-band backscattering coefficient (σ 0 ) was estimated from observations by the Advanced Land Observing Satellite Phased Array type L-band Synthetic Aperture Radar. Observed maximum absolute deviations from the mean over the forest area were 1.0 and 1.2 dB for σ HH 0 and σ HV 0 , respectively, and 4.0 and 3.0 dB over open land. σ 0 and rainfall correlations show that ε Tree and σ Forest 0 are proportional to precipitation integrated over seven or eight days; ε Soil and σ Open land 0 are proportional to precipitation integrated over three days. This finding indicates that ε Tree variations influence σ Forest areas 0 . A stronger correlation between σ HV 0 and precipitation is observed in several sites with low σ HV 0 , where less biomass is expected, and several sites with high σ HV 0 , where more biomass is expected. A weaker correlation between σ HV 0 and precipitation is observed for several sites with high σ HV 0 . These differences may be explained by the different contributions of double bounce scattering and potential transpiration, which is a measure of the ability of the atmosphere to remove water from the surface through the processes of transpiration. The two other results were as follows: 1) The functional relation between aboveground biomass and σ 0 showed dependence on precipitation data, this being an effect connected with seasonal changes of the ε Tree . This experiment reinforces the fact that the dry season is preferable for retrieval of woody biomass from inversion of the functional dependence of SAR backscatter and for avoiding the influence of rainfall. 2) The complex dielectric constant for a tree trunk, which is measured between 0.2 and 6 GHz, indicates that free water is dominant in the measured tree.","['Vegetation', 'Dielectric measurement', 'Dielectric constant', 'Biomass', 'Probes', 'L-band', 'Soil measurements']","['Biomass', 'dielectric constant', 'forest', 'Phased Array type L-band Synthetic Aperture Radar (PALSAR)', 'temporal variation']"
"The MEdium Resolution Spectral Imager (MERSI) is the keystone instrument onboard FengYun-3 (FY-3). After FY-3A, FY-3B MERSI is the second launched in November 2010. Nineteen of the 20 MERSI spectral bands are the reflective solar bands, which cannot be absolutely calibrated onboard. The annual vicarious calibration (VC) based on synchronous in situ measurements at the Dunhuang site is the baseline calibration method for MERSI. To assure frequent and stable calibration updates, a multisite calibration tracking method is developed. This paper presents the FY-3B MERSI postlaunch daily calibration updating method based on multisite calibration tracking with the Dunhuang VC correction, the long-term sensor response on-orbit change, and the calibration performance evaluation. A reflectance-based method is used for the Dunhuang VC, and the reflectance calibration uncertainties are within 3% for most MERSI bands. The multisite calibration tracking method relies on simulated radiation over several stable sites without synchronous in situ measurements. A postlaunch daily calibration updating model is established using a linear function of days since launch to describe the long-term trend. The calibration updating model is validated by the Dunhuang VC, showing the relative bias within 3.5% for most bands. It is found that the shortwave channels of MERSI experience large degradation, particularly the 412-nm band with an annual degradation rate of approximately 18%, whereas most red and near-infrared bands are relatively stable. Using the calibration updating model with the Dunhuang VC correction, the recalibrated MERSI data are validated against Moderate Resolution Imaging Spectroradiometer by near synchronous-nadir-observation analysis, and good agreement is achieved.","['Calibration', 'MODIS', 'Satellites', 'Atmospheric measurements', 'Aerosols', 'Optical surface waves']","['Dunhuang site vicarious calibration (VC)', 'FY-3B MEdium Resolution Spectral Imager (MERSI)', 'multisite calibration tracking', 'on-orbit response change', 'reflective solar bands (RSBs)']"
"Image segmentation techniques are challenging to apply to large-size remote sensing imagery. Indeed, if the data to be processed are larger than the computer's available memory, it must be split into smaller pieces. Without precaution, segmentation errors appear along the edges of these pieces. The goal of this paper is to present a tilewise processing method to overcome this issue for superpixel segmentation, applied in particular to the simple linear iterative clustering algorithm. Incidentally, tilewise methods allow for several pieces of the image to be processed simultaneously, which enables the deployment of these methods in a parallel processing environment. Estimations of the speed-up when using multiple processors are provided. Then, it is demonstrated that the result of the tilewise segmentation is equivalent to the segmentation of the complete image, with respect to a number of global unsupervised segmentation criteria. Finally, experimental results on a large-size Sentinel-2 time series validate the method's feasibility.","['Image segmentation', 'Remote sensing', 'Clustering algorithms', 'Time series analysis', 'Image edge detection', 'Iterative algorithms', 'Lattices']","['Image processing', 'image segmentation', 'scalability', 'superpixel segmentation']"
"The Alpha Jet Atmospheric eXperiment (AJAX) is a project to measure the atmospheric profiles of greenhouse gases (GHGs) and ozone (O 3 ) regularly over California and Nevada. Airborne instruments measuring GHGs and O 3 are installed in a wing pod of an Alpha Jet aircraft and operated from the National Aeronautics and Space Administration Ames Research Center at Moffett Field, CA. The instruments yield precise and accurate in situ vertical profiles of atmospheric carbon dioxide (CO 2 ), methane (CH 4 ), and O 3 . Measurements of vertical profiles of GHGs and O 3 over Railroad Valley, NV have been conducted directly under the Greenhouse gases Observing SATellite (GOSAT) over passes on a monthly basis as part of the AJAX project since June 2011. The purpose of this work is to calculate aircraft-based dry-air mole fractions of the GHGs for the validation of GOSAT data products. This study expands and improves our previous comparisons by evaluating three algorithms against 24 months of in situ data collected over a Gain-M target. We used three different algorithms: Atmospheric CO 2 Observations from Space (ACOS v3.4r3), Remote Sensing of Greenhouse Gases for Carbon Cycle Modeling (RemoteC v2.3.5FP), and National Institute for Environmental Studies (NIES v2.11). We find that the CO 2 average differences of ACOS and RemoteC from AJAX are 0.26% and 0.24%, respectively. The difference between NIES and AJAX is 0.96%, which is higher than that of ACOS and RemoteC. The CH 4 average differences for RemoteC and NIES are 2.1% and 1.7%, respectively.","['Atmospheric measurements', 'Extraterrestrial measurements', 'Sea measurements', 'NASA', 'Aircraft', 'Global warming', 'Satellites']","['Atmospheric measurements', 'remote sensing', 'satellites']"
"Synthetic aperture radar tomography (TomoSAR) has been extensively employed in 3-D reconstruction in dense urban areas using high-resolution SAR acquisitions. Compressive sensing (CS)-based algorithms are generally considered as the state-of-the art in super-resolving TomoSAR, in particular in the single look case. This superior performance comes at the cost of extra computational burdens, because of the sparse reconstruction, which cannot be solved analytically, and we need to employ computationally expensive iterative solvers. In this article, we propose a novel deep learning-based super-resolving TomoSAR inversion approach,γ-Net, to tackle this challenge.γ-Net adopts advanced complex-valued learned iterative shrinkage thresholding algorithm (CV-LISTA) to mimic the iterative optimization step in sparse reconstruction. Simulations show the height estimate from a well-trainedγ-Net approaches the Cramér-Rao lower bound (CRLB) while improving the computational efficiency by one to two orders of magnitude comparing to the first-order CS-based methods. It also shows no degradation in the super-resolution power comparing to the state-of-the-art second-order TomoSAR solvers, which are much more computationally expensive than the first-order methods. Specifically,γ-Net reaches more than 90% detection rate in moderate super-resolving cases at 25 measurements at 6 dB SNR. Moreover, simulation at limited baselines demonstrates that the proposed algorithm outperforms the second-order CS-based method by a fair margin. Test on real TanDEM-X data with just six interferograms also shows high-quality 3-D reconstruction with high-density detected double scatterers.","['Synthetic aperture radar', 'Superresolution', 'Deep learning', 'Neural networks', 'Reflectivity', 'Imaging', 'Image reconstruction']","['Complex-valued learned iterative shrinkage thresholding algorithm (LISTA)', 'compressive sensing (CS)', 'synthetic aperture radar~(SAR) tomography (TomoSAR)', 'super-resolution']"
"During the Geostationary Operational Environmental Satellite (GOES)-14 and -15 post-launch test (PLT) for science periods, an up to ~ 2 K mean brightness temperature (Tb) bias with respect to collocated Atmospheric Infrared Sounder (AIRS) and Infrared Atmospheric Sounding Interferometer (IASI) observations was observed in the absorptive IR channels of the GOES-14/15 Imagers. These large scene-dependent biases were believed to be caused mainly by spectral characterization errors. In this paper, we refined the spectral response function (SRF) shift algorithm which was developed during the GOES-13 PLT period to improve the GOES-14/15 Imager IR radiometric calibration accuracy by accurately calculating the impact of blackbody on the calibrated scene radiance. The uncertainty of the SRF shift algorithm was estimated and used to guide the final selection of the total amount of central wave-number shift. This refined algorithm was first verified with GOES-13 Imager Ch6 data and then used to evaluate and further revise the audited GOES-14/15 SRFs provided by the instrument vendor. Based on this algorithm, the optimal SRF shifts were -1.98 cm -1 for GOES-13 Ch6, -8.25 cm -1 for GOES-14 Ch3, -0.25 cm -1 for GOES-14 Ch6, -6.25 cm -1 for GOES-15 Ch3 and +0.50 cm -1 for GOES-15 Ch6. The newly shifted SRFs were operationally implemented into the GOES-14/15 Imager IR calibrations in the August of 2011 and successfully reduced the mean all-sky Tb bias with respect to the reference instrument to less than 0.15 K. The scene-dependent bias, which can be nonlinear at large erroneous SRF, was also greatly reduced. The same method was applied to correct the GOES-12 Imager Ch6 SRF which has a changing SRF error during its mission life. A strong linear relationship between the optimal SRF shifts and the mean Tb bias with respect to the AIRS data was observed at this channel. This strong linear relationship can be used to revise the GOES-12 Ch6 SRF for a better radiance simulation. The method described in this paper is particularly important to evaluate and revise the erroneous SRF, if it exists, after satellite launch yet before it becomes fully operational.","['Calibration', 'Instruments', 'Radiometry', 'Low earth orbit satellites', 'Aerospace electronics', 'Earth']","['Calibration', 'GOES Imager', 'GSICS', 'Infrared', 'inter-calibration', 'spectral response function']"
"Wildfires and their emissions reduce air quality in many regions of the world, contributing to thousands of premature deaths each year. Smoke forecasting systems have the potential to improve health outcomes by providing future estimates of surface aerosol concentrations (and health hazards) over a period of several days. In most operational smoke forecasting systems, fire emissions are assumed to remain constant during the duration of the weather forecast and are initialized using satellite observations. Recent work suggests that it may be possible to improve these models by predicting the temporal evolution of emissions. Here, we develop statistical models to predict fire activity one to five days into the future using Moderate Resolution Imaging Spectroradiometer (MODIS) satellite fire counts and weather data from ERA-interim reanalysis. Our predictive framework consists of two-Poisson regression models that separately represent new ignitions and the dynamics of existing fires on a coarse resolution spatial grid. We use ten years of active fire detections in Alaska to develop the model and use a cross-validation approach to evaluate model performance. Our results show that regression methods are significantly more accurate in predicting daily fire activity than persistence-based models (which suffer from an overestimation of fire counts by not accounting for fire extinction), with vapor pressure deficit being particularly effective as a single weather-based predictor in the regression approach.","['Predictive models', 'Weather forecasting', 'MODIS', 'Atmospheric modeling', 'Forecasting', 'Satellites']","['Daily fire forecasting', 'fire ignitions', 'moderate resolution imaging spectroradiometer (MODIS)', 'smoke aerosols', 'vapor pressure deficit (VPD)']"
"Low-frequency (LF) signal content in seismic data as well as a realistic initial model are key ingredients for robust and efficient full-waveform inversions (FWIs). However, acquiring LF data is challenging in practice for active seismic surveys. Data-driven solutions show promise to extrapolate LF data given a high-frequency counterpart. While being established for synthetic acoustic examples, the application of bandwidth extrapolation to field datasets remains nontrivial. Rather than aiming to reach superior accuracy in bandwidth extrapolation, we propose to jointly reconstruct LF data and a smooth background subsurface model within a multitask deep learning framework. We automatically balance data, model, and trace-wise correlation loss terms in the objective functional and show that this approach improves the extrapolation capability of the network. We also design a pipeline for generating synthetic data suitable for field data applications. Finally, we apply the same trained network to synthetic and real marine streamer datasets and run an elastic FWI from the extrapolated dataset.","['Data models', 'Extrapolation', 'Bandwidth', 'Time-domain analysis', 'Deep learning', 'Time-frequency analysis', 'Task analysis']","['Deep learning', 'full-waveform inversion (FWI)', 'low frequency', 'multitask learning (MTL)']"
"Predicting wildfire spread is critical for land management and disaster preparedness. To this end, we present “Next Day Wildfire Spread,” a curated, large-scale, multivariate dataset of historical wildfires aggregating nearly a decade of remote-sensing data across the United States. In contrast to existing fire datasets based on Earth observation satellites, our dataset combines 2-D fire data with multiple explanatory variables (e.g., topography, vegetation, weather, drought index, and population density) aligned over 2-D regions, providing a feature-rich dataset for machine learning. To demonstrate the usefulness of this dataset, we implement a neural network that takes advantage of the spatial information of these data to predict wildfire spread. We compare the performance of the neural network with other machine learning models: logistic regression and random forest. This dataset can be used as a benchmark for developing wildfire propagation models based on remote-sensing data for a lead time of one day.","['Indexes', 'Remote sensing', 'Vegetation mapping', 'Statistics', 'Sociology', 'Data models', 'Soft sensors']","['Earth Engine', 'machine learning', 'remote sensing', 'wildfire']"
"In this paper, we report the accurate estimation of vine grape yield from a 3-D radar imagery technique. Three ground-based frequency-modulated continuous-wave radars operating, respectively, at 24, 77, and 122 GHz are used for the contact-less estimation of grape mass in vineyards. The 3-D radar images are built from the beam scanning of the vine plants and allow estimating the mass of grapes from the computation of appropriate statistical estimators. These estimators are derived from the measured polarization and magnitude of radar echoes. It is shown that the estimation of grape mass from the proposed ground-based radar imagery technique at millimeter-wave frequency range may be accurate within 1%.","['Pipelines', 'Radar imaging', 'Estimation', 'Radar measurements', 'Optical sensors']","['Precision viticulture (PV)', 'proximal sensing', 'radar imagery']"
"In recent years, space-borne synthetic aperture radar (SAR) polarimetry has become a valuable tool for sea ice type retrieval. L-band SAR has proven to be sensitive toward deformed sea ice and is complementary compared with operationally used C-band SAR for sea ice type classification during the early and advanced melt seasons. Here, we employ an artificial neural network (ANN)-based sea ice type classification algorithm on a comprehensive data set of ALOS-2 PALSAR-2 fully polarimetric images acquired with a range of incidence angles and during different environmental conditions. The variability within the data set means that it is ideal for making a novel assessment of the robustness of the sea ice classification, investigating the intraclass variability, the seasonal variations, and the incidence angle effect on the sea ice classification results. The images coincide with two different Arctic campaigns in 2015: the Norwegian Young Sea Ice Cruise 2015 (N-ICE2015) and the Polarstern’s (PS92) Transitions in the Arctic Seasonal Sea Ice Zone (TRANSSIZ). We find that it is essential to take into account seasonality and intraclass variability when establishing training data for machine learning-based algorithms though moderate differences in incidence angle are possible to accommodate by the classifier during the dry and cold winter season. We also conclude that the incidence angle dependence of backscatter for a given ice type is consistent for different Arctic regions.","['Sea ice', 'L-band', 'Ocean temperature', 'Synthetic aperture radar', 'Arctic', 'Satellites', 'Temperature measurement']","['Artificial neural network (ANN)', 'L-band', 'operational service', 'polarimetry', 'sea ice', 'synthetic aperture radar (SAR)']"
"In recent years, synthetic aperture radar interferometry has become a recognized geodetic tool for observing ground motion. For monitoring areas with low density of coherent targets, artificial corner reflectors (CRs) are usually introduced. The required size of a reflector depends on radar wavelength and resolution and on the required deformation accuracy. CRs have been traditionally used to provide a high signal-to-clutter ratio (SCR). However, large dimensions can make the reflector bulky, difficult to install and maintain. Furthermore, if a large number of reflectors are needed for long infrastructure, such as vegetation-covered dikes, the total price of the reflectors can become unaffordable. On the other hand, small reflectors have the advantage of easy installation and low cost. In this paper, we design and study the use of small reflectors with low SCR for ground motion monitoring. In addition, we propose a new closed-form expression to estimate the interferometric phase precision of resolution cells containing a (strong or weak) point target and a clutter. Through experiments, we demonstrate that the small reflectors can also deliver displacement estimates with an accuracy of a few millimeters. To achieve this, we apply a filtering method for reducing clutter noise.","['Clutter', 'Monitoring', 'Standards', 'Radar cross-sections', 'Synthetic aperture radar', 'Signal to noise ratio']","['Artificial corner reflectors (CRs)', 'deformation monitoring', 'infrastructure monitoring', 'interferometric phase precision', 'radar interferometry', 'remote monitoring', 'small reflectors', 'synthetic aperture radar (SAR)']"
"An efficient electromagnetic inversion scheme for imaging sparse 3-D domains is proposed. The scheme achieves its efficiency and accuracy by integrating two concepts. First, the nonlinear optimization problem is constrained using L 0 or L 1 -norm of the solution as the penalty term to alleviate the ill-posedness of the inverse problem. The resulting Tikhonov minimization problem is solved using nonlinear Landweber iterations (NLW). Second, the efficiency of the NLW is significantly increased using a steepest descent algorithm. The algorithm uses a projection operator to enforce the sparsity constraint by thresholding the solution at every iteration. Thresholding level and iteration step are selected carefully to increase the efficiency without sacrificing the convergence of the algorithm. Numerical results demonstrate the efficiency and accuracy of the proposed imaging scheme in reconstructing sparse 3-D dielectric profiles.","['Electromagnetics', 'Inverse problems', 'Image reconstruction', 'Electromagnetic scattering', 'Imaging', 'Transmitters', 'Optimization']","['Accelerated steepest descent', 'electromagnetic imaging', 'electromagnetic inverse scattering', 'Landweber iterations', 'nonlinear ill-posed problem', 'numerical methods', 'sparsity']"
"The lack of in situ data has always posed challenges to remote-sensing-data product validation. Herein, the products of sea-ice concentration (SIC) data derived using the arctic radiation and turbulence interaction study (ARTIST) sea ice (ASI) algorithm were evaluated by comparing them with SICs from a high-resolution sea-ice aerial image obtained during the 27th China Antarctic expedition in January 2011. Results suggest that data obtained from the advanced microwave scanning radiometer for the earth-observing system (AMSR-E) underestimate SICs by 19%. We performed step-by-step comparisons among the aerial image, moderate-resolution-imaging spectroradiometer (MODIS), and AMSR-E SIC. These types of comparisons have not been made in previous validation studies. First, SICs acquired from MODIS-Terra imagery were acquired using a tie-point method and corrected by SICs derived from the aerial photography. Second, SICs of MODIS-Aqua images were trained based on the consistency of SIC results between MODIS-Terra and MODIS-Aqua over the selected region on the same day. Finally, the MODIS-Aqua SICs were employed to validate synchronous AMSR-E swath SIC products. The results show that the AMSR-E products underestimate SICs by 8.5% in the marginal ice zone in comparison with MODIS SICs. According to our further analysis between sea-ice types and AMSR-E biases, the higher the proportion of first-year ice, the smaller the AMSR-E SIC bias. In other words, results suggest that the higher the thin ice proportion, the more the AMSR-E underestimates the SIC.","['Silicon carbide', 'Sea ice', 'MODIS', 'Microwave radiometry', 'Antarctica', 'Spatial resolution']","['Aerial imagery', 'AMSR-E (advanced microwave scanning radiometer for the earth-observing system) data', 'step-by-step validation', 'sea-ice concentration (SIC)']"
"The benefits of composite products are well known to users of data from optical sensors: cloud-cleared composite reflectance or index products are commonly used as an analysis-ready data (ARD) layer. No analogous composite products are currently in widespread use that is based on spaceborne radar satellite backscatter signals. Here, we present a methodology to produce wide-area ARD composite backscatter images. They build on the existing heritage of geometrically and radiometrically terrain corrected level 1 products. By combining backscatter measurements of a single region seen from multiple satellite tracks (incl. ascending and descending), they are able to provide wide-area coverage with low latency. The analysis-ready composite backscatter maps provide flattened backscatter estimates that are geometrically and radiometrically corrected for slope effects. A mask layer annotating the local quality of the composite resolution is introduced. Multiple tracks are combined by weighting each observation by its local resolution, generating seamless wide-area backscatter maps suitable for applications ranging from wet snow monitoring to land cover classification or short-term change detection.","['Backscatter', 'Radar', 'Radiometry', 'Radar tracking', 'Radar imaging', 'Geometry', 'Spaceborne radar']","['Radar cross sections', 'radar scattering', 'radar signatures', 'radar terrain factors']"
"Data from the solar reflectance channels of the Along Track Scanning Radiometer (ATSR) series of instruments are being used in applications for monitoring trends in many climate variables, for example, clouds and aerosols. In order to provide quantitative information, the radiometric calibrations of the sensors must be consistent, stable, and ideally traced to international standards with uncertainties quantified. In this paper, the authors describe the current methodology used to monitor the long-term drifts and determine the relative biases of the ATSR solar channel radiometric calibrations. Top-of-atmosphere bidirectional reflectance factors (BRFs) over quasi-stable desert and ice sites are extracted from level-1 images and compared against a reference BRF model derived from averages of measurements over the site from a reference sensor. This enables comparisons to be performed where there is limited or no temporal overlap between sensors. The results of the drift monitoring and intercomparisons are used to provide lookup tables to be applied by users for existing products and in subsequent reprocessing of ATSR data. The method is extended to perform comparisons against the Medium Resolution Imaging Spectrometer and the Moderate Resolution Imaging Spectrometer. Results of the comparisons are presented and show that the sensors are stable throughout the mission lifetime and biases relative to the Advanced ATSR are presented. Improvements to the methodology are discussed to account for spectral mismatches of the sensors under comparison and to increase the range of view angles that the BRF model presently covers.","['Calibration', 'Azimuth', 'Satellites', 'MODIS', 'Clouds']","['Advanced Along Track Scanning Radiometer (ATSR) (AATSR)', 'calibration', 'intercomparison', 'reflectance']"
"A time or spatial series of drop counts is but one realization of a multiple stochastic process. In this paper, a method is presented that extracts more of the information contained in the time series of 1-min Joss-Waldvogel disdrometer counts in rain than a simple analysis of the magnitudes of the counts would provide. This is done by greatly increasing the size of a data set using a Bayesian analysis of drop count measurements in 17 size bins. Using the empirical copula statistical technique of probability density function transformations, a 1391-min time series of drop counts was expanded to the equivalent of 40 000 min. This dramatic increase in sample size permits a deeper characterization of the rain. Using this single disdrometer, it also allows one to translate these counts into a 200 × 200 grid filled at each point with drop size distributions of mean drop concentrations consistent with the observed statistical properties of the rain. Such a field can be used for remote sensing studies of the effect of partial beam filling and for algorithm development. Moreover, since there is nothing unique to this set of drop counts, this approach can be applied to any other set of count data, including snow and clouds.","['Correlation', 'Radar', 'Rain', 'Bayes methods', 'Time series analysis', 'Covariance matrices']","['Algorithms', 'atmospheric measurements', 'estimation', 'meteorology', 'multiplication', 'radar applications', 'rain', 'random media', 'remote sensing', 'statistics']"
"The primary goal of NASA's Ice, Cloud, and land Elevation Satellite (ICESat) mission was to detect centimeter-level changes in global ice sheet elevations at the spatial scale of individual ice streams. Confidence in detecting these small signals requires careful validation over time to characterize the uncertainty and stability of measured elevations. A common validation approach compares altimeter elevations to an independently characterized and stable reference surface. Using a digital elevation model (DEM) from geodetic surveys of one such surface, the salar de Uyuni in Bolivia, we show that ICESat elevations at this location have a 0.0-cm bias relative to the WGS84 ellipsoid, 4.0-cm (1-sigma) uncertainty overall, and 1.8-cm uncertainty under ideal conditions over short (50 km) profiles. We observe no elevation bias between ascending and descending orbits, but we do find that elevations measured immediately after transitions from low to high surface albedo may be negatively biased. Previous studies have reported intercampaign biases (ICBs) between various ICESat observation campaigns, but we find no statistically significant ICBs or ICB trends in our data. We do find a previously unreported 3.1-cm bias between ICESat's Laser 2 and Laser 3, and we find even larger interlaser biases in reanalyzed data from other studies. For an altimeter with an exact repeat orbit like ICESat, we also demonstrate that validation results with respect to averaged elevation profiles along a single ground track are comparable to results obtained using reference elevations from an in situ survey.","['Ice', 'Sea surface', 'Lasers', 'Orbits', 'Tracking', 'Extraterrestrial measurements', 'Standards']","['Ice sheets', 'laser ranging', 'lidar', 'remote sensing', 'satellite laser altimetry', 'system validation']"
"A synergetic approach for the estimation of stable boundary layer height (SBLH) using lidar and microwave radiometer (MWR) data is presented. Vertical variance of the backscatter signal from a ceilometer is used as an indicator of the aerosol stratification in the nocturnal stable boundary layer. This hypothesis is supported by a statistical analysis over one month of observations. Thermodynamic information from the MWR-derived potential temperature is incorporated as coarse estimate of the SBLH. Data from the two instruments are adaptively assimilated by using an extended Kalman filter (EKF). A first test of the algorithm is performed by applying it to collocated Vaisala CT25K ceilometer and humidity and temperature profiler MWR data collected during the HD(CP) 2 Observational Prototype Experiment (HOPE) campaign at Jülich, Germany. The application of the algorithm to different atmospheric scenarios reveals the superior performance of the EKF compared to a nonlinear least squares estimator, particularly in nonidealized conditions.","['Temperature measurement', 'Laser radar', 'Aerosols', 'Instruments', 'Estimation', 'Backscatter', 'Atmospheric measurements']","['Adaptive Kalman filtering', 'laser radar', 'microwave radiometry', 'remote sensing', 'signal processing']"
"Remote sensing deals with a plethora of sensors, a large number of classes/categories, and a huge variation in geography. Due to the difficulty of collecting labeled data uniformly representing all scenarios, data-hungry deep learning models are often trained with labeled data in a source domain that is limited in the above-mentioned aspects. However, during the test/inference phase, such deep learning models are often subjected to a distributional shift, also called out-of-distribution (OOD) samples, in the form of unseen classes, geographic differences, and multisensor differences. Deep learning models can behave in an unexpected manner when subjected to such distributional uncertainties. Vulnerability to OOD data severely reduces the reliability of deep learning models and trusting on such predictions in the absence of any reliability indicator may lead to wrong policy decisions or mishaps in time-bound remote sensing applications. Motivated by this, in this work, we propose a Dirichlet prior network-based model to quantify the distributional uncertainty of deep learning-based remote sensing models. The approach seeks to maximize the representation gap between the in-domain and OOD examples for better segregation of OOD samples at test time. Extensive experiments on several remote sensing image classification datasets demonstrate that the proposed model can quantify distributional uncertainty. To the best of our knowledge, this is the first work to elaborately study distributional uncertainty in context of remote sensing. The codes are publicly available at https://gitlab.lrz.de/ai4eo/Uncertainty/-/tree/main/DPN-RS .","['Uncertainty', 'Remote sensing', 'Data models', 'Sensors', 'Biological system modeling', 'Training', 'Deep learning']","['Distributional uncertainty', 'open-set recognition', 'out-of-distribution (OOD)', 'reliability', 'remote sensing', 'robustness', 'uncertainty']"
"This paper proposes a novel occlusion detection method for urban true orthophoto generation. In this new method, occlusion detection is performed using a ghost image; this method is therefore considerably different from the traditional Z-buffer method, in which occlusion detection is performed during the generation of a true orthophoto (to avoid ghost image occurrence). In the proposed method, a model is first established that describes the relationship between each ghost image and the boundary of the corresponding building occlusion, and then an algorithm is applied to identify the occluded areas in the ghost images using the building displacements. This theory has not previously been applied in true orthophoto generation. The experimental results demonstrate that the method proposed in this paper is capable of effectively avoiding pseudo-occlusion detection, with a success rate of 99.2%, and offers improved occlusion detection accuracy compared with the traditional Z-buffer detection method. The advantage of this method is that it avoids the shortcoming of performing occlusion detection and true orthophoto generation simultaneously, which results in false visibility and false occlusions; instead, the proposed method detects occlusions from ghost images and therefore provides simple and effective true orthophoto generation.","['Buildings', 'Cameras', 'Contracts', 'Iterative methods', 'Surface treatment', 'Geomagnetism', 'Tin']","['Aerial image', 'building', 'digital building model (DBM)', 'ghost image', 'occlusion detection', 'orthophoto']"
"TanSat is an important satellite in the Chinese Earth Observation Program which is designed to measure global atmospheric CO 2 concentrations from space. The first Chinese superhigh-resolution grating spectrometer for measuring atmospheric CO 2 is aboard TanSat. This spectrometer is a suite of three grating spectrometers that make coincident measurements of reflected sunlight in the near-infrared CO 2 band near 1.61 and2.06 μmand in the molecular oxygen A-band (O 2 A) at0.76 μm. Their spectral resolving power (λ/Δλ) is ~19 000, ~12 800, and ~12 250 in the O 2 A, weak absorption band of molecular carbon dioxide band, and strong absorption of carbon dioxide band, respectively. This paper describes the laboratory radiometric calibration of the spectrometer suite, which consists of measurements of the dark current response, gain coefficients, and signal-to-noise ratio (SNR). The SNRs of each channel meet the mission requirements for the O 2 A and weak CO 2 band but slightly miss the requirements in a few channels in the strong CO 2 band. The gain coefficients of the three bands have a negligible random error component and achieve very good stability. Most of the R-squared of gain coefficients model consist of five numbers of nine (e.g., 0.99999) after the decimal point, suggesting that the instrument has significant response linearity. The radiometric calibration results meet the requirements of an absolute calibration uncertainty of less than 5%.","['Radiometry', 'Calibration', 'Atmospheric measurements', 'Satellite broadcasting', 'Instruments', 'Detectors', 'Extraterrestrial measurements']","['Atmospheric measurements', 'carbon dioxide', 'radiometric calibration', 'spectrometer']"
"Airborne hyperspectral images are used for crop identification with a high classification accuracy because of their high spectral resolution, spatial resolution, and signal-to-noise ratio (SNR). However, the tradeoffs between the three core parameters of a hyperspectral imager (SNR, spatial resolution, and spectral resolution) should be considered for designing an efficient imaging system. Only a few reported studies on the analysis of the impact of SNR on identification accuracy are available. Further, the tradeoffs and mutual interactions among these parameters are rarely considered. In this empirical study, our aim was to understand the relationship among the core parameters and their effects on crop identification accuracy by analyzing the tradeoffs and mutual interactions among these parameters. We analyzed the hyperspectral images of a typical plain agricultural area in Xiongan, China, acquired by the newly developed sensor airborne multimodular imaging spectrometer (AMMIS). The fundamental images were transformed to form datasets with different ranges of spectral resolution, spatial resolution, and SNR using data reconstruction methods. We adopted the classification and regression tree (CART), random forest (RF), and k-nearest neighbor (kNN) classifiers, and observed the overall accuracy (OA) across the degraded hyperspectral datasets. The experimental results indicated that the OA decreased with a decreasing SNR. As the spectral resolution became coarser, the OA first increased, plateaued, and then decreased. However, the OA increased with decreasing spatial resolution. This study was performed with the goal of bridging the knowledge gap between the back-end hyperspectral sensor designing and its front-end applications.","['Spatial resolution', 'Hyperspectral imaging', 'Agriculture', 'Signal to noise ratio', 'Forestry', 'Instruments', 'Signal resolution']","['Hyperspectral imager', 'parameter optimization', 'system design', 'tradeoff']"
"For most Earth observation applications, passive microwave radiometry from the geostationary orbit requires prohibitively large apertures for conventional single-satellite platforms. This paper proposes a novel interferometric technique capable of synthesizing these apertures using satellite formation flight. The significance of such concept is in its capacity to synthesize microwave apertures of conceptually unconstrained size in space for the first time. The technique is implemented in two formation flight configurations: a formation of a single full-sized satellite with microsatellites and a formation of several full-sized satellites. Practical advantages and challenges of these configurations are explored by applying them to geostationary atmospheric sounding at 53 GHz, the lowest sounding frequency considered for future sounder concepts Geostationary Atmospheric Sounder, GeoSTAR, and Geostationary Interferometric Microwave Sounder. The two configurations produce apertures of 14.4 and 28.8 m, respectively, and a spatial resolution of 16.7 and 8.3 km, respectively, from the geostationary orbit. The performance of these interferometers is simulated, and the challenges identified are threefold. First, intersatellite ranging in micrometer-level precision is required. Second, the extremely sparse design suggests that further innovation is necessary to improve radiometric resolution. Third, the presence of long baselines suggests extreme decorrelation effects are expected. While the first requirement has already been demonstrated on ground, the other two remain for future research. This technique can be implemented at arbitrary microwave frequencies and arbitrary circular orbits, meaning it can also be applied to other geostationary applications, or to achieve unprecedented spatial resolution from lower orbits, or to extend the accessible frequencies into lower frequency radio waves.","['Microwave radiometry', 'Apertures', 'Satellite broadcasting', 'Antenna arrays', 'Microwave theory and techniques', 'Microwave imaging', 'Antenna measurements']","['Atmospheric sounding', 'microwave radiometry', 'mission concept', 'satellite formation flight', 'synthetic aperture imaging']"
"Automatically producing Arctic sea ice charts from Sentinel-1 synthetic aperture radar (SAR) images is challenging for convolutional neural networks (CNNs) due to ambiguous backscattering signatures. The number of pixels viewed by the CNN model in the input image used to generate an output pixel, or the receptive field, is important to detect large features or physical objects such as sea ice and correctly classify them. In addition, a noise phenomenon is present in the Sentinel-1 ESA Instrument Processing Facility (IPF) v2.9 SAR data, particularly in subswath transitions, visible as long vertical lines and grained particles resembling small sea ice floes. To overcome these two challenges, we suggest adjusting the receptive field of the popular U-Net CNN architecture used for semantic segmentation. It is achieved by symmetrically adding additional blocks of convolutional, pooling and upsampling layers in the encoder and decoder of the U-Net, constituting an increase in the number of levels. This shows great improvements in the performance and in the homogeneity of predictions. Second, training models on SAR data noise-corrected with an enhanced technique has demonstrated a significant increase in model performance and enabled better predictions in uncertain regions. An eight-level U-Net trained on the alternative noise-corrected SAR data is presented to be capable of correctly predicting many ambiguous SAR signatures and increased the performance by 8.44% points compared with the regular U-Net trained on the ordinary ESA IPF v2.9 noise-corrected SAR data. This is the first installment of this multi-series installment of articles related to AI applied to sea ice (in short AI4SeaIce).","['Sea ice', 'Synthetic aperture radar', 'Radar polarimetry', 'Convolutional neural networks', 'Backscatter', 'Image resolution', 'Arctic']","['AI4Arctic', 'AI4EO', 'cryosphere', 'deep learning', 'receptive field', 'SAR noise correction', 'sea ice charting', 'synthetic aperture radar (SAR) data', 'U-Net']"
"This article presents a novel technique for automatically locating tropical cyclone (TC) centers based on top cloud motions in consecutive geostationary satellite images. The high imaging rate and spatial resolution images of the Gaofen-4 geostationary satellite enable us to derive pixel-wise top cloud motion data of TCs, and from the data, TC spiral centers can be accurately determined based on an entirely different principle from those based on static image features. First, a physical motion field decomposition is proposed to eliminate scene shift and TC migration in the motion data without requiring any auxiliary geolocation data. This decomposition does not generate the artifacts that appear in the results of the previously published motion field decomposition. Then, an algorithm of a motion direction-based index embedded in a pyramid searching structure is fully designed to automatically and effectively locate the TC centers. The test shows that the TC concentric motions are more clearly revealed after the proposed motion field decomposition and the located centers are in good agreement with the cloud pattern centers in a visual sense and also with the best track data sets of four meteorological agencies.","['Tropical cyclones', 'Spirals', 'Satellites', 'Sensors', 'Cloud computing', 'Spatial resolution', 'Clouds']","['Geostationary satellite', 'motion field decomposition', 'tropical cyclone (TC)', 'typhoon eye']"
"Accurate global observations from space are critical for global climate change study. However, atmospheric temperature trend derived from spaceborne microwave instruments remains a subject of debate, due mainly to the uncertainty in characterizing the long-term drift of instrument calibration. Thus, a highly stable target with a well-known microwave radiation is required to evaluate the long-term calibration stability. This paper develops a new model to simulate the lunar emission at microwave frequencies, and the model is then used for monitoring the stability of the Advanced Technology Microwave Sounder (ATMS) onboard Suomi NPP satellite. It is shown that the ATMS cold space view of lunar radiation agrees well with the model simulation during the past five years and this instrument is capable of serving the reference instrument for atmospheric temperature trending studies, and connecting the previous generation of microwave sounders from NOAA-15 to the future Joint Polar Satellite System Microwave Sounder onboard NOAA-20 satellite.","['Climate change', 'Microwave radiometry', 'Calibration', 'Instruments', 'Space vehicles', 'Satellite broadcasting', 'Brightness temperature', 'Moon']","['Lunar calibration', 'microwave radiometer']"
"Learning the similarity between remote sensing (RS) images forms the foundation for content-based RS image retrieval (CBIR). Recently, deep metric learning approaches that map the semantic similarity of images into an embedding (metric) space have been found very popular in RS. A common approach for learning the metric space relies on the selection of triplets of similar (positive) and dissimilar (negative) images to a reference image called an anchor. Choosing triplets is a difficult task particularly for multilabel RS CBIR, where each training image is annotated by multiple class labels. To address this problem, in this article, we propose a novel triplet sampling method in the framework of deep neural networks (DNNs) defined for multilabel RS CBIR problems. The proposed method selects a small set of the most representative and informative triplets based on two main steps. In the first step, a set of anchors that are diverse to each other in the embedding space is selected from the current minibatch using an iterative algorithm. In the second step, different sets of positive and negative images are chosen for each anchor by evaluating the relevancy, hardness, and diversity of the images among each other based on a novel strategy. Experimental results obtained on two multilabel benchmark archives show that the selection of the most informative and representative triplets in the context of DNNs results in: 1) reducing the computational complexity of the training phase of the DNNs without any significant loss on the performance and 2) an increase in learning speed since informative triplets allow fast convergence. The code of the proposed method is publicly available at https://git.tu-berlin.de/rsim/image-retrieval-from-triplets .","['Feature extraction', 'Training', 'Semantics', 'Forestry', 'Sampling methods', 'Image retrieval', 'Remote sensing']","['Deep neural networks (DNNs)', 'metric learning', 'multilabel image retrieval', 'remote sensing (RS)', 'triplet selection']"
"When monitoring time series of remote sensing data, it is advisable to fill gaps, i.e., missing or distorted data, caused by atmospheric effects or technical failures. In this paper, a new method for filling these gaps called interpolation of the mean anomalies (IMA) is proposed and compared with some competitors. The method consists of: 1) defining a neighborhood for the target image from previous and subsequent images across previous and subsequent years; 2) computing the mean target image of the neighborhood; 3) estimating the anomalies in the target image by subtracting the mean image from the target image; 4) filtering the anomalies; 5) averaging the anomalies over a predefined window; 6) interpolating the averaged anomalies; and 7) adding the interpolated anomalies to the mean image. To assess the performance of the IMA method, both a real example and a simulation study are conducted with a time series of Moderate Resolution Imaging Spectroradiometer (MODIS) TERRA and MODIS AQUA images captured over the region of Navarre (Spain) from 2011 to 2013. We analyze the land surface temperature (LST) day and night, and the normalized difference vegetation index (NDVI). In the simulation study, seven sizes of artificial clouds are randomly introduced to each image in the studied time series. The square root of the mean-squared prediction error (RMSE) between the original and the filled data is chosen as an indicator of the goodness of fit. The results show that the IMA method outperforms Timesat, Hants, and Gapfill (GF) in filling small, moderate, and big cloud gaps in both the day and night LST and NDVI data, reaching RMSE reductions of up to 23%.","['MODIS', 'Clouds', 'Remote sensing', 'Land surface temperature', 'Time series analysis', 'Interpolation', 'Vegetation mapping']","['Geostatistics', 'moderate resolution imaging spectroradiometer (MODIS)', 'smoothing images', 'thin-plate splines']"
"With the steadily increasing spatial resolution of synthetic aperture radar images, the need for a consistent but locally adaptive image enhancement rises considerably. Numerous studies already showed that adaptive multilooking, able to adjust the degree of smoothing locally to the size of the targets, is superior to uniform multilooking. This study introduces a novel approach of multiscale and multidirectional multilooking based on intensity images exclusively but applicable to an arbitrary number of image layers. A set of 2-D circular and elliptical filter kernels in different scales and orientations (named Schmittlets) is derived from hyperbolic functions. The original intensity image is transformed into the Schmittlet coefficient domain where each coefficient measures the existence of Schmittlet-like structures in the image. By estimating their significance via the perturbation-based noise model, the best-fitting Schmittlets are selected for image reconstruction. On the one hand, the index image indicating the locally best-fitting Schmittlets is utilized to consistently enhance further image layers, e.g., multipolarized, multitemporal, or multifrequency layers, and on the other hand, it provides an optimal description of spatial patterns valuable for further image analysis. The final validation proves the advantages of the Schmittlets over six contemporary speckle reduction techniques in six different categories (preservation of the mean intensity, equivalent number of looks, and preservation of edges and local curvature both in strength and in direction) by the help of four test sites on three resolution levels. The additional value of the Schmittlet index layer for automated image interpretation, although obvious, still is subject to further studies.","['Synthetic aperture radar', 'Shape', 'Image representation', 'Kernel', 'Image resolution', 'Image enhancement', 'Smoothing methods']","['Adaptive filters', 'digital filters', 'image analysis', 'image edge analysis', 'image enhancement', 'image reconstruction', 'image representations', 'synthetic aperture radar (SAR)']"
"Multibaseline interferometric synthetic aperture radar (InSAR) techniques are effective approaches for retrieving the 3-D information of urban areas. In order to obtain a plausible reconstruction, it is necessary to use more than 20 interferograms. Hence, these methods are commonly not appropriate for large-scale 3-D urban mapping using TanDEM-X data, where only a few acquisitions are available in average for each city. This article proposes a new SAR tomographic processing framework to work with those extremely small stacks, which integrates the nonlocal filtering into SAR tomography inversion. The applicability of the algorithm is demonstrated using a TanDEM-X multibaseline stack with five bistatic interferograms over the whole city of Munich, Germany. A systematic comparison of our result with TanDEM-X raw digital elevation models (DEMs) and airborne LiDAR data shows that the relative height accuracy of two-third buildings is within 2 m, which outperforms the TanDEM-X raw DEM. The promising performance of the proposed algorithm paved the first step toward high-quality large-scale 3-D urban mapping.","['Synthetic aperture radar', 'Tomography', 'Estimation', 'Image reconstruction', 'Urban areas', 'Laser radar', 'Remote sensing']","['3-D urban mapping', 'digital elevation models (DEMs)', 'interferometric synthetic aperture radar (InSAR)', 'SAR tomography (TomoSAR)', 'TanDEM-X']"
"An adaptive classification is developed as a hybrid of the eigenvector-based and the model-based target decompositions for polarimetric synthetic aperture radar (PolSAR) data. The classification adopts the canonical scattering models that widely used in model-based decompositions to provide an improvement for the well-known H/α classification. First, a correspondence principle is adopted to adaptively identify the matched canonical models. The selected models are parallelly combined based on the scattering similarity for a fine depiction of the scattering mechanism then. Twelve classes are finally obtained, and each one carries a unique symbol to show a specific scattering. The classification does not depend on a particular data set, avoids the hard partitioning, and solves the obscures in H/α. Comparison on the real PolSAR data sets with H/α and the existing scattering similarity-based classification validates the better discrimination.","['Scattering', 'Adaptation models', 'Classification', 'Data models', 'Entropy', 'Radar polarimetry']","['Radar polarimetry', 'scattering model', 'scattering similarity', 'target decomposition', 'unsupervised classification']"
"We propose a framework that estimates the inundation depth (maximum water level) and debris-flow-induced topographic deformation from remote sensing imagery by integrating deep learning and numerical simulation. A water and debris-flow simulator generates training data for various artificial disaster scenarios. We show that regression models based on Attention U-Net and LinkNet architectures trained on such synthetic data can predict the maximum water level and topographic deformation from a remote sensing-derived change detection map and a digital elevation model. The proposed framework has an inpainting capability, thus mitigating the false negatives that are inevitable in remote sensing image analysis. Our framework breaks limits of remote sensing and enables rapid estimation of inundation depth and topographic deformation, essential information for emergency response, including rescue and relief activities. We conduct experiments with both synthetic and real data for two disaster events that caused simultaneous flooding and debris flows and demonstrate the effectiveness of our approach quantitatively and qualitatively. Our code and data sets are available at https://github.com/nyokoya/dlsim .","['Remote sensing', 'Strain', 'Floods', 'Data models', 'Training data', 'Optical sensors', 'Optical imaging']","['Convolutional neural network (CNN)', 'debris-flow mapping', 'flood mapping', 'numerical simulation']"
"The Earth’s surface is continually changing, and identifying changes plays an important role in urban planning and sustainability. Although change detection techniques have been successfully developed for many years, these techniques are still limited to experts and facilitators in related fields. In order to provide every user with flexible access to change information and help them better understand land-cover changes, we introduce a novel task: change detection-based visual question answering (CDVQA) on multitemporal aerial images. In particular, multitemporal images can be queried to obtain high-level change-based information according to content changes between two input images. We first build a CDVQA dataset, including multitemporal image–question–answer triplets using an automatic question–answer generation method. Then, a baseline CDVQA framework is devised in this work, and it contains four parts: multitemporal feature encoding, multitemporal fusion, multimodal fusion, and answer prediction. In addition, we also introduce a change enhancing module to multitemporal feature encoding, aiming at incorporating more change-related information. Finally, the effects of different backbones and multitemporal fusion strategies are studied on the performance of CDVQA task. The experimental results provide useful insights for developing better CDVQA models, which are important for future research on this task. The dataset will be available at https://github.com/YZHJessica/CDVQA .","['Task analysis', 'Semantics', 'Visualization', 'Remote sensing', 'Earth', 'Feature extraction', 'Question answering (information retrieval)']","['Change detection', 'deep learning', 'multitemporal aerial images', 'visual question answering (VQA)']"
"This paper presents a new damage-mapping algorithm based on coherence images estimated from multitemporal polarimetric-interferometric synthetic aperture radar (SAR) data. The interferometric coherence has been restricted in the conventional damage-mapping approaches because the decorrelation sources are too complicated to interpret accurately and temporal decorrelation effects caused by slowly occurring natural changes and disaster events are often coupled together. To overcome these limitations, we formulate a coherence model that accounts for temporal decorrelation in two simplified layers, ground and volume layers, for long-temporal repeat-pass scenarios with zero spatial baseline. The model parameters include: 1) ground-to-volume ratio, a factor to determine the relative scattering contribution of ground and volume layers; 2) temporally correlated change, which captures the exponentially decaying behavior of coherence with time; and 3) temporally uncorrelated change, which is associated with random temporal changes. We estimate the model parameters in three steps: coherence optimization, interferometric pair-invariant parameter estimation, and interferometric pair-variant parameter estimation. To isolate the effects of disaster events from background natural changes, we calculate the probability density functions of historical change pixel by pixel and produce a probability map of damage. We tested the algorithm with uninhabited aerial vehicle data acquired from 2009 to 2015 for mapping the area damaged by the 2015 Lake Fire in California. Based on performance evaluation using receiver operating characteristic curves for optimized coherences and averaged probability maps, the proposed method reduced the false alarm from 0.25 to 0.07 when the probability of detection was 0.85 compared to coherence products alone.","['Decorrelation', 'Coherence', 'Synthetic aperture radar', 'Data models', 'Solid modeling', 'Dielectrics', 'Radar imaging']","['Coherence model', 'damage mapping', 'interferometry', 'synthetic aperture radar (SAR)']"
"Since 1983, the International Satellite Cloud Climatology Project (ISCCP) has collected Earth radiance data from the succession of geostationary and polar-orbiting meteorological satellites operated by weather agencies worldwide. Meeting the ISCCP goals of global coverage and decade-length time scales requires consistent and stable calibration of the participating satellites. For the geostationary imager visible channels, ISCCP calibration provides regular periodic updates from regressions of radiances measured from coincident and collocated observations taken by Advanced Very High Resolution Radiometer instruments. As an independent check of the temporal stability and intersatellite consistency of ISCCP calibrations, we have applied lunar calibration techniques to geostationary imager visible channels using images of the Moon found in the ISCCP data archive. Lunar calibration enables using the reflected light from the Moon as a stable and consistent radiometric reference. Although the technique has general applicability, limitations of the archived image data have restricted the current study to Geostationary Operational Environmental Satellite and Geostationary Meteorological Satellite series. The results of this lunar analysis confirm that ISCCP calibration exhibits negligible temporal trends in sensor response but have revealed apparent relative biases between the satellites at various levels. However, these biases amount to differences of only a few percent in measured absolute reflectances. Since the lunar analysis examines only the lower end of the radiance range, the results suggest that the ISCCP calibration regression approach does not precisely determine the intercept or the zero-radiance response level. We discuss the impact of these findings on the development of consistent calibration for multisatellite global data sets.","['Moon', 'Calibration', 'Satellite broadcasting', 'Radiometry', 'Extraterrestrial measurements', 'Instruments']","['Calibration', 'Moon', 'radiometry', 'remote sensing']"
"Glacial landforms are a significant element of landscape in many regions of Earth. The increasing availability of high-resolution digital elevation models (DEMs) provides an opportunity to develop automated methods of glacial landscape exploration and classification. In this study, we aimed to: 1) identify glacial landforms based on high-resolution DEM datasets; 2) determine relevant geomorphometric and spectral parameters and object-based features for the mapping of glacial landforms; and 3) develop an accurate workflow for glacial landform classification based on DEM. The developed methodology included the extraction of secondary features from DEM, feature selection with the Boruta algorithm, object-based image analysis, and random forest supervised classification. We applied the workflow for three study sites: one in Svalbard and two in Poland. It allowed the identification of six categories of glacial landforms: till plains, end moraines, hummocky moraines, outwash/glaciolacustrine plains, valleys, and kettle holes. The majority of relevant secondary features represented DEM spectral parameters calculated from 2-D Fourier analysis. The supervised classification models with the highest performance exhibited up to 96% overall accuracy with regard to a groundtruth dataset. This study showed that glacial landforms can be identified using novel image-processing methodology and spectral parameters of high-resolution DEM. The complete classification workflow developed herein provides a solution for the transparent generation of thematic maps of glacial landforms that may be reproducible and transferrable to various glacial regions worldwide.","['Feature extraction', 'Laser radar', 'Remote sensing', 'Surface treatment', 'Satellites', 'Meters', 'Image analysis']","['ArcticDEM', 'digital elevation model (DEM)', 'feature selection', 'glacial landforms', 'Light Detection and Ranging (LiDAR)', 'object-based image analysis (OBIA)', 'Pleistocene', 'remote sensing', 'supervised classification', 'Svalbard']"
"The Northern Hemisphere is, to a large extent, underlain by permafrost, which is prone to thawing due to rapid warming in the Arctic during the 21st century. In this context, satellite-derived soil moisture data are valuable for modeling purposes. Assessing the applicability of such data at high latitudes is essential but has, until recently, been given little attention. Recent studies have pointed out that seasonal land cover variations and the presence of small water bodies, which are typical in the Arctic, can cause complications for soil moisture retrieval. Here, it is hypothesized that a bias related to water fraction is caused by variations in the water surface roughness. The impact is quantified for the Metop Advanced Scatterometer by investigations of the higher spatial resolution synthetic aperture radar (SAR) data acquired by ENVISAT Advanced SAR over 11 sites across the Siberian Arctic. The bias calculated as an average over time can be explained by the lake fraction: a water fraction higher than 20% causes a bias of more than 10% relative surface soil moisture. This can, to a great extent, be attributed to wind, based on which a bias correction was developed. The correction was applied and evaluated with in situ soil moisture data, which were available from one of the sites: the Lena Delta. Weak results are obtained because water surfaces correspond mainly to rivers at this site. Variations in discharge, water height, and streams may therefore also affect the water surface roughness.","['Soil moisture', 'Arctic', 'Rivers', 'Lakes', 'Satellites', 'Rough surfaces', 'Surface roughness']","['Arctic', 'C-band', 'lakes', 'microwave measurement', 'moisture', 'radar remote sensing', 'scatterometer', 'surface waves', 'synthetic aperture radar (SAR)']"
"In this paper, the Backus-Gilbert (B-G) method was used for the conversion from Advanced Technology Microwave Sounder (ATMS) FOVs to AMSU-A FOVs. This method provides not only an optimal combination of measurements within a specified region but also a quantitative measure of the tradeoff between resolution and noise. Based on a subpixel microwave antenna temperature simulation technique, ATMS observations at a specified FOV size with 1.1° sampling interval are simulated. Errors of remapping results were quantified by using simulated data sets and real AMSU observations. It is shown that the biases and/or standard deviations of brightness temperatures are significantly reduced by using the B-G generated remapping coefficients. For K/Ka bands, a resolution enhancement by the remap of ATMS observations introduces about 0.6 K increase in noise. For other bands, the channel sensitivity was improved for the remapped data.","['Antennas', 'Noise', 'Microwave radiometry', 'Brightness temperature', 'Instruments', 'Meteorology', 'Microwave theory and techniques']","['Advanced Microwave Sounding Unit (AMSU)', 'Advanced Technology Microwave Sounder (ATMS)', 'remapping']"
"Radio waves traversing the Earth's ionosphere suffer from Faraday rotation with noticeable effects on measurements from lower frequency space-based radars, but these effects can be easily corrected given estimates of the Faraday rotation angle, i.e., Ω. Several methods to derive Ω from polarimetric measurements are known, but they are affected by system distortions (crosstalk and channel imbalance) and noise. A first-order analysis for the most robust Faraday rotation estimator leads to a differentiable expression for the bias in the estimate of Ω in terms of the amplitudes and phases of the distortion terms and the covariance properties of the target. The analysis applies equally to L-band and P-band. We derive conditions on the amplitudes and phases of the distortion terms that yield the maximum bias and a compact expression for its value for the important case where Ω = 0. Exact simulations confirm the accuracy of the first-order analysis and verify its predictions. Conditions on the distortion amplitudes that yield a given maximum bias are derived numerically, and the maximum bias is shown to be insensitive to the amplitude of the channel imbalance terms. These results are important not just for correcting polarimetric data but also for assessing the accuracy of the estimates of the total electron content derived from Faraday rotation.","['Faraday effect', 'Noise', 'Crosstalk', 'Distortion measurement', 'Accuracy', 'Biomass', 'Rotation measurement']","['Calibration', 'Faraday rotation', 'ionospheric structure', 'radar imaging', 'radar polarimetry']"
"Geometric positioning of a remote sensing image is one of the core technologies for the quantitative application of the geostationary satellite data. Affected by the change of the incident angle of the sunlight, the spatial thermal environment surrounding the remote sensing cameras (RSCs), especially the geostationary RSCs, fluctuates greatly and has a noticeable impact on the installation matrix based on the reference to the satellite body. Therefore, the spatial thermal environment will ultimately influence the camera's geometric positioning model and the final positioning accuracy. This paper proposes a novel correction method based on stellar observations for correcting geometric positioning error caused by spatial thermal deformation (STD) of geostationary optical payloads. The proposed method overcomes the drawbacks associated with current stabilization methods that involve shutting down the camera to reduce STD effects. Experimental results show that the positioning error corrected by the proposed method can be within ±1.9 pixels (2σ) at a 95% confidence level and better than the ±18 pixels before correction.","['Satellites', 'Adaptive optics', 'Optical imaging', 'Optical sensors', 'Orbits', 'Cameras', 'Strain']","['Error correction', 'geometric positioning model (GPM)', 'spatial thermal deformation (STD)']"
"This study develops a deep learning (DL) model to extract the ship size from Sentinel-1 synthetic aperture radar (SAR) images, named SSENet. We employ a single shot multibox detector (SSD)-based model to generate a rotatable bounding box (RBB) for the ship. We design a deep-neural-network (DNN)-based regression model to estimate the accurate ship size. The hybrid inputs to the DNN-based model include the initial ship size and orientation angle obtained from the RBB and the abstracted features extracted from the input SAR image. We design a custom loss function named mean scaled square error (MSSE) to optimize the DNN-based model. The DNN-based model is concatenated with the SSD-based model to form the integrated SSENet. We employ a subset of the OpenSARShip, a data set dedicated to Sentinel-1 ship interpretation, to train and test SSENet. The training/testing data set includes 1500/390 ship samples. Experiments show that SSENet is capable of extracting the ship size from SAR images end to end. The mean absolute errors (MAEs) are under 0.8 pixels, and their length and width are 7.88 and 2.23 m, respectively. The hybrid input significantly improves the model performance. The MSSE reduces the MAE of length by nearly 1 m and increases the MAE of width by 0.03m compared to the mean square error (MSE) loss function. Compared with the well-performed gradient boosting regression (GBR) model, SSENet reduces the MAE of length by nearly 2 m (18.68%) and that of width by 0.06 m (2.51%). SSENet shows robustness on different training/testing sets.","['Marine vehicles', 'Radar polarimetry', 'Feature extraction', 'Synthetic aperture radar', 'Data mining', 'Radar imaging', 'Oceans']","['Custom loss function', 'deep learning (DL)', 'deep neural network (DNN) regression', 'ship size extraction', 'synthetic aperture radar (SAR) image']"
"One of the biggest challenges in forestry research is the effective and accurate measuring and monitoring of forest variables, as the exploitation potential of forest inventory products largely depends on the accuracy of estimates and on the cost of data collection. This paper presented a novel computational method of low-cost forest inventory using global navigation satellite system (GNSS) signals in a crowdsourcing approach. Statistical features of GNSS signals were extracted from widely available GNSS devices and were used for predicting forest attributes, including tree height, diameter at breast height, basal area, stem volume, and above-ground biomass, in boreal forest conditions. The basic evidence of the predictions is the physical correlations between forest variables and the responses of GNSS signals penetrating through the forest. The random forest algorithm was applied to the predictions. GNSS-derived prediction accuracies were comparable with those of the most accurate 2-D remote sensing techniques, and the predictions can be improved further by integration with other publicly available data sources without additional cost. This type of crowdsourcing technique enables the collection of up-to-date forest data at low cost, and it significantly contributes to the development of new reference data collection techniques for forest inventory. Currently, field reference can account for half of the total costs of forest inventory.","['Crowdsourcing', 'Geospatial analysis', 'Receivers', 'Data collection', 'Vegetation', 'Biomass', 'Remote sensing']","['Crowdsourcing', 'forestry', 'global navigation satellite systems (GNSSs)', 'laser scanning', 'mobile mapping', 'radio propagation losses']"
"The Earth Clouds, Aerosol, and Radiation Explorer (EarthCARE) is a satellite mission jointly developed by the Japan Aerospace Exploration Agency (JAXA) and the European Space Agency (ESA). One challenging feature of this mission is the observation of Doppler velocity by the Cloud Profiling Radar (EC-CPR). The Doppler measurement accuracy is affected by random errors induced by Doppler broadening due to the finite beamwidth and Doppler folding caused by the finite pulse repetition frequency. We investigated the impact of horizontal (along-track) integration and unfolding methods on the reduction of Doppler errors, in order to improve Doppler data processing in the JAXA standard algorithm. We simulated EC-CPR-observed Doppler velocities from pulse-pair covariances with the latest EC-CPR specifications using the radar reflectivity factor and Doppler velocity fields simulated by a satellite data simulator and a global cloud system resolving simulation. Two representative cases of a cirrus cloud and precipitation were examined. In the cirrus cloud case, the standard deviation of random error was decreased to 0.5 m/s for −10 dBZeafter 10-km horizontal integration. In the precipitation case, large falling speeds of precipitation caused Doppler folding errors due to larger Doppler velocities than that in the cirrus cloud case. WhenZeis larger than −15 dBZe, the standard deviations of random error were less than 1.0 m/s after 10-km horizontal integration and unfolding.","['Doppler effect', 'Clouds', 'Spaceborne radar', 'Satellites', 'Radar', 'Doppler radar', 'Standards']","['Doppler measurement accuracy', 'Earth Clouds Aerosol and Radiation Explorer (EarthCARE)', 'Global Cloud System Resolving Models (GCSRMs)', 'horizontal integration', 'spaceborne Doppler radar', 'unfolding method']"
"Geodetic stereo synthetic aperture radar (SAR) is capable of absolute 3-D localization of natural persistent scatterers, which allows for ground control point (GCP) generation using only SAR data. The prerequisite for the method to achieve high-precision results is the correct detection of common scatterers in SAR images acquired from different viewing geometries. In this contribution, we describe three strategies for automatic detection of identical targets in SAR images of urban areas taken from different orbit tracks. Moreover, a complete workflow for automatic generation of large number of GCPs using SAR data is presented and its applicability is shown by exploiting TerraSAR-X high-resolution spotlight images over the city of Oulu, Finland, and a test site in Berlin, Germany.","['Synthetic aperture radar', 'Satellites', 'Geometry', 'Timing', 'Radar tracking', 'Orbits', 'Target tracking']","['Geodetic stereo synthetic aperture radar (SAR)', 'ground control point', 'positioning', 'SAR', 'TerraSAR-X (TS-X)']"
"Global Navigation Satellite System (GNSS) Reflectometry uses reflected GNSS signals for Earth remote sensing applications. Absolute calibration of a Delay Doppler Map (DDM) requires an accurate estimate of the effective isotropic radiated power (EIRP) of the GNSS transmitter, e.g., Global Positioning System (GPS). However, variable transmit power by numerous Block II Follow-on (IIF) and II Replenishment-Modernized (IIR-M) GPS space vehicles has been observed due to their flex power mode. Nonuniformity in the GPS antenna gain patterns further complicates EIRP estimation. A dynamic calibration approach is developed to address GPS EIRP variability. It uses measurements by the direct received GPS signal to estimate GPS EIRP in the specular reflected direction and then incorporates it into the calibration of normalized bistatic radar cross section (NBRCS). Error analyses using Monte Carlo simulations and a root sum of squares (RSS) approach show that the resulting error in NBRCS is about 0.32 dB. Dynamic EIRP calibration instantaneously detects and corrects for power fluctuations in the GPS transmitters and significantly reduces errors due to GPS antenna gain azimuthal asymmetry. It allows observations with the most variable Block IIF transmitters (approximately 37% of the GPS constellation) to be included in the standard data products and further improves the calibration quality of NBRCS and geophysical data products.","['Global Positioning System', 'Calibration', 'Sea surface', 'Wind speed', 'Satellites', 'Gain', 'Sea measurements']","['Bistatic radar', 'calibration', 'Cyclone Global Navigation Satellite System (CYGNSS)', 'effective isotropic radiated power (EIRP)', 'flex power', 'Global Navigation Satellite System-Reflectometry (GNSS-R)', 'Global Positioning System (GPS)', 'remote sensing']"
"One of the challenges in ocean surface current retrieval from synthetic aperture radar (SAR) data is the estimation and removal of the wave-induced Doppler centroid (DC). This article demonstrates empirically the relationship between the dc derived from spaceborne X-band InSAR data and the ocean surface wind and waves. In this study, we analyzed over 300 TanDEM-X image pairs. It is found that the general characteristics of the estimated dc follow the theoretically expected variation with incidence angle, wind speed, and wind direction. An empirical geophysical model function (GMF) is fit to the estimated dc and compared to existing models and previous experiments. Our GMF is in good agreement (within 0.2 m/s) with other models and data sets. It is found that the wind-induced Doppler velocity contributes to the total Doppler velocity with about 15% of the radial wind speed. This is much larger than the sum of the contributions from the Bragg waves (~0.2 m/s) and the wind-induced drift current (~3% of wind speed). This indicates a significant (dominant) contribution of the long wind waves to the SAR dc. Moreover, analysis of dual-polarized data shows that the backscatter polarization ratio (PR=σ0VV/σ0HH) and the dc polarization difference (PD=|dcVV|−|dcHH|) are systematically larger than 1 and smaller than 0 Hz, respectively, and both increase in magnitude with incidence angle. The estimated PR and PD are compared to other theoretical and empirical models. The Bragg scattering theory-based (pure Bragg and composite surface) models overestimate both PR and PD, suggesting that other scattering mechanisms, e.g., wave breaking, are involved. In general, it is found that empirical models are more consistent with both backscatter and Doppler data than theory-based models. This motivates a further improvement of SAR dc GMFs.","['Doppler effect', 'Synthetic aperture radar', 'Sea surface', 'Spaceborne radar', 'Surface waves', 'Extraterrestrial measurements', 'Data models']","['Along-track interferometry (ATI)', 'Doppler centroid (DC)', 'Doppler oceanography', 'ocean surface winds', 'ocean surface currents', 'synthetic aperture radar (SAR)', 'TanDEM-X']"
"Space-based low-frequency (L-band and below) synthetic aperture radar (SAR) is affected by the ionosphere. In particular, the phase scintillation causes the sidelobes to rise in a manner that can be predicted by an analytical theory of the point spread function (PSF). In this paper, the results of an experiment, in which a 5 m corner reflector on Ascension Island, was repeatedly imaged by PALSAR-2 in the spotlight mode are described. Many examples of the effect of scintillation on the SAR PSF were obtained, and all fit the theoretical model. This theoretical model of the PSF has then been used to determine two ionospheric turbulence parameters p and C k L from the SAR PSF. The values obtained have been compared with those obtained from simultaneous GPS measurements. Although the comparison shows that the two measures are strongly correlated, the differing spatial and temporal scales of SAR and GPS make exact comparison difficult.","['Synthetic aperture radar', 'Ionosphere', 'Radar tracking', 'Satellites', 'Spaceborne radar', 'Extraterrestrial measurements']","['Ionosphere', 'ionospheric electromagnetic propagation', 'synthetic aperture radar']"
"Space-based synthetic aperture radar (SAR) can be affected by the ionosphere, particularly at L-band and below. A technique is described that exploits the reduction in SAR image contrast to measure the strength of ionospheric turbulence parameter C k L. The theory describing the effect of the ionosphere on the SAR point spread function (PSF) and the consequent effect on clutter is reviewed and extended. This theory can then be used to determine C k L from both corner reflectors (CRs) and K-distributed SAR clutter. Measuring the K-distribution order parameter allows CkL values much lower than those that defocus the image to be determined. The results of an experiment in which a CR on Ascension Island was repeatedly imaged by PALSAR-2 in the spotlight mode during the scintillation season are described. The value of C k L obtained by measuring the clutter was compared with that obtained from a nearby CR. The correlation between the two was good using a median value of the spectral index p. This correlation was improved by using the measured value of p derived from the CR PSF. The technique works for any homogeneous K-distributed SAR clutter and is thus applicable to extra-terrestrial bodies as well as PALSAR-2 images of Ascension Island.","['Clutter', 'Synthetic aperture radar', 'Ionosphere', 'Image resolution', 'Speckle', 'Correlation', 'Apertures']","['Ionosphere', 'ionospheric electromagnetic propagation', 'synthetic aperture radar (SAR)']"
"Horizontally stratified media are commonly used to represent naturally occurring and man-made structures, such as soils, roads, and pavements, when probed by ground-penetrating radar (GPR). Electromagnetic (EM) wave scattering from such multilayered media is dependent on the roughness of the interfaces. In this paper, we developed a closed-form asymptotic EM model considering random rough layers based on the scalar Kirchhoff-tangent plane approximation (SKA) model that we combined with planar multilayered media Green's functions. In order to validate our extended SKA model, we conducted simulations using a numerical EM solver based on the finite-difference time-domain (FDTD) method. We modeled a medium with three layers-a base layer of perfect electric conductor (PEC) overlaid by two layers of different materials with rough interfaces. The reflections at the first and at the second interface were both well reproduced by the SKA model for each roughness condition. For the reflection at the PEC surface, the extended SKA model slightly overestimated the reflection, and this overestimation increased with the roughness amplitude. Good agreement was also obtained between the FDTD simulation input values and the inverted root mean square (rms) height estimates of the top interface, while the inverted rms heights of the second interface were slightly overestimated. The accuracy and the performances of our asymptotic forward model demonstrate the promising perspectives for simulating rough multilayered media and, hence, for the full waveform inversion of GPR data to noninvasively characterize soils and materials.","['Media', 'Scattering', 'Ground penetrating radar', 'Rough surfaces', 'Surface roughness', 'Sea surface', ""Green's function methods""]","['Finite-difference time-domain (FDTD)', 'gprMax', 'Green’s function', 'ground-penetrating radar (GPR)', 'Kirchhoff-tangent plane approximation (KA)', 'model inversion', 'multilayered media', 'radar', 'rough interfaces', 'scattering']"
"Multipass SAR interferometry (InSAR) techniques based on meter-resolution spaceborne SAR satellites, such as TerraSAR-X or COSMO-SkyMed, provide 3D reconstruction and the measurement of ground displacement over large urban areas. Conventional methods such as persistent scatterer interferometry (PSI) usually requires a fairly large SAR image stack (usually in the order of tens) to achieve reliable estimates of these parameters. Recently, low rank property in multipass InSAR data stack was explored and investigated in our previous work (J. Kang et al. , “Object-based multipass InSAR via robust low-rank tensor decomposition,” IEEE Trans. Geosci. Remote Sens. , vol. 56, no. 6, 2018). By exploiting this low rank prior, a more accurate estimation of the geophysical parameters can be achieved, which in turn can effectively reduce the number of interferograms required for a reliable estimation. Based on that, this article proposes a novel tensor decomposition method in a complex domain, which jointly exploits low rank and variational prior of the interferometric phase in InSAR data stacks. Specifically, a total variation (TV) regularized robust low rank tensor decomposition method is exploited for recovering outlier-free InSAR stacks. We demonstrate that the filtered InSAR data stacks can greatly improve the accuracy of geophysical parameters estimated from real data. Moreover, this article demonstrates for the first time in the community that tensor-decomposition-based methods can be beneficial for large-scale urban mapping problems using multipass InSAR. Two TerraSAR-X data stacks with large spatial areas demonstrate the promising performance of the proposed method.","['Tensors', 'TV', 'Strain', 'Image reconstruction', 'Synthetic aperture radar', 'Parameter estimation', 'Semantics']","['Inteferometric SAR (InSAR)', 'low rank', 'synthetic aperture radar (SAR)', 'tensor decomposition', 'total variation (TV)']"
"The results of remote sensing temperature profiles measurements within a 0-600-m altitude range and total water content measurements during total (Kislovodsk, 2006; Novosibirsk, 2008) and partial (Moscow, 2011) solar eclipses, using microwave radiometers are presented. Initially, continuous data on temperature profiles are obtained at different altitudes before, during, and after total solar eclipses, using two single channel elevation scanning microwave temperature profilers. Terrestrial consequences of solar eclipses (especially total ones) are quite noticeable and important. Solar eclipses support unique, specific conditions, which gives the opportunity for various meteorological research. The most important indicator of thermodynamic processes occurring during solar eclipses is air temperature at different altitudes in the atmospheric boundary layer (ABL). The ABL temperature depends, in general, on the flux of solar radiation and some features of the ground (albedo, absorptivity, and emissivity) and the air (humidity). Temperature profile measurements are accompanied by solar radiation (with net-radiometer) and total water vapor (with microwave radiometers) measurements. The observation results of this paper will contribute detailed model calculations for clarifying meteorological effects of solar eclipses. Observations of the next total solar eclipse over Russia (August 12, 2026) can be used to verify our observational results.","['Temperature measurement', 'Microwave measurement', 'Microwave radiometry', 'Cities and towns', 'Ocean temperature', 'Temperature sensors', 'Microwave theory and techniques']","['Atmospheric measurements', 'humidity measurement', 'microwave radiometry', 'radiation effects', 'solar radiation', 'temperature measurement']"
"Associating a radar scatterer to a physical object is crucial for the correct interpretation of interferometric synthetic aperture radar measurements. Yet, especially for medium-resolution imagery, this is notoriously difficult and dependent on the accurate 3-D positioning of the scatterers. Here, we investigate the 3-D positioning capabilities of ENVISAT medium-resolution data. We find that the data are perturbed by range-and-epoch-dependent timing errors and calibration offsets. Calibration offsets are estimated to be about 1.58 m in azimuth and 2.84 m in range and should be added to ASAR products to improve geometric calibration. The timing errors involve a bistatic offset, atmospheric path delay, solid earth tides, and local oscillator drift. This way, we achieve an unbiased positioning capability in 2-D, while in 3-D, a scatterer was located at a distance of 28 cm from the true location. 3-D precision is now expressed as an error ellipsoid in local coordinates. Using the Bhattacharyya metric, we associate radar scatterers to real-world objects. Interpreting deformation of individual infrastructure is shown to be feasible for this type of medium-resolution data.","['Radar imaging', 'Azimuth', 'Synthetic aperture radar', 'Radar scattering', 'Spaceborne radar', 'Timing']","['3-D accuracy', 'ENVISAT', 'error ellipsoid', 'geocoding', 'geolocation', 'infrastructure monitoring', 'medium resolution', 'positioning', 'synthetic aperture radar (SAR)', 'target association']"
"Timely and up-to-date bathymetry maps over large geographical areas have been difficult to create, due to the cost and difficulty of collecting in situ calibration and validation data. Recently, combinations of spaceborne Ice, Cloud, and Elevation Satellite-2 (ICESat-2) lidar data and Landsat/sentinel-2 data have reduced these obstacles. However, to date, there have been no means of automatically extracting bathymetry photons from ICESat-2 tracks for model calibration/validation and no well-established open source workflows for generating regional scale bathymetric models. Here we provide an open source approach for generating bathymetry maps for the shallow water region around the island of Andros, Bahamas. We demonstrate an efficient means of processing 224 ICESat-2 tracks and 221 Landsat-8 scenes, using the classification of subaquatic height extracted photons (C-SHELPh) algorithm and Extra Trees Regression to provide 30 m pixel estimates of per-pixel depth and standard error. We map bathymetry with an RMSE of 0.32 m and RMSE% of 6.7%. Our workflow and results demonstrate a means of achieving accurate regional–scale bathymetry maps from purely spaceborne data.","['Photonics', 'Bathymetry', 'Earth', 'Sea surface', 'Remote sensing', 'Artificial satellites', 'Oceans']","['Bathymetry', 'Ice', 'Cloud and Elevation Satellite-2 (ICESat-2)', 'landsat8', 'machine learning']"
"The ionospheric propagation path delay is a major error source in synthetic aperture radar (SAR) interferograms and, therefore, has to be estimated and corrected. Various methods can be used to extract different kinds of information about the ionosphere from SAR images, with different accuracies. This paper presents a general technique, based on a Bayesian inverse problem, that combines various information sources in order to increase the estimation accuracy, and thus the correction. A physically realistic fractal modeling of the ionosphere turbulence and a data-based estimation of the model parameters allow the avoidance of arbitrary filtering windows and coefficients. To test the technique, the differential ionospheric phase screen was estimated by combining the split-spectrum method with the azimuth mutual shifts between interferometric pair images. This combination is convenient since it can benefit from the strengths of both sources: range and azimuth variations from the split-spectrum method and small-scale azimuth variations from more sensitive azimuth shifts. Therefore, the two methods can recover the long and short wavelength components of the ionospheric phase screen, respectively. A theoretical comparison between the Faraday rotation method and the split-spectrum method is also reported. For the use in the combination, precedence was then given to the split-spectrum method because of the comparable precision level, lower susceptibility to biases, and wider applicability. Finally, Advanced Land Observing Satellite Phased Array type L-band SAR L-band images are used to show how the combined result is more accurate than that obtained with the simple split-spectrum method.","['Ionosphere', 'Estimation', 'Azimuth', 'Synthetic aperture radar', 'Faraday effect', 'Extraterrestrial measurements', 'Bayes methods']","['Ionosphere estimation', 'interferometric synthetic aperture radar (SAR)', 'methods’ combination', 'SAR ionospheric effects']"
"Object retrieval and reconstruction from very-high-resolution (VHR) synthetic aperture radar (SAR) images are of great importance for urban SAR applications, yet highly challenging due to the complexity of SAR data. This article addresses the issue of individual building segmentation from a single VHR SAR image in large-scale urban areas. To achieve this, we introduce building footprints from geographic information system (GIS) data as a complementary information and propose a novel conditional GIS-aware network (CG-Net). The proposed model learns multilevel visual features and employs building footprints to normalize the features for predicting building masks in the SAR image. We validate our method using a high-resolution spotlight TerraSAR-X image collected over Berlin. Experimental results show that the proposed CG-Net effectively brings improvements with variant backbones. We further compare two representations of building footprints, namely, complete building footprints and sensor-visible footprint segments, for our task, and conclude that the use of the former leads to better segmentation results. Moreover, we investigate the impact of inaccurate GIS data on our CG-Net, and this study shows that CG-Net is robust against positioning errors in the GIS data. In addition, we propose an approach of ground truth generation of buildings from an accurate digital elevation model (DEM), which can be used to generate large-scale SAR image data sets. The segmentation results can be applied to reconstruct 3-D building models at level-of-detail (LoD) 1, which is demonstrated in our experiments.","['Buildings', 'Radar polarimetry', 'Image segmentation', 'Feature extraction', 'Synthetic aperture radar', 'Urban areas', 'Data mining']","['Deep convolutional neural network (CNN)', 'geographic information system (GIS)', 'individual building segmentation', 'large-scale urban areas', 'synthetic aperture radar (SAR)']"
"In the field of remote sensing image, how to transmit image information more efficiently with limited bandwidth has always been a research hotspot. Compared with other ground objects, cloud pixels in remote sensing image are invalid information, so it is a meaningful research work to remove cloud before transmitting image and reduce the waste of useless information. In remote sensing image, due to the existence of thin clouds and the complexity of the underlying surface, most of the cloud detection algorithms struggle to achieve effective separation of clouds and ground objects. A deep learning (DL) cloud detection algorithm based on attention mechanism and probability upsampling has been proposed in this article. In order to enhance the information of the key areas, in the channel attention module, crucial information is highlighted in the channel dimension of the encoder, and the useless information is weakened. The spatial attention module is in the spatial dimension. The information fusion between each point in the image is strengthened. To reduce the information loss caused by the down-sampling module, a probabilistic upsampling block (PUB) is proposed to restore the image. Eventually, experiments are performed on Gaofen-1WFV data, and the results indicate that the algorithm proposed in this article has better detection results than other cloud detection algorithms in different scenarios.","['Remote sensing', 'Image segmentation', 'Feature extraction', 'Convolution', 'Image coding', 'Visualization', 'Probabilistic logic']","['Channel attention', 'cloud detection', 'probabilistic upsampling', 'spatial attention']"
"Developments in computer vision, such as structure from motion and multiview stereo reconstruction, have enabled a range of photogrammetric applications using unmanned aerial vehicles (UAV)-based imagery. However, some specific cases still present reconstruction challenges, including survey areas composed of steep, overhanging, or vertical rock formations. Here, the suitability and geometric accuracy of four UAV-based image acquisition and data processing scenarios for topographic surveying applications in complex terrain are assessed and compared. The specific cases include the use of: 1) nadir imagery; 2) nadir and oblique imagery; 3) nadir and façade imagery; and 4) nadir, oblique, and façade imagery to reconstruct a topographically complex natural surface. Results illustrate that including oblique and façade imagery to supplement the more traditional nadir collections significantly improves the geometric accuracy of point cloud data reconstruction by approximately 35% when assessed against terrestrial laser scanning data of near-vertical rock walls. Most points (99.41%) had distance errors of less than 50 cm between the point clouds derived from the nadir imagery and nadir–oblique–façade imagery. Apart from delivering enhanced spatial resolution in façade details, the geometric accuracy improvements achieved from integrating nadir, oblique, and façade imagery provide value for a range of applications, including geotechnical and geohazard investigations. Such gains are particularly relevant for studies assessing rock integrity and stability, and engineering design, planning, and construction, where information on the position of rock cracks, joints, faults, shears, and bedding planes may be required.","['Rocks', 'Three-dimensional displays', 'Image reconstruction', 'Surface topography', 'Surface reconstruction', 'Spatial resolution', 'Sea surface']","['Computer vision', 'photogrammetry', 'structure from motion (SfM)', 'topographic survey', 'unmanned aerial vehicles (UAV)']"
"Near-earth hyperspectral big data present both huge opportunities and challenges for spurring developments in agriculture and high-throughput plant phenotyping and breeding. In this article, we present data-driven approaches to address the calibration challenges for utilizing near-earth hyperspectral data for agriculture. A data-driven, fully automated calibration workflow that includes a suite of robust algorithms for radiometric calibration, bidirectional reflectance distribution function (BRDF) correction and reflectance normalization, soil and shadow masking, and image quality assessments was developed. An empirical method that utilizes predetermined models between camera photon counts (digital numbers) and downwelling irradiance measurements for each spectral band was established to perform radiometric calibration. A kernel-driven semiempirical BRDF correction method based on the Ross Thick-Li Sparse (RTLS) model was used to normalize the data for both changes in solar elevation and sensor view angle differences attributed to pixel location within the field of view. Following rigorous radiometric and BRDF corrections, novel rule-based methods were developed to conduct automatic soil removal; and a newly proposed approach was used for image quality assessment; additionally, shadow masking and plot-level feature extraction were carried out. Our results show that the automated calibration, processing, storage, and analysis pipeline developed in this work can effectively handle massive amounts of hyperspectral data and address the urgent challenges related to the production of sustainable bioenergy and food crops, targeting methods to accelerate plant breeding for improving yield and biomass traits.","['Calibration', 'Hyperspectral imaging', 'Radiometry', 'Atmospheric measurements', 'Atmospheric modeling', 'Artificial intelligence', 'Cameras']","['Bidirectional reflectance distribution function (BRDF) correction', 'high-throughput phenotyping', 'image quality assessment', 'shadow compensation', 'soil removal']"
"Previous applications of machine learning in remote sensing for the identification of damaged buildings in the aftermath of a large-scale disaster have been successful. However, standard methods do not consider the complexity and costs of compiling a training data set after a large-scale disaster. In this article, we study disaster events in which the intensity can be modeled via numerical simulation and/or instrumentation. For such cases, two fully automatic procedures for the detection of severely damaged buildings are introduced. The fundamental assumption is that samples that are located in areas with low disaster intensity mainly represent nondamaged buildings. Furthermore, areas with moderate to strong disaster intensities likely contain damaged and nondamaged buildings. Under this assumption, a procedure that is based on the automatic selection of training samples for learning and calibrating the standard support vector machine classifier is utilized. The second procedure is based on the use of two regularization parameters to define the support vectors. These frameworks avoid the collection of labeled building samples via field surveys and/or visual inspection of optical images, which requires a significant amount of time. The performance of the proposed method is evaluated via application to three real cases: the 2011 Tohoku-Oki earthquake–tsunami, the 2016 Kumamoto earthquake, and the 2018 Okayama floods. The resulted accuracy ranges between 0.85 and 0.89, and thus, it shows that the result can be used for the rapid allocation of affected buildings.","['Buildings', 'Training', 'Remote sensing', 'Training data', 'Earthquakes', 'Support vector machines', 'Machine learning']","['Automatic labeling', 'building damage', 'multiregularization parameters', 'support vector machine (SVM)']"
"Remote sensing image classification task is challenging due to the characteristics of complex composition, so different geographic elements in the same image will interfere with each other, resulting in misclassification. To solve this problem, we propose a multibranch ensemble network to enhance the feature representation ability by fusing final output logits and intermediate feature maps. However, simply adding branches will increase the complexity of models and decline the inference efficiency. To reduce the complexity of multibranch network, we make multibranch share more weights and add feature augmentation modules to compensate for the lack of diversity caused by weight sharing. To improve the efficiency of inference, we embed self-distillation (SD) method to transfer knowledge from ensemble network to main branch. Through optimizing with SD, the main branch will have close performance as an ensemble network. In this way, we can cut other branches during inference. In addition, we simplify the process of SD and totally adopt two loss functions to self-distill the logits and feature maps. In this article, we design a compact multibranch ensemble network, which can be trained in an end-to-end manner. Then, we insert an SD method on output logits and feature maps. Our proposed architecture (ESD-MBENet) performs strongly on classification accuracy with compact design. Extensive experiments are applied on three benchmark remote sensing datasets, AID, NWPU-RESISC45, and UC-Merced with three classic baseline models, VGG16, ResNet50, and DenseNet121. Results prove that ESD-MBENet can achieve better accuracy than previous state-of-the-art complex deep learning models. Moreover, abundant visualization analyses make our method more convincing and interpretable.","['Remote sensing', 'Feature extraction', 'Image classification', 'Task analysis', 'Deep learning', 'Knowledge engineering', 'Sensors']","['Multibranch ensemble network', 'network pruning', 'remote sensing scene classification', 'self-distillation (SD)']"
"A number of endmember extraction methods have been developed to identify pure pixels in hyperspectral images (HSIs). The majority of them use only one spectrum to represent one kind of material, which ignores the spectral variability problem that particularly characterizes a HSI with high spatial resolution. Only a few algorithms have been developed to identify multiple endmembers representing the spectral variability within each class, called endmember bundle extraction (EBE). This article introduces multiobjective particle swarm optimization for the identification of multiple endmember spectra with variability. Unlike existing convex geometry-based EBE methods, which operate on a single geometry of the dataspace, the proposed method divides the observed data into subsets along the spectral dimension and simultaneously operates on multiple dataspaces to obtain candidate endmembers based on multiobjective particle swarm optimization. The candidate endmembers are then refined by spatial post-processing and sequential forward floating selection to produce the final result. Experiments are conducted on both synthetic and real hyperspectral data to demonstrate the effectiveness of the proposed method in comparison with several state-of-the-art methods.","['Optimization', 'Hyperspectral imaging', 'Indexes', 'Data mining', 'Particle swarm optimization', 'Linear programming', 'Pareto optimization']","['Endmember bundle extraction (EBE)', 'hyperspectral', 'multiobjective optimization', 'spectral variability']"
"This article reports the Phase A study results of the interferometric extension of the high-resolution wide-swath (HRWS) mission with three MirrorSAR satellites. According to the MirrorSAR concept, small, low-cost, transponder-like receive-only satellites without radar signal demodulation, digitization, memory storage, downlink, and synchronization are added to the planned German X-band HRWS mission. The MirrorSAR satellites fly a triple helix orbit in close formation around the HRWS orbit and span multiple single-pass interferometric baselines. A comprehensive system engineering and performance analysis is provided that includes orbit formation, MirrorLink, Doppler steering, antenna pattern and swath design, multi-static echo window timing, SAR performance, height performance, and coverage analysis. The overall interferometric system design analysis of Phase A is presented. The predicted performance of the global digital elevation model (DEM) is improved by one order of magnitude compared to presently available global DEM products such as the TanDEM-X DEM.","['Satellites', 'Synthetic aperture radar', 'Spaceborne radar', 'Orbits', 'Radar antennas', 'Azimuth', 'Radar remote sensing']","['Digital elevation model (DEM)', 'high-resolution wide-swath (HRWS)', 'interferometry', 'mirrorSAR', 'synthetic aperture radar (SAR)']"
"Agricultural drought monitoring and prediction technology are urgently needed. We applied an ecohydrological land data assimilation system (LDAS), which can simulate soil moisture and leaf area index (LAI) by data assimilation of microwave brightness temperature into a land surface model (LSM), to monitor and predict agricultural droughts in North Africa. We successfully monitor nationwide crop failures, which are characterized by the declines of the nationwide wheat production, in Morocco, Algeria, and Tunisia using LAI and soil moisture calculated by the LDAS. Our simulated LAI is well correlated with the nationwide wheat production (r = 0.70, 0.65, and 0.72 in Morocco, Algeria, and Tunisia, respectively). A general circulation model (GCM)-based seasonal meteorological prediction significantly contributes to accurately predicting LAI and agricultural droughts in 2-3-month lead time. In addition, it is found that initial conditions have an important role in predicting LAI. We demonstrate the capability of our framework to monitor and predict agricultural drought in North Africa. Our proposed framework can contribute to mitigating the negative impact of drought on agriculture in poorly gauged water-limited subcontinental regions.","['Monitoring', 'Vegetation mapping', 'Soil moisture', 'Temperature sensors', 'Agriculture', 'Temperature measurement', 'Production']","['Drought', 'land data assimilation', 'passive microwave remote sensing']"
"Spectral–spatial-based deep learning models have recently proven to be effective in hyper-spectral image (HSI) classification for various earth monitoring applications such as land cover classification and agricultural monitoring. However, due to the nature of “black-box” model representation, how to explain and interpret the learning process and the model decision, especially for vegetation classification, remains an open challenge. This study proposes a novel interpretable deep learning model—a biologically interpretable two-stage deep neural network (BIT-DNN), by incorporating the prior-knowledge (i.e., biophysical and biochemical attributes and their hierarchical structures of target entities)-based spectral–spatial feature transformation into the proposed framework, capable of achieving both high accuracy and interpretability on HSI-based classification tasks. The proposed model introduces a two-stage feature learning process: in the first stage, an enhanced interpretable feature block extracts the low-level spectral features associated with the biophysical and biochemical attributes of target entities; and in the second stage, an interpretable capsule block extracts and encapsulates the high-level joint spectral–spatial features representing the hierarchical structure of biophysical and biochemical attributes of these target entities, which provides the model an improved performance on classification and intrinsic interpretability with reduced computational complexity. We have tested and evaluated the model using four real HSI data sets for four separate tasks (i.e., plant species classification, land cover classification, urban scene recognition, and crop disease recognition tasks). The proposed model has been compared with five state-of-the-art deep learning models. The results demonstrate that the proposed model has competitive advantages in terms of both classification accuracy and model interpretability, especially for vegetation classification.","['Feature extraction', 'Biological system modeling', 'Deep learning', 'Vegetation mapping', 'Data models', 'Biology', 'Neural networks']","['Classification', 'deep learning', 'hyper-spectral images (HSIs)', 'interpretability']"
"The midinfrared (MIR) spectral region (3-5 μm), which penetrates most haze layers in the atmosphere and is less sensitive to variations in atmospheric water vapor, seems to be appropriate for retrieving land surface temperature (LST). However, there are currently few studies of LST retrieval with MIR data because it is difficult to eliminate solar irradiance from the total energy measured in the MIR during the daytime. This paper proposes a physics-based method to retrieve LST from MODIS daytime MIR data. The bidirectional reflectivity describing the reflected solar direct irradiance is determined using the method by Tang and Li. The directional emissivity, representing the surface emitted radiance, is determined by a kernel-driven bidirectional reflectance distribution function model, i.e., RossThick-LiSparse-R. Intercomparisons using the MODIS-derived LST product MYD11_L2, for the Baotou experimental site in Urad Qianqi, Inner Mongolia, China, have a maximum root-mean-square error (RMSE) of 1.69 K and a minimum RMSE of 1.31 K, for four scenes of MODIS images. Furthermore, in situ LSTs measured at the Hailar field site in northeastern Inner Mongolia, China, were also used to validate the proposed method. Comparisons of the LSTs retrieved from MODIS daytime MIR data and those calculated using in situ measurements have a bias and RMSE of -0.17 K and 1.42 K, respectively, which indicates that the proposed method can accurately retrieve LST from MODIS daytime MIR data.","['Land surface temperature', 'MODIS', 'Atmospheric measurements', 'Land surface', 'Satellites', 'Temperature measurement', 'Atmosphere']","['Daytime', 'land surface temperature (LST)', 'midinfrared (MIR)', 'MODIS']"
"Meteosat Third Generation (MTG) is the next generation of European meteorological geostationary satellites, set to be launched in 2021. Besides ensuring continuity with Meteosat Second Generation imagery mission, the new series will feature new instruments, such as the Lightning Imager (LI), a high-speed optical detector providing near real-time lightning detection capabilities over Europe and Africa. The instrument will register events on pixels, where a lightning pulse generates a transient in the acquired radiance. In parallel, signal variations due to a number of unwanted sources, e.g., acquisition noise or jitter movement, are expected to produce false events. The challenge for on-board and on-ground processing is, thus, to discard as many false events as possible while keeping a majority of the true lightning events. This paper discusses a chain of algorithms that can be used by the LI for the detection of lightning and for the filtering of false events. Some of these algorithms have been developed in the framework of internal research and simulations conducted by the MTG team at the European Space Agency on an in-house LI simulator and therefore will not necessarily reflect the ultimate operational processing chain. The application of the chain on a representative scenario shows that 99.5% of the false events can be eliminated while keeping 83.6% of the true events, before generating the LI higher level data products. Machine learning techniques have also been studied as an alternative for on-ground event processing, and preliminary results indicate promising potential.","['Lightning', 'Europe', 'Instruments', 'Space vehicles', 'Transient analysis', 'Satellites', 'Optical imaging']","['Filtering', 'jitter', 'lightning', 'machine learning', 'Meteosat Third Generation (MTG)', 'Satellite Meteorology', 'transient detection']"
"Aerial scene classification remains challenging as: 1) the size of key objects in determining the scene scheme varies greatly and 2) many objects irrelevant to the scene scheme are often flooded in the image. Hence, how to effectively perceive the region of interests (RoIs) from a variety of sizes and build more discriminative representation from such complicated object distribution is vital to understand an aerial scene. In this article, we propose a novel all grains, one scheme (AGOS) framework to tackle these challenges. To the best of our knowledge , it is the first work to extend the classic multiple instance learning (MIL) into multigrain formulation. Specifically, it consists of a multigrain perception (MGP) module, a multibranch multi-instance representation (MBMIR) module, and a self-aligned semantic fusion (SSF) module. First, our MGP module preserves the differential dilated convolutional features from the backbone, which magnifies the discriminative information from multigrains. Then, our MBMIR module highlights the key instances in the multigrain representation under the MIL formulation. Finally, our SSF module allows our framework to learn the same scene scheme from multigrain instance representations and fuses them, so that the entire framework is optimized as a whole. Notably, our AGOS is flexible and can be easily adapted to existing convolutional neural networks (CNNs) in a plug-and-play manner. Extensive experiments on UCM, aerial image dataset (AID), and Northwestern Polytechnical University (NWPU) benchmarks demonstrate that our AGOS achieves a comparable performance against the state-of-the-art methods.","['Image analysis', 'Semantics', 'Deep learning', 'Remote sensing', 'Computer vision', 'Feature extraction', 'Convolutional neural networks']","['Aerial scene classification', 'differential dilated convolution (DDC)', 'multigrain instance representation', 'multiple instance learning (MIL)', 'self-alignment strategy']"
"This article assesses the coherency of Global Navigation Satellite System (GNSS) signals reflected off the oceans and sea ice under grazing angle geometries and received aboard low Earth orbit (LEO) CubeSats for precision altimetry applications. The coherency is characterized as a function of ocean surface conditions and reflected signal parameters based on Spire Global CubeSat data collected from January to April 2019. The data contain 50-Hz GPS L1 and L2 carrier phase estimations obtained by open-loop tracking. Indicators based on the circular statistics of the excess-phase noise are developed to identify coherent and semicoherent reflections. Based on these indicators, we found that ~1% and 44% of GPS reflections over the ocean and sea ice, respectively, have potential for precision altimetry. The coherent and semicoherent reflection rates reach 23% in areas less than 200 km from the coastline and under calm sea conditions. Over young sea ice over the Arctic, this rate can be as high as 70%. There is a strong relationship between coherency and signal strength, and the coherency occurrence rate improves as the grazing angle decreases. The quality of the L1 and L2 coherent reflections is similar over sea ice, while, for reflections over the ocean, L1 signals are predominantly noisier and less coherent than the L2 signals. Using a postprocessing filtering method, the semicoherent reflections can achieve a similar level of altimetry precision as that of the coherent ones, thereby increasing the along-track length of the retrieved altimetry profile.","['Reflection', 'CubeSat', 'Altimetry', 'Sea surface', 'Global navigation satellite system', 'Sea ice', 'Global Positioning System']","['Carrier phase estimations', 'coherent reflections', 'GPS reflection', 'ocean surface', 'precision altimetry', 'sea ice', 'semicoherent reflections']"
"The highly reflective nature of high particulate inorganic carbon (PIC) from calcifying plankton, such as surface blooms of Emiliana huxleyi in the latter stages of their life cycle, can cause the saturation of the Moderate Resolution Imaging Spectrometer (MODIS) visible spectrum ocean color bands. This saturation results in errors in the standard MODIS oceanic PIC product, resulting in the highest PIC levels being represented as cloud-like gaps (missing data) in daily level 2 data, and as either gaps or erroneously low PIC values in temporally averaged data (e.g., 8-day level 3 data). A method is described to correct this error and to reconstruct the missing data in the ocean color band data by regressing the 1-km spatial resolution ocean color bands against MODIS higher resolution (500 m spatial resolution) bands with lower sensitivities. The method is applied to all North Atlantic MODIS data from 2002 to 2014. This shows the effect on mean PIC concentration over the whole North Atlantic to be less than 1% annually and 2% monthly, but with more significant regional effects, exceeding 10% in peak months in some coastal shelf regions. Effects are highly localized and tend to annually reoccur in similar geographical locations. Ignoring these missing data within intense blooms is likely to result in an underestimation of the influence that coccolithophores, and their changing distributions, are having on the North Atlantic carbon cycle. We see no evidence in this 12-year time series of a temporal poleward movement of these intense bloom events.","['Image color analysis', 'Oceans', 'MODIS', 'Spatial resolution', 'Sea measurements', 'Clouds', 'Satellites']","['Optical saturation', 'remote sensing', 'satellite applications', 'sea surface']"
"Rapidly and accurately estimating yields at field scales are very significant. Each type of currently yield estimation model has been well studied, yet all of them have certain limitations. Based on a coupled Carnegie-Ames-Stanford approach (CASA)-World Food Studies (WOFOST) model and time series Sentinel-2 imagery, we achieved daily crop simulations and crop yield estimations at two adjacent farms in China. The results indicated that the coupled model inherited the high computing speed of light use efficiency (LUE) models and the mechanistic advantages of crop growth models. TheR2of yield simulation was 0.64 for the CASA model, 0.84 for the coupled model, and 0.86 for the WOFOST model, and the root mean square error (RMSE) values were 948.32, 792.11, and 623.64 kg/ha, respectively. The operating times of the CASA model, CASA-WOFOST coupled model, and WOFOST model over the growing season of wheat were 37 min, 48 min, and 1 day 5 h 7 min, respectively. Compared with the WOFOST model, the coupled model provided a much faster running speed in yield simulations and a similar accuracy; therefore, the proposed model can be applied for assessments at large farms with high-spatial-resolution images to obtain accurate yield simulations. Compared with the CASA model, the coupled model provided higher simulation accuracy in mountainous areas and regions of uneven terrain. It can be concluded that the proposed coupled CASA-WOFOST model can improve the precision, reliability, and stability of crop yield estimation; provide theoretical support for crop yield estimation at field scales; and promote the development of precision agriculture.","['Agriculture', 'Biological system modeling', 'Data models', 'Soil', 'Yield estimation', 'Atmospheric modeling', 'Biomass']","['Carnegie-Ames-Stanford approach (CASA) model', 'data assimilation', 'high temporal resolution normalized differential vegetation index (NDVI)', 'remote sensing (RS)', 'world food studies (WOFOST) model', 'yield estimation']"
"The purpose of seismic reservoir characterization is to predict the spatial distribution of the subsurface rock properties from a set of direct and indirect measurements. Commonly, rock property volumes are obtained in a two-steps approach. First, the elastic properties are inverted from seismic reflection data and then used to compute rock properties volumes by applying a calibrated rock physics models. Such an approach is not only time consuming but may also lead to biased results as the uncertainties related to seismic inversion may not be propagated through the entire geomodeling workflow. Here, we propose an iterative geostatistical shale rock physics seismic AVA inversion method to invert seismic reflection data directly for shale rock properties. The workflow consists of three main steps starting from shale properties model generation of brittleness, total organic carbon (TOC), and porosity using stochastic sequential simulation and cosimulation and calculation of a volume of shale. In the following step, elastic property volumes are calculated based on a calibrated shale rock physics model using the self-consistent approximation. The elastic models are then used for the calculation of the synthetic seismic data. In the final step, the misfit between synthetic and real data is calculated and used as part of the stochastic update of the model parameter space. The whole process is repeated until a minimum misfit between the observed and synthetic seismic is achieved. The proposed method is successfully tested on an onshore Lower Paleozoic shale gas reservoir in Northern Poland, and the predicted shale rock properties agree with those observed at a blind-well location.","['Rocks', 'Data models', 'Computational modeling', 'Physics', 'Predictive models', 'Stochastic processes', 'Reservoirs']","['Geostatistics', 'rock physics', 'seismic inversion', 'stochastic inversion']"
"In this study, a new deep learning method was developed to estimate the spatiotemporal properties of the hourly aerosol optical depth (AOD) because existing physical models are limited in their abilities to separate the reflectance between aerosols and the underlying surface over land, accurately and effectively. By incorporating geostationary ocean color imagery (GOCI), multispectral bands were applied to train data-driven models to estimate the high-spatiotemporal-resolution AOD over Northeast Asia. Physical model and traditional machine learning (ML) models (the random forest (RF) and support vector regression (SVR) models) were compared with the deep neural network (DNN) model to evaluate its accuracy, implementing hold-out validation andk-fold cross-validation approaches. In the statistical results of the hold-out validation, the DNN model showed the higher accuracy (root mean square error (RMSE) = 0.112, mean bias error (MBE) = 0.007, and correlation coefficient(R)=0.863) relative to the traditional SVR (RMSE = 0.123, MBE = −0.010, andR=0.833) and RF (RMSE = 0.125, MBE = 0.004, andR=0.825) models. The DNN model also exhibited the best performance for most statistical metrics among the traditional SVR, RF, and selected physical models (except for the correlation coefficients and index of agreement) in the spatial and temporal cross-validation analyses. Although the DNN model was trained using the match-up dataset between the top of atmosphere (TOA) reflectance from GOCI multispectral bands and AErosol RObotic NETwork measurements, it showed high spatial and temporal generalization performance owing to its deeper and more complicated network structure. Hourly GOCI AOD data obtained using a deep learning approach with high accuracy are expected to be useful for the quantification of aerosol contents and monitoring of diurnal variations in the AOD.","['Atmospheric modeling', 'Aerosols', 'Optical imaging', 'Optical sensors', 'Atmospheric measurements', 'Sun', 'Land surface']","['Aerosol optical depth (AOD)', 'deep neural network (DNN)', 'geostationary ocean color imagery (GOCI) satellite', 'Northeast Asia', 'random forest (RF)', 'support vector regression (SVR)']"
"P-band synthetic aperture radar (SAR) is sensitive to above-ground biomass (AGB) but retrieval accuracy has been shown to deteriorate in topographic areas. In boreal forest, the signal penetrates through the canopy to interact with the ground producing variations in backscatter depending on ground topography, forest structure, and soil moisture. Tomographic processing of multiple SAR images Tomographic SAR (TomoSAR) provides information about the vertical backscatter distribution. This article evaluates the use of P-band TomoSAR data to improve AGB retrievals from backscattered intensity by suppressing the backscattered signal from the ground. This approach can be used even when the tomographic resolution is insufficient to resolve the vertical backscatter profile. The analysis is based on P-band data from two campaigns: BioSAR-1 (2007) in Remingstorp, southern Sweden, and BioSAR-2 (2008) in Krycklan (KR), northern Sweden. BioSAR airborne data were also processed to correspond as closely as possible to future BIOMASS TomoSAR acquisitions, with BioSAR-2-based results shown. A power law AGB model using volumetric HV polarized backscatter performs best in KR, with training residual root mean-squared error (RMSE) of 30%-36% (27-33 t/ha) for airborne data and 38%-39% for simulated BIOMASS data. Airborne TomoSAR data suggest that both vertical and horizontal tomographic resolution are of importance and that it is possible to greatly reduce AGB retrieval bias when compared with airborne P-band SAR backscatter intensity-based retrievals. A lack of significant ground slopes in Remningstorp reduces the benefit of using TomoSAR data which performs similar to retrievals based solely on P-band SAR backscatter intensity.","['Forestry', 'Synthetic aperture radar', 'Biomass', 'Tomography', 'Backscatter', 'Spaceborne radar']","['Biomass', 'boreal forest', 'P-band', 'tomography']"
"Visual question answering (VQA) for remote sensing scene has great potential in intelligent human–computer interaction system. Although VQA in computer vision has been widely researched, VQA for remote sensing data (RSVQA) is still in its infancy. There are two characteristics that need to be specially considered for the RSVQA task: 1) no object annotations are available in the RSVQA datasets, which makes it difficult for models to exploit informative region representation and 2) there are questions with clearly different difficulty levels for each image in the RSVQA task. Directly training a model with questions in a random order may confuse the model and limit the performance. To address these two problems, in this article, a multi-level visual feature learning method is proposed to jointly extract language-guided holistic and regional image features. Besides, a self-paced curriculum learning (SPCL)-based VQA model is developed to train networks with samples in an easy-to-hard way. To be more specific, a language-guided SPCL method with a soft weighting strategy is explored in this work. The proposed model is evaluated on three public datasets, and extensive experimental results show that the proposed RSVQA framework can achieve promising performance. Code will be available at https://gitlab.lrz.de/ai4eo/reasoning/VQA-easy2hard .","['Visualization', 'Task analysis', 'Feature extraction', 'Remote sensing', 'Computational modeling', 'Representation learning', 'Earth']","['Remote sensing', 'self-paced curriculum learning (SPCL)', 'spatial transformer', 'visual question answering (VQA)']"
"Spatial downscaling is an ill-posed, inverse problem, and information loss (IL) inevitably exists in the predictions produced by any downscaling technique. The recently popularized area-to-point kriging (ATPK)-based downscaling approach can account for the size of support and the point spread function (PSF) of the sensor, and moreover, it has the appealing advantage of the perfect coherence property. In this article, based on the advantages of ATPK and the conceptualization of IL, an IL-guided image fusion (ILGIF) approach is proposed. ILGIF uses the fine spatial resolution images acquired in other wavelengths to predict the IL in ATPK predictions based on the geographically weighted regression (GWR) model, which accounts for the spatial variation in land cover. ILGIF inherits all the advantages of ATPK, and its prediction has perfect coherence with the original coarse spatial resolution data which can be demonstrated mathematically. ILGIF was validated using two data sets and was shown in each case to predict downscaled images more accurately than the compared benchmark methods.","['Spatial resolution', 'Image fusion', 'Remote sensing', 'Earth', 'Coherence', 'Predictive models']","['Downscaling', 'geographically weighted regression (GWR)', 'geostatistics', 'image fusion', 'information loss (IL)']"
"Persistent scatterers (PS) interferometry tools are extensively used for the monitoring of slow, long-term ground deformation. High spatial resolution is typically required in urban areas to cope with the variability of the signal, whereas in rural regions, multilook shall be implemented to improve the coverage of monitored areas. Along this line, SqueeSAR and later Component extrAction and sElection SAR (CAESAR) were introduced for the monitoring of both persistent and (decorrelating) distributed scatterers (DS). Multilook generalized likelihood ratio test (MGLRT) is a detector derived in the context of tomographic SAR processing that has been investigated for a fixed multilook degree. In this work, we address MGLRT and CAESAR in the multiresolution context characterized by a spatially variable multilook degree. We compare the two schemes for the multiresolution selection of PS and DS, highlighting the pros and cons of each scheme, particularly the peculiarities of CAESAR that have important implications at the implementation stage. A performance analysis of both detectors in case of model mismatch is also addressed. Experiments carried out with data acquired by the COSMO-SkyMed constellation support both the theoretical argumentation and the results achieved by resorting to Monte Carlo simulations.","['Monitoring', 'Detectors', 'Spatial resolution', 'Signal resolution', 'Tomography', 'Covariance matrices', 'Strain']","['Detection', 'generalized likelihood ratio test (GLRT)', 'persistent scatterers (PS)', 'SAR tomography']"
"It is an important task to automatically and accurately map rooftops from very high resolution remote sensing images since buildings are very closely related to human activity. Two typical technologies are often utilized to accomplish the task, i.e., semantic segmentation and instance segmentation. The semantic segmentation is to independently allocate a label (e.g., “building” or not) to each pixel, resulting in blob-like segments. On the contrary, one might model the boundary of a rooftop as a polygon to improve the shape of the rooftop by encouraging vertices of polygon to adhere to the rooftop’s boundary. Following this line of work, we present a multitask learning approach to predict rooftop corners in a sequent way using the attention learned from where the boundaries are in a given image region. The approach simulates the process of manual delineation of rooftops’ outline in a given image, which can produce accurate boundaries of rooftops with sharp corners and straight lines between them. Specifically, the proposed method consists of three components, i.e., object detection, pixel-by-pixel classification of both edges and corners, and delineation of rooftops in a sequent manner using a convolutional recurrent neural network (RNN). It is called as object-oriented, edges and corners (OEC)-RNN in this article. Three image datasets of buildings are employed to validate the performance of the OEC-RNN, which are compared with state-of-the-art methods for instance segmentation. The experimental results show that the OEC-RNN achieves the best performance in terms of overlay, boundary adherence, and vertex location between ground-truth and predicted polygons.","['Image segmentation', 'Buildings', 'Image edge detection', 'Semantics', 'Recurrent neural networks', 'Object detection', 'Feature extraction']","['Building extraction', 'convolutional neural network (CNN)', 'recurrent neural network (RNN)', 'rooftop delineation']"
"Deep learning-based multi-task learning (MTL) methods have recently attracted attention for content-based image retrieval (CBIR) applications in remote sensing (RS). For a given set of tasks (e.g., scene classification, semantic segmentation, and image reconstruction), existing MTL methods employ a joint optimization algorithm on the direct aggregation of task-specific loss functions. Such an approach may provide limited CBIR performance when: 1) tasks compete or even distract each other; 2) one of the tasks dominates the whole learning procedure; or 3) characterization of each task is underperformed compared to single-task learning. This is mainly due to the lack of: 1) plasticity condition (which is associated with sensitivity to new information) or 2) stability condition (which is associated with protection from radical disruptions by new information) of the whole learning procedure. To avoid this issue, as a first time, we propose a novel plasticity-stability preserving MTL (PLASTA-MTL) approach to ensure the plasticity and the stability conditions of the whole learning procedure independently of the number and type of tasks. This is achieved by defining two novel loss functions. The first loss function is the plasticity preserving loss (PPL) function that aims to enforce the global image representation space to be sensitive to new information learned with each task. This is achieved by minimizing the difference of gradient magnitudes for the global representation and task-specific embedding spaces. The second loss function is the stability preserving loss (SPL) function that aims to protect the global representation space radically disrupted by a new task. This is achieved by minimizing the angular distances between the task gradients over global representation space. To effectively employ the proposed loss functions, we also introduce a novel sequential optimization algorithm. Experimental results show the effectiveness of the proposed approach compared to the state-of-the-art MTL methods in the context of CBIR.","['Task analysis', 'Image representation', 'Optimization', 'Image reconstruction', 'Semantics', 'Image analysis', 'Training']","['Image retrieval', 'multi-task learning (MTL)', 'remote sensing (RS)', 'representation learning']"
"To cope with global climate change and monitor global CO 2 concentration distribution, the first Chinese carbon dioxide satellite (TanSat) has been successfully launched in December 2016. In this study, we implemented a CO 2 retrieval scheme by calibrating the TanSat sun-glint (GL) mode spectra and adapting the Iterative MaximumAPosteriori Differential Optical Absorption Spectroscopy (IMAP-DOAS) algorithm for CO 2 spectral retrieval. The global terrestrial CO 2 total vertical column density (VCD) and column-averaged dry-air mole fractions of CO 2 (XCO2) were simultaneously retrieved from TanSat GL spectral observations. Then, a comprehensive verification was performed between TanSat CO 2 retrieval and other measurements including Total Carbon Column Observing Network (TCCON), the Japanese Greenhouse gases Observing SATellite (GOSAT), and the US Orbiting Carbon Observatory-2 (OCO-2). Further comparisons between our TanSat CO 2 retrieval and ground-based FTIR measurements from TCCON indicated a good correlation with the mean bias of −0.78 ppm, the standard deviation at 1.75 ppm, and the Pearson correlation coefficient of 0.81. In addition, cross-satellite CO 2 validations of TanSat with GOSAT and OCO-2 showed consistently spatiotemporal trends for both CO 2 VCD andXCO2. In summary, we can conclude that the presented CO 2 retrieval scheme has achieved global CO 2 retrieval from TanSat GL mode spectra with high precision and accuracy, as suggested by the results of independent ground-based and satellite validations.","['Satellites', 'Extraterrestrial measurements', 'Absorption', 'Clouds', 'Instruments', 'Carbon dioxide', 'Atmospheric measurements', 'Climate change']","['CO₂', 'Iterative Maximum A Posteriori Differential Optical Absorption Spectroscopy (IMAP-DOAS)', 'remote sensing', 'satellites', 'spectral analysis', 'TanSat', 'Xcₒ₂']"
"Freeze/Thaw (F/T) surface state retrieval is important to further understand hydrological patterns and climate change. This article investigates the use of Earth-reflected Global Positioning System (GPS) L-band signals as collected by the National Aeronautics and Space Administration NASA’s Cyclone Global Navigation Satellite System (CYGNSS) mission for F/T surface state retrieval over a target area in South America, covering the Andes Mountains and the Argentinian Pampas. In the study, CYGNSS responsiveness to changes in surface permittivity is leveraged to detect transitions of F/T surface state, at an improved spatio-temporal sampling as compared to traditional Remote Sensing missions. A Seasonal-Threshold Algorithm (STA) is developed and validated using surface temperature data as provided by the European Centre for Medium-Range Weather Forecast (ECMWF) ERA5-Land numerical reanalysis model. Then, the monthly evolution of CYGNSS-derived F/T surface state maps is evaluated and an inter-comparison with the Soil Moisture Active Passive (SMAP) F/T data product is performed.","['Surface states', 'Sea surface', 'Soil moisture', 'Permittivity', 'Land surface', 'Ocean temperature', 'L-band', 'Climate change']","['Andes mountains', 'Cyclone Global Navigation Satellite System (CYGNSS)', 'European Centre for Medium-Range Weather Forecast (ECMWF) ERA5-Land', 'freeze/thaw (F/T) retrieval', 'GNSS-R', 'Soil Moisture Active Passive (SMAP)']"
"For the quantitative applications of the Suomi National Polar-orbiting Partnership (SNPP) Advanced Technology Microwave Sounder (ATMS), the geolocation accuracy of its sensor data records must be quantified during its on-orbit operation. In this paper, a refined coastline inflection point method is used to evaluate the on-orbit geolocation accuracy of SNPP ATMS. It is disclosed that for SNPP ATMS, the static error term with scan-angle-dependent feature is a dominant part among all the geolocation error sources. A mathematical model is then developed to convert the in-track and cross-track geolocation errors to the beam pointing Euler angles defined in the spacecraft coordinate system, which can be further used to construct the correction matrix for on-orbit geolocation process. By using the correction matrix built in this paper, the geolocation error is obviously reduced both at nadir and at the edge of the scan. The total geolocation error at nadir before/after correction is 3.8/0.8 km at K-band, 5.6/0.8 km at Ka-band, 3.3/0.4 km at V-band, and 1.5/0.1 km at W-band. The geolocation bias at the edge of the scan line before/after correction is 4.6/1.3 km at K-band, 9.4/1.8 km at Ka-band, 4.4/2.4 km at V-band, and 3.2/0.8 km at W-band. After correction, the scan-angle-dependent feature in geolocation error is also largely reduced.","['Geology', 'Instruments', 'Mathematical model', 'Earth', 'Antenna measurements', 'Space vehicles', 'Microwave radiometry']","['Advanced Technology Microwave Sounder (ATMS)', 'coastline inflection point(CIP)', 'Euler angles', 'geolocation error', 'microwave radiometer', 'Suomi National Polar-orbiting Partnership (SNPP)']"
"Earth-observing satellites carrying multispectral sensors are widely used to monitor the physical and biological states of the atmosphere, land, and oceans. These satellites have different vantage points above the Earth and different spectral imaging bands resulting in inconsistent imagery from one to another. This presents challenges in building downstream applications. What if we could generate synthetic bands for existing satellites from the union of all domains? We tackle the problem of generating synthetic spectral imagery for multispectral sensors as an unsupervised image-to-image translation problem modeled with a variational autoencoder (VAE) and generative adversarial network (GAN) architecture. Our approach introduces a novel shared spectral reconstruction loss to constrain the high-dimensional feature space of multispectral images. Simulated experiments performed by dropping one or more spectral bands show that cross-domain reconstruction outperforms measurements obtained from a second vantage point. Our proposed approach enables the synchronization of multispectral data and provides a basis for more homogeneous remote sensing datasets.","['Satellites', 'Sensors', 'Image reconstruction', 'NASA', 'Monitoring', 'Earth', 'Atmospheric measurements']","['Geophysical image processing', 'neural networks (NNs)', 'remote sensing', 'unsupervised learning']"
"For long-wavelength space-based radars, such as the P-band radar on the recently selected European Space Agency BIOMASS mission, system distortions (crosstalk and channel imbalance), Faraday rotation, and system noise all combine to degrade the measurements. A first-order analysis of these effects on the measurements of the polarimetric scattering matrix is used to derive differentiable expressions for the errors in the polarimetric backscattering coefficients in the presence of Faraday rotation. Both the amplitudes and phases of the distortion terms are shown to be important in determining the errors and their maximum values. Exact simulations confirm the accuracy and predictions of the first-order analysis. Under an assumed power-law relation between σ hv and the biomass, the system distortions and noise are converted into biomass estimation errors, and it is shown that the magnitude of the deviation of the channel imbalance from unity must be 4-5 dB less than the crosstalk, or it will dominate the error in the biomass. For uncalibrated data and midrange values of biomass, the crosstalk must be less than -24 dB if the maximum possible error in the biomass is to be within 20% of its true value. A less stringent condition applies if the amplitudes and phases of the distortion terms are considered random since errors near the maximum possible are very unlikely. For lower values of the biomass, the noise becomes increasingly important because the σ hv signal-to-noise ratio is smaller.","['Biomass', 'Noise', 'Faraday effect', 'Backscatter', 'Distortion measurement', 'Crosstalk', 'Scattering']","['Biomass', 'calibration', 'Faraday rotation', 'long-wavelength radar', 'polarimetric measurements', 'system distortion']"
"Earth observation data have huge potential to enrich our knowledge about our planet. An important step in many Earth observation tasks is semantic segmentation. Generally, a large number of pixelwise labeled images are required to train deep models for supervised semantic segmentation. On the contrary, strong intersensor and geographic variations impede the availability of annotated training data in Earth observation. In practice, most Earth observation tasks use only the target scene without assuming availability of any additional scene, labeled or unlabeled. Keeping in mind such constraints, we propose a semantic segmentation method that learns to segment from a single scene, without using any annotation. Earth observation scenes are generally larger than those encountered in typical computer vision datasets. Exploiting this, the proposed method samples smaller unlabeled patches from the scene. For each patch, an alternate view is generated by simple transformations, e.g., addition of noise. Both views are then processed through a two-stream network and weights are iteratively refined using deep clustering, spatial consistency, and contrastive learning in the pixel space. The proposed model automatically segregates the major classes present in the scene and produces the segmentation map. Extensive experiments on four Earth observation datasets collected by different sensors show the effectiveness of the proposed method. Implementation is available at https://gitlab.lrz.de/ai4eo/cd/-/tree/main/unsupContrastiveSemanticSeg .","['Earth', 'Image segmentation', 'Semantics', 'Training', 'Deep learning', 'Task analysis', 'Supervised learning']","['Deep learning', 'self-supervised learning', 'semantic segmentation', 'single-scene training']"
"Remote sensing data have become increasingly vital in target detection, disaster monitoring, and military surveillance. Abundant pan-sharpening and super-resolution (SR) methods based on deep learning have been proposed and have achieved remarkable performance. However, pan-sharpening requires paired panchromatic (PAN) and multispectral (MS) images, and SR cannot increase the spectral resolution of PAN. Thus, we introduce a computational imaging-based method to recover or produce the incomplete data of single PAN or MS. This work also explores the integration of multiple tasks by a single neural network. We start with SR and colorization, study the feasibility of simultaneously finishing SR colorization, and use a model trained in SR colorization to finish pan-sharpening without MS. A generic neural network, remote sensing image improvement network (RSI-Net), is designed for remote sensing image SR, colorization, simultaneous SR colorization, and pan-sharpening. To verify its performance, RSI-Net is compared with the state-of-the-art SR and colorization methods. Experiments show that RSI-Net can be competitive in visual effects and evaluation indexes, and it performs well at simultaneous SR colorization, and RSI-Net finishes pan-sharpening and only needs to input PAN. Our experiments confirm the effect of integrating multiple tasks.","['Remote sensing', 'Task analysis', 'Neural networks', 'Image color analysis', 'Gray-scale', 'Sensors', 'Feature extraction']","['Convolutional neural network (CNN)', 'deep learning (DL)', 'image colorization', 'image super-resolution (SR)', 'remote sensing image']"
"Different aspects of the operational constraints of remote sensing inverse problems are thoroughly investigated by simulation studies, using a deterministic method, namely regularized total least squares (RTLS). For demonstration purposes, water vapor profiles retrievals from simulated Suomi NPP Cross-track Infrared Souder (CrIS) hyperspectral measurements are considered. Synthetic CrIS radiances are generated using a line-by-line radiative transfer model (GENSPECT) with ~424 realistic radiosonde profiles and US 1976 standard atmosphere as inputs. These results are also compared with those from a prevalent stochastic method. Our findings show that the stochastic method, even with additional deterministic constraints (truncated singular value decomposition) applied on top of it, is often unable to produce useful retrieval results, i.e., posterior error is more than the a priori error. In contrast, RTLS is able to produce deterministically unique results according to the available information content in the measurements, which could result in a paradigm shift in operational satellite inversion.","['Stochastic processes', 'Satellites', 'Jacobian matrices', 'Inverse problems', 'Atmospheric measurements', 'Measurement uncertainty', 'Atmospheric modeling']","['Hyperspectral infrared sounding', 'ill-conditioned inverse', 'regularized total least squares (RTLS)', 'Suomi NPP Cross-track Infrared Souder (CrIS)']"
"The 3-D inverse synthetic aperture radar (ISAR) tomography is an enabling technique for applications such as the exact diagnosis of scattering mechanisms for complex targets. Nevertheless, current ISAR tomography solutions still suffer from problems such as the great computational complexity and optimal utilization of the signatures acquired from limited baselines. In this work, we propose a fast ISAR tomography technique for fully polarimetric 3-D imaging of man-made targets. A stack of 2-D complex-valued images with different baselines and polarizations is first obtained through a phase error calibration (PEC) process and graphic processing unit accelerated polarimetric filtered backprojection. A polarimetric state-space decomposition (P-SSD) algorithm is then developed which could provide joint 3-D reconstruction results with low computational complexity. Examples from both numerical multibaseline data for the Sandia laboratories implementation of cylinders (SLICY) benchmark model and the outdoor range dataset collected by the Georgia Tech Research Institute (GTRI) are presented to demonstrate the superior performance and the usefulness of the proposed technique.","['Tomography', 'Scattering', 'Radar imaging', 'Radar', 'Image resolution', 'Radar antennas', 'Azimuth']","['Fully polarimetric 3-D imaging', 'inverse synthetic aperture radar (ISAR) tomography', 'polarimetric state-space decomposition (P-SSD)']"
"Many economically important minerals have absorption features in the short-wave infrared (SWIR; 2000-2500 nm). Sensors which measure this part of the spectrum cannot detect the wavelength minimum of a feature at '900 nm (F900), indicative of ferric iron mineralogy. A method based on Gaussian processes (GPs) was developed and compared with multiple linear regression (MLR) to estimate the wavelength position of F900 from SWIR data (1002-1355 nm). SWIR data with different signal-to-noise ratios were acquired from crushed rock samples by a nonimaging spectrometer and an imaging spectrometer. GP estimates of wavelength position were converted to the proportion of goethite using coefficients from a regression of the proportion of goethite determined from X-ray diffraction (XRD) on wavelength position measured directly from spectra. GP-estimated wavelength positions were within the 2-nm and '4-nm root-mean-square error of measurements made directly from spectra for nonimaging and imaging spectrometer data, respectively. Proportions of goethite derived from these estimates were respectively within 4% and 6% of the values measured by XRD. MLR performed poorly compared to GPs when applied to data with no added noise and failed when applied to data with added noise or to imaging spectrometer data. These findings indicate that the wavelength position of F900-an indicator of ferric iron mineralogy-can be estimated from data acquired at SWIR wavelengths (1002- 1355 nm). This opens up possibilities for using a single (SWIR) sensor to acquire information on ferric iron mineralogy (using F900) and other minerals with diagnostic absorptions between 1000 and 2500 nm.","['Sensors', 'Hyperspectral sensors', 'Wavelength measurement', 'Rocks', 'Noise', 'Absorption', 'Image sensors']","['Electromagnetic radiation', 'Gaussian processes (GPs)', 'geology', 'image sensors', 'infrared spectroscopy', 'iron', 'mining industry', 'remote sensing', 'signal processing', 'spectral analysis']"
"We propose an unsupervised method for iceberg detection over sea ice-free waters. The algorithm is based on the segmentation and nonparametric constant false alarm rate (SnP-CFAR) approach. Unlike in parametric CFAR detection, in our method, there is no need to define target, guard, and background areas explicitly. Instead, we apply the CFAR detection to the pixels within each detected segment and the background is formed of the nearby pixels not included in the target segment. By using nonparametric background probability density function (PDF) estimates, we also eliminate the need of assuming a specific type of a background PDF. We compared the detection results with the operational Danish Meteorological Institute (DMI) Gamma-CFAR algorithm results. The results were evaluated against icebergs manually identified by the Finnish Meteorological Institute (FMI) Ice analysts. Our method also exhibits a reduced number of false alarms. We present results of iceberg detection based on the SAR channel-cross-correlation (CCC). CCC was able to distinguish many of the true targets with a low number of false alarms. However, CCC seems to miss some of the true targets and its main use would be in confirming iceberg observations.","['Marine vehicles', 'Synthetic aperture radar', 'Image segmentation', 'Object detection', 'Radar polarimetry', 'Probability density function', 'Gamma distribution']","['C-band', 'constant false alarm rate (CFAR)', 'dual-polarized', 'iceberg detection', 'SAR', 'segmentation and nonparametric constant false alarm rate (SnP-CFAR)']"
"In recent years, portable laser scanning devices and their applications in the context of forest mensuration have undergone rapid methodological and technological developments. Devices have become smaller, lighter, and more affordable, whereas new data-driven methods and software packages have facilitated the derivation of information from point clouds. Thus, terrestrial laser scanning (TLS) is now well established, and laser–object interactions have been studied using theoretical, modeling, and experimental approaches. The representation of scanned objects in terms of accuracy and completeness is a key factor for successful feature extraction. Still, little is known about the influence of TLS and survey properties on point clouds in complex scattering environments, such as forests. In this study, we investigate the influence of laser beam diameter and signal triggering on the quality of point clouds in forested environments. Based on the Swiss National Forest Inventory data, we simulate the TLS measurements in 684 virtual forest stands using a 3-D content creation suite. We show that small objects lack sufficient representation in the point cloud and they are further negatively influenced by large laser beam diameters, dense stands, and large distances from the scanning device. We provide simulations that make it possible to derive a rationale for decisions regarding the appropriate choice of TLS device and survey configuration for forest inventories.","['Laser beams', 'Forestry', 'Measurement by laser beam', 'Three-dimensional displays', 'Surface emitting lasers', 'Laser theory', 'Laser noise']","['Forest inventory', 'occlusion', 'simulation', 'stem diameter distribution']"
"With the increasing need to construct long-term climate-quality data records to understand, monitor, and predict climate variability and change, it is vital to continue systematic satellite measurements along with the development of new technology for more quantitative and accurate observations. The Suomi National Polar-orbiting Partnership mission provides continuity in monitoring the Earth's surface and its atmosphere in a similar fashion as the heritage MODIS instruments onboard the National Aeronautics and Space Administration's Terra and Aqua satellites. In this paper, we aim at quantifying the consistency of Aqua MODIS and Suomi-NPP Visible Infrared Imaging Radiometer Suite (VIIRS) Land Surface Reflectance (LSR) and NDVI products as related to their inherent spatial sampling characteristics. To avoid interferences from sources of measurement and/or processing errors other than spatial sampling, including calibration, atmospheric correction, and the effects of the bidirectional reflectance distribution function, the MODIS and VIIRS LSR products were simulated using the Landsat-8's Operational Land Imager (OLI) LSR products. The simulations were performed using the instruments' point spread functions on a daily basis for various OLI scenes over a 16-day orbit cycle. It was found that the daily mean differences due to discrepancies in spatial sampling remain below 0.0015 (1%) in absolute surface reflectance at subgranule scale (i.e., OLI scene size). We also found that the MODIS-VIIRS product intercomparisons appear to be minimally impacted when differences in the corresponding view zenith angles (VZAs) are within the range of -15° to -35° (VZAV - VZAM), where VIIRS and MODIS footprints resemble in size. In general, depending on the spatial heterogeneity of the OLI scene contents, per-grid-cell differences can reach up to 20%. Further spatial analysis of the simulated NDVI and LSR products revealed that, depending on the user accuracy requirements for product intercomparisons, spatial aggregations may be used. It was found that if per-grid-cell differences on the order of 10% (in LSR or NDVI) are tolerated, the product intercomparisons are expected to be immune from differences in spatial sampling.","['MODIS', 'Extraterrestrial measurements', 'Atmospheric measurements', 'Earth', 'Satellites', 'Remote sensing']","['Biosphere', 'consistency', 'geometry', 'land surface', 'spatial resolution']"
"The Thermal Infrared Sensor 2 (TIRS-2) on Landsat 9 (L9) was launched on September 27, 2021, and underwent a variety of tests during its commissioning phase to establish its postlaunch performance. We report on the calibration updates performed to maintain its calibration and generate high-quality imagery. This is done by transferring the SI-traceable prelaunch calibration to on-orbit while accounting for changes in the TIRS-2 response as detected through on-board calibrator observations. Additional empirical corrections were implemented to mitigate image striping observed on-orbit. The detector arrays were monitored through its commissioning phase to ensure that stable detectors were chosen for operations. TIRS-2 has demonstrated ~0.025% instability over its orbit, ~80-mK noise equivalent delta temperature (NEdT), and an absolute radiometric uncertainty <1.4% in its nominal temperature range enabling a wide array of Earth science applications.","['Calibration', 'Detectors', 'Uncertainty', 'Temperature measurement', 'Temperature distribution', 'Earth', 'Extraterrestrial measurements']","['Calibration', 'Landsat', 'thermal infrared', 'traceability', 'uncertainty']"
"Traditionally, the modeling of the water vapor contents of the atmosphere is done through the estimation of precipitable water (PW)-the integrated value of the mass of water vapor over a vertical column expressed in millimeter equivalent height. This modeling method is justified by the fact that, to a high degree of approximation, the atmosphere can be seen as the stacking of horizontal layers, including water vapor, over distances larger than the height of the tropopause. Nevertheless, the cycle of the water vapor, the most prevalent of the greenhouse gases in the atmosphere, has a highly turbulent regime both in time and in space that the variations in PW cannot fully embrace. In this article, we explore the modeling method, as a series expansion in time and space, of the slant wet delays (SWDs) from one GPS receiver, as an extension of the usual modeling in zenith wet delays (ZWDs) and then PW values. In the first part, we assess, from a metrological point of view, the derivation of the SWDs computed from GPS carrier phase measurements, in the case of a very humid location, the tropical island of Tahiti, for a typical sample over the wet and dry seasons. In the second part, we introduce the series expansion of the SWDs, as seen from one GPS receiver, in terms of trigonometric functions of time and spherical harmonics of elevation and azimuth. This allows us to infer time and space correlations for the SWDs that are unreachable through the modeling of ZWD values alones. In a third part, to show that our approach also includes the zenith case, we make a comparison between the modeled SWDs in the zenith direction with wind velocities from a ground weather station and radiosonde soundings (RSs). The three main conclusions from our data case are: first, the SWDs are correlated in time by about four days, and in space, with an angular correlation distance of about 20°, both for the dry and wet seasons; second, the postfit residuals are almost uncorrelated with the SWDs from a temporal and spatial point of view, but with a diurnal component; third, there is a weak correlation between the SWDs and wind velocity, the pattern depends on the season.","['Global Positioning System', 'Delays', 'Global navigation satellite system', 'Receivers', 'Atmosphere', 'Terrestrial atmosphere', 'Tomography']","['Global positioning system (GPS)', 'precipitable water (PW)', 'slant wet delays (SWDs)', 'spherical harmonics', 'trigonometric functions', 'wind velocity', 'zenith wet delays (ZWDs)']"
"Estimating gross primary production (GPP), the gross uptake of CO 2 by vegetation, is a fundamental prerequisite for understanding and quantifying the terrestrial carbon cycle. Over the last decade, multiple approaches have been developed to derive spatiotemporal dynamics of GPP combining in situ observations and remote sensing data using machine learning techniques or semiempirical models. However, no high spatial resolution GPP product exists so far that is derived entirely from satellite-based remote sensing data. Sentinel-2 satellites are expected to open new opportunities to analyze ecosystem processes with spectral bands chosen to study vegetation between 10- and 20-m spatial resolutions with five-day revisit frequency. Of particular relevance is the availability of red-edge bands that are suitable for deriving estimates of canopy chlorophyll content that are expected to be much better than any previous global mission. Here, we analyzed whether red-edge-based and near-infrared-based vegetation indices (VIs) or machine learning techniques that consider VIs, all spectral bands, and their nonlinear interactions could predict daily GPP derived from 58 eddy covariance sites. Using linear regressions based on classic VIs, including near-infrared reflectance of vegetation (NIRv), we achieved prediction powers of $R^{2}_{\mathrm{10-fold}} = 0.51$ and an $RMSE_{\mathrm{10-fold}} = 2.95 $ [ $\mu \rm {mol \ CO_{2} m^{-2}s^{-1}}$ ] in a 10-fold cross validation. Chlorophyll index red (CIR) and the novel kernel NDVI (kNVDI) achieved significantly higher prediction powers of around $R^{2}_{\mathrm{10-fold}} \approx 0.61$ and $RMSE_{\mathrm{10-fold}} \approx 2.57$ [ $\mu \rm {mol \ CO_{2} m^{-2}s^{-1}}$ ]. Using all spectral bands and VIs jointly in a machine learning prediction framework allowed us to predict GPP with $R^{2}_{\mathrm{10-fold}} = 0.71$ and $RMSE_{\mathrm{10-fold}} = 2.68$ [ $\mu \rm {mol \ CO_{2} m^{-2}s^{-1}}$ ]. Despite the high-power prediction when machine learning techniques are used, under water-stress scenarios or heat waves, optical information alone is not enough to predict GPP properly. In general, our analyses show the potential of nonlinear combinations of spectral bands and VIs for monitoring GPP across ecosystems at a level of accuracy comparable to previous works, which, however, required additional meteorological drivers.","['Vegetation mapping', 'Machine learning', 'Indexes', 'Clouds', 'Biological system modeling', 'Remote sensing', 'Spatial resolution']","['Gross primary production', 'red edge', 'Sentinel-2']"
"The on-orbit calibration technology for internal and external parameters of remote sensing camera is the key to guaranteeing the imaging quality and positioning accuracy. Conventional landmark-based methods are not suitable for satellite missions where landmarks do not work nor for the on-orbit autonomous calibration of large remote sensing constellation. Hence, we introduced an on-orbit geometric calibration method based on star sources observation. By adjusting the satellite attitude, the camera and star sensor can simultaneously detect the star sources. Taking the star sources as control points, a high-accuracy “camera-star sensor” joint geometric calibration model integrating the space–time effects and optical imaging is established, which precisely decouples the deviations caused by the relativistic effects, the distortion of internal parameters, and the bias of external parameters. Furthermore, the procedures of the method are designed for on-orbit autonomous calibration. On-orbit experiments were carried out in several satellites, such as Jilin-1 ShiPin07 (JL-1 SP07). The results showed that the accuracy of internal calibration could reach 0.052” and the error of external installation matrix was within ±1.4”. Therefore, the method is attractive for actualizing on-orbit autonomous geometric calibration accurately and complements the conventional methods where the availability of landmarks is hard.","['Calibration', 'Satellites', 'Cameras', 'Remote sensing', 'Optical distortion', 'Optical imaging', 'Distortion']","['Astronomical observation', 'geometric calibration', 'remote sensing', 'stellar aberration']"
"Recent work has demonstrated a passive radio sounding approach that uses the Sun as a source for echo detection and ranging. As the Sun is a moving source with a position that is known a priori , we evaluate this technique’s capabilities to measure the echo’s phase history, map topography, and perform synthetic aperture radar (SAR) focusing. Here, we present our approach to implementing passive SAR using a compact, temporally incoherent radio-astronomical source as a signal of opportunity. We first evaluate the passive system’s capabilities to obtain an echo from a rough surface by determining the critical signal-to-noise ratio (SNR) for reliably observing the Sun’s echo reflection with our passive instrument. We then demonstrate that our technique can detect the necessary changes in range, phase, and reflectivity of an echo from the Sun. We next present the experimental results of our passive radar testing using the Sun at Dante’s View, Death Valley, to highlight this technique’s ability to perform 2-D imaging. Finally, with synthetic data, we demonstrate that we can use time-domain backprojection to focus a planar white noise signal, perform passive SAR imaging, and improve the measurement’s SNR and azimuth resolution. The results of passive SAR focusing on white noise highlight the potential for the Sun and Jupiter’s radio emissions to perform surface and subsurface imaging for planetary and terrestrial observations.","['Synthetic aperture radar', 'Passive radar', 'Sun', 'Radar', 'Focusing', 'Signal to noise ratio', 'Mathematical model']","['Passive radar', 'passive radio sounding', 'passive synthetic aperture radar (SAR)', 'radio echo sounding']"
"A vicarious calibration with reference to characterized surface tarps was conducted to determine the first radiometric characteristics of KOMPSAT-3. The 6S radiative transfer model was also used by inputting various initial parameters, such as the spectral response function of KOMPSAT-3, and atmospheric and geometric conditions. Moderate-Resolution Imaging Spectroradiometer atmospheric products, such as aerosol optical depth, precipitable water, and total ozone, were used as input parameters to interpret solar radiation reflection, scattering, and absorption effects. In the first field campaign, the radiometric coefficients from each of the spectral bands were estimated by calculating the predicted radiance at sensor level and the digital number (DN) of KOMPSAT-3 based on a linear least squares fit over a range of target reflectance levels. The second field campaign measurements were also used to upgrade the KOMPSAT-3 DNs to radiance coefficients. The root-mean-square error differences between simulated radiance and measured radiance during the second field campaign for “sensor-to-itself” calibration were 2.072 W/m 2 sr (blue), 6.80 W/m 2 sr (green), 7.512 W/m 2 sr (NIR), and 5.712 W/m 2 sr (red), respectively. This highlights that radiometric calibration with tarps is a reliable method. Furthermore, the gain ratio between the first and the second one was <; 5%, indicating reasonable radiometric calibration results. Additionally, cross-validation of KOMPSAT-3 with radiometrically well-calibrated Landsat-8 was performed over bright desert. Although the difference between the vicarious calibration with surface tarps and cross-validation with Landsat-8 was significant, reasonable results were obtained under close geometrical conditions, despite inherent vicarious calibration error.","['Calibration', 'Radiometry', 'Atmospheric modeling', 'Satellites', 'Atmospheric measurements', 'Satellite broadcasting', 'Imaging']","['Cross-calibration', 'Korea Multi-Purpose Satellite-3 (KOMPSAT-3)', 'Moderate-Resolution Imaging Spectroradiometer (MODIS)', 'radiometric calibration', 'surface tarp', 'vicarious calibration']"
"Multiple radar systems represent an attractive option for target tracking because they can significantly enlarge the area coverage and improve both the probability of trajectory detection and the localization accuracy. The presence of multiple extended targets or weak targets is a challenge for multiple radar systems. Moreover, their performance may be severely deteriorated by regions characterized by a high clutter density. In this article, an algorithm for detection and tracking of multiple targets, extended or weak, based on measurements provided by multiple radars in an environment with heavily cluttered regions, is proposed. The proposed method features three stages. In the first stage, past measurements are exploited to build a spatiotemporal clutter map in each radar; a weight is then assigned to each measurement to assess its significance. In the second stage, a track-before-detect algorithm, based on a weighted 3-D Hough transform, is applied to obtain target tracklets. In the third stage, a low-complexity tracklet association method, exploiting a lion reproduction model, is applied to associate tracklets of the same target. Three experiments are presented to illustrate the effectiveness of the proposed approach. The first experiment is based on synthetic data, the second one is based on actual data from a radar network with two homogeneous air surveillance radars, and the third one is based on actual data from a radar network with four different marine surveillance radars. The results reveal that the proposed method can outperform competing approaches.","['Radar tracking', 'Radar', 'Target tracking', 'Radar clutter', 'Clutter', 'Radar detection', 'Radar measurements']","['Maneuvering target', 'multiple radar system', 'radar data', 'remote sensing', 'target tracking', 'track-before-detect (TBD)']"
"Cross-polarized temporal coherence observations of a boreal forest, acquired using a tower-based radar, are presented in this article. Temporal coherence is analyzed with respect to frequency, temporal baseline, time of day of observation, season, meteorological variables, and biophysical variables. During the summer, P- and L-band temporal coherence exhibited diurnal cycles, which appeared to be due to high rates of transpiration and convective winds during the day. During the winter, freeze-thaw cycles and precipitation resulted in decorrelation. At temporal baselines of seconds to hours, a high temporal coherence was observed even at C-band. The best observation times of the day were midnight and dawn. Temporal coherence is the main limitation of accuracy in interferometric and tomographic forest applications. The observations from this experiment will allow for better spaceborne SAR mission designs for forest applications, better temporal decorrelation modeling, and more accurate forest parameter estimation algorithms using interferometric and tomographic SAR data.","['Forestry', 'Coherence', 'Synthetic aperture radar', 'Spaceborne radar', 'Decorrelation', 'Radar', 'L-band']","['BorealScat', 'boreal forest', 'C-band', 'coherence', 'decorrelation', 'L-band', 'P-band', 'tower']"
"Radar-derived refractivity from stationary ground targets can be used as a proxy of near-surface moisture field and has the potential to improve the forecast of convection initiation. Refractivity retrieval was originally developed for a single radar and was recently extended for a network of radars by solving a constrained least squares (CLS) minimization. In practice, the number of high-quality ground returns can be often limited, and consequently, the retrieval problem becomes ill-conditioned. In this paper, an emerging technology of compressive sensing (CS) is proposed to estimate the refractivity field using a network of radars. It has been shown that CS can provide an optimal solution for the underdetermined inverse problem under certain conditions and has been applied to different fields such as magnetic resonance imaging, radar imaging, etc. In this paper, a CS framework is developed to solve the inversion. The feasibility of CS for refractivity retrieval using single and multiple radars is demonstrated using simulations, where the model refractivity fields were obtained from the Advanced Regional Prediction System. The root-mean-squared error was introduced to quantify the performance of the retrieval. The performance of CS was assessed statistically and compared to the CLS estimates for various amounts of measurement errors, numbers of radars, and model refractivity fields. Our preliminary results have shown that CS can consistently provide relatively robust and high-quality estimates of the refractivity field.","['Compressive sensing (CS)', 'constrained least squares (CLS)', '$l_{1}$ minimization', 'networked radars', 'refractivity retrieval']","['Compressive sensing (CS)', 'constrained least squares (CLS)', '$l_{1}$ minimization', 'networked radars', 'refractivity retrieval']"
"Accuracy assessment of burned area maps has been traditionally performed using pixel-based metrics, with the objective of assessing the accuracy and precision of burned area estimates at local and regional scales. While these assessments are helpful for obtaining consistent estimates of the burned area across many fires and over large areas, pixel-based approaches do not necessarily characterize how well individual fires are mapped. At the individual fire scale, other factors like the shape of the fire have significance regarding ecology, fire succession, and landscape management and determining other fire properties such as the spread rate. We propose a method for evaluating wildfire classification maps, which retains the spatially explicit properties of the burn scar. Our method quantifies the edge error (EE) of burned area classifications and reference maps by calculating the average geometric normal of the evaluated burned area boundary along the burn edge and the two nearest neighbor samples from the reference burn boundary. The metric is a physically meaningful quantification of the EE, which represents the average distance between the boundaries of the reference and evaluated burn scars. The methods are demonstrated by comparing MODIS Burned Area (MCD64A1) maps to Monitoring Trends in Burn Severity (MTBS) maps for 173 total wildfires in the United States. The results indicate that when accounting for the minimum achievable EE (MAEE) due to differing spatial resolutions, the mean EE is less than two MODIS pixels and the magnitude of the errors does not appear to be related to fire size.","['Shape', 'MODIS', 'Remote sensing', 'Earth', 'Spatial resolution', 'Artificial satellites']","['Accuracy assessment', 'MODIS Burned Area', 'North America', 'wildfire']"
"Changes in the calving front position of marine-terminating glaciers strongly influence the mass balance of glaciers, ice caps, and ice sheets. At present, quantification of frontal position change primarily relies on time-consuming and subjective manual mapping techniques, limiting our ability to understand changes to glacier calving fronts. Here we describe a newly developed automated method of mapping glacier calving fronts in satellite imagery using observations from a representative sample of Greenland’s peripheral marine-terminating glaciers. Our method is adapted from the 2-D wavelet transform modulus maxima (WTMM) segmentation method, which has been used previously for image segmentation in biomedical and other applied science fields. The gradient-based method places edge detection lines along regions with the greatest intensity gradient in the image, such as the contrast between glacier ice and water or glacier ice and sea ice. The lines corresponding to the calving front are identified using thresholds for length, average gradient value, and orientation that minimize the misfit with respect to a manual validation data set. We demonstrate that the method is capable of mapping glacier calving fronts over a wide range of image conditions (light to intermediate cloud cover, dim or bright, mélange presence, etc.). With these time series, we are able to resolve subseasonal to multiyear temporal patterns as well as regional patterns in glacier frontal position change.","['Ice', 'Image segmentation', 'Remote sensing', 'Earth', 'Artificial satellites', 'Wavelet transforms', 'Image edge detection']","['Computational infrastructure', 'Cryosphere', 'geographic information systems (GIS)', 'optical data']"
"Investigations of the effects of clouds on Earth’s radiation budget demand accurate representations of cloud top parameters, which can be efficiently obtained by large-scale satellite remote sensing approaches. However, the insufficient utilization of multiband information is one of the major sources of uncertainty in cloud top products derived from geostationary satellites. In this study, we developed a new algorithm to estimate Cloud, Atmospheric Radiation and renewal Energy application (CARE) version 1.0 cloud top properties [cloud top height (CTH), cloud top pressure (CTP), and cloud top temperature (CTT)]. The algorithm is constructed from ten thermal spectral measurements in Himawari-8 observations by using the random forest (RF) method to comprehensively consider the contribution of each band to the cloud top parameters. We chose the highly accurate Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP) products in 2018 as the true values. The sensitivity analysis demonstrated that the products can be fully reproduced by using multiple Himawari-8 channels with the addition of the digital elevation model (DEM) data. The validation results of the 2019 CALIOP data confirm that the new algorithm shows an effective performance, with correlation coefficients (R) of 0.89, 0.89, and 0.90 for CTH, CTP, and CTT, respectively. Moreover, a significant improvement in the ice cloud estimation is achieved, in which the CTTRvalue increased from 0.46 to 0.70, as well as an improvement in the sea area, where the CTTRvalue increased from 0.71 to 0.84 compared with the Himawari-8 products of the Japan Aerospace Exploration Agency (JAXA) P-tree system. The further analyses performed here capture the diurnal cycle of cloud top parameters well in different temporal scales over the Asia–Pacific region.","['Clouds', 'Satellites', 'Ocean temperature', 'MODIS', 'Temperature measurement', 'Aerosols', 'Sea surface']","['Advanced Himawari Imager (AHI)', 'Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation satellite (CALIPSO)', 'cloud top parameters', 'ice cloud', 'remote sensing', 'random forests (RFs)']"
"In a recent publication, Ansari et al. (2021) claimed (see, in particular, the Discussion and Recommendation Section in their article) that the advanced differential SAR interferometry (InSAR) algorithms for surface deformation retrieval, based on the small baseline approach, are affected by systematic biases in the generated InSAR products. Therefore, to avoid such biases, they recommended a strategy primarily focused on excluding “the short temporal baseline interferograms and using long baselines to decrease the overall phase errors.” In particular, among various techniques, Ansari et al. (2021) identified the solution presented by Manunta et al. (2019) as a small baseline advanced InSAR processing approach where the presence of the above-mentioned biases (referred to as a fading signal) compromises the accuracy of the retrieved InSAR deformation products. We show that the claim of Ansari et al. (2021) is not correct (at least) for what concerns the mentioned approach discussed by Manunta et al. (2019). In particular, by processing the Sentinel-1 dataset relevant to the same area in Sicily (southern Italy) investigated by Ansari et al. (2021), we demonstrate that the generated InSAR products do not show any significant bias.","['Strain', 'Time series analysis', 'Synthetic aperture radar', 'Coherence', 'Systematics', 'Orbits', 'Interferometry']","['Distributed scatterers (DSs)', 'interferometric synthetic aperture radar (SAR) (InSAR)', 'multilook interferograms', 'parallel small baseline subset (P-SBAS)', 'phase inconsistencies', 'phase unwrapping errors', 'systematic bias', 'time series analysis']"
"We describe a lightweight wideband C-band radar sounder, which is designed for use in a small unmanned aerial vehicle, primarily for measuring snow and ice. The waveform used is a gated frequency modulated continuous wave, which enables transmission at low power with relatively high energy in the compressed pulse. Gating allows use of a single antenna, reducing the influence of the direct wave on the systems' dynamic range. The radar operates at 5.3-GHz center frequency and has a bandwidth of 1 GHz, for a nominal range resolution of 15 cm in air or 12 cm in snow. Laboratory testing and airborne acquisition of data over the glacier Hardangerjøkulen show that the radar appears to work well for the intended purpose of measuring snow layer thickness.","['Radar antennas', 'Spaceborne radar', 'Ground penetrating radar', 'Airborne radar', 'Receivers', 'Delays']","['CW radar', 'FM radar', 'geophysical measurements', 'ground penetrating radar', 'radar']"
"Tradeoffs among the spatial, spectral, and temporal resolutions of satellite sensors make it difficult to acquire remote sensing images at both high spatial and high temporal resolutions from an individual sensor. Studies have developed methods to fuse spatiotemporal data from different satellite sensors, and these methods often assume linear changes in surface reflectance across time and adopt empirical rules and handcrafted features. Here, we propose a dense spatiotemporal fusion (DenseSTF) network based on the convolutional neural network (CNN) to deal with these problems. DenseSTF uses a patch-to-pixel modeling strategy that can provide abundant texture details for each pixel in the target fine image to handle heterogeneous landscapes and models both forward and backward temporal dependencies to account for land cover changes. Moreover, DenseSTF adopts a mapping function with few assumptions and empirical rules, which allows for establishing reliable relationships between the coarse and fine images. We tested DenseSTF in three contrast scenes with different degrees of heterogeneity and temporal changes, and made comparisons with three rule-based fusion approaches and three CNNs. Experimental results indicate that DenseSTF can provide accurate fusion results and outperform the other tested methods, especially when the land cover changes abruptly. The structure of the deep learning networks largely impacts the success of data fusion. Our study developed a novel approach based on CNN using a patch-to-pixel mapping strategy and highlighted the effectiveness of the deep learning networks in the spatiotemporal fusion of the remote sensing data.","['Remote sensing', 'Earth', 'Artificial satellites', 'Reflectivity', 'Spatiotemporal phenomena', 'Spatial resolution', 'MODIS']","['Convolutional neural networks (CNNs)', 'deep learning', 'spatiotemporal fusion']"
"Developed here is an algorithm for determining the infrared (IR) cloud-top phase for advanced Himawari imager (AHI) measurements from the Japanese geostationary satellite Himawari-8. The tests and decision tree used in the AHI algorithm are different from those in the Moderate Resolution Imaging Spectroradiometer (MODIS) Level-2 cloud product algorithm. Verification of AHI cloud-top phase results with the Cloud-Aerosol Lidar with orthogonal polarization product over a four-month period from March to June of 2017 over the North Pacific gives hit rates of 80.20% (66.33%) and 86.51% (80.61%) for liquid-water and randomly oriented-ice cloud tops, respectively, if clear-sky pixels are excluded (included) from the statistics. Also made are intercomparisons between AHI and MODIS IR cloud-top phase products over the North Pacific in June 2017. AHI liquid-water-phase determinations agree with MODIS liquid-water-phase determinations at an agreement rate of 83.68%, showing a dependence on MODIS zenith angles. The agreement rate of ice phase classifications between AHI and MODIS is 93.54%. The MODIS IR product contains some unreasonable ice-phase pixels over oceans, as well as uncertain-phase pixels over land, and has limitations for daytime liquid-water-phase identifications over the Indo-China Peninsula. Limitations of the AHI cloud-top phase algorithm are mainly caused by difficulties in identifying liquid-water-phase clouds over sun-glint regions and during twilight.","['Clouds', 'MODIS', 'Ice', 'Laser radar', 'Integrated optics', 'Satellites', 'Temperature measurement']","['Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP)', 'cloud-top phase', 'Himawari', 'infrared (IR)', 'Moderate Resolution Imaging Spectroradiometer (MODIS)']"
"Ground-based measurements of the hemispherical directional reflectance factor (HDRF) of windblown snow covered Arctic tundra were measured at large solar zenith angles (79°-85°) for six sites near the international research base in Ny-Ålesund, Svalbard. Measurements were made with the Gonio RAdiometric Spectrometer System over the viewing angles 0°-50° and the azimuth angles 0°-360°, for the wavelength range 400-1700 nm. The HDRF measurements showed good consistency between sites for near-nadir and backward viewing angles, with a relative standard deviation of less than 10% between sites where the snowpack was smooth and the snow depth was greater than 40 cm. The averaged HDRF showed good symmetry with respect to the solar principal plane and exhibited a forward scattering peak that was strongly wavelength dependent, with greater than a factor of 2 increase in the ratio of maximum to minimum HDRF values for all viewing angles over the wavelength range 400-1300 nm. The angular effects on the HDRF had minimal influence for viewing angles less than 150 in the backward viewing direction for the averaged sites and agreed well with another study of snow HDRF for infrared wavelengths, but showed differences of up to 0.24 in the HDRF for visible wavelengths owing to light-absorbing impurities measured in the snowpack. The site that had the largest roughness elements showed the strongest anisotropy in the HDRF, a large reduction in forward scattering, and a strong asymmetry with respect to the solar principal plane.","['Snow', 'Goniometers', 'Wavelength measurement', 'Azimuth', 'Surface topography', 'Arctic', 'Calibration']","['Albedo', 'bidirectional reflectance distribution function (BRDF)', 'black carbon', 'calibration', 'goniometer', 'hemispherical-conical reflectance factor (HCRF)', 'hemispherical-directional reflectance factor (HDRF)', 'reflectance', 'remote sensing', 'scattering', 'snow', 'surface roughness']"
"In recent years, convolutional neural networks (CNNs) have achieved excellent performance in hyperspectral image classification and have been widely used. However, the convolution kernel used in traditional CNN has the limitation of single scale, which is not conducive to the improvement of hyperspectral classification performance. In addition, training a classification network of high-dimensional data based on limited labeled samples is still one of the challenges of hyperspectral image classification. To solve the above problems, a hyperspectral image classification method based on expansion convolution network (ECNet) is proposed. The expansion convolution injects holes into the standard convolution kernel to expand the receptive field (RF), so as to extract more context features. Because the shallow features of hyperspectral images contain more location and detail information, while the deep features contain stronger semantic information, in order to further enhance the correlation between deep and shallow information, inspired by ResNet, a similar feedback block (SFB) is introduced on the basis of ECNet, and the deep features and shallow features are fused through this feedback mechanism. Thus, an improved version of ECNet method is obtained, which is called feedback ECNet (FECNet). This study was tested on four commonly used hyperspectral datasets [i.e., Indian Pine (IP), Pavia University (UP), Kennedy Space Center (KSC), and Salinas Valley (SV)] and on a higher resolution and complexly distributed land cover dataset (University of Houston (HT). The experimental results show that the proposed method has better classification performance than some state-of-the-art methods, which shows that FECNet has a certain potential in hyperspectral image classification.","['Convolution', 'Feature extraction', 'Hyperspectral imaging', 'Image classification', 'Three-dimensional displays', 'Kernel', 'Training']","['Convolutional neural network (CNN)', 'expansion convolution block (ECB)', 'hyperspectral image (HSI) classification', 'similar feedback block (SFB)']"
"The development of accurate methods for multi-label scene classification (MLC) of remote sensing (RS) images is one of the most important research topics in RS. To address MLC problems, the use of deep neural networks that require a high number of reliable training images annotated by multiple land-cover class labels (multi-labels) has been found popular in RS. However, collecting such annotations is time consuming and costly. A common procedure to obtain annotations at zero labeling cost is to rely on thematic products or crowdsourced labels. As a drawback, these procedures come with the risk of label noise that can distort the learning process of the MLC algorithms. In the literature, most label noise robust methods are designed for single-label classification (SLC) problems in computer vision (CV), where each image is annotated by a single label. Unlike SLC, label noise in MLC can be associated with: 1) subtractive label noise (a land cover class label is not assigned to an image while that class is present in the image); 2) additive label noise (a land cover class label is assigned to an image, although that class is not present in the given image); and 3) mixed label noise (a combination of both). In this article, we investigate three different noise robust CV SLC methods (self-adaptive training (SAT), early-learning regularization, and joint co-regularized training) and adapt them to be robust for multi-label noise scenarios in RS. During experiments, we study the effects of different types of multi-label noise and evaluate the adapted methods rigorously. To this end, we also introduce a synthetic multi-label noise injection strategy that is more adequate to simulate operational scenarios compared to the uniform label noise injection strategy, in which the labels of absent and present classes are flipped at uniform probability. Further, we study the relevance of different evaluation metrics in MLC problems under noisy multi-labels. On the basis of the theoretical and experimental analyses, some guidelines for a proper design of label noise robust MLC methods are derived.","['Training', 'Noise robustness', 'Noise measurement', 'Annotations', 'Additive noise', 'Task analysis', 'Prototypes']","['Multi-label noise', 'multi-label scene classification (MLC)', 'noisy labels', 'remote sensing (RS)']"
"The Thermal Infrared Sensor-2 (TIRS-2) that will be onboard Landsat 9 has undergone a prelaunch testing campaign to characterize its radiometric, spectral, and spatial performances and demonstrate compliance to its requirements. This work reviews the key elements of the instrument-level radiometric testing using an SI-traceable source to derive its uncertainties. Those arising from on-orbit calibration using the TIRS-2 onboard blackbody are also discussed. We use a Monte Carlo approach to propagate the uncertainties through a nonlinear calibration equation and address both random and systematic uncertainty terms. Achieving the required performance demonstrates the instrument's potential for enhancing our understanding of the Earth's environment.","['Uncertainty', 'Calibration', 'Radiometry', 'Instruments', 'Earth', 'Testing', 'Remote sensing']","['Calibration', 'Landsat 9', 'prelaunch testing', 'spectral response', 'Thermal Infrared Sensor 2 (TIRS-2)', 'uncertainty']"
"Phase unwrapping, also known as ambiguity resolution, is an underdetermined problem in which assumptions must be made to obtain a result in SAR interferometry (InSAR) time series analysis. This problem is particularly acute for distributed scatterer InSAR, in which noise levels can be so large that they are comparable in magnitude to the signal of investigation. In addition, deformation rates can be highly nonlinear and orders of magnitude larger than neighboring point scatterers, which may be part of a more stable object. The combination of these factors has often proven too challenging for the conventional InSAR processing methods to successfully monitor these regions. We present a methodology which allows for additional environmental information to be integrated into the phase unwrapping procedure, thereby alleviating the problems described above. We show how problematic epochs that cause errors in the temporal phase unwrapping process can be anticipated by the machine learning algorithms which can create categorical predictions about the relative ambiguity level based on the readily available meteorological data. These predictions significantly assist in the interpretation of large changes in the wrapped interferometric phase and enable the monitoring of environments not previously possible using standard minimum gradient phase unwrapping techniques.","['Soil', 'Monitoring', 'Coherence', 'Strain', 'Standards', 'Time series analysis', 'L-band']","['Artificial intelligence', 'peatland', 'phase unwrapping', 'recurrent neural network (RNN)', 'SAR interferometry (InSAR)', 'subsidence']"
"Considering the trajectories of pulses from terrestrial laser scanners (TLS) can provide refined models of occlusion and improve the assessment of observation quality in forests and other ecosystems. By considering the space traversed by light detection and ranging (lidar) pulses, we can separate empty regions of an ecosystem sample from unobserved regions of an ecosystem sample. We apply this method of TLS observation quality assessment, and analyze Compact Biomass Lidar 2 (CBL2) TLS observations of a single tree and of a deciduous forest stand. We show the contribution of information from each TLS scan to be inconsistent and the combination of multiple scans to have diminishing returns for new information, without guaranteeing complete coverage of a sample. We quantitatively investigate the effects of imposing information quality requirements on TLS sampling, for example, requiring minimum numbers of observations in each region or requiring regions to be observed from a minimum number of independent scans. We show empirically that rigid, predefined TLS sampling schemes, even with hypothetically dense coverage, cannot guarantee successful samples in geometrically complex systems such as forests. Through these methods, we lay the groundwork for on-the-fly assessment of observation quality according to several modeling-relevant metrics which enhance TLS ecosystem assessment. We also establish the value of flexible deployment options for TLS instruments, including the ability to deploy at a variety of heights.","['Vegetation', 'Forestry', 'Laser radar', 'Ecosystems', 'Trajectory', 'Uncertainty', 'Instruments']","['Forestry', 'laser beams', 'laser measurements']"
"This article addresses the general problem of single-look multi-master SAR tomography. For this purpose, we establish the single-look multi-master data model, analyze its implications for the single and double scatterers, and propose a generic inversion framework. The core of this framework is the nonconvex sparse recovery, for which we develop two algorithms: one extends the conventional nonlinear least squares (NLS) to the single-look multi-master data model and the other is based on bi-convex relaxation and alternating minimization (BiCRAM). We provide two theorems for the objective function of the NLS subproblem, which lead to its analytic solution up to a constant phase angle in the 1-D case. We also report our findings from the experiments on different acceleration techniques for BiCRAM. The proposed algorithms are applied to a real TerraSAR-X data set and validated with the height ground truth made available by an SAR imaging geodesy and simulation framework. This shows empirically that the single-master approach, if applied to a single-look multi-master stack, can be insufficient for layover separation, and the multi-master approach can indeed perform slightly better (despite being computationally more expensive) even in the case of single scatterers. In addition, this article also sheds light on the special case of single-look bistatic SAR tomography, which is relevant for the current and future SAR missions such as TanDEM-X and Tandem-L.","['Tomography', 'Synthetic aperture radar', 'Data models', 'Spaceborne radar', 'Germanium', 'Remote sensing', 'Estimation']","['Bistatic SAR', 'nonconvex optimization', 'SAR tomography', 'sparse recovery', 'synthetic aperture radar (SAR)', 'Tandem-L', 'TanDEM-X']"
"Remote sensing instruments, both aircraft and on-orbit platforms, undergo extensive laboratory calibrations to determine their geometric, spectral, and radiometric responses. Additional in-flight radiometric calibrations can be performed using well-characterized earth targets. The Fire Influence on Regional to Global Environments and Air Quality (FIREX-AQ) campaign provided such an opportunity when the ER-2 aircraft overflew Railroad Valley on August 13 and 15, 2019. Surface reflectances were available from the August 4, 2019 field team and from the Radiometric Calibration Network (RadCalNet) portal, and spectral aerosol optical depths from an on-site AERosol RObotic NETwork (AERONET) sunphotometer. The Enhanced MODIS Airborne Simulator (eMAS), the Airborne Multiangle SpectroPolarimetric Imager (AirMSPI), and the “Classic” Airborne Visible/Infrared Imaging Spectrometer (AVIRIS-C) sensors individually performed a vicarious calibration using their respective methodologies and selection of input parameters. A comparison of the at-sensor radiances predicted from these independent analyses highlights some of the uncertainties in the inputs, including choice of solar irradiance model. Although good agreement, within 5%, is found at visible wavelengths, difference can be as large as 15% in the shortwave infrared (SWIR). This highlights the need for the remote sensing community to agree upon a standard solar model, to remove sensor-to-sensor biases derived from in-flight calibrations.","['Calibration', 'Optical surface waves', 'Instruments', 'Aerosols', 'Radiometry', 'Optical sensors', 'NASA']","['Airborne Multiangle SpectroPolarimetric Imager (AirMSPI)', 'Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)', 'Enhanced MODIS Airborne Simulator (eMAS)', 'Fire Influence on Regional to Global Environments and Air Quality (FIREX-AQ)', 'radiometric calibration', 'Railroad Valley (RRV)', 'vicarious calibration (VicCal)']"
"We consider zenith/nadir pointing atmospheric radars and explore the effects of different dual-polarization architectures on the retrieved variables: reflectivity, depolarization ratio, cross-polar coherence, and degree of polarization. Under the assumption of azimuthal symmetry, when the linear depolarization ratio (LDR) and circular depolarization ratio (CDR) modes are compared, it is found that for most atmospheric scatterers reflectivity is comparable, whereas the depolarization ratio dynamic range is maximized at CDR mode by at least 3 dB. In the presence of anisotropic (aligned) scatterers, that is, when azimuthal symmetry is broken, polarimetric variables at CDR mode do have the desirable property of rotational invariance and, further, the dynamic range of CDR can be significantly larger than the dynamic range of LDR. The physical meaning of the cross-polar coherence is revisited in terms of scattering symmetries, that is, departure from reflection symmetry for the LDR mode and departure from rotation symmetry for the CDR mode. The Simultaneous Transmission and Simultaneous Reception mode (STSR mode or hybrid mode orZDRmode) is also theoretically analyzed for the case of zenith/nadir pointing radars and, under the assumption of azimuthal symmetry, relations are given to compare measurements obtained at hybrid mode with measurements obtained from orthogonal (LDR and CDR) modes.","['Polarization', 'Covariance matrix', 'Coherence', 'Correlation', 'Radar polarimetry', 'Radar antennas']","['Circular depolarization ratio (CDR)', 'circular polarization', 'degree of polarization', 'linear depolarization ratio (LDR)', 'linear polarization', 'polarization response', 'reflectivity']"
"The widespread application of drones and associated miniaturization of imaging sensors has led to an explosion of remote sensing applications with very high spatial and spectral resolutions. The 3-D ultrahigh-resolution digital outcrop models created using drones and oblique imagery from ground-based sensors are now commonly used in the academic and industrial sectors, while the generation of spatially accurate models has been greatly facilitated by the development of computer vision tools, such as structure from motion, and the correction of spectral attributes to achieve material reflectance measurements remains challenging. Following the development of a topographical correction toolbox (mephysto), we now propose a series of new tools that can leverage the detailed geometry captured by digital outcrop models to correct for illumination effects caused by oblique viewing angles and the interaction of light with complex 3-D surfaces. This open-source code is integrated into hylite, a python toolbox for the full 3-D processing and fusion of digital outcrop models with hyperspectral imaging data. We validate the performance of our novel method using a case study at an open-pit mine in Tharsis, Spain, and demonstrate the importance of accurate illumination corrections for quantitative spectral analyses. Significantly, we show that commonly applied spectral analysis techniques can yield erroneous results for data corrected using current state-of-the-art approaches. Our proposed method ameliorates many of the issues with these established approaches.","['Atmospheric modeling', 'Lighting', 'Hyperspectral imaging', 'Calibration', 'Surface topography', 'Light sources', 'Geologic measurements']","['Digital outcrop models', 'geology', 'hyperspectral imaging', 'illumination correction']"
"This paper develops a model of the synthetic aperture, interferometric satellite radar altimeter echo power, and echo cross-product. The model uses the smallness of the satellite pitch and roll angles, and the limited range of satellite altitude to provide a semianalytical echo model, whose numerical dimensions are limited to two in the synthetic aperture case or three in the interferometric case at large roll angles, making its application to extensive data practical. By not implementing the synthetic beam formation, it is demonstrated that the model recovers the conventional case of a pulse-limited altimeter. A theoretical description of the fluctuations in the multilooked, interferometric cross-product, and the synthetic aperture and pulse-limited powers is given by extending the model to describe correlations between individual looks in each case. The model offers the potential to retrieve ocean-surface parameters from synthetic aperture and pulse-limited altimeter data simultaneously, with obvious application to the new generation of altimeters onboard CryoSat-2, Sentinel-3, and Sentinel-6 during the transition to operational synthetic aperture radar altimetry.","['Satellites', 'Synthetic aperture radar', 'Oceans', 'Sea measurements', 'Antenna measurements', 'Spaceborne radar', 'Apertures']","['Geophysical sea measurements', 'ocean surface', 'radar altimetry', 'radar remote sensing', 'spaceborne radar', 'synthetic aperture radar (SAR)']"
"We address the problem of automatically coregistering planetary images to a common baseline, introducing a novel generic technique that achieves an unprecedented robustness to different image inputs, thus making batch-mode coregistration achievable without requiring the usual parameter tweaking. We introduce a novel image matching technique, which boosts matching performance even under the most strenuous circumstances, and experimentally demonstrate validation through an extensive experimental multi-instrument setup that includes images from eight high-resolution data sets of the Mars and the Moon. The technique is further tested in a batch-mode processing, in which approximately 1.6% of all high-resolution Martian imagery is coregistered to a common baseline.","['Remote sensing', 'Cameras', 'Systematics', 'Image matching', 'Mars', 'Image resolution', 'Estimation']","['High-resolution imaging', 'image matching', 'image registration', 'multi-instrument coregistration', 'planetary images', 'remote sensing']"
"Unmanned aerial vehicles (UAVs) are widely applied for purposes of inspection, search, and rescue operations by the virtue of low-cost, large-coverage, real-time, and high-resolution data acquisition capacities. Massive volumes of aerial videos are produced in these processes, in which normal events often account for an overwhelming proportion. It is extremely difficult to localize and extract abnormal events containing potentially valuable information from long video streams manually. Therefore, we are dedicated to developing anomaly detection methods to solve this issue. In this article, we create a new dataset, named Drone-Anomaly, for anomaly detection in aerial videos. This dataset provides 37 training video sequences and 22 testing video sequences from seven different realistic scenes with various anomalous events. There are 87488 color video frames (51635 for training and 35853 for testing) with the size of 640×640at 30 frames/s. Based on this dataset, we evaluate existing methods and offer a benchmark for this task. Furthermore, we present a new baseline model, anomaly detection with Transformers (ANDTs), which treats consecutive video frames as a sequence of tubelets, utilizes a Transformer encoder to learn feature representations from the sequence, and leverages a decoder to predict the next frame. Our network models normality in the training phase and identifies an event with unpredictable temporal dynamics as an anomaly in the test phase. Moreover, to comprehensively evaluate the performance of our proposed method, we use not only our Drone-Anomaly dataset but also another dataset. We will make our dataset and code publicly available. A demo video is available at https://youtu.be/ancczYryOBY . We make our dataset and code publicly available ( https://gitlab.lrz.de/ai4eo/reasoning/drone-anomaly https://github.com/Jin-Pu/Drone-Anomaly ).","['Videos', 'Anomaly detection', 'Transformers', 'Task analysis', 'Autonomous aerial vehicles', 'Training', 'Spatiotemporal phenomena']","['Aerial videos', 'anomaly detection', 'convolutional neural networks (CNNs)', 'temporal reasoning', 'transformers', 'unmanned aerial vehicle (UAV)']"
"Motivated by prospective synthetic aperture radar (SAR) satellite missions, this paper addresses the problem of differential SAR tomography (D-TomoSAR) in urban areas using spaceborne bistatic or pursuit monostatic acquisitions. A bistatic or pursuit monostatic interferogram is not subject to significant temporal decorrelation or atmospheric phase screen and, therefore, ideal for elevation reconstruction. We propose a framework that incorporates this reconstructed elevation as deterministic prior to deformation estimation, which uses conventional repeat-pass interferograms generated from bistatic or pursuit monostatic pairs. By means of theoretical and empirical analyses, we show that this framework is, in the pursuit monostatic case, both statistically and computationally more efficient than the standard D-TomoSAR. In the bistatic case, its theoretical bound is no worse by a factor of 2. We also show that reasonable results can be obtained by using merely six TerraSAR-X add-on for digital elevation measurements (TanDEM-X) pursuit monostatic pairs, if additional spatial prior is introduced. The proposed framework can be easily extended for multistatic configurations or external sources of scatterer's elevation.","['Synthetic aperture radar', 'Strain', 'Satellites', 'Satellite broadcasting', 'Spaceborne radar', 'Decorrelation', 'Tomography']","['Synthetic aperture radar (SAR)', 'SAR tomography', 'Tandem-L', 'TerraSAR-X add-on for digital elevation measurements (TanDEM-X)']"
"Cloud detection is of great significance for the subsequent analysis and application of remote-sensing images, and it is a critical part of remote-sensing image preprocessing. In this article, we propose a cloud detection method using convolutional neural networks based on cascaded feature attention and channel attention (CFCA-Net). The CFCA-Net uses cascaded feature attention module (CFAM) to enhance the attention of the network toward important color feature and texture feature. The CFAM cascaded the color feature attention and texture feature attention module in the encoder. The CFAN-Net also uses channel attention to highlight the important information in the channel dimensions. The attention module is based on multi-scale features and uses dilated convolution with different dilation rates to obtain information about multiple receptive fields. Moreover, a loss function combined quadtree and binary cross-entropy (BCE) was also introduced to make the network focus on the edge of cloud area. We validated our CFCA-Net on the Gaofen-1 wide field-of-view (WFV) imagery dataset. The experimental results show that the CFCA-Net performs well under different scenarios, and its overall accuracy reaches 97.55%. Moreover, subjective cloud detection results also prove the effectiveness of our algorithm.","['Feature extraction', 'Remote sensing', 'Convolution', 'Image segmentation', 'Image color analysis', 'Deep learning', 'Convolutional neural networks']","['Attention mechanism', 'cascaded feature attention', 'channel attention', 'cloud detection', 'loss function', 'quadtree segmentation']"
"We present a novel digital signal processing procedure, named eigenvalue signal processing (henceforth ESP), patented by the first author with Brookhaven Science Associates in 2013. The method enables the removal of the bias due to antenna coherent cross-channel coupling and is applicable in the LDR mode, the ATSR mode and the STSR orthogonal mode of weather radar measurements. In this paper, we focus on the LDR mode and consider copolar reflectivity at horizontal transmit (Z HH ), cross-polar reflectivity at horizontal transmit (Z VH ), linear depolarization ratio at horizontal transmit (LDRH) and degree of polarization at horizontal transmit (DOP H ). The ESP (ESP) method is substantiated by an experiment carried out in November 2012 using C-band weather radar with a parabolic reflector located at the Selex ES-Gematronik facilities in Neuss, Germany. The experiment involved comparison of weather radar measurements taken 1.5 minutes apart in two hardware configurations, namely with cross-coupling on (cc-on) and cross-coupling off (cc-off). It is experimentally demonstrated that eigenvaluederived variables are invariant with respect to antenna coherent cross-channel coupling. This property had to be expected, since the eigenvalues of the Coherency matrix are SU(2) invariant.","['Correlation', 'Covariance matrices', 'Eigenvalues and eigenfunctions', 'Meteorology', 'Radar polarimetry', 'Antenna measurements']","['Antenna radiation pattern', 'coherency matrix', 'copolar radiation pattern', 'covariance matrix', 'cross-channel coupling', 'cross-polar correlation coefficient', 'cross-polar radiation pattern', 'degree of polarization at horizontal transmit', 'eigenvalues', 'linear depolarization ratio', 'polarimetric phased array weather radar']"
"In recent years, there has been growing interest in developing oriented bounding box (OBB)-based deep learning approaches to detect arbitrary-oriented ship targets in synthetic aperture radar (SAR) images. However, most existing OBB-based detection methods suffer from boundary discontinuity problems for bounding box angle prediction and key point regression challenges. In this article, we present a novel OBB-based detection algorithm that utilizes ellipse encoding to effectively exploit the geometric and scattering properties of ship targets. Specifically, the ship contour is fit by an OBB inscribed ellipse that is encoded as a set of distances between dynamic key points on the bow and target center. By combining the bow angle interval and the decoding process, the negative impact of the boundary discontinuity problem is avoided. In addition, we propose an elliptical Gaussian distribution heatmap and a pooling strategy termed double-peak max-pooling (DPM) to deal with the challenge of separating densely distributed ships in inshore scenes. The former can enhance the heatmap’s ship-side score gap between neighboring ship targets, while the latter can solve the problem of target center responses being suppressed after max-pooling. Simulation experiments conducted on the benchmark Rotating SAR Ship Detection Dataset (RSSDD) and Rotated Ship Detection Dataset in SAR Images (RSDD-SAR) demonstrate the superior performance of our method for ship target detection compared to several state-of-the-art OBB-based algorithms. Ablation experiments show that elliptical Gaussian distribution heatmap and DPM can further improve the inshore detection performance.","['Marine vehicles', 'Feature extraction', 'Detection algorithms', 'Synthetic aperture radar', 'Radar polarimetry', 'Heating systems', 'Scattering']","['Arbitrary-oriented', 'boundary discontinuity', 'ellipse encoding', 'ship detection', 'synthetic aperture radar (SAR)']"
"Calibration consistency between Ku-band radars flown on the Tropical Rainfall Measuring Mission's (TRMM's) precipitation radar (PR) and the global precipitation measurement (GPM) mission's dual-frequency PR (DPR) can be attained by the use of the normalized radar cross section (NRCS) or σ 0 over the oceans. With the use of the sea surface echo (SSE) data obtained from the spaceborne PRs, this article aims to estimate the radar parameters of pulsewidth and beamwidth and to evaluate the bias in the NRCS estimates caused by the discrete range sampling. Since the SSE shape is closely related to the received pulsewidth and the two-way cross-track beamwidth, those parameters are individually estimated from the SSE shapes. The SSE shapes are also used to evaluate the impact of the discrete range sampling on the NRCS statistics. The pulsewidth and beamwidth estimated from the SSEs compare well with the level-1 values and accurately reflect changes in the configuration of the radars. The NRCS statistics in GPM version 06 show that the calibration consistency between GPM KuPR and TRMM PR is evaluated within the range of -0.39 to +0.03 dB (-0.48 to +0.11 dB) with (without) the peak correction.","['Spaceborne radar', 'Shape', 'Band-pass filters', 'Sea surface', 'Radar measurements', 'Pulse measurements']","['Global precipitation measurement (GPM)', 'intercomparison', 'sea surface echo (SSE)', 'spaceborne precipitation radar (PR)', 'tropical rainfall measuring mission (TRMM)']"
"Attenuation from clouds and precipitation hinders the use of Ka-band in SARs, radar altimeters and in satellite link communications. The NASA-JAXA Global Precipitation Measurement (GPM) mission, with its core satellite payload including a dual-frequency (13.6 and 35.5 GHz) radar and a multifrequency passive microwave radiometer, offers an unprecedented opportunity for better quantifying such attenuation effects. Based on four years of GPM products, this article presents a global climatology of Ka-band attenuation caused by clouds and precipitation and analyses the impact of the precipitation diurnal cycle. As expected, regions of high attenuation mirror precipitation patterns. Clouds and precipitation cause two-way attenuation at 35.5 GHz in excess of 3 dB about 1.5% of the time in the regions below 65°, peaking at as much as 10% in the tropical rain belt and the South Pacific Convergence Zone and at circa 5% along the storm tracks of the North Atlantic and Pacific Oceans. Confirming previous findings, the diurnal cycle is particularly strong over the land and during the summer period; while over the ocean, the diurnal cycle is generally weaker some coherent features emerge in the tropical oceans and in the northern hemisphere. Results are useful for estimating data loss from (sun-synchronous) satellite adopting active instruments/links at a frequency close to 35 GHz.","['Clouds', 'Attenuation', 'Rain', 'Spaceborne radar', 'Oceans', 'Microwave radiometry']","['Attenuation', 'cloud and precipitation', 'Ka-band', 'radar']"
"This paper investigates at the example of bathymetry how much an application can profit from comprehensive characterizations required for an improved calibration of data from a state-of-the-art commercial hyperspectral sensor. A NEO HySpex VNIR-1600 sensor is used for this paper, and the improvements are based on measurements of sensor properties not covered by the manufacturer, in particular, detector nonlinearity and stray light. This additional knowledge about the instrument is used to implement corrections for nonlinearity, stray light, spectral smile distortion and nonuniform spectral bandwidth and to base the radiometric calibration on a SI-traceable radiance standard. Bathymetry is retrieved from a data take from the lake Starnberg using WASI-2D. The results using the original and improved calibration procedures are compared with ground reference data, with an emphasis on the effect of stray-light correction. For our instrument, stray-light biases the detector response from 416-500 nm up to 8% and from 700-760 nm up to 5%. Stray-light-induced errors affect bathymetry mainly in water deeper than Secchi depth, whereas in shallower water, the dominant error source is the calibration accuracy of the light source used for radiometric calibration. Stray-light correction reduced the systematic error of water depth by 19% from Secchi depth to three times Secchi depth, whereas the relative standard deviation remained stable at 5%.","['Calibration', 'Detectors', 'Wavelength measurement', 'Radiometry', 'Instruments', 'Stray light', 'Measurement by laser beam']","['Bathymetry', 'calibration', 'hyperspectral', 'imaging spectrometer', 'nonlinearity', 'remote sensing', 'stray light']"
"The Thermal Infrared Sensor 2 (TIRS-2) payload for the Landsat 9 mission closely follows the design of the TIRS instrument currently flying aboard Landsat 8. The TIRS-2 instrument, however, incorporates an important design change to mitigate the stray light issue that plagued the TIRS instrument. Shortly after the launch of Landsat 8 in 2013, calibration errors due to stray light artifacts were observed in Earth imagery from TIRS with magnitudes of 4% (10.8μmband) and 8% (12.0μmband). Out-of-field scans of the Moon were conducted to map the angles from which off-axis radiance was detected on the focal plane arrays. Optical modeling, constrained by reverse ray traces of the lunar data, identified the primary scattering sources within the TIRS telescope, and these results informed the locations and design of mitigating baffles for TIRS-2. The effect of the modifications to the TIRS-2 instrument was tested preflight through thermal vacuum (TVAC) characterization tests, and the optical models were updated to be consistent with the measured data. Preliminary assessments indicated at least an order of magnitude reduction of the total signal due to scattering in TIRS-2. On-orbit lunar scans provided the final confirmations and demonstrated that the new design changes to TIRS-2 have reduced the primary out-of-field scattering by over 40x from the original TIRS design bringing the total scattering to 1% or less. More importantly, Earth imagery produced by Landsat 9 TIRS-2 does not show any stray-light-related artifacts, as was prevalent in the Landsat 8 TIRS imagery.","['Earth', 'Detectors', 'Optical scattering', 'Optical imaging', 'Scattering', 'Remote sensing', 'Optical variables measurement']","['Landsat', 'lunar scans', 'optical model', 'scattering', 'stray light', 'thermal infrared']"
"In order to improve the understanding of the dynamical deformation processes of sea ice in the seasonal ice zone (SIZ), measures to detect deformed ice were developed and validated using satellite L-band synthetic aperture radar (ScanSAR) images for the southern Sea of Okhotsk. To approach, sea ice was categorized into three ice types, typical of the sea ice in this region: nilas (thin level), pancake ice (thin rough), and deformed ice (thick rough), and then the measures to classify into these categories were developed using ALOS/Phased Array type L-band Synthetic Aperture Radar (PALSAR) as a function of backscatter coefficients at HH polarization (σ0HH) and incidence angle (θi), based on the field observations. Comparative analysis confirmed that PALSAR can detect deformed ice more efficiently than RADARSAT-2 (C-band SAR). The temporal evolution of the area, judged as deformed ice from these measures, shows significant variability with both time and space, and deformed ice regions appear in relatively linear alignments with a width of a few tens of kilometers in the inner ice pack region, consistent with ice drift convergence. To confirm the results, PALSAR-2 images at HH and HV polarizations were examined as a function ofθi, based on the four-year field observations in the same area. The results revealed thatσ0HHandσ0HVare both subject to floe sizes as well as deformed ice, andσ0HVis more sensitive. This indicates that care should be taken when applying these measures to the ice areas where significantly small floes are dominant like the marginal ice zone.","['Ice', 'Sea ice', 'Sea measurements', 'Synthetic aperture radar', 'L-band', 'Radar polarimetry', 'Satellites']","['Cryosphere', 'deformed sea ice', 'dynamics', 'remote sensing', 'sea ice', 'synthetic aperture radar (SAR)']"
"In this article, ALOS-2/PALSAR-2 dual-polarized [horizontal-transmit-horizontal-receive and horizontal-transmit-vertical-receive (HH/HV)] ScanSAR mode L-band synthetic aperture radar (SAR) imagery over an Arctic study area was evaluated for their suitability for operational sea ice (SI) monitoring. The L-band SAR data are studied for the estimation of different SI parameters: SI concentration, SI thickness, SI type, and SI drift. Also, some comparisons with nearly coincident C-band data over the same study area have been made. The results indicate that the L-band SAR data from ALOS-2/PALSAR-2 are very useful for estimating the studied SI parameters and equally good or better than using the conventional operational dual-polarized C-band SAR satellite data.","['Ice', 'Synthetic aperture radar', 'L-band', 'Image segmentation', 'Estimation', 'Arctic', 'Parameter estimation']","['ALOS-2/PALSAR-2', 'dual polarized', 'L-band', 'synthetic aperture radar (SAR)', 'sea ice (SI)']"
"An adaptive selection of the near/shortwave infrared (NIR/SWIR) reflectance correction and the quasi-analytic algorithms (QAAs) is proposed for the Moderate Resolution Imaging Spectroradiometer (MODIS-Aqua) to utilize the strengths of different correction algorithms and QAAs in a single satellite scene with water types ranging from turbid coastal to clear open ocean waters. A blended satellite product is generated by merging three atmospheric-correction algorithms(AD-ATCOR): 1) iterative NIR correction; 2) management unit of the north sea mathematical models (MUMM); and 3) SWIR, using a spectral threshold-based selection for different water types. The validation analysis of a blended remote sensing reflectance product showed overall good agreement with AERONET-OC observations followed by NASA bio-optical marine algorithm data set (NOMAD) at the blue wavelengths and the estuarine data set at the green and red wavelengths. The results suggest that the adaptive method is a better alternative to address the challenging problem of selecting different correction algorithms for different water types in a single satellite scene. Likewise, an adaptive selection of a QAA (AD-QAA) used the QAA-v5 and the QAA-V to obtain merged inherent optical property (IOP) products in a single MODIS-Aqua scene with varying water types. As a case study, the two adaptive selection procedures were sequentially applied to the MODIS-Aqua imagery representing four environmental conditions in the northern Gulf of Mexico. Improved retrievals of the total absorption and backscattering coefficients along an estuarine to ocean continuum demonstrated the effectiveness of this method in an optically complex and dynamic river-dominated system.","['Oceans', 'Optical sensors', 'Adaptive optics', 'Biomedical optical imaging', 'Image color analysis', 'Sea measurements']","['Atmospheric-correction algorithm', 'Moderate Resolution Imaging Spectroradiometer (MODIS-Aqua)', 'northern Gulf of Mexico (nGoM)', 'quasi-analytic algorithm (QAA)']"
"Radar tomography of glaciers promises to improve imaging and estimates of subsurface ice-sheet structures and properties, including temperature distributions, basal materials, ice fabric, and englacial water content. However, bistatic radar data with long (i.e., larger than the ice thickness) walk-away surveys are required to constrain high-fidelity tomographic inversions. These long-offset data have proven difficult to collect due to the hardware complexity of existing synchronization techniques. Therefore, we remove the hardware complexity required for real-time synchronization by synchronizing in postprocessing. Our technique transforms an Autonomous phase-sensitive Radio Echo Sounder (ApRES) system and a software-defined radio receiver into a coherent bistatic radar capable of recovering basal echoes at long offsets. We validated our system at Whillans Ice Stream, West Antarctica, with a walk-away survey up to 1300 m (797 m thick) and at Store Glacier, Greenland, up to 1450 m (1028 m thick). At both field sites, we measured the basal echo at angles beyond the point of total internal reflection (TIR), whose previous literature had set as a hard physical limit. We support our experimental results with high-frequency structure simulation, which shows that ground-based radar systems capture evanescent waves and are not hindered by TIR. Our analysis and experiments demonstrate a system capable of executing wide-angle bistatic radar surveys for improved geometric and radiometric resolution of inversions for englacial and subglacial properties.","['Ice', 'Synchronization', 'Radar', 'Radar antennas', 'Tomography', 'Hardware', 'Antennas']","['Bistatic radar', 'bistatic tomography', 'direct-path synchronization', 'ice-antenna coupling', 'phase alignment', 'radio echo sounder', 'signal-to-noise ratio (SNR) gain', 'summation noise statistics']"
"In the context of ship monitoring in the ocean, targets are usually sparsely distributed. Thus, synthetic aperture radar (SAR) imaging of the whole scene is usually quite redundant and costly. However, raw SAR echo data were considered to be useless before focusing. Few studies have attempted to detect ships from raw SAR echo data. It seems to be an impossible task since the resolution of raw SAR echo data is too low. This article proposes a ship detection method for raw SAR echo data in view of a nonimaging target sensing paradigm. The core idea is that we can sense the existence of ships from raw SAR echo data without imaging. The underlying rationale is that the radar always speaks the same sentence, i.e., usually an exactly identical linear frequency modulated (LFM) signal, while target and clutter answer differently. The difference spread into each part of the whole echo sequence rather than only the focused energy after match filtering. Thus, the ships can be found by pattern analysis on one-dimension sequence data rather than two-dimension images. The experimental results based on simulation and typical real data validate our assumption. This study shows that SAR imaging is an unnecessary intermediate process and opens up new significant possibilities for ship detection in the vast ocean.","['Synthetic aperture radar', 'Marine vehicles', 'Imaging', 'Radar imaging', 'Sensors', 'Detectors', 'Oceans']","['Raw echo data', 'ship detection', 'synthetic aperture radar (SAR)']"
"Detecting changes on the ground in multitemporal Earth observation data is one of the key problems in remote sensing. In this article, we introduce Sibling Regression for Optical Change detection (SiROC), an unsupervised method for change detection (CD) in optical satellite images with medium and high resolutions. SiROC is a spatial context-based method that models a pixel as a linear combination of its distant neighbors. It uses this model to analyze differences in the pixel and its spatial context-based predictions in subsequent time periods for CD. We combine this spatial context-based CD with ensembling over mutually exclusive neighborhoods and transitioning from pixel to object-level changes with morphological operations. SiROC achieves competitive performance for CD with medium-resolution Sentinel-2 and high-resolution Planetscope imagery on four datasets. Besides accurate predictions without the need for training, SiROC also provides a well-calibrated uncertainty of its predictions.","['Remote sensing', 'Spatial resolution', 'Optical sensors', 'Optical imaging', 'Earth', 'Satellites', 'Mathematical models']","['Change detection (CD)', 'multitemporal', 'optical images', 'unsupervised', 'urban analysis']"
"The purpose of this paper is to assess the height and strength of the surface echo clutter from the wide swath operation of a future spaceborne precipitation radar (PR) using the wide swath observation data during the end-of-mission experiment of the PR onboard the Tropical Rainfall Measuring Mission (TRMM) satellite. In this experiment, the maximum incident angle was expanded up to 32.5° from nadir, while the maximum incident angle is 18° for normal observation of the TRMM/PR. Although the results show that the clutter height monotonically increases with the incident angle as expected, the clutter height is suppressed for wide angles because of the weakening of the surface echo strength. As a result, the clutter height is less than 2 km if the rain echo is 15 dB higher than the noise level (i.e., about 34 dBZ echo for the TRMM/PR). The clutter height and the normalized backscattering cross section (0r0) are compared between ocean and land. Suppression of the clutter height is significant over ocean because of the relatively smaller 0r0 (smoother surface) and flat surface (no topography). The impact of the 0r0 for wide swath observation on the rain retrieval algorithm, especially on the surface reference technique, is also examined. The operation with a swath width almost twice the current TRMM/PR is achievable if relatively intense echoes are targeted; however, relatively weak and shallow precipitation will be masked by the clutter.","['Sea surface', 'Clutter', 'Spaceborne radar', 'Satellites', 'Surface topography', 'Standards']","['Radar clutter', 'radar cross-section', 'spaceborne radar']"
"With the development of airborne light detection and ranging (LiDAR) technology, it has become a common and efficient way to collect large-scale 3-D spatial information. However, efficient and automatic semantic segmentation of LiDAR data, in the form of 3-D point clouds, remains a persistent challenge. To address this, a dual attention neural network (DA-Net) is proposed, consisting of two different blocks, namely, augmented edge representation (AER) and elevation attentive pooling (EAP). First, the AER can adaptively represent local orientation and position, thereby effectively enhancing geometric information. Second, the captured local features of centroid points are utilized to further encode discriminative features using the EAP with the learned attention scores. Finally, a location homogeneity (LH) module is devised to explore the long-range relationship in an encoder–decoder network. Benefiting from the dual attention module, geometric information hidden in unorganized point clouds can be effectively propagated. Besides, the LH forces the network to pay attention to the semantic consistency of elevated objects, which facilitates both point- and object-level point cloud semantic segmentation for scene understanding. A benchmark dataset is used to assess the proposed method, which achieves an overall accuracy of 85.98% and an average F1 score of 72.31%. In addition, comparisons with other latest deep learning methods on the 2019 Data Fusion Contest dataset further demonstrate the robustness and generalization ability of the proposed method.","['Point cloud compression', 'Semantics', 'Three-dimensional displays', 'Convolutional neural networks', 'Neural networks', 'Laser radar', 'Deep learning']","['Airborne light detection and ranging (LiDAR)', 'dual attention neural network (DA-Net)', 'location-homogeneity module', 'point cloud classification', 'Vaihingen dataset']"
"As an improvement of the four-component scattering power decomposition with rotation of coherency matrix (Y4R) and extension of volume model (S4R), the general four-component decomposition with unitary transformation (G4U) was devised to make the full use of the polarimetric information in coherency matrix. This article enables an extension to G4U by deriving the scattering balance equation system in G4U to investigate the role of unitary transformation first. Despite self-contained, the scattering balance equation system in Y4R and S4R is independent of the T 13 entry of coherency matrix. To include T13 in decomposition, the unitary transformation in G4U adds a T 13 -related but redundant balance equation into the original system. As a result, T 13 is accounted for by G4U, and we attain no exact solution to the equation system but some approximate ones. By deducing the general expression of the approximate solutions, a generalized G4U (GG4U) is then created and denoted as G(ψ). The decomposition constant ψ determines a GG4U by producing a ψ-rotated double-bounce scattering matrix. We treat this as the scattering preference of (ψ) to characterize the physical mechanism. By assigning appropriate values to ψ, we attain GG4U of different preferences, while G(0) and G(+π/8) just correspond to S4R and G4U. A dual G4U G(-π/8) is also achieved. The duality G(±π/8) provides us an adaptive improvement to both G4U and S4R by strengthening the double-bounce scattering over urban and building area while enhancing the surface scattering over water and land area. Both theoretical derivation and experiments on ten polarimetric synthetic aperture radar data sets validate the outperformance. Nonetheless, for whatever unitary transformation employed, there is, forever, a T 13 -related residual component in GG4U. Thus, the incorporation of unitary transformation into Y4R and S4R for the full modeling of polarimetric information is impossible in theory only when the canonical scattering model with nonzero (1, 3) entry of coherency matrix is used to add the balance equation system an independent T 13 -related equation rather than a redundant one.","['Mathematical model', 'Matrix decomposition', 'Radar scattering', 'Radar', 'Apertures', 'Data models']","['Polarimetric decomposition', 'radar polarimetry', 'scattering model', 'unitary transformation']"
This paper presents a novel automatic real-time remote sensing algorithm that uses radar images and global positioning satellite system measurements to estimate the ice-drift velocity vector in a region around a free-floating and potentially moving vessel. It is motivated by the low image frequency of satellite systems together with the inconvenience of deploying and retrieving ice trackers (beacons) on the ice. The algorithm combines radar image processing with two Kalman filters to produce the estimated local drift vector decoupled from the ship motion. The proposed design is verified using a full-scale data set from an ice management operation north of Svalbard in 2015. It is found that the performance of the algorithm is comparable with that of trackers on the ice.,"['Ice', 'Radar tracking', 'Spaceborne radar', 'Sea measurements', 'Marine vehicles', 'Radar imaging']","['Arctic', 'ice', 'Kalman filtering', 'marine vehicles', 'radar', 'remote Sensing']"
"This article presents an assessment of the horizontal accuracy and precision of the laser altimetry observations collected by NASA’s Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) mission. We selected the terrain-matching method to determine the position of laser altimeter profiles within a precisely known surface, represented by a digital elevation model (DEM). We took this classical approach a step further, approximated the DEM by planar surfaces, and calculated the optimal position of the laser profile by minimizing the square sum of the elevation differences between reference DEMs and ICESat-2 profiles. We found the highly accurate DEMs of the McMurdo Dry Valleys (DV), Antarctica, ideal for this research because of their stable landscape and rugged topography. We computed the 3-D shift parameters of 379 different laser altimeter profiles along two reference ground tracks collected within the first two years of the mission. Analyzing these results revealed a total geolocation error (mean +1σ) of 4.93 m for version 3 and 4.66 m for version 4 data. These numbers are the averages of the six beams, expressed as mean +1σand lie well within the mission requirement of 6.5 m.","['Laser beams', 'Measurement by laser beam', 'Geology', 'Surface emitting lasers', 'Ice', 'Surface topography', 'Antarctica']","['Calibration', 'Ice', 'Cloud', 'and Land Elevation Satellite-2 (ICESat-2)', 'laser altimetry']"
"In this study application of convolutional neural networks (CNNs) preceded by synthetic aperture radar (SAR), image segmentation for sea ice concentration (SIC) estimation over the Baltic Sea from dual-polarized C-band SAR imagery is studied. Three algorithm variants were studied and trained using FMI ice chart SIC or a synthetic SIC dataset with different SIC values generated by combining pure open water and sea ice blocks by applying binary masks. The first two algorithm variants were trained using only open water and fully ice-covered patches, based on the FMI ice charts, and they had a similar CNN structure. These two algorithm variants differ only in deriving the segmentwise SIC from the CNN output. In the third algorithm, variant synthetic SIC data derived as a mixture of open water and fully ice-covered ice patches according to the ice charts were used in training. The estimation results were evaluated with respect to the FMI ice chart SIC for an independent test dataset. The results were very encouraging for operational purposes and significantly better than for our earlier SAR-based SIC estimation algorithms. The algorithm version trained with synthetic SIC data clearly outperformed the two other algorithm versions and our earlier SIC estimation results based on dual-polarized SAR, using independent FMI ice chart SIC as a reference.","['Ice', 'Silicon carbide', 'Estimation', 'Sea ice', 'Synthetic aperture radar', 'Image segmentation', 'Radar polarimetry']","['Convolutional neural network (CNN)', 'sea ice', 'sea ice concentration (SIC)', 'synthetic aperture radar (SAR)']"
"Finding sparse solutions of underdetermined linear systems commonly requires the solving ofL1regularized least-squares minimization problem, which is also known as the basis pursuit denoising (BPDN). They are computationally expensive since they cannot be solved analytically. An emerging technique known as deep unrolling provided a good combination of the descriptive ability of neural networks, explainable, and computational efficiency for BPDN. Many unrolled neural networks for BPDN, e.g., learned iterative shrinkage thresholding algorithm and its variants, employ shrinkage functions to prune elements with small magnitude. Through experiments on synthetic aperture radar tomography (TomoSAR), we discover the shrinkage step leads to unavoidable information loss in the dynamics of networks and degrades the performance of the model. We propose a recurrent neural network (RNN) with novel sparse minimal gated units (SMGUs) to solve the information loss issue. The proposed RNN architecture with SMGUs benefits from incorporating historical information into optimization and, thus, effectively preserves full information in the final output. Taking TomoSAR inversion as an example, extensive simulations demonstrated that the proposed RNN outperforms the state-of-the-art deep learning-based algorithm in terms of super-resolution power and generalization ability. It achieved 10%–20% higher double-scatterer detection rate and is less sensitive to phase and amplitude ratio difference between scatterers. Test on real TerraSAR-X spotlight images also shows the high-quality 3-D reconstruction of the test site.","['Logic gates', 'Imaging', 'Recurrent neural networks', 'Image reconstruction', 'Thresholding (Imaging)', 'Deep learning', 'Signal processing algorithms']","['Basis pursuit denoising (BPDN)', 'recurrent neural network (RNN)', 'sparse reconstruction', 'synthetic aperture radar tomography (TomoSAR)']"
"Building footprint generation is a vital task in a wide range of applications, including, to name a few, land use management, urban planning and monitoring, and geographical database updating. Most existing approaches addressing this problem fall back on convolutional neural networks (CNNs) to learn semantic masks of buildings. However, one limitation of their results is blurred building boundaries. To address this, we propose to learn attraction field representation for building boundaries, which is capable of providing an enhanced representation power. Our method comprises two elemental modules: an Img2AFM module and an AFM2Mask module. More specifically, the former aims at learning an attraction field representation conditioned on an input image, which is capable of enhancing building boundaries and suppressing the background. The latter module predicts segmentation masks of buildings using the learned attraction field map. The proposed method is evaluated on three datasets with different spatial resolutions: the ISPRS dataset, the INRIA dataset, and the Planet dataset. From experimental results, we find that the proposed framework can well preserve geometric shapes and sharp boundaries of buildings, which brings significant improvements over other competitors. The trained model and code are available at https://github.com/lqycrystal/AFM_building .","['Buildings', 'Semantics', 'Image segmentation', 'Remote sensing', 'Feature extraction', 'Convolutional neural networks', 'Task analysis']","['Attraction field map (AFM)', 'building footprint', 'convolutional neural network (CNN)', 'semantic segmentation']"
"This article presents the bistatic operation mode and the performance analysis of KAPRI, a terrestrial frequency-modulated continuous-wave (FMCW) Ku-band polarimetric radar interferometer capable of acquiring bistatic full-polarimetric datasets with high spatial and temporal resolution. In the bistatic configuration, the system is composed of two independently operating KAPRI devices, one serving as a primary transmitter and receiver and the other as a secondary receiver. The secondary bistatic dataset is affected by possible offsets between the two devices’ reference clocks, as well as distortions arising from the bistatic geometry. To correct for this, we present a two-chirp bistatic FMCW signal model, which accounts for the distortions, and a reference chirp transmission procedure, which allows correcting the clock offsets in the deramped signal time domain. The second challenge of operation of a bistatic polarimetric system is polarimetric calibration since it is not possible to employ purely monostatic targets such as corner reflectors. For this purpose, we developed a novel active calibration device Variable-Signature Polarimetric Active Radar Calibrator (VSPARC), designed for monostatic and bistatic calibration of all polarimetric channels. VSPARC and its associated novel polarimetric calibration method were then used to achieve full calibration of both KAPRI devices with polarimetric phase calibration accuracy of 20° and 30-dB polarization purity in field conditions. This article thus presents a complete measurement configuration and data processing pipeline necessary for synchronization, coregistration, and polarimetric calibration of bistatic and monostatic datasets acquired by a real-aperture FMCW radar.","['Calibration', 'Spaceborne radar', 'Radar', 'Radar polarimetry', 'Radar antennas', 'Receiving antennas', 'Geometry']","['Active radar calibrator', 'bistatic radar', 'polarimetric calibration', 'real-aperture radar']"
"Advanced satellite-borne remote sensing instruments produce high-resolution multispectral data for much of the globe at a daily cadence. These datasets open up the possibility of improved understanding of cloud dynamics and feedback, which remain the biggest source of uncertainty in global climate model projections. As a step toward answering these questions, we describe an automated rotation-invariant cloud clustering (RICC) method that leverages deep learning autoencoder technology to organize cloud imagery within large datasets in an unsupervised fashion, free from assumptions about predefined classes. We describe both the design and implementation of this method and its evaluation, which uses a sequence of testing protocols to determine whether the resulting clusters: 1) are physically reasonable (i.e., embody scientifically relevant distinctions); 2) capture information on spatial distributions, such as textures; 3) are cohesive and separable in latent space; and 4) are rotationally invariant (i.e., insensitive to the orientation of an image). Results obtained when these evaluation protocols are applied to RICC outputs suggest that the resultant novel cloud clusters capture meaningful aspects of cloud physics, are appropriately spatially coherent, and are invariant to orientations of input images. Our results support the possibility of using an unsupervised data-driven approach for automated clustering and pattern discovery in cloud imagery.","['Clouds', 'Satellites', 'Protocols', 'Feature extraction', 'Unsupervised learning', 'Supervised learning', 'MODIS']","['Autoencoder', 'cloud classification (CLDCLASS)', 'clustering', 'moderate resolution imaging spectroradiometer (MODIS) rotation-invariant loss', 'unsupervised learning']"
"This article presents a new index, polarization-conversion ratio (PCR) to characterize depolarized bistatic scattering from rough surfaces with dielectric inhomogeneity and spatial anisotropy. We then investigate the dependence of PCR on both surface and radar parameters. Numerical results show that the distribution of PCR on the scattering plane varies with the polarization state of the incident wave and incident angle. The PCR clusters more in the cross-plane for horizontally polarized incidence. However, for vertically polarized incidence, the PCR disperses as “triangular shape” on the whole scattering plane with a sharp valley occurring in the incident plane. The following points can be drawn: 1) the inhomogeneity effectively enhances the PCR in the cross-plane; 2) the effect of anisotropy on the PCR is relatively weak, because the scattering is less affected by correlation length; 3) the impacts of surface rms height on the PCR are negative on the whole scattering plane; and 4) as the background permittivity increases, at the horizontally polarized incidence, the PCR is enhanced in the backward and forward regions, while at vertically polarized incidence, it is enhanced in the incident plane and the forward region. As is demonstrated, the PCR is an effective measure of the sensitivity of depolarization, making it potentially useful as a new reliable index for surface parameter inversion.","['Scattering', 'Rough surfaces', 'Surface roughness', 'Dielectrics', 'Anisotropic magnetoresistance', 'Nonhomogeneous media', 'Sea surface']","['Anisotropy', 'bistatic scattering', 'depolarization', 'inhomogeneous', 'polarization conversion', 'rough surface']"
"A flash flood is a rapid and intense response of a drainage area to heavy rainfall events. In the arid and semiarid parts of the Eastern Mediterranean (EM) region, the spatiotemporal distribution of rainfall is the most important factor for flash flood generation. A possible precursor to heavy rainfall events is the rise in tropospheric water vapor amount, which can be remotely sensed using ground-based global navigation satellite system (GNSS) stations. Here, we use the precipitable water vapor (PWV) derived from nine GNSS ground-based stations in the arid part of the EM region in order to predict flash floods. Our approach includes using three types of machine learning (ML) models in a binary classification task, which predicts whether a flash flood will occur given 24 h of PWV data. We train our models with 107 unique flash flood events and vigorously test them using a nested cross-validation technique. The results indicate a good agreement between all three types of models and across various score metrics. In addition, the models are further improved by adding more features such as surface pressure measurements. Finally, a feature importance analysis shows that the most important features are the PWV values from 2 to 6 h prior to a flash flood. These promising results indicate that it is possible to augment the current flash flood warning systems with a near real-time GNSS ground-based data-driven approach as demonstrated in this work.","['Floods', 'Global navigation satellite system', 'Delays', 'Receivers', 'Meteorology', 'Predictive models', 'Global Positioning System']","['Eastern Mediterranean (EM)', 'flash floods', 'global navigation satellite system (GNSS)', 'machine learning (ML)', 'path delays', 'precipitable water vapor (PWV)']"
"We present new high-resolution snow depth data on Arctic sea ice derived from airborne microwave radar measurements from the IceBird campaigns of the Alfred Wegener Institute (AWI) together with a new retrieval method using signal peakiness based on an intercomparison exercise of colocated data at different altitudes. We aim to demonstrate the capabilities and potential improvements of radar data, which were acquired at a lower altitude (200 ft) and slower speed (110 kn) and had a smaller radar footprint size (2-m diameter) than previous airborne snow radar data. So far, AWI Snow Radar data have been derived using a 2–18-GHz ultrawideband frequency-modulated continuous-wave (FMCW) radar in 2017–2019. Our results show that our method in combination with thorough calibration through coherent noise removal and system response deconvolution significantly improves the quality of the radar-derived snow depth data. The validation against a 2-D grid of in situ snow depth measurements on level landfast first-year ice indicates a mean bias of only 0.86 cm between radar and ground truth. Comparison between the radar-derived snow depth estimates from different altitudes shows good consistency. We conclude that the AWI Snow Radar aboard the IceBird campaigns is able to measure the snow depth on Arctic sea ice accurately at higher spatial resolution than but consistent with the existing airborne snow radar data of NASA Operation IceBridge. Together with the simultaneous measurements of the total ice thickness and surface freeboard, the IceBird campaign data will be able to describe the whole sea-ice column on regional scales.","['Snow', 'Radar', 'Sea ice', 'Airborne radar', 'Spaceborne radar', 'Sea measurements', 'Radar measurements']","['Airborne', 'microwave', 'radar', 'sea ice', 'snow']"
"Cyanobacterial harmful algal blooms are an increasing threat to coastal and inland waters. These blooms can be detected using optical radiometers due to the presence of phycocyanin (PC) pigments. The spectral resolution of best-available multispectral sensors limits their ability to diagnostically detect PC in the presence of other photosynthetic pigments. To assess the role of spectral resolution in the determination of PC, a large (N=905) database of colocated in situ radiometric spectra and PC are employed. We first examine the performance of selected widely used machine-learning (ML) models against that of benchmark algorithms for hyperspectral remote sensing reflectance (Rrs) spectra resampled to the spectral configuration of the Hyperspectral Imager for the Coastal Ocean (HICO) with a full-width at half-maximum (FWHM) of < 6 nm. Results show that the multilayer perceptron (MLP) neural network applied to HICO spectral configurations (median errors < 65%) outperforms other ML models. This model is subsequently applied toRrsspectra resampled to the band configuration of existing satellite instruments and of the one proposed for the next Landsat sensor. These results confirm that employing MLP models to estimate PC from hyperspectral data delivers tangible improvements compared with retrievals from multispectral data and benchmark algorithms (with median errors between∼73% and 126%) and shows promise for developing a globally applicable cyanobacteria measurement approach.","['Hyperspectral imaging', 'Optical sensors', 'Pigments', 'Monitoring', 'Lakes', 'Absorption', 'Sea measurements']","['Cyanobacteria harmful algal bloom (Cyano HAB)', 'hyperspectral', 'machine learning (ML)', 'neural network', 'phycocyanin (PC)', 'spectral resolution']"
"The orbit of the tropical rainfall measuring mission (TRMM) satellite was boosted from 350 to 402.5 km in August 2001 to extend its lifetime by conserving fuel. Since the timing between transmission and reception by the precipitation radar (PR) onboard the TRMM satellite was a fixed constant for measurement from an original altitude of 350 km, the PR encountered a mismatch between the transmitting and receiving beams after the TRMM orbit boost. Although the PR algorithm in TRMM Version 7 employs a correction for the beam mismatch, its error remains as an underestimation of the precipitation estimates near surface in the second half of the swath. This paper aims to mitigate the beam-mismatch correction error in Version 7. The beam-mismatch correction needs to estimate the radar echo that would be measured in the virtual intermediate beam between the beam in question and the previous adjacent beam. The beam-mismatch correction developed in this paper assumes that the surface and precipitation echoes change linearly in the horizontal direction in parallel to the surface between the two beams. The new correction is tested with observational data, indicating that the method improves the accuracy of the correction at off-nadir angles. Statistics of the surface normalized radar cross sections and the bright band peak intensities at off-nadir angles are improved using the method. The asymmetric bias of the precipitation estimates with respect to the scan angle in Version 7 is mitigated by 95.9% over ocean and 72.5% over land with the new correction.","['Orbits', 'Spaceborne radar', 'Extraterrestrial measurements', 'Degradation', 'Satellites', 'Radar measurements']","['Algorithm', 'beam mismatch', 'precipitation radar (PR)', 'tropical rainfall measuring mission (TRMM)']"
"Volumetric synthetic aperture radar (VolSAR) analysis techniques and image formation algorithms suitable for short ranges and large scenes are presented. From a diffractive wave field inversion perspective, ultrawide beamwidth and near range SAR imaging scenarios can be viewed as a form of in situ SAR. Novel Huygens–Fresnel processing methods are introduced that empower in situ volumetric imaging with 2-D and 3-D aperture synthesis. These methods support coherent fusion across multiple separated frequency bands and also support spatial subaperture fusion of data from sparse sensor swarms. A novel signal analysis tool we call a chirp couplet is developed and shown to be useful in the expression and exploitation of temporal and spatial SAR chirp signals. Chirp couplets provide a physically motivated and unifying tool for both temporal and spatial elements in SAR signal analysis. In the temporal domain, chirp couplets provide a generalized formulation of the coupling of time and frequency that exists, for example, in linear frequency modulated waveforms. In the spatial domain, chirp couplets describe the coupling of sensor position and sensed wavenumber. Formulations of chirp couplets suitable for ray tomographic SAR algorithms (i.e., polar format algorithms) are contrasted with chirp couplets capable of supporting diffractive tomographic SAR algorithms (i.e., Stolt format algorithms). Scenarios in which diffraction limited resolution can be achieved with in situ VolSAR and finite length synthetic apertures are explored. VolSAR imaging methods are shown to escape the approximations that constrain the application and performance of range-Doppler and polar format methods.","['Apertures', 'Chirp', 'Synthetic aperture radar', 'Radar imaging', 'Tomography', 'Image resolution']","['3D radar imaging', 'Huygens–Fresnel theory', 'radar signal processing', 'radar theory', 'synthetic aperture radar (SAR)', 'volumetric radar imaging']"
"Reflectometry of Global Navigation Satellite Systems (GNSS) signals from the ocean surface has provided a new source of observations to study the ocean-atmosphere interaction. We investigate the sensitivity and performance of GNSS-Reflectometry (GNSS-R) data to retrieve sea surface roughness (SSR) as an indicator of sea state. A data set of one-year observations in 2016 is acquired from a coastal GNSS-R experiment in Onsala, Sweden. The experiment exploits two sea-looking antennas with right- and left-hand circular polarizations (RHCP and LHCP). The interference of the direct and reflected signals captured by the antennas is used by a GNSS-R receiver to generate complex interferometric fringes. We process the interferometric observations to estimate the contributions of direct signals and reflections to the total power. The power estimates are inverted to the SSR using the state-of-the-art model. The roughness measurements from the RHCP and LHCP links are evaluated against match-up wind measurements obtained from the nearest meteorological station. The results report on successful roughness retrieval with overall correlations of 0.76 for both links. However, the roughness effect in LHCP observations is more pronounced. The influence of surrounding complex coastlines and the wind direction dependence are discussed. The analysis reveals that the winds blowing from land have minimal impact on the roughness due to limited fetch. A clear improvement of roughness estimates with an overall correlation of 0.82 is observed for combined polarimetric observations from the RHCP and LHCP links. The combined observations can also improve the sensitivity of GNSS-R measurements to the change of sea state.","['Antennas', 'Sea surface', 'Receivers', 'Satellites', 'Sea state', 'Sea measurements', 'Global navigation satellite system']","['Global Navigation Satellite Systems (GNSS)-Reflectometry', 'polarimetric observations', 'sea state', 'sea surface roughness (SSR)']"
"Compared with the traditional remote sensing image, there is a large amount of spectral information in the hyperspectral image (HSI), which makes HSI better reflect the actual condition of surface features. However, due to the limitations of imaging conditions, HSI tends to have a lower spatial resolution. In order to overcome this issue, we propose a spectral–spatial attention-based U-Net named SSAU-Net for HSI and multispectral image (MSI) fusion. The SSAU-Net constructs a spectral–spatial attention module by a coordinate-attention (CA) module and an efficient pyramid split attention (EPSA) module, which can enhance the image’s spectral information and spatial information. Meanwhile, the proposed network fully extracts the shallow and deep features of the images and finally generates high-resolution (HR) HSIs. Compared with the state-of-the-art HSI-MSI fusion methods, the experimental results verify that the proposed method has a better subjective and objective fusion effect.","['Feature extraction', 'Matrix decomposition', 'Image fusion', 'Hyperspectral imaging', 'Spatial resolution', 'Tensors', 'Image reconstruction']","['Deep learning', 'hyperspectral image (HSI)', 'image fusion', 'multispectral image (MSI)', 'spectral–spatial attention']"
"Due to the complexity of backscattering mechanisms in built-up areas, the synthetic aperture radar (SAR)-based mapping of floodwater in urban areas remains challenging. Open areas affected by flooding have low backscatter due to the specular reflection of calm water surfaces. Floodwater within built-up areas leads to double-bounce effects, the complexity of which depends on the configuration of floodwater concerning the facades of the surrounding buildings. Hence, it has been shown that the analysis of interferometric SAR coherence reduces the underdetection of floods in urbanized areas. Moreover, the high potential of deep convolutional neural networks for advancing SAR-based flood mapping is widely acknowledged. Therefore, we introduce an urban-aware U-Net model using dual-polarization Sentinel-1 multitemporal intensity and coherence data to map the extent of flooding in urban environments. It uses a priori information (i.e., an SAR-derived probabilistic urban mask) in the proposed urban-aware module, consisting of channel-wise attention and urban-aware normalization submodules to calibrate features and improve the final predictions. In this study, Sentinel-1 single-look complex data acquired over four study sites from three continents have been considered. The qualitative evaluation and quantitative analysis have been carried out using six urban flood cases. A comparison with previous methods reveals a significant enhancement in the accuracy of urban flood mapping: the F1 score of flooded urban increased from 0.3 to 0.6 with few false alarms in urban area using our method. Experimental results indicate that the proposed model trained with limited datasets has strong potential for near-real-time urban flood mapping.","['Floods', 'Coherence', 'Synthetic aperture radar', 'Urban areas', 'Buildings', 'Training', 'Soil']","['Deep learning (DL)', 'multitemporal synthetic aperture radar (SAR)', 'U-Net', 'urban flood mapping', 'urban-aware']"
"Single-photon airborne light detection and ranging (LiDAR) systems provide high-density data from high flight altitudes. We compared single-photon and linear-mode airborne LiDAR for the prediction of species-specific volumes in boreal coniferous-dominated forests. The LiDAR data sets were acquired at different flight altitudes using Leica SPL100 (single-photon, 17 points⋅ m−2), Riegl VQ-1560i (linear-mode, 11 points⋅ m−2), and Leica ALS60 (linear-mode, 0.6 points⋅ m−2) LiDAR systems. Volumes were predicted at the plot-level using Gaussian process regression with predictor variables extracted from the LiDAR data sets and aerial images. Our findings showed that the Leica SPL100 produced a greater mean root-mean-squared error (RMSE) value (41.7m3 ⋅ha −1 ) than the Leica ALS60 (39.3m3 ⋅ha −1 ) in the prediction of species-specific volumes. Correspondingly, the Riegl VQ-1560i (mean RMSE = 33.0m3 ⋅ha −1 ) outperformed both the Leica ALS60 and the Leica SPL100. We found that the cumulative distributions of the first echo heights >1.3 m were rather similar among the data sets, whereas the last echo distributions showed larger differences. We conclude that the Leica SPL100 data set is suitable for area-based LiDAR inventory by tree species although the prediction errors are greater than with data obtained using the modern linear-mode LiDAR, such as Riegl VQ-1560i.","['Laser radar', 'Forestry', 'Vegetation', 'Photonics', 'Vegetation mapping', 'Measurement by laser beam', 'Green products']","['Airborne laser scanning', 'Gaussian processes (GPs)', 'light detection and ranging (LiDAR) intensity', 'photon-counting LiDAR']"
"This article proposes a method for estimating the Doppler power spectrum (DPS) of a weather radar via minimum mean square error (MMSE). In order to detect severe weather phenomena that mostly occur within the lowest few kilometers of the atmosphere, weather radars have to direct their beams at low elevation angles, and the received signals from such observations usually contain reflections from the ground and buildings, so-called ground clutter. The MMSE estimator, which is an adaptive spectral estimator, allows weather radar DPSs to be obtained with excellently reduced ground clutter contaminations. The MMSE estimator was examined by numerical simulations, which supposed various precipitation and ground clutter scenarios and DPS estimation parameter values. The MMSE estimator provided DPSs almost as accurate as those from the traditional Fourier and windowed Fourier estimators in simulations with no ground clutter and much better DPSs than those estimators in the presence of ground clutter. Furthermore, the MMSE estimator gave better suppression of ground clutter contamination than the Capon estimator, which is another adaptive spectral estimator. As a result of statistical evaluations, ground clutter signals with a strong clutter-to-noise ratio of 70 dB appeared only in a narrow velocity range of the MMSE DPS, from −2 to 2 m/s, and caused degradation of the mean and standard errors outside this velocity range by just 1 dB. The MMSE estimator was also applied to signals received by actual weather radar, and DPS estimation of precipitation signals with similarly low ground clutter contamination was demonstrated.","['Meteorology', 'Radar', 'Meteorological radar', 'Clutter', 'Doppler effect', 'Estimation', 'Doppler radar']","['Adaptive filter', 'Doppler spectrum', 'weather radar']"
"The concurrent imaging technique enables parallel acquisitions with different beams or modes, e.g., a wide area Stripmap mode with a High-Resolution Spotlight mode. Such a concurrent Stripmap/Spotlight imaging technique is investigated for TerraSAR-X. This technique employs a pulse-to-pulse interleaving scheme to acquire two acquisitions-even of disjunctive areas-at the same time, offering products with different resolution and coverage portfolios. This capability is especially interesting for customers interested in an overview of a larger area but at the same time observing an area of interest with higher resolution, e.g., for infrastructure monitoring or reconnaissance applications. The basic concepts, as well as the driving system parameters, are discussed in detail, together with a coverage analysis revealing the high availability rate of the mode combinations on a global scale. A processing approach re-using a substantial part of the existing infrastructure is described and exemplary acquisitions are shown, together with a detailed performance analysis with respect to resolution and ambiguities.","['Imaging', 'Azimuth', 'Image resolution', 'Radar imaging', 'Synthetic aperture radar', 'Timing', 'Radar antennas']","['Concurrent imaging', 'high resolution', 'staring spotlight', 'synthetic aperture radar (SAR)', 'TerraSAR-X']"
"Incorporation of mid-wave infrared (MWIR) channel/s into the prevalent regression-based split-window technique (SWT) for operational daytime sea surface temperature (SST) retrieval is challenging. However, the MWIR channels are highly desirable to obtain unambiguous information from the surface since these channels offer high transparency with respect to the earth's atmosphere and are very sensitive to the thermal emission from the surface. On the other hand, the MWIR channel/s can be easily incorporated into any physical-based SST retrieval scheme. Daytime SST retrieval using various physical-based methods is studied and it is found that the physical deterministic sea surface temperature (PDSST) retrieval scheme is the best choice. This article discusses various scientific aspects of the daytime PDSST retrieval including MWIR channels from a theoretical point of view and its application on real data from Moderate Resolution Imaging Spectroradiometer (MODIS)-Aqua. Daytime SST retrievals from PDSST, including MWIR channels, are also compared with the currently operational SWT-based SSTs from MODIS-Aqua and MODIS-Terra by NASA, without MWIR channels. The root-mean-square differences in PDSST from the in situ buoys using the global matchup data for daytime MODIS-Aqua SSTs is ~0.28 K for complete cloud-free set and is ~0.38 K for MODIS-Aqua and MODIS-Terra when quasi-deterministic cloud and error masking algorithm is applied for cloud detection. The information gain is defined by combining the two metrics, quality improvement and the increase in cloud-free data. The PDSST suite rendered two to three times as much information as the NASA-produced daytime regression-based SST.","['Ocean temperature', 'Temperature measurement', 'Sea measurements', 'Sea surface', 'Data models', 'Atmospheric modeling', 'Predictive models']","['Information retrieval', 'infrared image sensor (Moderate Resolution Imaging Spectroradiometer (MODIS)-Aqua and MODIS-Terra)', 'inverse problem [physical deterministic (PD)]', 'radiative transfer (RT)', 'remote sensing', 'sea surface temperature (SST)']"
"The detection of small metallic objects buried in mineralized soil poses a challenge for metal detectors, especially when the response from the metallic objects is orders of magnitude below the response from the soil. This article describes a new, handheld, detector system based on magnetic induction spectroscopy (MIS), which can be used to detect buried metallic objects, even in challenging soil conditions. Experimental results consisting of 1669 passes across either buried objects or empty soil are presented. Fourteen objects were buried at three different depths in three types of soil including nonmineralized and mineralized soils. A novel processing algorithm is proposed to demonstrate how spectroscopy can be used to detect metallic objects in mineralized soils. The algorithm is robust across all types of soil, objects, and depths used in this experiment and achieves a true positive rate over 99% at a false-positive rate of less than 5%, based on just a single pass over the object. It has also been shown that the algorithm does not have to be trained separately for each soil type. The data gathered in the experiment are also published to enable more research on the processing algorithms for MIS-based detectors.","['Soil', 'Detectors', 'Metals', 'Landmine detection', 'Magnetic separation', 'Ground penetrating radar', 'Spectroscopy']","['Landmines', 'magnetic induction spectroscopy', 'metal detection', 'mineralized soil', 'minimum metal', 'multifrequency']"
"Doppler velocity can be derived by calculating Doppler shift anomalies between predicted and estimated Doppler centroids. The predicted Doppler centroid is calculated based on a geometric model of satellite assuming that the target is not moving. The estimated Doppler centroid can be directly extracted from the raw SAR signal data by applying the average cross-correlation coefficient method. It is known that wind-generated ocean waves can significantly contribute to Doppler velocity due to the correlation between orbital motions of the waves and (tilt and hydrodynamic) modulated radar cross sections, in addition to what sea surface current contributes. In this study, the characteristics of Doppler velocities under hurricane conditions were investigated using RADARSAT-1 ScanSAR raw data. Five different hurricanes (Hurricane Dean, Hurricane Ivan, Hurricane Kyle, Hurricane Lili, and Typhoon Xangsane) and sequential acquisitions of two hurricanes (Hurricane Kyle and Hurricane Lili) were selected to study the contribution of wind-induced waves to Doppler velocities and compared with in situ measurements of drifting buoys. The results show that hurricane-generated seas and associated winds and waves appear to be different from ordinary sea state. This leads to lower estimates of Doppler velocities than expected and much closer to sea surface current velocities.","['Hurricanes', 'Tropical cyclones', 'Sea surface', 'Synthetic aperture radar', 'Surface waves', 'Doppler shift']","['Doppler velocity', 'drift current', 'hurricane', 'RADARSAT', 'wind-induced waves']"
"Typical synthetic aperture radar imaging techniques neglect the dispersive nature of the so-called image “reflectivity” function over the bandwidth of the transmitted waveform. In this paper, we form an image of the complex scene reflectivity as it depends on (x, y, and frequency), or equivalently (x, y, and time delay), a technique we refer to as hyperspectral synthetic aperture radar (HSAR). Our approach is based on a signal model that allows arbitrary flight trajectories and arbitrary waveforms (including continuously transmitting signals such as noise waveforms), and incorporates the causal, dispersive nature of the scene reflectivity without resorting to resolution-degrading frequency-domain sub-banding as others have previously proposed. We describe the resulting joint time-space resolution of HSAR in terms of the imaging point spread function for a selection of geometries and waveform bandwidths, and provide numerical examples to illustrate the approach.","['Synthetic aperture radar', 'Mathematical model', 'Scattering', 'Imaging', 'Antennas', 'Dispersion', 'Radar imaging']","['Material dispersion', 'structural dispersion', 'synthetic aperture radar (SAR) imaging']"
"Space-based infrared tiny ship detection aims at separating tiny ships from the images captured by Earth-orbiting satellites. Due to the extremely large image coverage area (e.g., thousands of square kilometers), candidate targets in these images are much smaller, dimer, and more changeable than those targets observed by aerial- and land-based imaging devices. Existing short imaging distance-based infrared datasets and target detection methods cannot be well adopted to the space-based surveillance task. To address these problems, we develop a space-based infrared tiny ship detection dataset (namely, NUDT-SIRST-Sea) with 48 space-based infrared images and $17\,598$ pixel-level tiny ship annotations. Each image covers about $10\,000$ km 2 of area with $10 \ 000\,\, \times \ 10 \ 000$ pixels. Considering the extreme characteristics (e.g., small, dim, and changeable) of those tiny ships in such challenging scenes, we propose a multilevel TransUNet (MTU-Net) in this article. Specifically, we design a vision Transformer (ViT) convolutional neural network (CNN) hybrid encoder to extract multilevel features. Local feature maps are first extracted by several convolution layers and then fed into the multilevel feature extraction module [multilevel ViT module (MVTM)] to capture long-distance dependency. We further propose a copy–rotate–resize–paste (CRRP) data augmentation approach to accelerate the training phase, which effectively alleviates the issue of sample imbalance between targets and background. Besides, we design a FocalIoU loss to achieve both target localization and shape description. Experimental results on the NUDT-SIRST-Sea dataset show that our MTU-Net outperforms traditional and existing deep learning-based single-frame infrared small target (SIRST) methods in terms of probability of detection, false alarm rate, and intersection over union. Our code is available at https://github.com/TianhaoWu16/Multi-level-TransUNet-for-Space-based-Infrared-Tiny-ship-Detection","['Marine vehicles', 'Feature extraction', 'Task analysis', 'Seaports', 'Imaging', 'Shape', 'Deep learning']","['FocalIoU loss', 'infrared ship detection', 'space-based detection', 'tiny ship', 'vision Transformer (ViT)']"
"Aerial scene recognition is a fundamental research problem in interpreting high-resolution aerial imagery. Over the past few years, most studies focus on classifying an image into one scene category, while in real-world scenarios, it is more often that a single image contains multiple scenes. Therefore, in this article, we investigate a more practical yet underexplored task—multiscene recognition in single images. To this end, we create a large-scale dataset, called MultiScene, composed of 100 000 unconstrained high-resolution aerial images. Considering that manually labeling such images is extremely arduous, we resort to low-cost annotations from crowdsourcing platforms, e.g., OpenStreetMap (OSM). However, OSM data might suffer from incompleteness and incorrectness, which introduce noise into image labels. To address this issue, we visually inspect 14 000 images and correct their scene labels, yielding a subset of cleanly annotated images, named MultiScene-Clean. With it, we can develop and evaluate deep networks for multiscene recognition using clean data. Moreover, we provide crowdsourced annotations of all images for the purpose of studying network learning with noisy labels. We conduct experiments with extensive baseline models on both MultiScene-Clean and MultiScene to offer benchmarks for multiscene recognition in single images and learning from noisy labels for this task, respectively. To facilitate progress, we make our dataset and trained models available on https://gitlab.lrz.de/ai4eo/reasoning/multiscene .","['Image recognition', 'Task analysis', 'Annotations', 'Noise measurement', 'Earth', 'Sports', 'Feature extraction']","['Convolutional neural network (CNN)', 'crowdsourced annotations', 'large-scale aerial image dataset', 'learning from noisy labels', 'multiscene recognition in single images']"
"While deep learning technologies for computer vision have developed rapidly since 2012, modeling of remote sensing systems has remained focused around human vision. In particular, remote sensing systems are usually constructed to optimize sensing cost-quality tradeoffs with respect to human image interpretability. While some recent studies have explored remote sensing system design as a function of simple computer vision algorithm performance, there has been little work relating this design to the state of the art in computer vision: deep learning with convolutional neural networks. We develop experimental systems to conduct this analysis, showing results with modern deep learning algorithms and recent overhead image data. Our results are compared to standard image quality measurements based on human visual perception, and we conclude not only that machine and human interpretability differ significantly but also that computer vision performance is largely self-consistent across a range of disparate conditions. This paper is presented as a cornerstone for a new generation of sensor design systems that focus on computer algorithm performance instead of human visual perception.","['Measurement', 'Visualization', 'Image recognition', 'Image quality', 'Remote sensing', 'Signal to noise ratio', 'Mathematical model']","['Convolutional neural network (CNN)', 'deep learning', 'image system design', 'remote sensing', 'satellite imagery', 'transfer learning']"
"Fast soil moisture content (SMC) mapping is necessary to support water resource management and to understand crop growth, quality, and yield. Therefore, earth observation (EO) plays a key role due to its ability of almost real-time monitoring of large areas at a low cost. This study aimed to explore the possibility of taking advantage of freely available Sentinel-1 (S1) and Sentinel-2 (S2) EO data for the simultaneous prediction of SMC with cycle-consistent adversarial network (CycleGAN) for time-series gap filling. The proposed methodology, first, learns latent low-dimensional representation of the satellite images, then learns a simple machine learning (ML) model on top of these representations. To evaluate the methodology, a series of vineyards, located in South Australia ’s Eden valley are chosen. Specifically, we presented an efficient framework for extracting latent features from S1 and S2 imagery. We showed how one could use S1 to S2 feature translation based on CycleGAN using S1 and S2 time series when there are missing images acquired over an area of interest. The resulting data in our study is then used to fill gaps in time-series data. We used the resulting latent representations to predict SMC with various ML tools. In the experiments, CycleGAN and the autoencoders were trained with data randomly chosen around the site of interest, so we could augment the existing dataset. The best performance was demonstrated with random forest (RF) algorithm, whereas linear regression model demonstrated significant overfitting. The experiments demonstrate that the proposed methodology outperforms the compared state-of-the-art methods if there are missing optical and synthetic-aperture radar (SAR) images.","['Soil moisture', 'Feature extraction', 'Satellites', 'Data models', 'Predictive models', 'Training', 'Soil measurements']","['Agriculture', 'generative adversarial networks (GANs)', 'machine learning (ML)', 'Sentinel-1', 'Sentinel-2', 'soil moisture (SM)', 'unsupervised domain adaptation']"
"An updated algorithm for the removal of thermal noise in Sentinel-1 synthetic aperture radar (SAR) level-1 ground range detected (GRD) data in cross-polarization is presented. The algorithm is comprised of two steps: correction of the annotated thermal noise magnitude (previously proposed in Park et al. , 2018) and a novel correction of the annotated thermal noise range dependence. The magnitude of the annotated thermal noise is corrected by applying scale and offset coefficients tuned on a few hundred of Sentinel-1 data acquired over surfaces with low backscatter in interferometric wide and extra-wide swath modes in HV and VH polarizations. The values of coefficients for all modes and polarizations for data processing with the Instrument Processing Facility (IPF) version 3.1–3.3 are provided. The range dependence is corrected by minimizing a cost function between the annotated range profiles of thermal noise and antenna pattern gain (APG). An objective validation metric based on a comparison of averaged backscatter at interswath boundaries is proposed. Validation is performed on hundreds of Sentinel-1 scenes acquired over the open ocean, doldrums, deserts, and sea ice. It shows that the new algorithm outperforms the standard thermal noise removal algorithm proposed by European Space Agency in almost all cases. Analysis shows that the new algorithm worsens noise correction in cases when the range dependence of the annotated APG does not match with the observed signal, indicating either problems with signal processing on IPF or imprecise annotation of APG.","['Thermal noise', 'Synthetic aperture radar', 'Sea ice', 'Training', 'Table lookup', 'Sea surface', 'Ocean temperature']","['Remote sensing', 'synthetic aperture radar (SAR)', 'Terrain Observation with Progressive Scans SAR (TOPSAR)', 'thermal noise']"
"The domain adaptation (DA) approaches available to date are usually not well suited for practical DA scenarios of remote sensing image classification since these methods (such as unsupervised DA) rely on rich prior knowledge about the relationship between label sets of source and target domains, and source data are often not accessible due to privacy or confidentiality issues. To this end, we propose a practical universal DA (UniDA) setting for remote sensing image scene classification that requires no prior knowledge on the label sets. Furthermore, a novel UniDA method without source data is proposed for cases when the source data are unavailable. The architecture of the model is divided into two parts: the source data generation stage and the model adaptation stage. The first stage estimates the conditional distribution of source data from the pretrained model using the knowledge of class separability in the source domain and then synthesizes the source data. With this synthetic source data in hand, it becomes a UniDA task to classify a target sample correctly if it belongs to any category in the source label set or mark it as “unknown” otherwise. In the second stage, a novel transferable weight that distinguishes the shared and private label sets in each domain promotes the adaptation in the automatically discovered shared label set and recognizes the “unknown” samples successfully. Empirical results show that the proposed model is effective and practical for remote sensing image scene classification, regardless of whether the source data are available or not. The code is available at https://github.com/zhu-xlab/UniDA .","['Remote sensing', 'Adaptation models', 'Data models', 'Uncertainty', 'Image classification', 'Entropy', 'Earth']","['Remote sensing image classification', 'source data generation (SDG)', 'transferable weight', 'universal domain adaptation (DA)']"
"The modular Snow Microwave Radiative Transfer (SMRT) model simulates microwave scattering behavior in snow via different selectable theories and snow microstructure representations, which is well suited to intercomparisons analyses. Here, five microstructure models were parameterized from X-ray tomography and thin-section images of snow samples and evaluated with SMRT. Three field experiments provided observations of scattering and absorption coefficients, brightness temperature, and/or backscatter with the increasing complexity of snowpack. These took place in Sodankylä, Finland, and Weissfluhjoch, Switzerland. Simulations of scattering and absorption coefficients agreed well with observations, with higher errors for snow with predominantly vertical structures. For simulation of brightness temperature, difficulty in retrieving stickiness with the Sticky Hard Sphere microstructure model resulted in relatively poor performance for two experiments, but good agreement for the third. Exponential microstructure gave generally good results, near to the best performing models for two field experiments. The Independent Sphere model gave intermediate results. New Teubner–Strey and Gaussian Random Field models demonstrated the advantages of SMRT over microwave models with restricted microstructural geometry. Relative model performance is assessed by the quality of the microstructure model fit to micro-computed tomography (CT) data and further improvements may be possible with different fitting techniques. Careful consideration of simulation stratigraphy is required in this new era of high-resolution microstructure measurement as layers thinner than the wavelength introduce artificial scattering boundaries not seen by the instrument.","['Snow', 'Microstructure', 'Scattering', 'Microwave theory and techniques', 'Object oriented modeling', 'Microwave radiometry', 'Correlation']","['Microstructure', 'microwave scattering', 'SMRT model', 'snow', 'snow microwave radiative transfer (SMRT)']"
"This paper reports the first use of a 3-D radar imaging for observing E-region field-aligned plasma irregularities (FAIs) in the midlatitude ionosphere. Five carrier frequencies equally spaced between 46.25 and 46.75 MHz were transmitted alternately with consecutive radar pulses, and 20 receivers were employed for receiving the radar echoes. The experiments were carried out using the middle and upper atmosphere radar (34.85°N, 136.10°E), with the radar beams steered to the geographic north and 6.6° north by east, at the zenith angles of 51.0° and 51.5°, respectively. The 2-D imaging with the echoes collected by the 19 of the 20 receivers revealed that in zonal dimension, the off-beam direction of arrival (DOA) of the echo region changed alternately between positive and negative values, which can be attributed to the highly localized FAIs drifting zonally through the radar beam. On the other hand, the off-beam DOA in meridional dimension was negative and positive, respectively, at higher and lower range locations, which were supposed to be due to the meridional drift component of FAIs. A combination of 5 frequencies and 19 receivers achieved a 3-D imaging of the FAI structures, illustrating small-scale FAI striations and bubblelike plasma structures in the radar volume. Moreover, declination of geomagnetic field line was examined from the imaged 3-D brightness distribution in the radar volume; the estimated declination of geomagnetic field line was in agreement with that computed from the International Geomagnetic Reference Field model in 2011. This paper has demonstrated some capabilities of radar imaging for ionospheric investigation.","['Radar imaging', 'Imaging', 'Receivers', 'Radar antennas', 'Plasmas', 'Atmosphere']","['Direction of arrival (DOA) estimation', 'ionosphere', 'radar imaging', 'very high-frequency (VHF) radar']"
"Global-scale canopy height mapping is an important tool for ecosystem monitoring and sustainable forest management. Various studies have demonstrated the ability to estimate canopy height from a single spaceborne multispectral image using end-to-end learning techniques. In addition to texture information of a single-shot image, our study exploits multitemporal information of image sequences to improve estimation accuracy. We adopt a convolutional variant of a long short-term memory (LSTM) model for canopy height estimation from multitemporal instances of Sentinel-2 products. Furthermore, we utilize the deep ensembles technique for meaningful uncertainty estimation on the predictions and postprocessing isotonic regression model for calibrating them. Our lightweight model ( {\sim }320{\mathrm {k}}trainable parameters) achieves the mean absolute error (MAE) of 1.29 ~{\mathrm {m}}in a European test area of 79 ~{\mathrm {km}}^{2}. It outperforms the state-of-the-art methods based on single-shot spaceborne images as well as costly airborne images while providing additional confidence maps that are shown to be well calibrated. Moreover, the trained model is shown to be transferable in a different country of Europe using a fine-tuning area of as low as {\sim }2 ~{\mathrm {km}}^{2}with {\mathrm {MAE}}=1.94 {\mathrm {m}}.","['Forestry', 'Estimation', 'Vegetation mapping', 'Atmospheric modeling', 'Satellites', 'Laser radar', 'Uncertainty']","['Calibration', 'canopy height estimation', 'multitemporal regression', 'recurrent neural network (RNN)', 'Sentinel-2', 'uncertainty estimation']"
"The comb beam transmission (TX) approach, which forms a power antenna radiation pattern with multiple mainlobes, was studied for application to phased array weather radars. Combining the use of comb TX and a digital beamforming receiver enables a weather radar to observe multiple directions simultaneously. Numerical simulations show that the two-way antenna radiation patterns formed by comb TXs have properties comparable to conventional weather radar observation: almost the same mainlobe widths are achieved, and with reference to the mainlobe peak, the maximum sidelobe level is less than −37 dB, and the sidelobe level reaches −60 dB at an angle of 13.77°. These properties are superior to the wide TXs that are normally utilized by phased array weather radars. A simulation that applied the comb TX approach to a current C-band weather radar showed that the volume scan time can be reduced from almost 500 s to 1 min.","['Meteorology', 'Phased arrays', 'Meteorological radar', 'Antenna radiation patterns', 'Radar antennas']","['C-band frequency', 'phased array radar', 'weather observation']"
"Phase decorrelation, caused by changes in the surface scattering properties between two radar acquisitions, is a major limiting factor for interferometric synthetic aperture radar (InSAR) surface deformation analysis over vegetated terrain. Persistent Scatterer (PS) techniques have been developed to identify high-quality radar pixels suffering from minimal decorrelation artifacts. However, existing PS selection algorithms are often based on the statistics of InSAR amplitude and phase measurements at each individual radar pixel, and scattering signal models that take into account the phase correlation of nearby PS pixels have not been fully developed. Here, we present a new PS selection algorithm based on the similarity of phase observations between nearby radar pixels. We used this algorithm to analyze 25 C-band Envisat SAR scenes acquired over the San Luis Valley, Colorado, and 93 C-band Sentinel-1 SAR scenes acquired over the Greater Houston area, Texas. At both the test sites, the presence of dense vegetation leads to severe phase decorrelation artifacts even in some interferograms with short temporal baselines. Our algorithm can reduce the number of false positive and false negative PS pixels identified from an existing PS identification algorithm. The improved PS identification accuracy allows us to substantially increase the total number of high-quality interferograms that are suitable for time series analysis. We reconstructed spatially coherent InSAR phase observations through an interpolation between PS pixels, and recovered subtle deformation signals that are otherwise undetectable. In both the cases, the superior performance of our PS processing strategy was demonstrated using a large number of independent ground-truth data.","['Radar', 'Strain', 'Synthetic aperture radar', 'Phase measurement', 'Radar measurements', 'Decorrelation', 'Radar scattering']","['Decorrelation', 'Interferometric Synthetic Aperture Radar (InSAR)', 'Persistent Scatterer (PS)', 'phase similarity', 'surface deformation']"
"The reflected light from the Moon can be utilized as a reference for radiometric calibration by employing a model to generate reference values corresponding to the Moon observations made by instruments. Using a calibration target that is outside the atmosphere provides a distinct advantage for space-based instruments; however, the lunar irradiance sensed by satellite instruments naturally changes as the host spacecraft traverses its orbit. This article presents a study of the potential impact on lunar radiometric measurements due to their acquisition from an orbiting platform. A simulation of a Sun-synchronous orbit was coupled to the U.S. Geological Survey (USGS) lunar model to generate predicted irradiances for points along orbit passes through several lunations. These irradiance values exhibit variations tied to the spacecraft motion, arising primarily from changes in the Moon-sensor distance and the phase angle. The two effects are similar in overall magnitude, but their respective contributions depend on the time of month and the orbit. Relative changes in irradiance mostly fall within an envelope of ±0.006% per second, except at the smallest phase angles. These studies enable planning space-based Moon observations to minimize the change in the target irradiance, an important consideration for measurements acquired for radiometric characterization of the Moon.","['Moon', 'Extraterrestrial measurements', 'Radiometry', 'Calibration', 'Space vehicles', 'Instruments', 'Atmospheric measurements']","['Calibration', 'Moon', 'radiometry']"
"Radiometric calibration of the medium-resolution satellite data is critical for monitoring and quantifying changes in the Earth’s environment and resources. Many medium-resolution satellite sensors have irregular revisits and, sometimes, have a large difference in illumination viewing geometry compared with a reference sensor, posing a great challenge for routine cross-calibration practices. To overcome these issues, this study proposed a cross-calibration method to calibrate medium-resolution multispectral data. The Chinese Gaofen-4 (GF-4) panchromatic and multispectral sensor (PMS) data with large viewing angles were used as the test data, and Landsat-8 operational land imager (OLI) data were used as the reference data. A bidirectional reflectance distribution function (BRDF) correction method was proposed to eliminate the effects of differences in illumination viewing geometry between GF-4 and Landsat-8. The validation using concurrent image shows that the mean relative error (MRE) of cross calibration is less than 6.65%. Validation using ground measurements shows that our calibration results have an improvement of around 14.8% compared with the official released calibration coefficients. The time series cross calibration reveals that, without the requirements of simultaneous nadir observations (SNOs), our calibration activities can be carried out more often in practice. Gradual and continuous radiometric sensor degradation is identified with the monthly updated calibration coefficients, demonstrating the reliability and importance of the timely cross calibration. Besides, the cross-calibration approach does not rely on any specific calibration site, and the difference in illumination viewing geometry can be well considered. Thus, it can be easily adapted and applied to other optical satellite data.","['Calibration', 'Remote sensing', 'Earth', 'Artificial satellites', 'Satellite broadcasting', 'Radiometry', 'MODIS']","['Bidirectional reflectance distribution function (BRDF)', 'Gaofen-4 (GF-4)', 'Landsat-8', 'radiometric calibration']"
"A combined analysis of ocean surface wind vector measurements by the European Advanced Scatterometer (ASCAT) and the National Aeronautics and Space Administration QuikSCAT (QS) scatterometer using buoy measurements, numerical weather prediction model analyses, and spectral decomposition reveals significant statistical differences between the two data sets. While QS wind speeds agree better with buoy wind speeds than ASCAT above 15 m s -1 , ASCAT wind directions agree better with buoy directions overall than QS. In contrast, it is shown that sea-level pressure (SLP) fields derived from ASCAT and QS measurements compare better with each other than the winds in a statistical sense, even though ASCAT bulk pressure gradients (BPGs) are slightly weaker than buoy pressure gradients and have slightly lower spectral energy than QS. Weaker BPGs in ASCAT are consistent with the low bias in ASCAT wind speeds. Thus, it is proposed that scatterometer-derived SLP fields can be used as a filter to improve the wind directions. This improves the QS wind directions but has less effect on the more accurate ASCAT wind directions. The unfiltered ASCAT wind vector statistics compare well with the statistics of the direction-filtered QS winds. It is suggested that this methodology might provide a basis for minimizing the discrepancies between various satellite wind measurement data sets in view of producing a long-term record of satellite-derived SLP fields and ocean surface wind vectors.","['Sea measurements', 'Vectors', 'Wind speed', 'Radar measurements', 'Sea surface']","['Advaned Scatterometer (ASCAT)', 'ocean surface winds', 'planetary boundary layer modeling', 'QuikSCAT', 'scatterometer', 'sea-level pressure', 'spaceborne radar', 'wind validation']"
"Surface state data derived from spaceborne microwave sensors with suitable temporal sampling are to date only available in low spatial resolution (25-50 km). Current approaches do not adequately resolve spatial heterogeneity in landscape-scale freeze-thaw processes. We propose to derive a frozen fraction instead of binary freeze-thaw information. This introduces the possibility to monitor the gradual freezing and thawing of complex landscapes. Frozen fractions were retrieved from Advanced Scatterometer (ASCAT, C-band) backscatter on a 12.5-km grid for three sites in noncontinuous permafrost areas in northern Finland and the Austrian Alps. To calibrate the retrieval approach, frozen fractions based on Sentinel-1 synthetic aperture radar (SAR, C-band) were derived for all sites and compared to ASCAT backscatter. We found strong relationships for ASCAT backscatter with Sentinel-1 derived frozen fractions (Pearson correlations of -0.85 to -0.96) for the sites in northern Finland and less strong relationships for the Alpine site (Pearson correlations -0.579 and -0.611, including and excluding forested areas). Applying the derived linear relationships, predicted frozen fractions using ASCAT backscatter values showed root mean square error (RMSE) values between 7.26% and 16.87% when compared with Sentinel-1 frozen fractions. The validation of the Sentinel-1 derived freeze-thaw classifications showed high accuracy when compared to in situ near-surface soil temperature (84.7%-94%). Results are discussed with regard to landscape type, differences between spring and autumn, and gridding. This article serves as a proof of concept, showcasing the possibility to derive frozen fraction from coarse spatial resolution scatterometer time series to improve the representation of spatial heterogeneity in landscape-scale surface state.","['Temperature measurement', 'Backscatter', 'Spatial resolution', 'Monitoring', 'Radar measurements', 'Synthetic aperture radar', 'Land surface temperature']","['Advanced Scatterometer (ASCAT)', 'freeze-thaw', 'permafrost', 'Sentinel-1', 'surface state']"
"Twenty-one years of Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) global thermal infrared (TIR) acquisitions provide a large amount of data for volcano monitoring. These data, with high spatial and spectral resolution, enable routine investigations of volcanoes in remote and inaccessible regions, including those with no ground-based monitoring. However, the dataset is too large to be manually analyzed on a global basis. Here, we systematically process the data over several volcanoes using a deep learning algorithm to automatically extract volcanic thermal anomalies. We explore the application of a convolutional neural network (CNN), specifically UNET, to detect subtle to intense anomalies exploiting the spatial relationships of the volcanic features. We employ a supervised UNET network trained with the largest (1500) labeled dataset of ASTER TIR images from five different volcanoes, namely, Etna (Italy), Popocatépetl (Mexico), Lascar (Chile), Fuego (Guatemala), and Kliuchevskoi (Russia). We show that our approach achieves high accuracy (93%) with excellent generalization capabilities. The effectiveness of our model for detecting the full range of thermal emission is shown for volcanoes with very different styles of activity and tested at Vulcano (Italy). The results demonstrate the potential applicability of the proposed approach to the development of automated thermal analysis systems at the global scale using future TIR data such as the planned NASA Surface Biology and Geology (SBG) mission.","['Feature extraction', 'Volcanoes', 'Training', 'Deep learning', 'Convolutional neural networks', 'Thermal analysis', 'Thermal pollution']","['Deep learning (DL) classifier', 'satellite remote sensing', 'thermal infrared (TIR) image data', 'volcanic eruptions']"
"Statistical approaches for quantitative precipitation nowcasts (QPNs) have emerged with recent advances in sensors in geostationary orbits, which provide more frequent observations at higher spatial resolutions. Advanced Meteorological Imager (AMI) onboard South Korea's second geostationary satellite (GEO-KOMPSAT-2A), scheduled for launch in early 2018, is an example of these sensors. This paper introduces operational prototype algorithms that attempt to produce QPN products for GEO-KOMPSAT-2A. The AMI QPN products include the potential accumulated rainfall and the probability of rainfall (PoR) for a 3-h lead time. The potential accumulated rainfall algorithm consists of two major procedures: 1) identification of rainfall features on the outputs from the GEO-KOMPSAT-2A rainfall rate algorithm; and 2) tracking of these rainfall features between two consecutive images. The potential accumulated rainfall algorithm extrapolates precipitation fields every 15 min. Rainfall rates at each time step are accumulated to yield the 3-hourly rainfall. In addition, the extrapolated precipitation fields at 15-min intervals are used as inputs for the PoR algorithm, which produces the probability of precipitation during the same 3-h period. The QPN products can be classified as extrapolated features associated with precipitation. The validation results show that the extrapolated features tend to meet the designated accuracy for the prototype development stage. We also confirm a tendency for decreasing accuracy of the QPN products with increasing forecast lead time. Mitigating the dependence on lead time may remain a challenge that can be incorporated into the next generation of QPN algorithms.","['Satellites', 'Clouds', 'Prediction algorithms', 'Prototypes', 'Extrapolation', 'Radar tracking', 'Estimation']","['Geostationary satellite', 'potential accumulated rainfall', 'probability of rainfall (PoR)', 'quantitative precipitation nowcast']"
"Numerical weather prediction (NWP) models are used to predict the weather based on current observations in combination with physical and mathematical models. Yet, they are limited by the spatial density and the accuracy of the available observations. Satellite radar interferometry (InSAR) is known to be extremely sensitive to the 3-D atmospheric refractivity distribution and has a high spatial resolution, providing information that can be used for assimilation in NWP models. However, due to the inherent superposition of two or more atmospheric states, only biased and temporally differenced signals can be retrieved, which can also be contaminated by deformation signals and decorrelation. Here, we present a method to estimate single-epoch absolute atmospheric delays by combining InSAR time series with prior NWP model prediction time series, using a constrained least-squares estimation. We show that this leads to a solution that reliably extracts the single-epoch relative delays from InSAR data and uses prior NWP model data to find the absolute reference for these delays while mitigating long-term deformation and decorrelation signal. This approach leads to repetitive delay updates with a spatial resolution of 500 m, which can be directly assimilated into numerical weather models.","['Atmospheric modeling', 'Delays', 'Predictive models', 'Strain', 'Data models', 'Numerical models', 'Meteorology']","['Atmospheric delay', 'InSAR', 'numerical weather prediction (NWP) model', 'single epoch']"
"In recent years, leaps and bounds have developed spatiotemporal fusion (STF) methods for remote sensing (RS) images based on deep learning. However, most existing methods use 2-D convolution (Conv) to explore features. 3-D Conv can explore time-dimensional features, but it requires more memory footprint and is rarely used. In addition, the current STF methods based on convolutional neural networks (CNNs) are mainly the following two: 1) use 2-D Conv to extract features from multiple bands of the input image together and fuse the features to predict the multiband image directly and 2) use 2-D Conv to extract features from individual bands of the image, predict the reflectance data of individual bands, and finally stack the predicted individual bands directly to synthesize the multiband image. The former method does not sufficiently consider the spectral and reflectance differences between different bands, and the latter does not consider the similarity of spatial structures between adjacent bands and the spectral correlation. To solve these problems, we propose a 2-D/3-D hybrid CNN called HCNNet, in which the 2D-CNN branch extracts the spatial information features of single-band image, and the 3D-CNN branch extracts spatiotemporal features of single-band images. After fusing the features of the dual branches, we introduce neighboring band features to share spatial information so that the information is complementary to obtain single-band features and images, and finally stack each single-band image to generate multiband images. Visual assessment and metric evaluation of the three publicly available datasets showed that our method predicted better images compared with the five methods.","['Feature extraction', 'Image reconstruction', 'Spatiotemporal phenomena', 'Spatial resolution', 'Data mining', 'Reflectivity', 'Earth']","['Feature fusion', 'hybrid convolution (Conv)', 'spatiotemporal fusion (STF)', 'spectral correlation']"
"Access to fresh water is a key issue for the next decades in the context of global warming. The water level of lakes is a fundamental variable that needs to be monitored for this purpose. The radar altimetry constellation brings a worldwide means to this question. Recent advances in radar altimeter onboard tracking modes have allowed monitoring thousands of lakes and rivers. Now, measurements are widely available with better resolution: it is time to drastically improve the processing. The altimetry waveforms over lakes are difficult to analyze and very different from the ocean ones. We face a large variety of signals due to surface roughness, lake geometry, and environment. The inversion process, named retracking, shall be able to describe all these components. We propose here a retracking based on physical simulations taking as inputs the lake contour and the instrument characteristics. Fitting the simulation on the waveforms gives the water surface height. The algorithm has been tested on the Sentinel-3A and Sentinel-3B time series over Occitan reservoirs (France) and Swiss lakes and compared to in situ references. Over small Occitan reservoirs (few ha to few km 2 ), the unbiased root-mean-square error (ub-RMSE) is better than 14 cm. Over the medium-size Swiss lakes, the ub-RMSE is better than 10 cm for most of them.","['Lakes', 'Synthetic aperture radar', 'Sea surface', 'Surface waves', 'Radar tracking', 'Surface treatment', 'Surface topography', 'Global warming']","['Hydrology', 'lake', 'radar altimetry', 'retracking', 'Sentinel-3', 'synthetic aperture radar (SAR)']"
"In this paper, we revise the relative radiometric calibration of synthetic-aperture-radar stacks which exploits natural persistent scatterers (PSs). We introduce a new model to estimate a slight error in the sensor pointing in elevation and a new coherent method that makes use of phases evaluated by averaging the complex data on the local window. We show that the proposed approach outperforms the conventional one, as it is mostly insensitive to the “differential” biasing that affects the noncoherent permanent scatterer calibration (PScal). Results from processing COnstellation of small Satellites for the Mediterranean basin Observation (COSMO)-SkyMed and European Remote Sensing (ERS) satellites stacks are presented.","['Signal to noise ratio', 'Nickel', 'Accuracy', 'Calibration', 'Antennas', 'Estimation']","['Inverse problems', 'radar cross section', 'radar signal analysis', 'synthetic aperture radar (SAR)']"
"The pixel-offset method has been utilized as a powerful tool to measure large ground movements. However, L-band spaceborne synthetic aperture radar (SAR) data are often affected by the ionosphere, which produces serious noises in the azimuth component of the pixel offset field, called as azimuth streaks. Here, we propose a new method to mitigate azimuth streaks based on physical modeling. Azimuth streaks cannot be removed by simply combining the known relationship between ionospheric azimuth offset and the ionospheric phase delay with the phase delay obtained by the split-spectrum method. Thus, taking into account that image matching (coregistration) affects the measurement of azimuth offsets, we formulate a theoretical correction formula of azimuth streaks by subtracting the coregistration-induced effects approximated by a polynomial function from ionospheric azimuth offsets modeled using the split-spectrum method. Applying the method to two pairs of Advanced Land Observing Satellite-2 (ALOS-2)/Phased Array type L-band Synthetic Aperture Radar-2 (PALSAR-2) stripmap images which are severely affected by the ionosphere, we demonstrate effective mitigation of azimuth streaks. In the application to the case that no significant ground movement is detected by Global Navigation Satellite System (GNSS), while the standard deviation of azimuth pixel offset before the correction is 87.2 cm, the value after the correction is 29.2 cm, which is comparable to the theoretical measurement accuracy of the azimuth pixel offset. In the application to the 2016 Kumamoto earthquake, we substantially reduce the azimuth streaks and successfully extract the ground movements from the azimuth offset fields within an accuracy of about 20 cm. The result suggests the proposed method enables more accurate and operational estimation of the 3-D ground displacement.","['Azimuth', 'Delays', 'Synthetic aperture radar', 'Standards', 'Spaceborne radar', 'Area measurement', 'Motion measurement']","['Advanced Land Observing Satellite-2 (ALOS-2)', 'azimuth streaks estimation', 'pixel-offset method', 'split-spectrum method', 'synthetic aperture radar (SAR)']"
"The global navigation satellite system (GNSS) has the capacity for remote sensing of water vapor content in the atmosphere. Post-processing of GNSS data can provide integrated water vapor (IWV) with accuracies comparable to measurements of traditional sensors, i.e., water vapor radiometers. While GNSS meteorology benefits from thousands of permanent GNSS stations operating worldwide the spatial resolution of GNSS-derived IWV is limited to tens of kilometers. Further densification of GNSS networks is achievable with low-cost GNSS receivers. We investigated the feasibility of low-cost multi-GNSS receivers for monitoring IWV. The post-processing and the real-time (RT) solution are validated against: 1) the results from a geodetic-grade GNSS receiver; 2) colocated water vapor radiometer; and 3) numerical weather model (NWM). Despite the high variability of the IWV during the validation period, the standard deviation of IWV differences with respect to the water vapor radiometer was 1.0 and 1.5 kg/m2in post-processing and RT, respectively. The city-scale variability of water vapor content in the atmosphere was monitored by a network of 16 low-cost GNSS receivers deployed in the city of Wroclaw, Poland. During rapidly changing weather conditions, the disagreement between the low-cost GNSS-derived IWV field and the NWM reached up to 5.4 kg/m2and interstation IWV differences exceeded 5 kg/m2. It has been demonstrated that low-cost GNSS receivers are reliable tools for precise determination of IWV, also in RT. This study is the first to measure the water vapor content with a spatial resolution of single kilometers and to present a significantly diversified city-scale IWV field.","['Global navigation satellite system', 'Meteorology', 'Terrestrial atmosphere', 'Monitoring', 'Real-time systems', 'Satellites', 'Delays']","['Global navigation satellite system (GNSS)', 'GNSS meteorology', 'integrated water vapor (IWV)', 'low-cost', 'numerical weather model (NWM)', 'troposphere', 'water vapor', 'zenith total delay (ZTD)']"
"Spectral data offer a means of estimating the critical parameters of sediments, including sediment composition, moisture content, surface roughness, density, and grain-size distribution. Macroscopic surface roughness in particular has a substantial impact on the structure of the bidirectional reflectance factor (BRF) and the angular distribution of scattered light. In developing the models to invert the properties of the surface beyond just surface composition, roughness must also be accounted for in order to achieve reliable and repeatable results. This paper outlines laboratory studies in which the BRF and surface digital elevation measurements were performed on dry clay sediments. The results were used to explore the suitability of various roughness metrics to account for the radiometric effect of surface roughness. The metrics that are specifically addressed in this paper include random roughness and sill variance. Relative accuracy and tradeoffs between these metrics are described. We find that spectral variability, especially near spectral absorption features, correlates strongly with the quantified measures of surface roughness. We also find that spectral variability is sensitive to the sensor fore-optic size. The results suggest that roughness parameters might be directly determined from the spectrum itself. The relationship between spectral variability and macroscopic surface roughness was particularly strong in some broad spectral ranges of the visible, near infrared, and shortwave infrared, including the near-infrared region between 600 and 850 nm.","['Rough surfaces', 'Surface roughness', 'Measurement by laser beam', 'Absorption', 'Soil', 'Sediments', 'Geophysical measurements']","['Bidirectional reflectance factor (BRF)', 'digital elevation model (DEM)', 'goniometer of the Rochester Institute of Technology-two (GRIT-T)', 'roughness', 'sediment']"
"Warm convective clouds play a significant role in the earth's energy and water budgets. However, they still pose a challenge in climate research as their feedback to predicted thermodynamic changes is highly uncertain and considered critical to the overall climate system's response. The focus of this study is continental, organized shallow convective clouds that, although they are spread globally and form in a variety of environments, seem to have common properties. One of these properties seems to be their preferred formation over vegetated areas, thus referred hereafter as green Cu. In this article, we present new observations of emerging universality and explore them using a method that combines fine- and coarse-resolution remote-sensing data sets. First, we use Moderate Resolution Imaging Spectroradiometer (MODIS) true-color images to visually classify cloud fields into different classes and identify green Cu fields. We show that the level and type of organization and the properties of these fields (e.g., cloud size distribution and cloud fraction) are similar throughout the world, regardless of their location. Second, we match the corresponding MODIS level-3 cloud properties to the identified cloud classes, and based on this data sets statistics, we develop a detection method for green Cu along ten years of measurements (2003-2012). We examine the geographical distribution and seasonality of this class and show that these fields are highly abundant over many continental areas and indeed mostly in the vicinity of vegetated regions.","['Clouds', 'Meteorology', 'MODIS', 'Europe', 'Atmospheric modeling', 'Green products', 'Surface morphology']","['Cloud classification', 'cloud remote sensing', 'continental clouds', 'fair weather cumulus', 'Moderate Resolution Imaging Spectroradiometer (MODIS)', 'satellite applications', 'shallow cumulus']"
"Observations of microwave backscatter from shoaling and breaking surface waves acquired with a shore-based, coherent-on-receive X-band marine radar are presented. The radar was located at the dune cliff of a sandy beach with two breaker bars. Waves were approximately shore-normal (inclination < 10°) during the study period. Consistent with other studies, the backscatter intensity from breaking waves is significantly increased ( \approx 10dB compared to nonbreaking) with Doppler velocities close to the wave phase velocity in shallow water. The strong backscatter from active breakers can cause a significant amount of signal artifacts due to the leakage of pulse energy into adjacent range cells, in particular behind the breaking crests. In the near range, the backscatter from the undisturbed surface and such pulse smearing artifacts appear as distinct peaks inside the Doppler spectra. Thus, the velocity of both sources of scatterering can be retrieved using a dedicated peak separation algorithm. In the far range ( r > 500m), the artifacts dominate the Doppler signal behind breaking wave crests. Therefore, when investigating the spatio-temporal evolution of breaking wave-induced Doppler velocities with marine radar, the analysis should be restricted to the wave crests and the well-illuminated front faces of the waves. The evolution of Doppler spectra tracked along the crest of an exemplary individual breaking wave is extracted during the steepening, active- and post-breaking stage.","['Radar', 'Doppler radar', 'Radar imaging', 'Doppler effect', 'Backscatter', 'Sea surface', 'Radar antennas']","['Doppler radar', 'marine radar', 'remote sensing', 'sea surface electromagnetic scattering', 'surface waves', 'wave breaking']"
"This work explores the properties characterizing the phase nonclosure of multilook (ML) synthetic aperture radar (SAR) interferograms. Specifically, we study the implications of ML phase time incongruences on the generation of ground displacement time series through small baseline (SB) multitemporal InSAR (Mt-InSAR) methods. Our research clarifies how these phase inconsistencies can propagate through a time-redundant network of SB interferograms and contribute, along with phase unwrapping (PhU) errors, to the quality of the generated ground displacement products. Moreover, we analyze the effects of short-lived phase bias signals that could happen in sequences of SB interferograms and propose a strategy for their mitigation. The developed methods have been tested using both simulated and real SAR data. The latter were collected by the Sentinel-1 A/B (C-band) sensors over the study areas of Nevada, USA, and Sicily, Italy.","['Synthetic aperture radar', 'Radar polarimetry', 'Strain', 'Sociology', 'Speckle', 'Sensors', 'Reactive power']","['Ground deformations', 'multitemporal InSAR (Mt-InSAR) algorithms', 'phase closure']"
"Radio frequency interference (RFI) is becoming a major concern for future synthetic aperture radar (SAR) missions due to the increased user demand for frequency occupation in a number of applications. Each occurrence of interference introduces artifacts in the radar imagery, biasing the measurements and leading to erroneous results. In addition to conventional techniques, the use of multichannel SAR for RFI mitigation has been proposed, because its digital beamforming (DBF) capability allows for a spatial filtering of the received signals. Thereby, it becomes possible to remove RFI that arrives from a different direction than the SAR signal. Past publications on the topic presented highly flexible spatial filtering techniques. Those methods require either additional on-board processing or a substantial increase in the downlink capacity. This article shows that by slightly reducing the flexibility of the spatial filtering, DBF can be utilized for RFI mitigation without either drawbacks: the processing is performed on-ground after downlinking the data and the data volume remains manageable. This is achieved with auxiliary beams. Their concept and limitations are discussed in detail in this article and are supported with simulated RFI mitigation results. Furthermore, it is shown that the information collected with an auxiliary beam can also be used to filter the RFI signal when it is spatially nonorthogonal to the SAR signal.","['Synthetic aperture radar', 'Interference', 'Antennas', 'Array signal processing', 'Radar antennas', 'Spaceborne radar', 'Spatial filters']","['Digital beamforming (DBF)', 'radio frequency interference (RFI)', 'synthetic aperture radar (SAR)']"
"Although domain adaptation approaches have been proposed to tackle cross-regional, multitemporal, and multisensor remote sensing applications since they do not require any human interpretation in the target domain, most current works assume identical label space across the source and the target domains. However, in real-world applications, we often transfer knowledge from a large-scale dataset with rich annotations to a small-scale target dataset with scarcity of labels. In most cases, the label space of the source domain is usually large enough to subsume that of the target domain, which is termed partial domain adaptation. In this article, we propose a new partial domain adaptation algorithm for remote sensing scene classification and our proposed method contains three major parts. First, we employ a progressive auxiliary domain module to alleviate the negative transfer effect caused by outlier classes. Second, we adopt an improved domain adversarial neural network (DANN) with multiweights to better encourage domain confusion. Last but not least, we design an attentive complement entropy regularization to improve the prediction confidence for samples and avoid untransferable samples (such as the samples belonging to outlier classes in the source domain) being mistakenly classified. We collect three common remote sensing datasets to evaluate our proposed method. Our method achieves an average accuracy of 79.36%, which considerably outperforms other state-of-the-art partial domain adaptation methods with an average accuracy improvement of 1.90%–12.45% and attaining a 13.67% gain compared to the straightforward deep learning model (ResNet-50). The experiment results indicate that our approach shows promising prospects for solving more general and practical domain adaptation problems where the label space of the source domain subsumes that of the target domain.","['Remote sensing', 'Standards', 'Annotations', 'Rivers', 'Airports', 'Adaptation models', 'Histograms']","['Adversarial learning', 'deep learning', 'negative transfer effect', 'partial domain adaptation', 'remote sensing', 'scene classification']"
"Mixture density networks (MDNs) have emerged as a powerful tool for estimating water-quality indicators, such as chlorophyll-a (Chl a) from multispectral imagery. This study validates the use of an uncertainty metric calculated directly from Chl aestimates of the MDNs. We consider multispectral remote sensing reflectance spectra ( R_{\text {rs}}) for three satellite sensors commonly used in aquatic remote sensing, namely, the ocean and land colour instrument (OLCI), multispectral instrument (MSI), and operational land imager (OLI). First, a study on a labeled database of colocated in situ Chl aand R_{\text {rs}}measurements clearly illustrates that the suggested uncertainty metric accurately captures the reduced confidence associated with test data, which is drawn for a different distribution than the training data. This change in distribution maybe due to: 1) random noise; 2) uncertainties in the atmospheric correction; and 3) novel (unseen) data. The experiments on the labeled in situ dataset show that the estimated uncertainty has a correlation with the expected predictive error and can be used as a bound on the predictive error for most samples. To illustrate the ability of the MDNs in generating consistent products from multiple sensors, per-pixel uncertainty maps for three near-coincident images of OLCI, MSI, and OLI are produced. The study also examines temporal trends in OLCI-derived Chl aand the associated uncertainties at selected locations over a calendar year. Future work will include uncertainty estimation from MDNs with a multiparameter retrieval capability for hyperspectral and multispectral imagery.","['Uncertainty', 'Predictive models', 'Remote sensing', 'Data models', 'Biological system modeling', 'Measurement', 'Instruments']","['Aquatic remote sensing', 'chlorophyll-a (Chl \\$a \\$)', 'inland and coastal waters', 'Landsat-8', 'machine learning (ML)', 'Sentinel-2', 'Sentinel-3', 'uncertainty']"
"The mystery of the asteroid (4179) Toutatis was revealed by Chang'e-2 spacecraft during a close flyby on December 13, 2012. Optical imaging and navigation of the probe during the flyby were performed entirely under ground-based radio tracking and default sequence built on ground. This paper establishes a set of estimation algorithms of the relative trajectory between Chang'e-2 and Toutatis based on dynamics, optical, and radio constraints that are determined by the unique flyby mode. This study is the first time to precisely reproduce the core process of Chang'e-2's encounter with Toutatis based on several optical images. In addition to constructing a strict photogrammetric model, the shadowing effects caused by the illumination and the deviation of the center-of-mass (COM) from the center-of-figure (COF) in optical images are also considered. The spacecraft trajectory with regard to the COF of the body is estimated using images taken from 120 km or less. The formal one sigma uncertainty is (67, 20, and 11 m) in the principal axes frame of the position error ellipse, and the closest approaching distance between Chang'e-2 and Toutatis's COF is calculated as 1557 ± 11 m, which is more precise than previous results with an uncertainty of hundreds of meters. The spacecraft trajectory with regard to the COM of the body is estimated with an uncertainty of (211, 34, and 17 m), and the corresponding closest distance is estimated as 1451 ± 18 m based on the previously developed shape model of Toutatis. The algorithms and results in this study are important for evaluating the performance of this flyby mission and are also valuable for any similar optical navigation during a close approach. In addition, our results can help in precisely determining the axis of Toutatis and sizes of impact craters, which are critical for understanding the formation and evolution of Toutatis.","['Trajectory', 'Optical imaging', 'Space vehicles', 'Cameras', 'Estimation', 'Optical sensors']","[""Chang'e-2 probe"", 'constraints', 'optical imaging', 'relative trajectory', 'the closest approaching distance']"
"Layover separation has been fundamental to many synthetic aperture radar applications, such as building reconstruction and biomass estimation. Retrieving the scattering profile along the mixed dimension (elevation) is typically solved by inversion of the synthetic aperture radar (SAR) imaging model, a process known as SAR tomography. This article proposes a nonlinear blind scatterer separation method to retrieve the phase centers of the layovered scatterers, avoiding the computationally expensive tomographic inversion. We demonstrate that conventional linear separation methods, for example, principle component analysis (PCA), can only partially separate the scatterers under good conditions. These methods produce systematic phase bias in the retrieved scatterers due to the nonorthogonality of the scatterers' steering vectors, especially when the intensities of the sources are similar or the number of images is low. The proposed method artificially increases the dimensionality of the data using kernel PCA, hence mitigating the aforementioned limitations. In the processing, the proposed method sequentially deflates the covariance matrix using the estimate of the brightest scatterer from kernel PCA. Simulations demonstrate the superior performance of the proposed method over conventional PCA-based methods in various respects. Experiments using TerraSAR-X data show an improvement in height reconstruction accuracy by a factor of one to three, depending on the used number of looks.","['Synthetic aperture radar', 'Covariance matrices', 'Principal component analysis', 'Tomography', 'Radar imaging', 'Mathematical model']","['Blind source separation', 'kernel principle component analysis (PCA)', 'multibaseline InSAR', 'nonlinear kernel', 'synthetic aperture radar (SAR) tomography']"
"This article proposes a new sea surface wind speed (SSWS) retrieval modeling algorithm based on the empirical orthogonal function (EOF) analysis for observations acquired by the global navigation satellite system reflectometry (GNSS-R). As a nonparametric modeling algorithm, it is simpler compared with the nonlinear methods. The influence of wind speed and incident angle on the modeling error is analyzed for the first time using a spectrum analysis. Three types of data from 80% CYGNSS 2019–2020 observations [delay Doppler map average (DDMA) and leading edge slope (LES)], signal incident angle, and the European Centre for Medium-Range Weather Forecasts Reanalysis V5 (ERA5) reference wind speed are used in the EOF analysis to establish two retrieval models. The remaining 20% of the data are used for accuracy evaluation after getting the final wind speed by the minimum variance (MV) estimator. As a result, when using three 0–20-m/s wind speeds of ERA5, Advanced Scatterometer (ASCAT), and the Modern-Era Retrospective Analysis for Research and Applications V2 (MERRA2) as contrasts, the root mean squared errors (RMSEs) are 1.51, 1.45, and 1.43 m/s, respectively. Compared with CYGNSS wind product, the performance of this algorithm is closer to the L2 Climate Data Record (CDR) V1.1 product than V1.0. The results demonstrate that the EOF algorithm has a good performance in retrieving SSWS and can better retain the influence of the incident angle on the observations.",['Licenses'],"['Delay Doppler map average (DDMA)', 'empirical orthogonal function (EOF)', 'global navigation satellite system reflectometry (GNSS-R)', 'leading edge slope (LES)', 'sea surface wind speed (SSWS) retrieval']"
"Synthetic aperture radar (SAR) is a valuable tool for lake ice monitoring. The recently proposed SAR configuration for Earth observation called compact polarimetric (CP) SAR could be a good compromised choice between conventional (single or dual) and fully polarimetric (FP) SAR for operational ice applications, including lake ice. Given its enhanced radar target information compared with conventional SAR systems over wider swath coverage compared with FP SAR, CP systems could play important role in the new generation of Earth observation systems. Herein, we study the evolution of CP SAR parameters from simulated CP SAR data in relation to early ice growth. Focus of the study is on four lakes located in Cornwallis Island, Canadian Central Arctic. We adopt parameters extracted from dual circular polarimetric and right circular transmit, linear (horizontal and vertical) receive configurations. In this study, we consider the ice thickness calculated from an established empirical model. Meteorological and ice climatological data were used to support the analysis. Results demonstrated a potential connection between a number of CP parameters and lake ice growth. Furthermore, we were able to highlight the relationship between the density of air bubbles in ice layer and the intensity of volume scattering mechanism, leading to the identification of lakes with increased gas production activities. Thus, differences between lakes in terms of density of air bubbles were detected and statistically evaluated.","['Lakes', 'Ice', 'Synthetic aperture radar', 'Ice thickness', 'Snow', 'Radar', 'Atmospheric modeling']","['Bubble in lake ice', 'compact polarimetric (CP) synthetic aperture radar (SAR)', 'ice thickness', 'lake ice']"
"Atmospheric correction (AC) algorithms for ocean color (OC) data processing usually rely on ancillary data documenting the atmosphere and the sea state to help the calculation of the remote sensing reflectanceRRSfrom the radiance measured by a space sensor. This study aims at assessing the impact that the uncertainties associated with these ancillary data have on the AC outputs. For this objective, a full year of global Sea-viewing Wide Field-of-view Sensor (SeaWiFS) imagery is processed with the standard AC algorithm l2gen of the National Aeronautics and Space Administration with different sets of ancillary data, the reference case with National Centers for Environmental Prediction (NCEP) Reanalysis-2 meteorological data and satellite ozone products, as well as with ten ensemble members from the European Centre for Medium-Range Weather Forecast (ECMWF) CERA-20C data. The spread within the ensemble data and the differences with respect to the reference case are taken as a measure of the uncertainties associated with ancillary data. The impact onRRSof perturbations in ancillary variables vary in space, the variables having the largest effects being wind speed and relative humidity, and ozone at bands where ozone absorption is largest, while sea-level pressure and precipitable water have the smallest effect. Sensitivity coefficients quantifying the relationship between perturbations in ancillary variables and effects onRRSchange with variable and wavelength. At the global scale, the variations found onRRSwhen ancillary data are perturbed are usually small but not negligible and should be considered in the ocean color (OC) data uncertainty budget.","['Uncertainty', 'Image color analysis', 'Sensitivity', 'Remote sensing', 'Distributed databases', 'Satellites', 'Aerosols']","['Ocean color (OC)', 'sea-viewing wide field-of-view sensor (SeaWiFS)', 'uncertainties']"
"The Global Navigation Satellite System (GNSS) polarimetric effects on the propagation of radio occultations (ROs) are studied here. Polarimetric ROs have been suggested as a technique to detect heavy rain events using opportunity signals from GNSS satellites. The systematic effects that hinder the isolation of the precipitation information are described and their significance and separability are assessed. A method that relies on the received phase difference between polarizations is presented. A dual-frequency extension is capable to completely separate the hydrometeor information from the other effects, including the ionospheric influence.","['Global navigation satellite system', 'Receivers', 'Satellite broadcasting', 'Ionosphere', 'Rain', 'Systematics']","['Faraday effect', 'Global Navigation Satellite System (GNSS)', 'microwave propagation', 'polarimetry', 'radio occultation (RO)', 'rainfall effects']"
"In order to measure the variance of wind velocity, which is contributed from turbulence, via radar observations, it is necessary to remove the unwanted contribution from strong horizontal velocity components through the finite beamwidth of the radar. This effect is referred to as beam broadening. Although the amount of beam broadening has thus far been calculated based on the approximating assumption that the pattern of the beam is rotationally symmetric and has very low sidelobes, we need to take a more theoretical approach to radar-one that does not have a simple beam pattern like the Antarctic Program of the Antarctic Syowa Station (PANSY) radar (69S, 39E). In this article, we clarify the theoretical relationship in a very simple form between the turbulence spectrum, which is directly associated with the variance of turbulence, two-way beam patterns, and the observed spectrum, using autocorrelation functions (ACFs). The theory is thoroughly universal and applicable to any type of atmospheric radar, such that we can quantitatively analyze radar observation systems. Furthermore, we propose a “debroadening” algorithm based directly on this theory and from calculations of the general maximum likelihood (ML). We performed numerical simulations that validate our theory and the algorithm.","['Radar antennas', 'Correlation', 'Atmospheric measurements', 'Wind speed', 'Radar remote sensing', 'Antarctica']","['Atmospheric radar', 'mesosphere-stratosphere-troposphere (MST) radar', 'beam broadening', 'debroadening', 'turbulence']"
"Satellite remote sensing acquisitions are usually processed after downlink to a ground station. The satellite travel time to the ground station adds to the total latency, increasing the time until a user can obtain the processing results. Performing the processing and information extraction onboard of the satellite can significantly reduce this time. In this study, synthetic aperture radar (SAR) image formation as well as ship detection and extreme weather detection were implemented in a multiprocessor system on a chip (MPSoC). Processing steps with high computational complexity were ported to run on the programmable logic (PL), achieving significant speed-up by implementing a high degree of parallelization and pipelining as well as efficient memory accesses. Steps with lower complexity run on the processing system (PS), allowing for higher flexibility and reducing the need for resources in the PL. The achieved processing times for an area covering 375 km 2 were approximately 4 s for image formation, 16 s for ship detection, and 31 s for extreme weather detection. These developments combined with new downlink concepts for low-rate information data streams show that the provision of satellite remote sensing results to end users in less than 5 min after acquisition is possible using an adequately equipped satellite.","['Synthetic aperture radar', 'Hardware', 'Radar polarimetry', 'Marine vehicles', 'Field programmable gate arrays', 'Software algorithms', 'Software']","['Constant false alarm rate (CFAR)', 'field-programmable gate array (FPGA)', 'multiprocessor system on a chip (MPSoC)', 'onboard processing', 'sea state detection', 'ship detection', 'synthetic aperture radar (SAR)']"
"The spectral and spatial resolutions of modern optical Earth observation data are continuously increasing. To fully utilize the data, integrate them with other information sources, and create applications relevant to real-world problems, extensive training data are required. We present TAIGA, an open dataset including continuous and categorical forestry data, accompanied by airborne hyperspectral imagery with a pixel size of 0.7 m. The dataset contains over 70 million labeled pixels belonging to more than 600 forest stands. To establish a baseline on TAIGA dataset for multitask learning, we trained and validated a convolutional neural network to simultaneously retrieve 13 forest variables. Due to the size of the imagery, the training and testing sets were independent, with strictly no overlap for patches up to45×45pixels. Our retrieval results show that including both spectral and textural information improves the accuracy of mapping key boreal forest structural characteristics, compared with an earlier study including only spectral information from the same image. TAIGA responds to the increased availability of hyperspectral and very high resolution imagery, and includes the forestry variables relevant for forestry and environmental applications. We propose the dataset as a new benchmark for spatial–spectral methods that overcomes the limitations of widely used small-scale hyperspectral datasets.","['Forestry', 'Hyperspectral imaging', 'Training', 'Spatial resolution', 'Data models', 'Soil', 'Deep learning']","['Boreal forest', 'convolutional neural networks', 'hyperspectral imaging (HSI)', 'multitask learning']"
"This article introduces the extended timing annotation dataset (ETAD) product for Sentinel-1 (S-1) which was developed in a joint effort of German Aerospace Center (DLR) and European Space Agency (ESA). It allows to correct range and azimuth timing of S-1 images for geophysical effects and for inaccuracies in synthetic aperture radar (SAR) image focusing. In combination with the precise orbit solution, these effects determine the absolute geolocation accuracy of S-1 SAR images and the relative collocation accuracy of repeat pass image stacks. ETAD contains the gridded timing corrections for the tropospheric and ionospheric path delays, the tidal-based surface displacements, and the SAR processing effects, all of which are computed for each data taken using standard models from geodesy and auxiliary atmospheric data. The ETAD product helps S-1 users to significantly improve the geolocation accuracy of the S-1 SAR products to better than 0.2 m and offers a potential solution for correcting large-scale interferometric phase variations. The product layout and product generation are described schematically. This article also reports first the results for different SAR techniques: first, the improvement in geolocation accuracy down to a few centimeters by verification of accurately surveyed corner reflector positions in the range–azimuth plane; second, the well-established offset-tracking technique, which is used for systematic ice velocity monitoring of ice sheets and glaciers, where ETAD can reduce velocity biases down to subcentimetric values; and third, the correction of atmospheric phase contributions in wide-area interferograms used for national and European ground motion services. These early results proof the added value of the ETAD corrections and that the product design is well-suited to be integrated into the processing flows of established SAR applications such as absolute ranging of targets, speckle/feature tracking, and interferometry.","['Synthetic aperture radar', 'Azimuth', 'Ice', 'Global navigation satellite system', 'Geology', 'Orbits', 'Delays']","['Atmospheric phase screen', 'glaciology', 'imaging geodesy', 'InSAR', 'ionospheric phase screen', 'natural hazards', 'radar remote sensing', 'SAR geodesy', 'Sentinel-1', 'speckle tracking', 'synthetic aperture radar']"
"Pavement survey is one of the most important applications for ground penetrating radar (GPR) in civil engineering. In the case of centimeter scale of GPR waves, the influence of interface roughness cannot be neglected and should be taken into account in the radar data model. The objective of this article is to estimate the time-delay in the presence of interface roughness by GPR. Using the property of noncircular signals, we propose a modified orthogonal matching pursuit method to estimate the pavement parameters for both overlapped and nonoverlapped echoes. Compared with subspace-based methods in coherent scenarios, the proposed method can estimate the time delays of backscattered echoes without applying the cumbersome interpolation and spatial smoothing procedures, which are more practical in real applications. The performance of the proposed method is tested on both simulated and experimental data. The estimation results show the good performance of the proposed method.","['Ground penetrating radar', 'Matching pursuit algorithms', 'Estimation', 'Frequency measurement', 'Media', 'Data models']","['Ground penetrating radar (GPR)', 'interface roughness', 'orthogonal matching pursuit (OMP)', 'time-delay estimation (TDE)']"
"Oil spills in the Arctic are becoming more likely as shipping traffic increases in response to climate-related sea ice loss. To improve oil spill detection capability, we used a controlled mesocosm to analyze the multipolarized C-band backscatter response of oil in newly formed sea ice (NI). Artificial sea ice was grown in two cylindrical tubs at the Sea-ice Environmental Research Facility, University of Manitoba. The sea ice physical characteristics, including surface roughness, thickness, temperature, and salinity, were measured before and after oil injection below the ice sheet. Time-series C-band radar backscatter measurements detected the differences in the sea ice evolution and oil migration to the sea ice surface in the oil-contaminated tub, which was compared to uncontaminated ice in a control tub. Immediately prior to the presence of oil on the ice surface, the copolarized backscatter is increased by 13-dB local maximum, while the cross-polarized backscatter is decreased by 9-dB. Ice physical properties suggest that the local backscatter maximum and minimum, which occurred immediately before oil migrated onto the surface, were related to a combination of brine and oil upward migration. The findings of this work provide a baseline data interpretation for oil detection in the Arctic Ocean using current and future C-band multipolarization radar satellites.","['Sea ice', 'Oils', 'Ice', 'Ocean temperature', 'Sea surface', 'Backscatter', 'Spaceborne radar']","['Arctic', 'detection', 'oil spill', 'radar backscatter', 'sea ice']"
"Ship tracking facilitates a comprehensive insight into maritime traffic situations and ensures its safety and security. However, with the current operational surveillance systems, detecting several maritime threats is still a major challenge. In this article, we propose a supportive ship tracking concept using an airborne-based radar sensor. The proposed tracking algorithm is suitable for dense multitarget scenarios. Tracking is performed in the range-Doppler domain. The primary advantage of using the range-Doppler domain is that ships even with low radar cross section moving with certain line-of-sight velocity appear out of the clutter region, thus improving their detectability. In addition, a powerful track management system is also developed to handle false targets. The simulated and real experimental results from the DLR’s airborne radar sensors, F-SAR and DBFSAR, are presented to prove the concept.","['Radar tracking', 'Target tracking', 'Marine vehicles', 'Radar', 'Airborne radar', 'Radar cross-sections', 'Radar antennas']","['Airborne radar', 'maritime safety', 'radar detections', 'radar tracking']"
"Deep learning methods have become ubiquitous tools in many Earth observation applications, delivering state-of-the-art results while proving to generalize for a variety of scenarios. One such domain concerns the Sentinel-2 (S2) satellite mission, which provides multispectral images in the form of 13 spectral bands, captured at three different spatial resolutions: 10, 20, and 60 m. This research aims to provide a super-resolution mechanism based on fully convolutional neural networks (CNNs) for upsampling the low-resolution (LR) spectral bands of S2 up to 10-m spatial resolution. Our approach is centered on attaining good performance with respect to two main properties: consistency and synthesis. While the synthesis evaluation, also known as Wald’s protocol, has spoken for the performance of almost all previously introduced methods, the consistency property has been overlooked as a viable evaluation procedure. Recently introduced techniques make use of sensor’s modulation transfer function (MTF) to learn an approximate inverse mapping from LR to high-resolution images, which is on a direct path for achieving a good consistency value. To this end, we propose a multiobjective loss for training our architectures, including an MTF-based mechanism, a direct input–output mapping using synthetically degraded data, along with direct similarity measures between high-frequency details from already available 10-m bands, and super-resolved images. Experiments indicate that our method is able to achieve a good tradeoff between consistency and synthesis properties, along with competitive visual quality results.","['Spatial resolution', 'Superresolution', 'Training', 'Neural networks', 'Monitoring', 'Degradation', 'Optimization']","['Consistency', 'convolutional neural networks (CNNs)', 'Sentinel-2 (S2)', 'super-resolution', 'synthesis']"
"Compared with remote sensing image (RSI) captioning methods based on the traditional encoder–decoder model, two-stage RSI captioning methods include an auxiliary remote sensing task to provide prior information, which enables them to generate more accurate descriptions. In previous two-stage RSI captioning methods, however, the image captioning and the auxiliary remote sensing tasks are handled separately, which is time-consuming and ignores mutual interference between tasks. To solve this problem, we propose a novel joint-training two-stage (JTTS) RSI captioning method. We use multilabel classification to provide prior information, and we design a differentiable sampling operator to replace the traditional nondifferentiable sampling operation to index the multilabel classification result. In contrast to previous two-stage RSI captioning methods, our method can implement joint training, and the joint loss allows the error of the generated description to flow into the optimization of the multilabel classification via backpropagation. Specifically, we approximate the Heaviside step function with the steep logistic function to implement a differentiable sampling operator for the multilabel classification. We propose a dynamic contrast loss function for multilabel classification tasks to ensure that a certain margin is maintained between the probabilities of the positive label and the negative label during sampling. We design an attribute-guided decoder to filter the multilabel prior information obtained by the sampling operator to generate more accurate image captions. The results of extensive experiments show that the JTTS method achieves state-of-the-art performance on the RSI captioning dataset (RSICD), the University of California, Merced (UCM)-captions, and the Sydney-captions datasets.","['Task analysis', 'Feature extraction', 'Decoding', 'Semantics', 'Training', 'Remote sensing', 'Logic gates']","['Image captioning', 'image understanding', 'joint training', 'multilabel attributes', 'remote sensing image (RSI)']"
"This article proposes a novel polarization orientation angle (POA) estimation algorithm for steep terrain by applying physical scattering characteristics of natural surface. The algorithm extends the range of POA from [-45°, 45°] to (-90°,90°] and achieves consistent estimation with the widely used circular polarization algorithm (CPA) and minimum cross-polarization algorithm (MXPA) over flat area, but avoids the POA wrapping caused by the restriction of CPA and MXPA over precipitous area. Besides, detailed comparison among these deorientation algorithms is presented for clarifying their relations and analyzing the wrapping and unwrapping issue of POA. Both simulated data of Bragg scattering model and PolSAR data of L-band ALOS PALSAR and P-band NASA/JPL AIRSAR are used to substantiate the proposals.","['Radar imaging', 'Estimation', 'Polarization', 'Wrapping', 'Geometry']","['Circular polarization algorithm (CPA)', 'polarimetric synthetic aperture radar (PolSAR)', 'polarization orientation angle (POA)', 'range extension', 'steep terrain']"
"The usage of unmanned aerial vehicles (UAVs)-based ground penetrating radar (GPR) systems has gained interest over the last years thanks to advantages over ground-based systems, such as contactless inspection and capability, to reach difficult-to-access areas. The former is of paramount importance concerning the detection of buried threats, such as improvised explosive devices (IEDs) and landmines. Current state-of-the-art UAV-based GPR systems are able to provide centimeter-level resolution thanks to the use of GPR-synthetic aperture radar (SAR) processing techniques. One of the challenges to keep improving these systems is the scanning throughput, that is, the area that can be scanned in a given time. This contribution presents an array-based GPR-SAR system for subsurface imaging, aiming at maximizing the scanning throughput without jeopardizing the imaging capabilities of the system. First, the antenna array is mounted on a portable setup to evaluate its performance and imaging capabilities. Next, the antenna array is integrated into the UAV platform, and the UAV-based GPR-SAR system with the array is tested in realistic scenarios with different kinds of buried targets. Results show that the scanning throughput is significantly improved and, furthermore, the coherent combination of all transmitting–receiving channels of the array provides enhanced detection capabilities.","['Antenna arrays', 'Radar antennas', 'Landmine detection', 'Radar', 'Throughput', 'Three-dimensional displays', 'Radar imaging']","['Antenna array', 'ground penetrating radar (GPR)', 'improvised explosive devices (IEDs)', 'landmines', 'sampling techniques', 'synthetic aperture radar (SAR)', 'unmanned aerial vehicles (UAVs)']"
"Attenuated backscatter measurements from a Vaisala CL31 ceilometer and a modified form of the well-known slope method are used to derive the ceilometer extinction profiles during rain events, restricted to rainfall rates (RRs) below approximately 10 mm/h. RR estimates from collocated S-band radar and portable disdrometer are used to derive the RR-to-extinction correlation models for the ceilometer-radar and ceilometer-disdrometer combinations. Data were collected during an intensive observation period of the Verification of the Origins of Rotation in Tornadoes Experiment Southeast (VORTEX-SE) conducted in northern Alabama. These models are used to estimate the RR from the ceilometer observations in similar situations that do not have collocated radar or the disdrometer. Such correlation models are, however, limited by the different temporal and spatial resolutions of the measured variables, measurement capabilities of the instruments, and the inherent assumption of a homogeneous atmosphere. An empirical method based on extinction and RR uncertainty scoring and covariance fitting are proposed to solve, in part, these limitations.","['Rain', 'Laser radar', 'Atmospheric measurements', 'Instruments', 'Optical variables measurement', 'Optical sensors']","['Atmospheric observation', 'geophysics', 'laser radar', 'lidar', 'meteorological radar', 'rainfall rate (RR)']"
"Synthetic aperture radar (SAR) missions with short repeat times enable opportunities for near real-time deformation monitoring. Traditional multitemporal interferometric SAR (MT-InSAR) is able to monitor long-term and periodic deformation with high precision by time-series analysis. However, as time series lengthen, it is time-consuming to update the current results by reprocessing the whole dataset. Additionally, the number of coherent scatterers varies over time due to disappearing and emerging scatterers due to inevitable changes in surface scattering, and potential deformation anomalies require changes in the prevailing deformation model. Here, we propose a novel method to analyze InSAR time series recursively and detect both significant changes in scattering as well as deformation anomalies based on the new acquisitions. Sequential change detection is developed to identify temporary coherent scatterers (TCSs) using amplitude time series. Based on the predicted phase residuals, scatterers with abnormal deformation displacements are identified by a generalized ratio test, while the parameters of stable scatterers are updated using Kalman filtering. The quality of the anomaly detection is assessed based on the detectability power and the minimum detectable deformation. This facilitates (near) real-time data processing and decreases the false alarm likelihood. Experimental results show that the technique can be used for the real-time evaluation of deformation risks.","['Strain', 'Time series analysis', 'Synthetic aperture radar', 'Deformable models', 'Decorrelation', 'Real-time systems', 'Coherence']","['Anomaly detection', 'change detection', 'multitemporal InSAR', 'recursive process']"
"In this study, snow slab data collected from the Arctic Snow Microstructure Experiment were used in conjunction with a six-directional flux coefficient model to calculate individual slab absorption and scattering coefficients. These coefficients formed the basis for a new semiempirical extinction coefficient model, using both frequency and optical diameter as input parameters, along with the complex dielectric constant of snow. Radiometric observations, at 18.7, 21.0, and 36.5 GHz at both horizontal polarization (H-Pol) and vertical polarization (V-Pol), and snowpit data collected as part of the Sodankylä Radiometer Experiment were used to compare and contrast the simulated brightness temperatures produced by the multi-layer Helsinki University of Technology snow emission model, utilizing both the original empirical model and the new semiempirical extinction coefficient model described here. The results show that the V-Pol RMSE and bias values decreased when using the semiempirical extinction coefficient; however, the H-Pol RMSE and bias values increased on two of the lower microwave bands tested. The unbiased RMSE was shown to decrease across all frequencies and polarizations when using the semiempirical extinction coefficient.","['Snow', 'Slabs', 'Radiometry', 'Grain size', 'Microstructure', 'Data models', 'Scattering']","['Extinction coefficient modeling', 'HUT snow emission model', 'microwave scattering', 'remote sensing', 'snow emission model']"
"Change detection is a long-standing and challenging problem in remote sensing. Very often, features about changes are difficult to model beforehand, thus making the collection of changed samples a challenging task. In comparison, it is much easier to collect numerous no-change samples. It is possible to define a change detection approach using only easily available annotated no-change samples, which we henceforth call one-class change detection. Autoencoder networks being trained on no-change data are natural candidates for addressing this task due to their superior performance when compared with other one-class classification models. However, the autoencoders usually suffer from the problem of overgeneralization, i.e., they tend to generalize too well, thus risking properly reconstructing changed samples. In this article, we propose a novel data-enclosing-ball minimizing autoencoder (DebM-AE) that is trained with dual objectives—a reconstruction error criterion and a minimum volume criterion. The network learns a compact latent space, where encodings of no-change samples have low intraclass variance, which as counterpart has the identification of changed instances. We conducted extensive experiments on three real-world datasets. Results demonstrate advantages of the proposed method over other competitors. We make our data and code publicly available ( https://gitlab.lrz.de/ai4eo/reasoning/DebM-AE; https://github.com/lcmou/DebM-AE ).","['Task analysis', 'Feature extraction', 'Training', 'Remote sensing', 'Earth', 'Support vector machines', 'Image reconstruction']","['Autoencoder network', 'change detection', 'one-class classification', 'remote sensing']"
"Several techniques have been proposed to observe long-term land surface deformation using phase information of synthetic aperture radar (SAR), for example, interferometric SAR (InSAR), differential interferometric SAR (DInSAR), permanent scatterer interferometric SAR (PS-InSAR), and small baseline subset (SBAS) using images of JERS-1, ALOS-1/2, ENVISAT, ERS-1/2, RADARSAT-1/2, TerraSAR-X, SENTINEL-1A/B, and so on. These methods have also been employed to investigate land deformation and the impact caused by the hot mudflow accident at the study area of regency of Sidoarjo, East Java province, Indonesia, where this disaster happened on May 29, 2006, and has been flowing until now. But these methods only derived the land surface deformation and did not figure geological situation in the Earth, especially the current of geological material outflow. Therefore, this research proposed the continuity equation of the law of conservation of material to estimate the current of material outflow to investigate the impact of the land surface disaster in the study area. The information of vertical deformation or subsidence is derived by the consecutive DInSAR using multispaceborne SAR and substituted into the proposed equation to estimate the volume and current of hot mud outflow. The differential global positioning system (GPS) data collected since 2006 were employed to validate the analysis result of land surface deformation, and the root means square (rms) error is 0.46 m. The result obtained the current at the center of the study area at the beginning of the mudflow period was 6800 m 3 /day and decreased from 2018 until now that matched well with the local reports. The proposed equation could also be applied for observation of land deformation, volcano, fault activity, underground water, and so on, using remote sensing technologies.","['Strain', 'Synthetic aperture radar', 'Mathematical models', 'Satellites', 'Interferometry', 'Land surface', 'Geology']","['Consecutive', 'continuity equation', 'differential interferometric synthetic aperture radar (DInSAR)', 'law of conservation of material', 'synthetic aperture radar (SAR)']"
"Recently, remote sensing image super-resolution (RSISR) has drawn considerable attention and made great breakthroughs based on convolutional neural networks (CNNs). Due to the scale and richness of texture and structural information frequently recurring inside the same remote sensing images (RSIs) but varying greatly with different RSIs, state-of-the-art CNN-based methods have begun to explore the multiscale global features in RSIs by using attention mechanisms. However, they are still insufficient to explore significant content attention clues in RSIs. In this article, we present a new hybrid attention-based U-shaped network (HAUNet) for RSISR to effectively explore the multiscale features and enhance the global feature representation by hybrid convolution-based attention. It contains two kinds of convolutional attention-based single-scale feature extraction modules (SEM) to explore the global spatial context information and abstract content information, and a cross-scale interaction module (CIM) as the skip connection between different scale feature outputs of encoders to bridge the semantic and resolution gaps between them. Considering the existence of equipment with poor hardware facilities, we further design a lighter HAUNet-S with about 596K parameters. Experimental attribution analysis method LAM results demonstrate that our HAUNet is a more efficient way to capture meaningful content information and quantitative results can show that our HAUNet can significantly improve the performance of RSISR on four remote sensing test datasets. Meanwhile, HAUNET-S also maintains competitive performance. Our code is available at https://github.com/likakakaka/HAUNet_RSISR .","['Feature extraction', 'Remote sensing', 'Superresolution', 'Transformers', 'Image reconstruction', 'Task analysis', 'Semantics']","['Convolutional network', 'multiscale features', 'remote sensing image super-resolution (RSISR)']"
"Deep learning models, such as convolutional neural networks (CNNs), have made significant progress in hyperspectral image (HSI) classification. However, these models require a large number of parameters, which occupy a lot of storage space and suffer from overfitting, thus resulting in performance loss. To solve the above problems, in this article, we propose a new compression network [namely, a Hybrid Fully Connected Tensorized Compression Network (HybridFCTCN)] by considering the high dimensionality of HSI data. First, using the low-rank fully connected tensor network decomposition (FCTND), three novel units, i.e., FCTN-FC, FCTNConv2D, and FCTNConv3D, are designed to compress the weight tensor of standard fully connected (FC) layer and kernel tensor of convolutional layer, reducing their parameters. In the novel units, the intrinsic correlation of the decomposed factors is adequately exploited by the FC structures, which enhances their feature extraction and classification abilities. Then, benefiting from the hybrid network backbone composed of the FCTNConv3D and FCTNConv2D units, HybridFCTCN can extract more discriminative features with fewer parameters, while it has great generalization capability and robustness, enabling better HSI classification. Finally, the rank of above-designed units is defined, and its determination is discussed to facilitate the application of the proposed model. Extensive experiments on three widely used HSI datasets reveal that the proposed model achieves state-of-the-art classification performance for different training sample sizes with a very small number of parameters.","['Tensors', 'Convolution', 'Feature extraction', 'Kernel', 'Standards', 'Hyperspectral imaging', 'Correlation']","['Convolutional neural networks (CNNs)', 'fully connected tensor network decomposition (FCTND)', 'hyperspectral image (HSI) classification', 'network compression']"
"The Concordiasi campaign aimed to improve satellite data assimilation at high latitudes and, particularly, the assimilation of the Infrared Atmospheric Sounding Interferometer (IASI) radiances over Antarctica. This study focuses on the IASI data retrieval using a 1-D variational data assimilation system, which was carried out at the Concordia station and within the framework of Concordiasi. The study period lasted from November 20 to December 12, 2009. Radiosonde measurements are utilized to validate temperature and water vapor retrieved profiles. Baseline Surface Radiation Network data and manned measurements in Concordia are used to verify skin temperature retrievals and derive information about cloudy conditions. This study assesses the impact of several parameters on the retrieved profile quality. In particular, the background error specification is crucial. The background error covariance matrix is optimally tuned to provide the best possible retrievals, modifying the shape of these covariances for stratospheric temperatures, computing and maximizing the degree of freedom for signal (DFS). The DFS characterizes how the assimilation system uses the observation to pull the signal from the background. For the study period, the humidity and temperature retrieved profiles are optimally improved compared with background profiles, with the largest reduction in error for the skin temperature.","['Humidity', 'Temperature measurement', 'Atmospheric measurements', 'Atmospheric modeling', 'Satellites', 'Satellite broadcasting', 'Antarctica']","['Antarctica', 'Concordiasi', 'Infrared Atmospheric Sounding Interferometer (IASI)', 'retrieval', 'skin temperature', '1-D variational (1D-Var)']"
"In this paper, we present a new technique for automated detection of ice and open water from sequential RADARSAT-2 ScanSAR dual-polarization HH-HV images. The technique is based on combining a previously developed approach to ice and water detection applied to single synthetic aperture radar (SAR) images with the ice motion information derived from sequential SAR images. To evaluate the new approach, it was applied to 736 SAR image pairs acquired in 2013. Compared with the previous approach, the new approach produced an increase in the fraction of correctly classified water samples from 57.7% to 72.6% while the fraction of correctly classified ice samples did not change appreciably. The overall accuracy stayed at a high level exceeding 99.8%, when compared against the Canadian Ice Service Image Analysis pure ice and water samples. Verification results for different regions and months showed that the detection accuracy exceeds 99.5% for the most regions and months. The proposed approach can also assign enhanced quality to ice and water retrievals found in the reference image. The results are particularly relevant in light of the upcoming Canadian RADARSAT Constellation Mission which will significantly increase the amount and frequency of SAR observations over the Arctic region.","['Ice', 'Synthetic aperture radar', 'Wind speed', 'Probability', 'Image analysis', 'Data assimilation', 'Arctic']","['Ice motion', 'ice probability', 'RADARSAT-2', 'Regional Ice-Ocean Prediction System (RIOPS)', 'sequential images', 'synthetic aperture radar (SAR)']"
"Background reconstruction is a key step of moving object detection in satellite videos. Most existing model-based methods exploit low-rank prior to recover background, which has achieved good performance but suffered degradation under complex and dynamic scenes. In this article, we introduce a deep background prior into model-based methods for moving vehicle detection in satellite videos. Our deep background prior is obtained by a background reconstruction network, which can learn to reconstruct the background from consecutive frames. By applying our deep background prior into model-based methods, a closed-form solution can be obtained via the alternating direction method of multipliers (ADMM), and then, detection results can be acquired through iterative optimization. More importantly, our background reconstruction network can be trained in an unsupervised way by introducing specifically designed loss, thus relieving the dependence on large-scale labeled datasets. Extensive experimental results demonstrate the efficiency and effectiveness of the proposed method.","['Videos', 'Satellites', 'Object detection', 'Image reconstruction', 'Iterative methods', 'Vehicle detection', 'Tensors']","['Background reconstruction', 'iterative optimization', 'moving object detection (MOD)', 'satellite videos', 'unsupervised learning']"
"The NOAA Wide Swath Radar Altimeter (WSRA) and its processing are described. The WSRA provides real-time measurements of sea surface significant wave height and directional wave spectra during flights in hurricanes and other environments. The characteristics of near nadir scattering from the sea surface and the resulting distortion of the wave topography measured by the WSRA are discussed, as well as the simulation which generated a matrix to correct the directional wave spectra produced from the WSRA wave topography.","['Sea surface', 'Surface topography', 'Surface waves', 'Hurricanes', 'Aircraft', 'Radar', 'Ocean waves']","['Airborne radar remote sensing', 'digital beamforming technique', 'directional ocean wave spectra', 'radar altimetry', 'sea surface electromagnetic scattering', 'sea surface measurements']"
"We address fast image stitching for large image collections while being robust to drift due to chaining transformations and minimal overlap between images. We focus on scientific applications where ground-truth accuracy is far more important than visual appearance or projection error, which can be misleading. For common large-scale image stitching use cases, transformations between images are often restricted to similarity or translation. When homography is used in these cases, the odds of being trapped in a poor local minimum and producing unnatural results increases. Thus, for transformations up to affine, we cast stitching as minimizing reprojection error globally using linear least-squares with a few, simple constraints. For homography, we observe that the global affine solution provides better initialization for bundle adjustment compared to an alternative that initializes with a homography-based scaffolding and at lower computational cost. We evaluate our methods on a very large translation dataset with limited overlap as well as four drone datasets. We show that our approach is better compared to alternative methods such as MGRAPH in terms of computational cost, scaling to large numbers of images, and robustness to drift. We also contribute ground-truth datasets for this endeavor.","['Bundle adjustment', 'Image stitching', 'Drones', 'Optimization', 'Cameras', 'Global Positioning System', 'Feature extraction']","['Bundle adjustment', 'image geocorrection', 'image stitching', 'linear least squares', 'remote sensing']"
"This article introduces the experiment design for Moon imaging based on Sanya incoherent scatter radar (SYISR) and algorithm research in data processing. The peak power of SYISR is 2 MW. The transmitted frequency used for Moon imaging experiments is 430 MHz. We conducted Moon imaging experiments using two types of waveforms, 13-bit Barker code, and linear frequency modulation (LFM) chirp. Considering both resolution and signal-to-noise ratio (SNR), the use of an LFM chirp with a bandwidth of 0.3 MHz and a pulsewidth of 2 ms can give higher SNR and resolution for Moon imaging using SYISR. Several key techniques were applied in the experiment design and data processing: 1) for the reliability of the imaging algorithm, the range-Doppler imaging algorithm commonly used in synthetic aperture imaging was applied; 2) to avoid the sidelobe effect of the 13-bit Barker code matched filter, a sidelobe-free filter was used; and 3) to mitigate the problem of “north–south ambiguity,” mosaic imaging of the Doppler northern and southern hemispheres of the nearside of the Moon was adopted. Two types of imaging results are obtained: mosaic images of the northern and southern hemispheres of the Moon and local regional images. The results demonstrate the feasibility and reliability of Moon imaging based on SYISR, which enables potential further lunar geology investigations in the future.","['Moon', 'Radar', 'Radar imaging', 'Imaging', 'Surface waves', 'Observatories', 'Signal resolution']","['Moon', 'north–south ambiguity', 'range-Doppler (RD) map', 'Sanya incoherent scatter radar (SYISR)']"
"Quantum algorithms are designed to process quantum data (quantum bits) in a gate-based quantum computer. They are proven rigorously that they reveal quantum advantages over conventional algorithms when their inputs are certain quantum data or some classical data mapped to quantum data. However, in a practical domain, data are classical in nature, and they are very big in dimension, size, and so on. Hence, there is a challenge to map (embed) classical data to quantum data, and even no quantum advantages of quantum algorithms are demonstrated over conventional ones when one processes the mapped classical data in a gate-based quantum computer. For the practical domain of earth observation (EO), due to the different sensors on remote-sensing platforms, we can map directly some types of EO data to quantum data. In particular, we have polarimetric synthetic aperture radar (PolSAR) images characterized by polarized beams. A polarized state of the polarized beam and a quantum bit are the Doppelganger of a physical state. We map them to each other, and we name this direct mapping a natural embedding , otherwise an artificial embedding . Furthermore, we process our naturally embedded data in a gate-based quantum computer by using a quantum algorithm regardless of its quantum advantages over conventional techniques; namely, we use the QML network as a quantum algorithm to prove that we naturally embedded our data in input qubits of a gate-based quantum computer. Therefore, we employed and directly processed PolSAR images in a QML network. Furthermore, we designed and provided a QML network with an additional layer of a neural network, namely, a hybrid quantum-classical network, and demonstrate how to program (via optimization and backpropagation) this hybrid quantum-classical network when employing and processing PolSAR images. In this work, we used a gate-based quantum computer offered by an IBM Quantum and a classical simulator for a gate-based quantum computer. Our contribution is that we provided very specific EO data with a natural embedding feature, the Doppelganger of quantum bits, and processed them in a hybrid quantum-classical network. More importantly, in the future, these PolSAR data can be processed by future quantum algorithms and future quantum computing platforms to obtain (or demonstrate) some quantum advantages over conventional techniques for EO problems.","['Qubit', 'Stokes parameters', 'Scattering', 'Logic gates', 'Quantum algorithm', 'Water', 'Radar imaging']","['Natural embedding', 'parameterized quantum circuit', 'polarimetric synthetic aperture radar (PolSAR)', 'quantum machine learning (QML)']"
"Compact and low-cost radar transponders are an attractive alternative to corner reflectors (CRs) for interferometric synthetic aperture radar (InSAR) deformation monitoring, datum connection, and geodetic data integration. Recently, such transponders have become commercially available for C-band sensors, which poses relevant questions on their characteristics in terms of radiometric, geometric, and phase stability. Especially for extended time series and for high-precision geodetic applications, the impact of secular or seasonal effects, such as variations in temperature and humidity, has yet to be proven. In this article, we address these challenges using a multitude of short baseline experiments with four transponders and six CRs deployed at test sites in The Netherlands and Slovakia. Combined together, we analyzed 980 transponder measurements in Sentinel-1 time series to a maximum extent of 21 months. We find an average radar cross section (RCS) of over 42 dBm 2 within a range of up to 15° of elevation misalignment, which is comparable to a triangular trihedral CR with a leg length of 2.0 m. Its RCS shows the temporal variations of 0.3–0.7 dBm 2 (standard deviation), which is partially correlated with surface temperature changes. The precision of the InSAR phase double differences over short baselines between a transponder and a stable reference CRs is found to be 0.5–1.2 mm (one sigma). We observe a correlation with surface temperature, leading to seasonal variations of up to ±3 mm, which should be modeled and corrected for in high-precision InSAR applications. For precise SAR positioning, we observe antenna-specific constant internal electronic delays of 1.2–2.1 m in slant range, i.e., within the range resolution of the Sentinel-1 interferometric wide (IW) product, with a temporal variability of less than 20 cm. Comparing similar transponders from the same series, we observe distinct differences in performance. Our main conclusion is that these characteristics are favorable for a wide range of geodetic applications. For particular demanding applications, individual calibration of single devices is strongly recommended.","['Transponders', 'Synthetic aperture radar', 'Radar', 'Time series analysis', 'Radar cross-sections', 'C-band', 'Antennas']","['C-band', 'compact radar transponders', 'corner reflector (CR)', 'radar-cross-section (RCS)', 'SAR positioning', 'synthetic aperture radar (SAR)', 'synthetic aperture radar interferometry (InSAR)', 'transponders']"
"Airborne synthetic aperture radar (SAR) data focusing in the presence of highly variable squint angles is addressed in this work. To do this, a time-domain SAR focusing algorithm has been developed to account for the target-dependent nature of the acquisition squint angle. For comparison purposes, we have considered also a SAR processor based on the use of an azimuth-invariant processing squint angle. Moreover, real data acquired by two different X-band airborne SAR systems have been analyzed. The aims of the work are twofold. First, we show that the adoption of an azimuth-invariant processing squint angle, typically pursued by computationally efficient focusing strategies, may become inappropriate in several airborne missions, which are typically corrupted by motion errors induced by atmospheric turbulence. On the contrary, these problems are circumvented through the developed time-domain focusing strategy, which exploits a target-dependent processing squint angle that is coincident with the acquisition one. Second, we show that the proposed focusing strategy must be properly revised in the case of interferometric processing. In particular, we show that if the (target-dependent) acquisition squint angle is different for the two interferometric channels used, we have to move toward a different focusing paradigm, say, an interferometric focusing solution. This is again based on considering a target-dependent processing squint angle, but for each target, this processing angle should be equal neither to the acquisition squint angle of the “main” acquisition nor to that of the “secondary” one: it must instead lie halfway between them. We prove that this approach is appropriate to obtain high interferometric coherence, and it is suboptimal to separately focus the interferometric channels.","['Focusing', 'Synthetic aperture radar', 'Radar tracking', 'Radar', 'Time-domain analysis', 'Radar antennas', 'Azimuth']","['Airborne synthetic aperture radar (SAR)', 'back-projection algorithm', 'squint']"
"We explore a novel paradigm for light detection and ranging (LIDAR) point classification in mobile laser scanning (MLS). In contrast to the traditional scheme of performing classification for a 3-D point cloud after registration, our algorithm operates on the raw data stream classifying the points on-the-fly before registration. Hence, we call it preregistration classification (PRC). Specifically, this technique is based on spatial correlations, i.e., local range measurements supporting each other. The proposed method is general since exact scanner pose information is not required, nor is any radiometric calibration needed. Also, we show that the method can be applied in different environments by adjusting two control parameters, without the results being overly sensitive to this adjustment. As results, we present classification of points from an urban environment where noise, ground, buildings, and vegetation are distinguished from each other, and points from the forest where tree stems and ground are classified from the other points. As computations are efficient and done with a minimal cache, the proposed methods enable new on-chip deployable algorithmic solutions. Broader benefits from the spatial correlations and the computational efficiency of the PRC scheme are likely to be gained in several online and offline applications. These range from single robotic platform operations including simultaneous localization and mapping (SLAM) algorithms to wall-clock time savings in geoinformation industry. Finally, PRC is especially attractive for continuous-beam and solid-state LIDARs that are prone to output noisy data.","['Laser radar', 'Three-dimensional displays', 'Correlation', 'Simultaneous localization and mapping', 'Vegetation', 'Forestry', 'Automobiles']","['Classification algorithms', 'laser radar', 'machine vision', 'remote sensing', 'simultaneous localization and mapping']"
"The wide field of view (WFV) imaging system of the GaoFen-6 (GF-6) satellite processes four popular spectral bands [blue, green, red, and near-infrared (NIR)] and four new spectral bands (costal, yellow, and two red edges). However, the corresponding reference spectral bands for these eight spectral bands from one reference sensor are insufficient in the cross-calibration process of the WFV; therefore, current cross-calibration methods should be improved. To address this problem, the Moderate-Resolution Imaging Spectroradiometer (MODIS), which has high radiometric performance with the aid of an onboard calibration system, is used as a reference sensor, and cross-calibration methods using the top of atmosphere (TOA) and the bottom of atmosphere (BOA) bidirectional reflectance distribution function (BRDF) models are developed and compared. The results reveal that the cross-calibration results in eight spectral bands with the BOA BRDF model that can obtain higher consistency with the official calibration coefficients (OCCs) compared with the TOA BRDF model. In addition, after various influencing factors are comprehensively analyzed, the spectral band adjustment factor (SBAF) correction in the shortwave infrared (SWIR) spectral bands can be neglected; moreover, by using five spectral bands (blue, green, red, NIR, and SWIR) of the MODIS sensor, the BOA BRDF model and the cubic polynomial interpolation method provide optimal cross-calibration schemes for WFV sensor. The total radiometric cross-calibration uncertainties of the proposed cross-calibration methods with the BOA BRDF model and the TOA BRDF model are less than 5.73% and 6.32%, respectively.","['Radiometry', 'MODIS', 'Calibration', 'Satellite broadcasting', 'Atmospheric modeling', 'Geometry', 'Earth']","['Bidirectional reflectance distribution function (BRDF) model', 'cross-calibration', 'GaoFen-6 (GF-6)', 'Moderate-Resolution Imaging Spectroradiometer (MODIS)', 'spectral interpolation']"
"Automated solutions for sea ice-type classification from synthetic aperture radar (SAR) imagery offer an opportunity to monitor sea ice, unimpeded by cloud cover or the arctic night. However, there is a common struggle to obtain accurate classifications year round, particularly in the melt and freeze-up seasons. During these seasons, the radar backscatter signal is affected by wet snow cover, obscuring information about underlying ice types. By using additional spatiotemporal contextual data and a combination of convolutional neural networks and a dense conditional random field, we can mitigate these problems and obtain a single classifier that is able to classify accurately at 3.5-m spatial resolution for five different classes of sea ice surface from October to May. During the near year-long drift of the Multidisciplinary Drifting Observatory for the Study of the Arctic Climate (MOSAiC) expedition, we collected satellite scenes of the same patch of Arctic pack ice with X-band SAR with a revisit time of less than a day on average. Combined with in situ observations of the local ice properties, this offers up the unprecedented opportunity to perform a detailed and quantitative assessment of the robustness of our classifier for level, deformed, and heavily deformed ice. For these three classes, we can perform accurate classification with a probability >95% and calculate a lower bound for the robustness between 85% and 88%.","['Ice', 'Synthetic aperture radar', 'Convolutional neural networks', 'Robustness', 'Image resolution', 'Backscatter', 'Training']","['Machine learning', 'robustness', 'sea ice', 'synthetic aperture radar (SAR)']"
"This article presents a novel Bayesian approach for hyperspectral image unmixing. The observed pixels are modeled by a linear combination of material signatures weighted by their corresponding abundances. A spike-and-slab abundance prior is adopted to promote sparse mixtures and an Ising prior model is used to capture spatial correlation of the mixture support across pixels. We approximate the posterior distribution of the abundances using the expectation-propagation (EP) method. We show that it can significantly reduce the computational complexity of the unmixing stage and meanwhile provide uncertainty measures, compared to expensive Monte Carlo strategies traditionally considered for uncertainty quantification. Moreover, many variational parameters within each EP factor can be updated in a parallel manner, which enables mapping of efficient algorithmic architectures based on graphics processing units (GPUs). Under the same approximate Bayesian framework, we then extend the proposed algorithm to semi-supervised unmixing, whereby the abundances are viewed as latent variables and the expectation-maximization (EM) algorithm is used to refine the endmember matrix. Experimental results on synthetic data and real hyperspectral data illustrate the benefits of the proposed framework over state-of-art linear unmixing methods.","['Bayes methods', 'Hyperspectral imaging', 'Graphics processing units', 'Correlation', 'Computational modeling', 'Estimation', 'Approximation algorithms']","['Compute Unified Device Architecture (CUDA)', 'expectation-maximization (EM)', 'expectation-propagation (EP)', 'graphics processing unit (GPU) programming', 'spectral unmixing (SU)']"
"We employ a previously developed statistical method to evaluate the performance of the Sentinel-3 Ocean and Land Colour Instrument (OLCI) global ocean color data relying on the temporal stability of the retrievals. We analyze the normalized water-leaving reflectanceρwN(λ)spectra generated by the National Oceanic and Atmospheric Administration (NOAA) Multi-Sensor Level-1 to Level-2 (MSL12) ocean color data processing system from the OLCI measurements and the European Organization for the Exploitation of Meteorological Satellites (EUMETSAT)-Instrument Processing Facility for OLCI Level-2 (IPF-OL-2) OLCI reflectanceρwN(λ)spectra. The deviations inρwN(λ)spectra from temporally and spatially averaged baseline data are statistically evaluated corresponding to various parameters, including the solar-sensor geometry, various ancillary data (i.e., surface wind speed, sea-level atmospheric pressure, water vapor amount, and ozone concentration), and other related parameters. Our results show that, under most conditions, both NOAA-MSL12 and EUMETSAT-IPF-OL-2 data processing systems produce statistically consistent ocean color products in the open ocean with respect to all corresponding parameters analyzed but with some underestimates ofρwN(λ)spectra by EUMETSAT retrievals in moderate sun glint conditions being the notable exception.","['Oceans', 'Image color analysis', 'Satellite broadcasting', 'Sensors', 'Reflectivity', 'Sea surface', 'Sea measurements']","['European Organization for the Exploitation of Meteorological Satellites (EUMETSAT)', 'European Space Agency (ESA)', 'Multi-Sensor Level-1 to Level-2 (MSL12)', 'National Oceanic and Atmospheric Administration (NOAA)', 'Ocean and Land Colour Instrument (OLCI)', 'ocean color remote sensing', 'Visible Infrared Imaging Radiometer Suite (VIIRS)']"
"Standard noise radars, as well as noise-type radars such as quantum two-mode squeezing (QTMS) radar, are characterized by a covariance matrix with a very specific structure. This matrix has four independent parameters: the amplitude of the received signal, the amplitude of the internal signal used for matched filtering, the correlation between the two signals, and the relative phase between them. In this article, we derive estimators for these four parameters using two techniques. The first is based on minimizing the Frobenius norm between the structured covariance matrix and the sample covariance matrix; the second is maximum likelihood (ML) parameter estimation. The two techniques yield the same estimators. We then give probability density functions (pdf’s) for all four estimators. Because some of these pdf’s are quite complicated, we also provide approximate pdf’s. Finally, we apply our results to the problem of target detection and derive expressions for the receiver operating characteristic (ROC) curves of two different noise radar detectors. In summary, our work gives a broad overview of the basic statistical behavior of noise-type radars.","['Radar', 'Covariance matrices', 'Radar detection', 'Probability density function', 'Quantum radar', 'Maximum likelihood estimation', 'Correlation coefficient']","['Covariance matrix', 'noise radar', 'parameter estimation', 'quantum radar', 'quantum two-mode squeezing (QTMS) radar']"
"This article proposes a 3-D imaging approach for contactless ground penetrating radar surveys. The imaging problem is formulated in the linear inverse scattering context and solved by using the singular values decomposition tool. A ray-based model accounting for the electromagnetic (EM) signal propagation into an inhomogeneous medium is developed to accurately evaluate the kernel of the integral equation to be inverted. Under the proposed model, an analysis of the spatial resolution performance is carried out as a function of the geometrical and EM parameters of the scenario. To this end, theoretical concepts based on diffraction tomography and the singular value decomposition of the scattering operator are exploited. Reconstruction results based on full-wave simulated data assess the feasibility of the imaging approach.","['Imaging', 'Soil', 'Radar imaging', 'Permittivity', 'Image reconstruction', 'Electromagnetics', 'Tomography']","['Contactless ground penetrating radar (GPR)', 'linear inverse scattering', 'ray-based propagation', 'resolution analysis']"
"Due to the rapidly growing remote-sensing (RS) image archives, images are usually stored in a compressed format for reducing their storage sizes. Thus, most of the existing content-based RS image retrieval systems require fully decoding images (i.e., decompression) that is computationally demanding for large-scale archives. To address this issue, we introduce a novel approach devoted to simultaneous RS image compression and indexing for scalable content-based image retrieval (denoted as SCI-CBIR). The proposed SCI-CBIR prevents the requirement of decoding RS images before image search and retrieval. To this end, it includes two main steps: 1) deep-learning-based compression and 2) deep-hashing-based indexing. The first step effectively compresses RS images by employing a pair of deep encoder and decoder neural networks and an entropy model. The second step produces hash codes with a high discrimination capability for RS images by employing pairwise, bit-balancing, and classification loss functions. For the training of the SCI-CBIR approach, we also introduce a novel multistage learning procedure with automatic loss weighting techniques to characterize RS image representations that are appropriate for both RS image indexing and compression. The proposed learning procedure enables automatically weighting different loss functions considered for the proposed approach instead of computationally demanding grid search. Experimental results show the effectiveness of the proposed approach when compared to widely used approaches in RS. The code of the proposed approach is available at https://git.tu-berlin.de/rsim/SCI-CBIR .","['Image coding', 'Codes', 'Indexing', 'Training', 'Decoding', 'Transform coding', 'Image retrieval']","['Deep-learning-based compression', 'hashing-based indexing', 'image retrieval', 'remote sensing (RS)']"
"Change detection (CD) by comparing two bitemporal images is a crucial task in remote sensing. With the advantages of requiring no cumbersome labeled change information, unsupervised CD has attracted extensive attention in the community. However, existing unsupervised CD approaches rarely consider the seasonal and style differences incurred by the illumination and atmospheric conditions in multitemporal images. To this end, we propose a CD with a domain shift setting for remote sensing images. Furthermore, we present a novel unsupervised CD method using a lightweight transformer, called UCDFormer. Specifically, a transformer-driven image translation composed of a lightweight transformer and a domain-specific affinity weight is first proposed to mitigate domain shift between two images with real-time efficiency. After image translation, we can generate the difference map between the translated before-event image and the original after-event image. Then, a novel reliable pixel extraction module is proposed to select significantly changed/unchanged pixel positions by fusing the pseudochange maps of fuzzy c-means clustering and adaptive threshold. Finally, a binary change map is obtained based on these selected pixel pairs and a binary classifier. Experimental results on different unsupervised CD tasks with seasonal and style changes demonstrate the effectiveness of the proposed UCDFormer. For example, compared with several other related methods, UCDFormer improves performance on the Kappa coefficient by more than 12%. In addition, UCDFormer achieves excellent performance for earthquake-induced landslide detection when considering large-scale applications. The code is available at https://github.com/zhu-xlab/UCDFormer .","['Transformers', 'Remote sensing', 'Feature extraction', 'Reliability', 'Real-time systems', 'Sensors', 'Earth']","['Change detection (CD)', 'domain shift', 'transformer', 'UCDFormer', 'unsupervised learning']"
"Deep convolutional neural networks (CNNs) have achieved much success in remote sensing image change detection (CD) but still suffer from two main problems. First, the existing multiscale feature fusion methods often use redundant feature extraction and fusion strategies, which often lead to high computational costs and memory usage. Second, the regular attention mechanism in CD is difficult to model spatial–spectral features and generate 3-D attention weights at the same time, ignoring the cooperation between spatial features and spectral features. To address the above issues, an efficient ultralightweight spatial–spectral feature cooperation network (USSFC-Net) is proposed for CD in this article. The proposed USSFC-Net has two main advantages. First, a multiscale decoupled convolution (MSDConv) is designed, which is clearly different from the popular atrous spatial pyramid pooling (ASPP) module and its variants since it can flexibly capture the multiscale features of changed objects using cyclic multiscale convolution. Meanwhile, the design of MSDConv can greatly reduce the number of parameters and computational redundancy. Second, an efficient spatial–spectral feature cooperation (SSFC) strategy is introduced to obtain richer features. The SSFC differs from the existing 2-D attention mechanisms since it learns 3-D spatial–spectral attention weights without adding any parameters. The experiments on three datasets for remote sensing image CD demonstrate that the proposed USSFC-Net achieves better CD accuracy than most CNNs-based methods and requires lower computational costs and fewer parameters, even it is superior to some Transformer-based methods. The code is available at https://github.com/SUST-reynole/USSFC-Net .","['Feature extraction', 'Remote sensing', 'Convolution', 'Three-dimensional displays', 'Task analysis', 'Kernel', 'Correlation']","['Change detection (CD)', 'convolutional neural network (CNN)', 'multiscale feature extraction', 'spatial–spectral feature cooperation (SSFC)']"
"The San Francisco Estuary and Sacramento–San Joaquin River Delta (Bay Delta) is a highly sensitive and critical habitat for the Delta Smelt, an endangered endemic fish, with water temperature being a key determinant of habitat suitability. This study investigates the relationship between open water surface and subsurface conditions from spaceborne thermal measurements (ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) and Landsat-8) and in situ sensor data from the California Data Exchange Center (CDEC) to produce estimates of spatially continuous bulk temperature in the Bay Delta. We found that ECOSTRESS and Landsat-8 surface temperature measurements are well-correlated with bulk water temperatures (N=236andr=0.907andN=226andr=0.976, respectively). For the ECOSTRESS- in situ comparison, accounting for time of day improved the correlation between surface and subsurface conditions (r=0.946, 0.881, and 0.944 for morning, midday, and evening, respectively). We found that ECOSTRESS surface temperatures were warmer than bulk temperatures in the midday period (2 °C peak at 2 P.M.) and cooler in the morning and evening periods (−1°C peak at 6 A.M.). We also found that a simple harmonic regression model can capture the diurnal variability of the skin effect to predict bulk water temperature (root-mean-square error (RMSE) = 0.809°C). With ECOSTRESS, we found that across the Bay Delta, including open waters and pelagic bays, temperature conditions causing stress and mortality for the Delta Smelt were persistent throughout the day during summer months. ECOSTRESS is a unique dataset capable of informing conservation efforts in the Bay Delta.","['Temperature measurement', 'Temperature sensors', 'Remote sensing', 'Temperature distribution', 'Ocean temperature', 'Earth', 'Artificial satellites']","['Aquatic habitat', 'Delta Smelt', 'diurnal patterns', 'ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS)', 'Landsat-8', 'skin effect', 'water temperature']"
"This article experimentally investigates relationships between copol backscattering at a wide range of frequencies (L- to Ka-bands) and snow–ground state parameters (SPs) in different evolution phases during the full winter cycle of 2019/2020. Backscattering coefficients from 1 to 40 GHz, in situ snow–ground SPs, and meteorological data are measured at the Davos-Laret Remote Sensing Field Laboratory (Switzerland). Relative strengths of the snow–ground system’s three primary scattering elements (air–snow interface, snow volume, and snow–ground interface) on backscattering are assessed. An anticorrelation between reasonably high snow wetness and backscattering coefficient is found, especially at higher microwave frequencies. For small amounts of snow wetness, backscatter coefficients at L- and S-bands are intensified via increasing snow volume and snow surface scattering. Snow–ground SPs influence backscattering according to their characteristic time scales of temporal evolution. Under dry snow conditions and at low and intermediate frequencies, ground permittivity is the major influencer of backscatter at a time scale of roughly two weeks. Snowfall is the major influencer of backscatter at a time scale of a few hours to a few days. The findings of this article are valuable to the development of retrieval algorithms using machine learning while maintaining a grasp on the ongoing physical processes. Another key message is that multifrequency active microwave measurements are critical to maximize the number of retrievable SPs and their estimation accuracy. For example, while Ka-band performs well in the detection of snow cover, L-band measurements are more responsive to changes of snow water equivalent (SWE) under moist or wet snow conditions.","['Snow', 'Backscatter', 'Microwave measurement', 'Microwave radiometry', 'Remote sensing', 'L-band', 'Microwave theory and techniques']","['Alpine snow cover', 'backscattering', 'microwave remote sensing', 'snow water equivalent (SWE)']"
"Road surface extraction from high-resolution remote sensing images has many engineering applications; however, extracting regularized and smooth road surface maps that reach the human delineation level is a very challenging task, and substantial and time-consuming manual work is usually unavoidable. In this article, to solve this problem, we propose a novel regularized road surface extraction framework by introducing a graph neural network (GNN) for processing the road graph that is preconstructed from the easily accessible road centerlines. The proposed framework formulates the road surface extraction problem as two-sided width inference of the road graph and consists of a convolutional neural network (CNN)-based feature extractor and a GNN model for vertex attribute adjustment. The CNN extracts the high-level abstract features of each vertex in the graph as the input of the GNN and also the road boundary features that allow us to distinguish roads from the background. The GNN propagates and aggregates the features of the vertices in the graph to achieve global optimization of the regression of the regularized widths of the vertices. At the same time, a biased centerline map can also be corrected based on the width prediction result. To the best of the authors’ knowledge, this is the first study to have introduced a GNN to regularized human-level road surface extraction. The proposed method was evaluated on four diverse datasets, and the results show that the proposed method comprehensively outperforms the recent CNN-based segmentation methods and other regularization methods in the intersection over union (IoU) and smoothness score, and a visual check shows that a majority of the prediction results of the proposed method approach the human delineation level.","['Roads', 'Feature extraction', 'Remote sensing', 'Data mining', 'Task analysis', 'Convolutional neural networks', 'Satellites']","['Convolutional neural network (CNN)', 'graph neural network (GNN)', 'regularization', 'road extraction']"
"In this work, we present a new method based on the compressive sensing (CS) theory to correct phase unwrapping (PhU) errors in the multitemporal sequence of interferograms exploited by advanced differential interferometric synthetic aperture radar (DInSAR) techniques to generate deformation time series. The developed algorithm estimates the PhU errors by using a modifiedL1-norm estimator applied to the interferometric network built in the temporal/spatial baseline plane. Indeed, in order to search the minimumL1-norm sparse solution, we apply the iterative reweighted least-squares method with an improved weight function that takes account of the baseline characteristics of the interferometric pairs. Moreover, we also introduce a quality function to identify those solutions that have no physical meaning. Although the proposed approach can be applied to different multitemporal DInSAR approaches, our analysis is tailored to the full-resolution small baseline subset (SBAS) processing chain that we properly modify to implement the proposed CS-based algorithm. To assess the performance of the developed technique, we carry out an extended experimental analysis based on simulated and real SAR data. In particular, we process two wide SAR datasets acquired by Sentinel-1 and COSMO-SkyMed constellations over central Italy between 2011 and 2019. The achieved experimental results clearly demonstrate the effectiveness of the developed approach in retrieving PhU errors and generating displacement time series related to strongly nonlinear deformation phenomena. Indeed, the developed CS-based technique significantly increases the number of detected coherent points and improves the accuracy of the retrieved deformation time series.","['Synthetic aperture radar', 'Interferometry', 'Strain', 'Time series analysis', 'Compressed sensing', 'Signal processing algorithms', 'Urban areas']","['Compressive sensing (CS)', 'differential synthetic aperture radar interferometry (DInSAR)', 'phase unwrapping (PhU)', 'small baseline subset (SBAS)']"
"The synthetic aperture radar (SAR) imaging mode described in this article utilizes the available signal bandwidth to form a narrow frequency-scanning transmit antenna beam illuminating the swath of interest from far to near range. The imaging technique is named frequency scan for time-of-echo compression (f-STEC), because, for a proper choice of mode parameters, the radar echo duration is reduced, i.e., compressed. This article provides a detailed analysis of the f-STEC imaging technique and derives the operational and performance parameters as well as—to the authors’ knowledge—for the first time, analytic expressions for the f-STEC timing constraints. Furthermore, the performance and trade space are reported and compared to both conventional and modern, i.e., digital beamforming imaging modes. The f-STEC imaging technique is shown to be specifically advantageous for SAR systems operating at higher carrier frequencies or an attractive add-on for state-of-the-art SAR instruments.","['Time-frequency analysis', 'Imaging', 'Radar imaging', 'Synthetic aperture radar', 'Antenna radiation patterns', 'Directive antennas', 'Chirp']","['Frequency scaning radar', 'imaging radar', 'performance analysis', 'radar imaging techniques', 'spaceborne radar', 'synthetic aperture radar (SAR)']"
"A trihedral corner reflector has been used to evaluate the capability of passive reflectors to calibrate radar altimeters, such as the Poseidon-4 altimeter on board Sentinel- 6A. The reflector location, placed on the top of a mountain ridge and about 4-km off-nadir of the Sentinel- 6A subsatellite track, allows capturing echoed signals with a signal-to-clutter ratio (SCR) around 40 dB when processing the received data with a fully-focused synthetic aperture radar (FF-SAR) algorithm. Results obtained show a range bias of 33.9 with 8.5 mm of standard deviation and datation bias of−2.3μswith a standard deviation of1.8 μsfor the measurement campaign between September 2021 and April 2022. Such values are comparable to what is currently achieved by means of active transponders, and therefore, it is demonstrated that passive reflectors may be of interest to support radar altimeter regular calibration.","['Clutter', 'Radar tracking', 'Calibration', 'Transponders', 'Spaceborne radar', 'Signal resolution', 'Synthetic aperture radar']","['Altimetry', 'calibration', 'corner reflector', 'fully focused synthetic aperture radar (FF-SAR)', 'radar altimeter', 'Sentinel-6']"
"We present an analytical formulation of the correlation coefficient of the electromagnetic fields scattered at near-specular direction by a rough or gently undulating surface and measured at two spatially separated positions occupied by a moving receiver at slightly different times. This allows us obtaining an explicit expression of the correlation time of the received signal in terms of radar and surface parameters. This work provides a contribution to the discussion, currently ongoing in the Global Navigation Satellite System Reflectometry (GNSS-R) scientific community, about the behavior of received signal fluctuations, especially when surface profile variations are such that the scattering is neither coherent nor completely incoherent. The scattering surface is here modeled as randomly rough, and the Kirchhoff approximation (KA) or the first-order small slope approximation (SSA1) is employed to compute the scattered field. In fact, the expression of the correlation coefficient is the same for both approximations. The obtained closed-form expression shows that as the surface correlation length increases, the degree of coherence smoothly increases from the value obtained with the expression already available in the literature for very rough surfaces to a value close to unity for gently undulating surfaces. The obtained behavior of correlation time as a function of surface parameters, system resolution, and observation geometry is in agreement with numerical simulations available in the literature. In general, obtained analytical results are in agreement with the observed behavior of GNSS-R signals over flat land surfaces.","['Sea surface', 'Land surface', 'Surface roughness', 'Rough surfaces', 'Correlation', 'Surface topography', 'Receivers']","['Bistatic radar', 'coherence', 'Global Navigation Satellite System Reflectometry (GNSS-R)']"
"An intrinsic challenge in the geophysical interpretation of low-frequency synthetic aperture radar (SAR) imagery of semitransparent media, such as ice sheets, is the position ambiguity of the scattering structures within the glacial volume. Commonly tackled by applying interferometric and tomographic techniques, their spaceborne implementation exhibits by orders higher complexity compared to missions relying on single SAR images, making them cost expensive or, in the context of planetary missions, even impossible due to limited navigation capability. Besides, even these sophisticated techniques are commonly biased due to inaccurate permittivity estimates, leading to geometric distortions up to several meters. We present a novel inversion procedure to estimate volume parameters of ice sheets, namely, the depth of the scattering layer within the glacial volume and the dielectric permittivity of the ice, based on single-image single-polarization SAR acquisitions. The information is inherent in the processed SAR data as phase errors on the azimuth signals resulting from uncompensated nonlinear propagation of the radar echoes through ice. We suggest a local map-drift autofocus approach to quantify and spatially resolve the phase errors and an inversion model to relate them to the penetration depth and permittivity. Testing the proposed technique using P-band SAR data acquired using DLR’s airborne sensor F-SAR during the ARCTIC15 campaign in Greenland shows promising results and good agreement with tomographic products of the analyzed test site.","['Ice', 'Permittivity', 'Synthetic aperture radar', 'Radar', 'Spaceborne radar', 'Scattering', 'Radar imaging']","['Autofocus', 'cryosphere', 'depth', 'glacier', 'penetration', 'permittivity', 'synthetic aperture radar (SAR)', 'tomography']"
"Floating plastic debris represent an environmental threat to the maritime environment as they drift the oceans. Developing tools to detect and remove them from our oceans is critical. We present an approach to detect and distinguish suspect plastic debris from other floating materials (i.e., driftwood, seaweed, sea snot, sea foam, and pumice) using Sentinel-2 data. We use extreme gradient boosting trained with data compiled from published works complemented by manual interpretation of satellite images. The method is trained with two spectral bands and seven spectral indices computed from the Sentinel-2 spectral bands. We consider three application scenarios. The first uses the database created under the scope of this work. While the classification achieved a 98% accuracy rate for suspect plastic debris, we acknowledge the need for ground-truth validation. The second, to enlarge the training dataset, uses synthetic data generated through a Wasserstein generative adversarial network. A supervised model trained exclusively with synthetic data successfully classified suspect plastic pixels with an accuracy of 83%. The third comprises an ensemble model that quantifies uncertainty about the predictions obtained with the classifier. We correctly classified 75% of the suspect plastic pixels. However, while the classification accuracy decreased, with the integration of uncertainty in the predictions, the number of misclassifications also significantly decreased when compared to the model with the highest accuracy of the previous scenarios. Due to the mixed band nature of the sensor and subpixel coverage of debris within a pixel, the application to other datasets might not be straightforward.","['Plastics', 'Satellites', 'Spatial resolution', 'Indexes', 'Synthetic data', 'Sea surface', 'Reflectivity']","['Floating plastic debris', 'marine pollution', 'remote sensing', 'Sentinel-2']"
"Low-resolution ship detection from optical satellite image sequences is critical in high-orbit remote sensing satellite applications. However, it is still a difficult problem due to the following challenges: 1) the size of the ship is tiny in the low-resolution image; 2) the ship target is dim and the contrast with the background is low; and 3) the interference of cloud and fog covering is complex and changeable. For these reasons, the targets are easily lost during the detection. In fact, the C learer the O bjects against to the background, the more C onfident the O bservers can detect it. In light of these considerations, we propose a COCO-Net to detect the small dynamic objects on low-resolution images in this article. First, the multiframe images are associated by introducing motion information as an effective compensation for small object features. Second, an integrated dual-supervised network that processes single-level tasks hierarchically is presented to adaptively enhance the input data quality of object detection without being limited by diverse scene disturbances. Third, a unified region of interest (ROI)-loss scheme that modulates the loss function of the first component by introducing ROI-masks from the second component is utilized to make the first component also work for object detection. In addition, we construct a new dataset for the small dynamic object detection based on the GaoFen-4 satellite imagery. Comprehensive experiments on a self-assembled dataset from the GaoFen-4 satellite show the superior performance of the proposed method compared to state-of-the-art object detectors.","['Object detection', 'Satellites', 'Remote sensing', 'Marine vehicles', 'Task analysis', 'Feature extraction', 'Optical imaging']","['Dual-supervised network', 'low-resolution imagery', 'optical remote sensing (RS) images', 'ship detection']"
"This article presents two supervised change detection algorithms (CDAs) based on convolutional neural networks (CNNs) that use stacks of coregistered wavelength-resolution (WR) synthetic aperture radar (SAR) images to detect changes in an image under monitoring. The additional information of a scene of interest provided by SAR image stacks can be explored to enhance the performance of CDAs. In particular, stacks of images with similar statistics can be obtained for ultrawideband (UWB) very high-frequency (VHF) SAR systems, as they produce images highly stable in time. The proposed CDAs can be summed up into four stages: difference image (DI) formation, semantic segmentation, clustering, and change classification. The CNN-GSP algorithm is based on a ground scene prediction (GSP) image, which is used as a reference to form a DI. A CNN-based model then analyzes the DI. The CNN-MDI algorithm feeds multiple DIs with identical monitored images to a CNN-based model, which will concurrently analyze their features. Tests with CARABAS-II data show that the proposed CDAs can outperform other state-of-the-art algorithms that also use stacks of WR-SAR images. Beyond that, the proposed algorithms outperformed a CNN-based CDA that does not use image stacks, which shows that CNN-based algorithms can use the additional information provided by stacks of SAR images to reduce false alarm occurrences while increasing the probability of detection of changes.","['Synthetic aperture radar', 'Radar polarimetry', 'Convolutional neural networks', 'Monitoring', 'Detection algorithms', 'Complexity theory', 'Ultra wideband technology']","['CARABAS-II', 'classification', 'convolutional neural network (CNN)', 'deep learning', 'remote sensing', 'semantic segmentation']"
"Sea-surface temperature (SST) images obtained by satellites contain noise and missing SSTs due to cloud covers. We propose a method for reconstructing denoised cloud-free SST images via deep-learning-based image inpainting. For denoizing, we use data-assimilation images to train a reconstruction network by considering the physical correctness of SSTs. For reconstruction stability, we introduce anomaly inpainting network , which does not directly complete missing SSTs but estimates the difference between the unobserved SSTs and the average SSTs. SSTs do not fluctuate much over a few days; thus, we can use recent average SSTs as a rough estimation of SSTs and can assume that the SST difference will be within a specific range. We conducted experiments to evaluate our method with satellite SST images and in situ SST data. The results indicate that our method with anomaly inpainting network qualitatively and quantitatively outperformed conventional SST image inpainting methods.","['Image reconstruction', 'Clouds', 'Satellites', 'Ocean temperature', 'Data models', 'Spatiotemporal phenomena', 'Training']","['Deep learning', 'inpainting', 'sea-surface temperature (SST)']"
"A recent publication claims that closure phases in SAR interferometry bear no relationship to physical changes of the scatterer, but only to the statistical properties of the averaged pixels. We disprove this claim with a simple counterexample and remind the reader of cases in which closure phases indicate a clear physical content, including the exploitation of closure phases in other fields.","['Moisture', 'Synthetic aperture radar', 'Interferometry', 'Scattering', 'Imaging', 'Image color analysis', 'Sociology']","['Closure phase', 'SAR interferometry']"
"For numerous applications in image registration, sub-pixel translation estimation is a fundamental task, and increasing attention has been given to methods based on image phase information. However, we have found that none of these methods is universal. In other words, for any one of these methods, we can always find some image pairs which will not be well matched. In this paper, by introducing the cyclic shift matrix (CSM), we present a new model for the translation matching problem and derive a least squares solution for the model. In addition, by repeatedly applying the CSM to the matching image, an iterative CSM method is proposed to further improve the matching accuracy. Furthermore, we show that the traditional phase-based matching algorithms can only achieve an exact solution when there is a cyclic shift relationship between the images to be matched. The proposed method is evaluated using simulated and real images and demonstrates a better performance in both accuracy and robustness compared with the state-of-the-art methods.","['Correlation', 'Matrix decomposition', 'Tools', 'Estimation', 'Iterative methods', 'Frequency-domain analysis', 'Optimization']","['Cyclic shift matrix (CSM)', 'phase correlation', 'sub-pixel translation']"
"Synthetic aperture radar (SAR) interferometry is a well-established technique for producing high-resolution digital elevation models (DEMs) of the Earth’s surface and measuring displacements on different time scales. Observations of SAR interferograms, however, show that azimuth ambiguities can be coherently imaged and may lead to phase biases and coherence losses that significantly degrade the interferometric performance. Whereas imposing very low ambiguity levels may represent a severe design constraint for a spaceborne SAR system, a slight variation of the pulse repetition interval (PRI) is a new, simple, yet effective technique to decorrelate ambiguities, which, in turn, reduces the phase biases and coherence losses without substantially affecting the imaged swath width. An additional benefit of the PRI variation is that range ambiguities also become decorrelated. This article addresses two cases. For the repeat-pass case, slightly different pulse repetition frequencies (PRFs) can be used for the two acquisitions, and the minimum required PRF difference can be analytically derived resorting to the power spectral density of the ambiguous signals. For the single-pass case, a slight variation of the PRI during the common acquisition is an effective solution in case an along-track baseline is present. In particular, a square wave PRI variation scheme outperforms sinusoidal or random ones. Finally, simulations using TanDEM-X data are presented to show the improvement in interferogram and DEM quality resulting from ambiguity decorrelation. This work is relevant for the design of future spaceborne interferometric SAR systems and for the enhanced exploitation of current ones.","['Azimuth', 'Decorrelation', 'Coherence', 'Synthetic aperture radar', 'Autocorrelation', 'Interferometry', 'Spaceborne radar']","['Azimuth ambiguities', 'interferometry', 'microwave remote sensing', 'synthetic aperture radar (SAR)']"
"Seismic interpretation is a fundamental approach for obtaining static and dynamic information about subsurface reservoirs, such as geological faults/salt bodies and associated fluid types and distribution. Due to the exponential growth in seismic data volume and considerable uncertainty in manual interpretation, deep learning (DL) algorithms have been introduced to assist seismic interpretation. Our investigation of the trained neural networks suggests that they underperform on seismic data with different noise characteristics. One of the main issues is that the noise patterns of seismic data are highly inconsistent due to many factors, including geological features, sampling parameters, and human intervention. To address this problem, we propose a noise pattern transfer (NPT) framework to transfer or remove seismic noise style between datasets by treating noise patterns as styles of image, which can also improve the generality of automatic seismic interpretation algorithms. Extensive experiments on three synthetic datasets and two field seismic datasets demonstrate the promising performance of our proposed NPT approach. Pairs of clean and stylized seismic data are generated by extending the use of the neural style transfer algorithm beyond the artistic domain. We then demonstrate how our method achieves superior noise pattern transferability between datasets and denoising performance on field datasets. Associated improvements in accuracy and generalization of the neural-network-based fault recognition tasks successfully demonstrate the practicality of our NPT approach. The source code is made publicly available online at https://github.com/Magnomic/npt-code .","['Noise reduction', 'Data models', 'Transfer learning', 'Task analysis', 'Neural networks', 'Geology', 'Computational modeling']","['Convolutional neural networks (CNNs)', 'deep learning (DL)', 'image processing', 'neural style transfer (NST)', 'seismic image']"
"Vertically and horizontally inhomogeneous distributions of hydrometeors are often observed in precipitating clouds. The 3-D characteristics can then cause errors in the passive microwave rainfall measurements with the current off-nadir viewing sensors' specifications. This result is due to the fact that the same surface rainfall could be associated with different amounts of hydrometeors depending on the sensors' viewing paths. In this paper, we confirmed that the plane-parallel radiative treatment to the atmosphere leaves a notable deficiency in the microwave radiometric signatures, particularly at the higher frequency channels for different viewing directions when largely inhomogeneous precipitating clouds are accompanied by significant ice particles. The mean differences between the two brightness temperature fields with two opposite azimuthal viewing directions were up to approximately 40 °K for the vertically polarized channel at 85.5 GHz in the case study. The impact of the 3-D effect on the passive microwave rainfall estimations was also examined by synthetic retrievals employing a Bayesian methodology. The results showed that the uncertainty in the rainfall estimations due to the 3-D effect depended on the viewing directions considered in the a priori information. It was also found that taking more viewing angles or the azimuth angles in the a priori information into consideration tended to moderate the retrieval difference that resulted from the different viewing directions. In addition, the retrieval uncertainty related to the 3-D effect appeared to be more significant for heavy rainfall cases with large amounts of ice particles, as expected.","['Brightness temperature', 'Clouds', 'Solid modeling', 'Atmospheric modeling', 'Microwave FET integrated circuits', 'Microwave integrated circuits', 'Microwave measurement']","['Passive microwave remote sensing', 'precipitation', '3-D effect', '3-D radiative transfer']"
"Synthetic aperture radar (SAR) is a well-established technique for observing the Earth on a global scale. As applications become more demanding, it is desirable to overcome the limitations imposed by the SAR principle, one of which is the tradeoff between the swath width and the instantaneous azimuth bandwidth, determining the resolution. Recombination of multiple channels with displaced phase centers has been proposed as a convenient way to create high resolution wide-swath images. We analyze various approximations made in the channel transfer functions and their impact on the reconstruction result using examples inspired by current imaging modes of the TerraSAR-X and TanDEM-X missions. In order to do so, we introduce an efficient method to assess the quality of reconstruction filters for an arbitrary number of channels without the need of full time-domain simulations.","['Synthetic aperture radar', 'Silicon', 'Azimuth', 'Bandwidth', 'Transfer functions', 'Image resolution', 'Image reconstruction']","['Radar signal processing', 'signal reconstruction', 'spaceborne radar', 'synthetic aperture radar (SAR)']"
"Despite the plethora of successful super-resolution (SR) reconstruction (SRR) models applied to natural images, their application to remote sensing imagery tends to produce poor results. Remote sensing imagery is often more complicated than natural images, has its peculiarities such as being of lower resolution, contains noise, and often depicts large textured surfaces. As a result, applying nonspecialized SRR models like the enhanced SR generative adversarial network (ESRGAN) on remote sensing imagery results in artifacts and poor reconstructions. To address these problems, we propose a novel strategy for enabling an SRR model to output realistic remote sensing images: instead of relying on feature-space similarities as a perceptual loss, the model considers pixel-level information inferred from the normalized digital surface model (nDSM) of the image. This allows the application of better-informed updates during the training of the model which sources from a task (elevation map inference) that is closely related to remote sensing. Nonetheless, the nDSM auxiliary information is not required during production, i.e., the model infers an SR image without additional data. We assess our model on two remotely sensed datasets of different spatial resolutions that also contain the DSMs of the images: the Data Fusion 2018 Contest (DFC2018) dataset and the dataset containing the national LiDAR flyby of Luxembourg. We compare our model with ESRGAN, and we show that it achieves better performance and does not introduce any artifacts in the results. In particular, the results for the high-resolution DFC2018 dataset are realistic and almost indistinguishable from the ground-truth images.","['Remote sensing', 'Task analysis', 'Image reconstruction', 'Training', 'Spatial resolution', 'Superresolution', 'Loss measurement']","['Deep learning (DL)', 'normalized digital surface model (nDSM)', 'perceptual loss', 'remote sensing', 'super-resolution (SR) reconstruction (SRR)']"
"Estimates of cloud droplet effective radius (re) and optical thickness (τ) can be derived using reflected sunlight in a visible non-absorbing channel combined with reflectances from a near IR channel that is absorbing [e.g., the bispectral method (BSM)]. Discrepancies between BSM-estimatedreand collocated in situ measurements are commonly attributed to a violation of the assumptions used in the BSM algorithm such as plane parallel geometry, and a single mode droplet size distribution (DSD). This research uses Markov chain Monte Carlo (MCMC) experiments to examine the impact of precipitation on BSM-retrievedrenear optical cloud top by comparing the retrievals and associated uncertainties obtained from two types of experiments assuming a unimodal or bimodal drop size distribution. Where rain is present, BSM-retrievedreoverestimates the true cloud modere. Moreover, there is no longer a unique measure ofrewithin the precipitating liquid-phased clouds, resulting in a substantial increase in retrieval uncertainties. This leads to a corresponding loss of information on the total number concentration and liquid water content (LWC) near the cloud top. It is found thatrebiases are not strongly correlated with properties exclusively pertaining to rain, such as rain water content (RWC) or precipitation rates, but tend to be a function of the ratio between rain and cloud water content (CWC) and the cloud total number concentration. These results highlight the need for additional independent information such as from an active or passive microwave sensor that can identify the presence of precipitation and constrain additional aspects of bimodal droplet distributions.","['Clouds', 'Rain', 'Cloud computing', 'Optical variables measurement', 'Computational modeling', 'Satellites', 'Optical sensors']","['Bayesian methods', 'cloud microphysics', 'cloud remote sensing', 'error analysis', 'Moderate Resolution Imaging Spectroradiometer (MODIS)']"
"Supercooled liquid clouds are very frequent in high-latitude regions. In addition to their substantial effect on visible and infrared radiation, they affect the signal of millimeter radars by producing nonnegligible attenuation. Such attenuation must be properly corrected if the information of millimeter radars is used in quantitative retrievals for inferring ice microphysical properties. This study proposes a multisensor scheme for refining the vertical distribution of supercooled liquid water content (SLWC) compared to state-of-the-art methods that equipartition the liquid water path measured by microwave radiometer to all pixels identified as cloudy by the radars and warmer than −40 °C. Our methodology is applicable in high-latitude, mixed-phase environments based on the synergy between radar and lidar binary cloud phase masking, microwave radiometer, and radio sounding observations. The technique is demonstrated via data collected by the U.S. Department of Energy (DoE) Atmospheric Radiation Measurement (ARM) Program climate research facility at the North Slope of Alaska (NSA) and compared with the state-of-the-art methods. Path integrated attenuation (PIA) at W- and G-band frequencies (>95 GHz) is then assessed. Results indicate that the different in-cloud distributions of the liquid condensate lead to round-trip PIA discrepancies of cloudy volumes that range in [2, 5] dB at W- and G-band frequencies. These differences far exceed those encountered when changing some of the algorithm’s arbitrary assumptions and weighting functions.","['Clouds', 'Radar', 'Liquids', 'Attenuation', 'Microwave radiometry', 'Laser radar', 'Ice']","['Attenuation measurement', 'clouds', 'electromagnetics for remote sensing', 'lidar data', 'radar data']"
"Because of the imaging mechanism complexity of long-linear-array and wide-swath whisk-broom thermal infrared spectrometer (TIS) of the first Sustainable Development Goals Satellite (SDGSAT-1), how to achieve a high geometric positioning accuracy (GPA) becomes the core factor in subsequent geometric quantitative applications. Here, in this article, a three-step in-orbit geometric calibration (GC) strategy comprising the estimations of exterior orientation parameters (EOPs), interior orientation parameters (IOPs), and scanning compensation parameters (SCPs) is proposed to correct the geo-location displacements for whisk-broom TIS. First, in accordance with the optical-mechanical structure and pinhole imaging theory, we establish the rigorous geometric positioning model (RGPM) of TIS and analyze the error resources term-by-term along the error propagation link elaborately. Second, the corresponding rigorous geometric calibration model (RGCM) is constructed in detail based on the 2-D look-angle model and the generalized bias correction matrix. Especially for eliminating the systematic nonlinear errors in the scanning direction, a fifth-degree polynomial is put forward to be employed to fit and compensate for the angular measurement errors of the scanning mirror. Finally, a three-step estimation method is presented to estimate the calibration parameters with ground control points (GCPs). Experimental results based on the spatial references of Landsat 8 panchromatic images and version 2 of advanced spaceborne thermal emission and reflection radiometer (ASTER) global digital elevation model (GDEM2) show that the GPA of the proposed method in along-track and cross-track directions can be better than 1.0 pixels for all three bands, which makes a great sense for associated geometric measurements.","['Calibration', 'Cameras', 'Payloads', 'Satellites', 'Stars', 'Spatial resolution', 'Optical sensors']","['Geometric calibration (GC)', 'geometric positioning accuracy (GPA)', 'rigorous geometric positioning model (RGPM)', 'Sustainable Development Goals Satellite (SDGSAT-1)', 'whisk-broom']"
"Groundwater depletion is one of the serious geoenvironmental issues causing ground subsidence, which damages buildings, infrastructures, and causes loss of life. The quantitative and qualitative evaluation of groundwater variability requires multiple approaches to measure hydraulic head level and geodetic deformation. In this study, we have made efforts to integrate multiple hierarchical space-borne data, including Gravity Recovery and Climate Experiment (GRACE), Sentinel-1 interferometric synthetic aperture radar (InSAR), and geological and hydrological data, to quantify subsidence in Chandigarh city and its surroundings. First, we conducted New-Small BAseline Subsets (NSBASs) and pointwise persistent scatterer (PS) InSAR techniques in parallel, using three-year Sentinel-1 data showing a vertical subsidence up to 120 mm/year around fluvial sediment deposits. Furthermore, correlation analysis of hydraulic/climatic measurements clearly shows the subsidence associated with the groundwater depletion. The pattern of PS points shows the instability of structures associated with the ground subsidence over the central city areas. The monumental architectures designed by Le Corbusier in the northern sectors are outside of the main subsidence area. In the target area, the magnitude of subsidence and surface deformation due to groundwater depletion depends on the subsurface geophysical environment and the anthropogenic activities within the region and surroundings. The results provided a case of monitoring scheme using multiresolution satellite data about the subsidence and associated consequences due to groundwater depletion.","['Urban areas', 'Deformation', 'Time series analysis', 'Buildings', 'Monitoring', 'Orbits', 'Sediments']","['Groundwater depletion', 'interferometric synthetic aperture radar (InSAR)', 'risk assessment', 'subsidence', 'time series analysis']"
"Freeze/thaw (FT) processes at the earth’s surface can have a considerable effect on global carbon, energy, and hydrologic cycles. Therefore, an accurate representation of FT is valuable to adequately monitor and model these processes. In this study, we assess the relationship between satellite-based FT products and modeled surface and soil temperatures over North America. In addition, hourly land surface temperature (LST) from the Geostationary Operational Environmental Satellite (GOES) system is also compared to FT classifications. Utilizing the higher spatial resolution temperatures (~5 km), we assess subgrid-scale variability and its relationship to coarser microwave FT classifications (>25 km). We also examine product agreement and subpixel characteristics across the land cover, climate, and topography. FT classifications are shown to vary widely depending on these variables, leading to an ambiguous definition of frozen and thawed states. Our results suggest that current products can characterize FT transitions with consistent subfreezing surface characteristics in far northern regions (>50 °N). However, uncertainty associated with FT classifications is shown to increase considerably as latitude decreases. Our results also suggest that fractional FT products, utilizing data inputs, such as LST, would provide a considerable improvement in mountainous regions with high intergrid cell heterogeneity, in regions characterized by ephemeral FT events (i.e., regions < 40 °N), as well as during freeze and thaw onset periods. This study also provides insight to improving the representation of surface FT state by providing a clearer definition of the subpixel scale temperature characteristics that govern existing frozen classifications.","['Land surface temperature', 'Land surface', 'Surface topography', 'Ocean temperature', 'Temperature sensors', 'Surface treatment', 'Sea surface']","['Earth observing systems', 'geoscience', 'North America', 'passive microwave remote sensing']"
"Radio-echo sounding (RES) reveals patches of high backscatter in basal ice units, which represent distinct englacial features in the bottom parts of glaciers and ice sheets. Their material composition and physical properties are largely unknown due to their direct inaccessibility but could provide significant information on the physical state as well as on present and past processes at the ice-sheet base. Here, we investigate the material properties of basal ice units by comparing measured airborne radar data with synthetic radar responses generated using electromagnetic (EM) forward modeling. The observations were acquired at the onset of the Jutulstraumen Ice Stream in western Dronning Maud Land (DML) (East Antarctica) and show strong continuous near-basal reflections of up to 200-m thickness in the normally echo-free zone (EFZ). Based on our modeling, we suggest that these high-backscatter units are most likely composed of point reflectors with low dielectric properties, suggesting thick packages of englacial entrained debris. We further investigate the effects of entrained particle size, and concentration in combination with different dielectric properties, which provide useful information to constrain the material composition of radar-detected units of high backscatter. The capability and application of radar wave modeling in complex englacial environments is therefore a valuable tool to further constrain the composition of basal ice and the physical conditions at the ice base.","['Ice', 'Radar', 'Backscatter', 'Antarctica', 'Dielectrics', 'Sediments', 'Atmospheric modeling']","['Antarctic ice sheet', 'basal freeze-on', 'basal ice', 'electromagnetic (EM) forward modeling', 'gprMax', 'ice accretion', 'Jutulstraumen ice stream', 'radio-echo sounding (RES)', 'sediment entrainment']"
"Interferometric synthetic aperture radar (InSAR) is a promising tool for the retrieval of snow water equivalent (SWE) from space. Due to refraction, the interferometric phase changes with snow depth and density, which is exploited by the InSAR method. While the method was first proposed two decades ago, qualitative research using experimental data analyzing factors affecting retrieval performance remains scarce. In this work, a tower-based 1–10-GHz, fully polarimetric SAR with InSAR capabilities was used to analyze the effect of meteorological events (air temperature, precipitation intensity, and wind) on the observed temporal decorrelation of interferometric image pairs at L-, S-, C-, and X-bands. These factors were found to be causes of decorrelation in snow, being the temperature the critical variable in the case of snowmelt events. Of the analyzed bands, the L-band presented the best coherence conservation properties. In addition, the phase change between pairs with sufficient coherence was applied to generate estimates of changes in SWE, studying the retrieval errors at different bands and over different temporal baselines. SWE accumulation was calculated from 6 h up to 12 days’ temporal baseline over a nonvegetated area. SWE accumulation profiles were successfully reconstructed for short temporal baselines and low frequencies, while an increase in the retrieval error was observed for high frequencies and long temporal baselines, indicating the limitations of higher frequencies for repeat-pass InSAR retrieval. The analysis was also reproduced over a forested area at the L-band with similar results as to the nonvegetated area.","['Snow', 'Temperature measurement', 'Coherence', 'Synthetic aperture radar', 'Microwave radiometry', 'Decorrelation', 'Radar']","['Decorrelation', 'interferometric synthetic aperture radar (InSAR)', 'SAR', 'snow water equivalent (SWE)', 'snow', 'Sodankylä SAR (SodSAR)', 'temporal coherence']"
"This work develops a method by which a kilometer-spaced array of Global Navigation Satellite System (GNSS) scintillation receivers can be used to estimate the ionospheric irregularity layer height and thickness and associated uncertainties on those estimates. Spectra of filtered signal power and phase data are used to estimate these quantities by comparing the observed ratio of the log of the power spectrum to the phase spectrum with the Rytov weak scatter theoretical ratio. A Monte Carlo simulation of noise on the input signal and the irregularity drift velocity is used to quantify the error in estimates of height and thickness. The method is tested using data from the Scintillation Auroral Global Positioning System (GPS) Array (SAGA) sited in the auroral zone at Poker Flat Research Range, Alaska. For the 30-min scintillation period studied, the technique identifies ionospheric scattering from a thick F layer, which correlates well with on-site incoherent scatter radar measurements of peak electron density, for an event previously identified in the literature as likely due to F layer.","['Global navigation satellite system', 'Scattering', 'Receivers', 'Arrays', 'Global Positioning System', 'Satellite broadcasting', 'Plasmas']","['Geophysical signal processing', 'Global Navigation Satellite System (GNSS)', 'Global Positioning System (GPS)', 'ionosphere', 'measurement uncertainty', 'radiowave propagation', 'scintillation']"
"Variations in water levels of seasonally ice-covered subarctic lakes are indicators of environmental and climatic change. Satellite altimetry enables remote sensing of these lakes, but the lake phenology is problematic as radar reflection surfaces include water, snow, and ice. Reflection from multiple surfaces gives rise to two-peak waveforms across ice-covered lakes. Misinterpretation of the altimetric height has caused extracted water levels to be low compared with gauge data. In this study, a modified retracker is used to determine heights from the first altimetric subwaveform. Using in situ snow depth and ice thickness, the first reflection surface is shown to correspond closely to the snow/ice interface when the lake is frozen. The modified retracker is applied to the Great Bear Lake (GBL), Great Slave Lake (GSL), and Lake Athabasca (ATL) of the Mackenzie River Basin for the period 1992–2020. Standard deviations (Std) of differences between lake levels from Jason-2 waveforms and in situ data across GBL and GSL are 0.06 m with the new methodology compared with 0.11 and 0.08 m, respectively, using the standard Ice retracker. With an Std of 0.11 m between altimetric and gauge lake levels, TOPEX/Poseidon is less accurate than the combined Jason missions (Std: 0.07 m).","['Lakes', 'Ice', 'Altimetry', 'Snow', 'Rivers', 'Sea surface', 'Satellites']","['Lake level', 'Mackenzie River basin', 'reflection surface', 'satellite altimetry', 'seasonal ice-covered lake']"
"Surface albedo plays a key role in global climate modeling as a factor controlling the energy budget. Satellite observations were utilized to estimate surface albedo at global and regional scales with good precision over flat areas. However, because topography greatly complicates radiative transfer (RT) processes, estimating the albedo of rugged terrain with satellite data remains a challenge. In addition, albedo definitions over sloping terrain differ from that for flat areas. They include horizontal/horizontal sloped surface albedo (HHSA) and inclined/inclined sloped surface albedo (IISA). Methods for retrieving HHSA and IISA in mountains have not been well-explored. Here, we retrieved HHSA and IISA on sloping terrain from Landsat 8 using a direct estimation algorithm. We simulated a dataset of Landsat top-of-atmosphere (TOA) reflectance and surface albedo with discrete anisotropic radiative transfer (DART) model, for variable atmospheric, vegetation, soil, and topography properties. Then, we used artificial neural networks (ANNs) to derive an empirical relationship between TOA reflectance and surface albedo. The accuracy of our method was verified with in situ measurements: root mean squared error (RMSE) and bias equal to 0.029 and −0.010 for HHSA, and 0.023 and −0.001 for IISA, respectively. Several albedo results (HHSA, IISA, values without topographic consideration) were evaluated and compared. HHSA was found similar to albedo without topographic consideration, but IISA, considered as the “true albedo” for sloping terrain, showed large difference from them. This study demonstrated the feasibility of surface albedo estimation from Landsat TOA reflectance directly in rugged terrains and advanced our understanding of energy budget in mountains.","['Surface topography', 'Estimation', 'Atmospheric modeling', 'Remote sensing', 'Reflectivity', 'Satellites', 'Surfaces']","['Artificial neural network (ANN)', 'direct estimation algorithm', 'Landsat', 'discrete anisotropic radiative transfer (DART)', 'sloping terrain', 'surface albedo']"
"This article considers the 3-D collaborative trajectory optimization (CTO) of multiple unmanned aerial vehicles to improve multitarget tracking performance with an asynchronous angle of arrival measurements. The predicted conditional Cramér–Rao lower bound is adopted as a performance measure to predict and subsequently control tracking error online. Then, the CTO problem is cast as a time-varying nonconvex problem subjected to constraints arising from dynamic and security (height, collision, and obstacle/target/threat avoidance). Finally, a comprehensive solution method (CSM) is presented to tackle the resulting problem, according to its unique structures. Specifically, if all security constraints are inactive, the CTO can be simplified as a nonconvex problem with convex dynamic constraints, which can be solved by the nonmonotone spectral projected gradient (NSPG) method. Oppositely, an alternating direction penalty method (ADPM) is presented to solve the CTO problem with some positive security constraints. The ADPM introduces auxiliary vectors to decouple the complex constraints and separates the CTO into several subproblems and tackles them alternately, while locally adjusting the penalty factor at each iteration. We show the subproblem w.r.t. the position vector is nonconvex but with convex constraints, which can be efficiently solved by the NSPG method. The subproblems w.r.t. the auxiliary vectors are separable and have closed-form solutions. Simulation results demonstrate that the CSM outperforms the unoptimized method in terms of tracking performance. Besides, the CSM achieves the near-optimal performance provided by the genetic algorithm with much lower computational complexity.","['Target tracking', 'Sensors', 'Three-dimensional displays', 'Security', 'Location awareness', 'Time measurement', 'Collision avoidance']","['Asynchronous target tracking', 'passive sensor', 'resource allocation', 'trajectory optimization (TO)', 'unmanned aerial vehicle (UAV)']"
"Motivated by the state-of-the-art optical sensing and image processing technologies, remote urban sensing (RUS) has emerged as a powerful sensing paradigm to capture abundant visual information about the urban environment for intelligent city monitoring, planning, and management. In this article, we focus on a classification and super-resolution coupling (CSC) problem in RUS applications, where the goal is to explore the interdependence between two critical tasks (i.e., classification and super-resolution ) to concurrently boost the performance of both the tasks. Two fundamental challenges exist in solving our problem: 1) it is challenging to obtain accurate classification results and generate high-quality reconstructed images without knowing either of them a priori and 2) the noise embedded in the image data could be amplified infinitely by the complex interdependence and coupling between the two tasks. To address these challenges, we develop SCLearn , a novel deep convolutional neural network architecture, to couple the classification task with the super-resolution task in an integrated learning framework to concurrently boost the performance of both the tasks. The evaluation results on a real-world RUS application over two different cities in Europe (Barcelona and Berlin) show that SCLearn consistently outperforms the state-of-the-art baselines by simultaneously achieving better land usage classification accuracy and higher reconstructed image quality under various application scenarios.","['Task analysis', 'Superresolution', 'Sensors', 'Image reconstruction', 'Visualization', 'Convolutional neural networks', 'Satellites']","['Classification', 'integrated deep learning', 'smart urban sensing', 'super-resolution']"
"This study aims at estimating the dry snow water equivalent (SWE) by using X-band synthetic aperture radar (SAR) data from the COSMO-SkyMed (CSK) satellite constellation. The time series of CSK acquisitions have been collected during the dry snow period in the Alto Adige test site, in the Italian Alps, during the winter seasons from 2013 to 2015 and from 2019 to 2021. The SAR data have been analyzed and compared with the in situ measurements to understand the X-band SAR sensitivity to SWE, which has been further assessed by dense media radiative transfer (DMRT) model simulations. The sensitivity analysis provided the basis for addressing the SWE retrieval from the CSK data, by exploiting two different machine learning (ML) techniques, namely, artificial neural networks (ANNs) and support vector regression (SVR). To ensure statistical independence of training and validation processes, the algorithms are trained and tested using SWE predictions of the fully distributed snow model AMUNDSEN as reference data and are subsequently validated on the experimental dataset. Due to its influence on the CSK estimates, the effect of forest canopy was accounted for in the analysis. Depending on the algorithm, the validation resulted in a correlation coefficient 0.78\le R \le0.91and a root-mean-square error (RMSE) 55.5 mm \leRMSE \le87.4mm between estimated and in situ SWE. Further analysis and validation are needed; however, the obtained results seem suggesting the CSK constellation as an effective tool for the retrieval of the dry SWE in alpine areas.","['Snow', 'Monitoring', 'Synthetic aperture radar', 'Training', 'Machine learning', 'Artificial neural networks', 'Time series analysis']","['Alpine environment', 'artificial neural networks (ANNs)', 'COSMO-SkyMed (CSK)', 'machine learning (ML)', 'snow depth (SD)', 'snow water equivalent (SWE)', 'support vector regression (SVR)', 'X-band synthetic aperture radar (SAR)']"
"During the melting season, predicting the daily sea ice concentration (SIC) of the Pan-Arctic at a subseasonal scale is strongly required for economic activities and a challenging task for current studies. We propose a deep-learning-based data-driven model to predict the 90 days SIC of the Pan-Arctic, named SICNet 90 . SICNet 90 takes the historical 60 days’ SIC and its anomaly and outputs the SIC of the next 90 days. We design a physically constrained loss function, normalized integrated ice-edge error (NIIEE), to constrain the SICNet90′soptimization by the spatial morphology of SIC. The satellite-observed SIC trains (1991–2011/1997–2017) and tests the model (2012/2018–2020). For each test year, a 90-day SIC prediction is made daily from May 1 to July 2. The binary accuracy (BACC) of sea ice extent (SIC>15%) and the mean absolute error (MAE) are evaluation metrics. Experiments show that SICNet 90 significantly outperforms the Climatology benchmark on 90 days prediction, with a BACC/MAE improvement/reduction of 5.41%/1.35%. The data-driven model shows a late-spring-early-summer predictability barrier (around June 20) and a prediction challenge (around July 10), consistent with SIC’s autocorrelation. The NIIEE loss optimizes the predictability barrier/challenge with a BACC increase of 4%. Using a 60 days historical SIC to predict 90 days SIC is better than a historical SIC of 30/90 days. The historical 2-m surface air temperature shows positive contributions to the prediction made from May 1 to mid-June, but negative contributions to the prediction made after mid-June. The historical sea surface temperature and 500 hp geopotential height show negative contributions.","['Predictive models', 'Atmospheric modeling', 'Sea ice', 'Numerical models', 'Arctic', 'Ocean temperature', 'Data models']","['Deep learning', 'Pan-Arctic', 'physically constrained loss function', 'sea ice concentration (SIC) prediction', 'subseasonal scale']"
"In recent years, pixel-wise hyperspectral image (HSI) classification has received growing attention in the field of remote sensing. Plenty of spectral–spatial convolutional neural network (CNN) methods with diverse attention mechanisms have been proposed for HSI classification due to the attention mechanisms being able to provide more flexibility over standard convolutional blocks. However, it remains a challenge to effectively extract multiscale features of high-resolution HSI in a real-world complex environment. In this article, we propose a pyramidal multiscale spectral–spatial convolutional network with polarized self-attention for pixel-wise HSI classification. It contains three stages: channel-wise feature extraction network, spatial-wise feature extraction network, and classification network, which are used to extract spectral features, extract spatial features, and generate classification results, respectively. Pyramidal convolutional blocks and polarized attention blocks are combined to extract spectral and spatial features of HSI. Furthermore, residual aggregation and one-shot aggregation are employed to better converge the network. The experimental results on several public HSI datasets demonstrate that the proposed network outperforms other related methods.","['Feature extraction', 'Convolutional neural networks', 'Data mining', 'Three-dimensional displays', 'Convolution', 'Kernel', 'Support vector machines']","['Convolutional neural network (CNN)', 'hyperspectral image (HSI) classification', 'multiscale feature extraction', 'polarized self-attention (PSA) mechanism']"
"Satellite-based passive microwave (PMW) remote sensing is an essential technique to clarify long-term and large-scale distribution patterns of cloud water content (CWC). However, most CWC estimation methods are not implemented over land because of high heterogeneity of land radiation, and the detailed characteristics of microwave (MW) radiative transfer between land and atmosphere including clouds have not been elucidated. This study aims to elucidate these characteristics and reveal the accuracy of land emissivity representation necessary for adequate CWC estimation over land using satellite-based PMW under various CWC conditions. First, important parameters related to MW radiative transfer between land and atmosphere at CWC-relevant frequencies in the presence of clouds are determined theoretically. Next, the relationship between errors in these parameters and the brightness temperatures used for CWC estimation is clarified through considerations based on radiative transfer equations. Then, ground-based PMW observations and numerical simulation are used to reveal the actual values of these important parameters and the size of errors. Finally, the results show that for any cloud liquid water path (LWP) value greater than 1.6 kg/m 2 at 89 GHz and 5 kg/m 2 at 36 GHz, we can reasonably neglect the heterogeneity of emissivity and radiation from the land surface for CWC estimation. However, when LWP values are below the threshold, the error in the representation of land emissivity should be kept below 0.015 for both 89- and 36-GHz data, and volumetric soil moisture content should have an error lower than 5%-6% for both frequencies.","['Estimation', 'Clouds', 'Atmosphere', 'Land surface', 'Soil', 'Brightness temperature', 'Atmospheric measurements']","['Cloud water content (CWC) retrieval over land', 'passive microwave (PMW) remote sensing', 'representation of land emissivity']"
"Small Baseline Subset InSAR (SBAS InSAR) utilizes a series of synthetic aperture radar (SAR) interferograms to generate a time series that can analyze the surface displacements of coherent points. Still, atmospheric errors in interferometric SAR (InSAR) measurements can seriously affect the reliability of the surface displacement time series. In this article, a new approach incorporating the Generic Atmospheric Correction Online Service for InSAR (GACOS) and principal component analysis (PCA) is proposed to reduce atmospheric errors in SBAS InSAR. Its application to Southern California, USA suggests that the incorporation of GACOS and PCA can efficiently reduce atmospheric effects on InSAR observations and hence improve the accuracy of InSAR-derived surface displacements. The overall standard deviations of the SAR interferograms were reduced from 4.97 to 2.02 rad after applying GACOS and PCA with the root mean square error (RMSE) reducing by 61.1% from 18 to 7 mm. In addition, comparisons between different PCA processing strategies suggest that the more principal components are removed, the smaller the standard deviations of the interferograms, but the lower the accuracy of InSAR-derived surface displacements.","['Atmospheric modeling', 'Principal component analysis', 'Atmospheric measurements', 'Delays', 'Global navigation satellite system', 'Numerical models', 'Radar polarimetry']","['Atmospheric errors', 'Generic Atmospheric Correction Online Service for InSAR (GACOS)', 'principal component analysis (PCA)', 'Small Baseline Subset InSAR (SBAS InSAR)', 'time series']"
"Highly short-term forecasting, or nowcasting, of heavy rainfall due to rapidly evolving mesoscale convective systems (MCSs) is particularly challenging for traditional numerical weather prediction (NWP) models. To overcome such a challenge, a growing number of studies have shown significant advantages of using machine learning (ML) modeling techniques with remote sensing data, especially weather radar data, for high-resolution rainfall nowcasting. To improve ML model performance, it is essential first and foremost to quantify the importance of radar variables and identify pertinent predictors of rainfall that can also be associated with domain knowledge. In this study, a set of MCS types consisting of convective cell (CC), mesoscale CC, diagonal squall line (SLD), and parallel squall line (SLP), was adopted to categorize MCS storm cells, following the fuzzy logic algorithm for storm tracking (FAST), over the Korean Peninsula. The relationships between rain rates and over 15 variables derived from data products of dual-polarimetric weather radar were investigated and quantified via five ML regression methods and a permutation importance algorithm. As an applicational example, ML classification models were also developed to predict locations of storm cells. Recalibrated ML regression models with identified pertinent predictors were coupled with the ML classification models to provide early warnings of heavy rainfall. Results imply that future work needs to consider MCS type information to improve ML modeling for nowcasting and early warning of heavy rainfall.","['Storms', 'Radar', 'Rain', 'Predictive models', 'Meteorological radar', 'Data models', 'Input variables']","['Artificial neural network (ANN)', 'convolutional neural network (CNN)', 'deep learning', 'dual-polarimetric weather radar', 'early warning', 'flash flood', 'hydrometeorological hazard', 'Lasso', 'mesoscale convective system (MCS)', 'permutation importance', 'random forest', 'remote sensing', 'storm', 'support vector regression (SVR)']"
"This article investigates the capabilities of 24 GHz frequency-modulated continuous-wave (FMCW) multiple-input–multiple-output (MIMO) radar technology to retrieve sea surface currents and directional wave spectra. A procedure based on the dispersion relation, which was previously applied to process X-band marine radar data, is here exploited. The estimation performance of the radar sensor is first assessed by numerical tests in the case of synthetic sea wave fields with known characteristics in terms of wave direction and surface currents. Finally, the estimation procedure is assessed on real data collected at San Vincenzo quay in the port area of Naples, Italy. The achieved results are encouraging and highlight that 24 GHz FMCW MIMO radar is a viable technology for sea wave monitoring.","['Radar', 'Radar antennas', 'Sea surface', 'Radar cross-sections', 'Surface waves', 'Monitoring', 'MIMO radar']","['Directional spectrum', 'frequency-modulated continuous wave (FMCW)', 'multiple-input--multiple-output (MIMO) radar', 'sea wave monitoring', 'surface currents']"
"Traditional synthetic aperture radar (SAR) target detection methods use matched filtered SAR images as input, and the detection performance is restricted due to the high sidelobes and speckle noise of these images. Sparse SAR imaging methods developed in recent years provide the advantages of reducing sidelobes, noise, and clutter. The imaging results obtained with these methods could help improve the SAR target detection performance. In this article, to improve the target detection performance using sparse SAR images as input, we proposed a convolutional sparse feature enhancement method to meet the needs of Bayesian saliency detection. The proposed Bayesian saliency joint target detection method comprised the following three steps: first, to obtain sparse SAR images with continuous contours and fewer holes in the target area, we proposed a convolutional L1 sparse regularization method. Second, a regularization parameter optimization method was derived to quickly obtain optimal regularization parameters for saliency detection. Finally, target detection results were obtained through a superpixel-based Bayesian saliency joint detector. Extensive experiments verified that the proposed method could improve the SAR target detection accuracy in complex backgrounds.","['Radar polarimetry', 'Synthetic aperture radar', 'Object detection', 'Bayes methods', 'Saliency detection', 'Feature extraction', 'Speckle']","['Bayesian inference', 'saliency detection', 'sparse synthetic aperture radar (SAR) feature enhancement', 'SAR target detection', 'sparse SAR images']"
"Semantic segmentation necessitates approaches that learn high-level characteristics while dealing with enormous quantities of data. Convolutional neural networks (CNNs) can learn unique and adaptive features to achieve this aim. However, due to the large size and high spatial resolution of remote sensing images, these networks cannot efficiently analyze an entire scene. Recently, deep transformers have proven their capability to record global interactions between different objects in the image. In this article, we propose a new segmentation model that combines CNNs with transformers and show that this mixture of local and global feature extraction techniques provides significant advantages in remote sensing segmentation. In addition, the proposed model includes two fusion layers that are designed to efficiently represent multimodal inputs and outputs of the network. The input fusion layer extracts feature maps summarizing the relationship between image content and elevation maps [digital surface model (DSM)]. The output fusion layer uses a novel multitask segmentation strategy where class labels are identified using class-specific feature extraction layers and loss functions. Finally, a fast-marching method (FMM) is used to convert unidentified class labels into their closest known neighbors. Our results demonstrate that the proposed method improves segmentation accuracy compared with state-of-the-art techniques.","['Transformers', 'Feature extraction', 'Remote sensing', 'Semantics', 'Semantic segmentation', 'Image resolution', 'Data models']","['Convolutional neural networks (CNNs)', 'EfficientNet', 'fusion networks', 'semantic segmentation', 'transformers']"
"Accurate and reliable building footprint maps are vital to urban planning and monitoring, and most existing approaches fall back on convolutional neural networks (CNNs) for building footprint generation. However, one limitation of these methods is that they require strong supervisory information from massive annotated samples for network learning. State-of-the-art semi-supervised semantic segmentation networks with consistency training can help deal with this issue by leveraging a large amount of unlabeled data, which encourages the consistency of model output on data perturbation. Considering that rich information is also encoded in feature maps, we propose to integrate the consistency of both features and outputs in the end-to-end network training of unlabeled samples, enabling to impose additional constraints. Prior semi-supervised semantic segmentation networks have established cluster assumption, in which the decision boundary should lie in the vicinity of low sample density. In this work, we observe that for building footprint generation, low-density regions are more apparent at the intermediate feature representations within the encoder than the encoder’s input or output. Therefore, we propose an instruction to assign the perturbation to the intermediate feature representations within the encoder, which considers the spatial resolution of input remote sensing imagery and the mean size of individual buildings in the study area. The proposed method is evaluated on three datasets with different resolutions: Planet dataset (3 m/pixel), Massachusetts dataset (1 m/pixel), and Inria dataset (0.3 m/pixel). Experimental results show that the proposed approach can well extract more complete building structures and alleviate omission errors.","['Buildings', 'Remote sensing', 'Training', 'Semantics', 'Image segmentation', 'Perturbation methods', 'Task analysis']","['Building footprint', 'consistency training', 'semantic segmentation', 'semi-supervised']"
"Remotely sensed soil moisture (SM) dataset with well accuracy and fine spatiotemporal resolution is very valuable in various fields. Downscaling is a promising way to obtain such an SM dataset. There are currently two basic methodologies in the downscaling with satellite datasets, i.e., the machine learning (ML) methods and the physical or semiphysical (PH) models. This study focuses on exploring feasible ways to integrate them for boosting the performance of downscaling. Here, three parallel modes, i.e., arithmetic average (Ari), geometric average (Geo), and weighted average (Wei), and two series modes, i.e., first PH then ML (PH-ML) and first ML then PH (ML-PH), were developed and evaluated. A representative of PH, DSCALE_mod16, and three ML algorithms, random forest (RF), extreme gradient boosting (XGBoost), and LightGBM, were used in this study. Microwave SM datasets from SM Active and Passive (SMAP) Mission, satellite datasets from Moderate Resolution Imaging Spectroradiometer (MODIS), and in situ SM observations spanning from April 2015 to December 2018 were employed in the evaluation. Spatial dynamic range, energy conservation, and precision preservation analyses were conducted as evaluation methods. Results demonstrated that the PH-ML series mode outperformed the other coupling modes, which was even better than the better one of the PH and ML. The PH can first generate a valuable initial estimate, and the initial estimate’s advantages can be preserved, while its errors can be suppressed by the ML. Resultantly, the ideology of first using the PH to obtain an initial estimate and then putting the initial estimate into the ML is suggested for further microwave SM downscaling.","['Microwave theory and techniques', 'Spatial resolution', 'Random forests', 'Land surface', 'Satellites', 'Microwave measurement', 'Machine learning algorithms']","['Coupling', 'downscaling', 'machine learning (ML)', 'physical model', 'remote sensing', 'soil moisture (SM)']"
"We introduce a comprehensive method for space-borne 3-D volumetric scattering-tomography of cloud microphysics, developed for the CloudCT mission. The retrieved microphysical properties are the liquid-water-content (LWC) and effective droplet radius within a cloud. We include a model for a perspective polarization imager and an assumption of 3-D variation of the effective radius. Elements of our work include computed tomography initialization by a parametric horizontally uniform microphysical model. This results in smaller errors than the prior art. The mean absolute errors of the retrieved LWC and effective radius are reduced from 62% and 28% to 40% and 9%, respectively. The parameters of this initialization are determined by a grid search of a cost function. Furthermore, we add viewpoints in the cloudbow region, to better sample the polarized scattering phase function. The suggested advances are evaluated by retrieval of a set of clouds generated by large-eddy simulations.","['Clouds', 'Three-dimensional displays', 'Cloud computing', 'Scattering', 'Tomography', 'Computed tomography', 'Spatial resolution']","['3-D scattering-tomography', 'clouds', 'initialization', 'polarization', 'pySHDOM']"
"The AisaOWL is a recent-to-market thermal hyperspectral instrument. As such, there is little information about the sensor performance in the literature. The sensor covers the 7.6-12.6 μm part of the long-wave infrared region with 102 continuous bands, and is capable of imaging in low-light conditions. This paper presents an independent characterization of the AisaOWL sensor, examining the spectral accuracy of black body measurements at different temperatures and validating manufacturer recommendations for warm-up, integration, and calibration times. This analysis is essential for establishing high quality operational procedures and in giving confidence to users of the data. In this paper, the sensor has been found to have a maximum error of 2 °C in absolute temperature measurement, and provides spectra most accurate in the 8-9 μm region. The recommended warm-up time of 15 min has been confirmed, with a 1% increase in error identified for data collected only 7 min after switch on. The optimal integration time of 1.18 ms has been validated and an exponential decrease in performance observed outside the 0.85-1.2 ms range. The detector used by the sensor is shown to have stability issues and this has been examined by comparing black body data processed with different calibration data. While the detector is operating in a stable regime compatible with the calibration, these black body readings stay within 5% across the central bands, approaching 10% below 8 μm and just exceeding 20% above 11 μm.","['Temperature measurement', 'Calibration', 'Detectors', 'Temperature sensors', 'Wavelength measurement', 'Hyperspectral imaging']","['AisaOWL', 'calibration', 'hyperspectral', 'thermal']"
"Geolocation accuracy is a critical issue for remote sensing applications. To achieve subpixel accuracy, geolocation errors need to be systematically identified and corrected. In this study, we propose a geometric sensor model for FengYun-3D (FY-3D) MERSI-II, a second-generation visible (VIS)/infrared (IR) spectroradiometer, to generate the geolocation lookup table (GLT). The geometric sensor model retrieves the imaging rays from the focal plane to the K-mirrors, 45° scanning mirrors, the platform, and the earth’s surface. After refining the attitude errors with ground control points (GCPs), the rigorous sensor model can achieve subpixel geolocation accuracy. However, significant systematic geolocation errors were identified from the residuals, especially for the area with large view angles. To study the errors of MERSI-II, we proposed a homogenous coordinate in the focal plane. As proven by both theory and experiments, the attitudes were adjusted to a wrong value and introduced systematic errors when there were principal point errors. The pitch angle error of K-mirrors caused the oscillation in the flight direction. The principal distance error introduced line coordinate-related error in the flight direction. Meanwhile, the initial phase angle error between the K-mirror and 45° scanning mirrors caused the line coordinate-related errors in the scanning direction. After correcting all the above-mentioned errors, the systematic geolocation errors of MERSI-II were removed. With 23 independent datasets, the root mean square errors (RMSEs) of 250 m bands were approximately 0.4 pixels, 100 m at nadir.","['Geology', 'Mirrors', 'Imaging', 'Detectors', 'Satellite broadcasting', 'Systematics', 'Earth']","['Geolocation', 'geometric sensor model', 'medium resolution spectrum imager-II (MERSI-II)']"
"Many atmospheric correction schemes of radiance-based optical satellite data require the selection of normalized solar spectral irradiance models at the top of atmosphere (TOA). However, there is no scientific consensus in literature as to which available model is most suitable. This article examines five commonly used models applied to Landsat 8 Operational Land Imager (OLI) TOA radiance and reflectance products to assess the accuracy and stability between models used to derive surface reflectance products. It is assumed that the calibration of the United States Geological Survey (USGS) Landsat 8 OLI TOA reflectance and radiance products are accurate to currently claimed levels. The results show that the retrieved surface reflectance can exhibit significant variations when different solar irradiance models are used, especially in the OLI coastal blue band at 443 nm. From the five solar irradiance models, the Kurucz 2005 model showed the least bias compared with OLI TOA reflectance product and least variance in surface reflectance. Furthermore, improvement was obtained by adjusting the total solar irradiance (TSI) normalization, and additional validation was provided using observed in situ water leaving reflectance data. The results from this article are particularly relevant to aquatic applications and to satellite sensors that provide TOA radiance such as previous Landsat and other current and historical missions.","['Atmospheric modeling', 'Earth', 'Artificial satellites', 'Remote sensing', 'Calibration', 'Data models', 'Atmospheric waves']","['Atmospheric correction', 'Landsat 8 Operational Land Imager (OLI)', 'sensor calibration', 'solar irradiance', 'spectral solar constant']"
"High-resolution optical cameras have always been important scientific payloads in Mars exploration missions, and the Mars topographic data produced by their detection data can provide support for scientific research on Mars geomorphology and geological structure evolution. As of December 2021, there are still relatively few high-resolution image data at the submeter level on the Martian surface, with about 2.6% global coverage and even more limited stereo coverage (just about 0.4%). At the same time, there are still some difficulties in data acquisition and terrain reconstruction processing methods for high-resolution Mars images that need to be solved. This article described how we designed the in-orbit stereo imaging strategy based on the characteristics of the high-resolution imaging camera (HiRIC) of China’s first Mars exploration mission (Tianwen-1), studied the technical solutions for HiRIC stereo image photogrammetry processing, and produced a topographic dataset for the “Tianwen-1” landing area, including a digital orthophoto map (DOM) with a ground sample distance (GSD) of 0.7 and 3.5 m and a digital elevation model (DEM) with a GSD of 3.5 m. Precision analysis results show that these topographic data have good consistency in planar position and elevation compared with the existing Mars terrain data and have advantages in spatial resolution and terrain detail expression, which will be widely used in the geological background study of the “Tianwen-1” landing area, as well as landing site positioning, Martian surface remote operation planning, and other Mars scientific research and engineering tasks.","['Mars', 'Imaging', 'Surface topography', 'Planetary orbits', 'Interplanetary exploration', 'Cameras', 'Spatial resolution']","['Digital topographic models', 'high-resolution imaging camera (HiRIC)', 'photogrammetry', 'stereo imaging', 'Tianwen-1', 'topographic dataset']"
"Active faults release tectonic stress imposed by plate motion through a spectrum of slip modes, from slow, aseismic slip, to dynamic, seismic events. Slow earthquakes are often associated with tectonic tremor, nonimpulsive signals that can easily be buried in seismic noise and go undetected. We present a new methodology aimed at improving the detection and location of tremors hidden within seismic noise. After identifying tremors with a classic convolutional neural network (CNN), we rely on neural network attribution to extract core tremor signatures. We observe that the signals resulting from the neural network attribution analysis correspond to a waveform traveling in the Earth’s crust and mantle at wavespeeds consistent with seismological estimates. We then use these waveforms signatures to locate the source of tremors with standard array-based techniques. We apply this method to the Cascadia subduction zone, where we identify tremor patches consistent with existing catalogs. This approach allows us to extract small signals hidden within the noise, and to locate more tremors than in existing catalogs.","['Spectrogram', 'Earthquakes', 'Convolutional neural networks', 'Training', 'Standards', 'Licenses', 'Feature extraction']","['Neural network attribution', 'tremor location', 'waveform extraction']"
"In latest years, deep learning (DL) has gained a leading role in the pansharpening of multiresolution images. Given the lack of ground truth data, most DL-based methods carry out supervised training in a reduced-resolution domain. However, models trained on downsized images tend to perform poorly on high-resolution target images. For this reason, several research groups are now turning to unsupervised training in the full-resolution domain, through the definition of appropriate loss functions and training paradigms. In this context, we have recently proposed a full-resolution training framework that can be applied to many existing architectures. Here, we propose a new DL-based pansharpening model that fully exploits the potential of this approach and provides cutting-edge performance. Besides architectural improvements with respect to previous work, such as the use of residual attention modules, the proposed model features a novel loss function that jointly promotes the spectral and spatial quality of the pansharpened data. In addition, thanks to a new fine-tuning strategy, it improves inference-time adaptation to target images. Experiments on a large variety of test images, performed in challenging scenarios, demonstrate that the proposed method compares favorably with the state-of-the-art both in terms of numerical results and visual output. The code is available online at https://github.com/matciotola/Lambda-PNN .","['Pansharpening', 'Training', 'Image resolution', 'Spatial resolution', 'Sensors', 'Multiresolution analysis', 'Correlation']","['Deep learning (DL)', 'image enhancement', 'image fusion', 'image registration', 'super resolution']"
"Gravity measurement is an important geophysical prospecting method for mineral exploration. With most of the shallow ore deposits in China being exploited, the future targets for exploration will be aimed at the deep mineral resources about 4-km underground. To improve the resolving ability in the vertical direction of the 3-D density inversion to find deep-source minerals, we propose to achieve the inversion of gravity anomalies by combining power-spectrum derived adaptive weighting and cross-gradient regularization. First, spectral analysis is utilized to conduct the source-depth separation, and the adaptive weighting function is designed depending on the slope of the radial logarithmic power spectrum of the gravity anomaly, which can enhance the correspondence for the field sources with different depths to improve the vertical resolution relatively. Thus, each separated source with different depths could be given an adaptive weighting coefficient in each separated inversion. Second, the cross-gradient technique is introduced as a structural constraint in the objective function to further constrain the separated inversion process. By performing two-cuboid-source cases, it can be seen that this combination calculated approach has great potential to improve the quality of the inversion results. When examining the inversion of the actual gravity data from certain ore district in the west of Zhen’an, South Qinling, we speculate that a potential deep ore body would exist in the deposit via utilizing the recovered density model obtained by the proposed method.","['Gravity', 'Three-dimensional displays', 'Linear programming', 'Data models', 'Licenses', 'Kernel', 'Fitting']","['Adaptive weighting function', 'combination inversion', 'gravity anomaly', 'logarithmic power spectrum']"
"Signal-phase measurements from global navigation satellite systems (GNSSs) have become an important tool for various remote sensing applications, including measuring ionosphere plasma content, atmospheric radio occultation, and water and ice reflectometry. In these types of scenarios, GNSS signals often experience harsh propagation conditions, such as low signal-to-noise ratios, multipath, and semicoherent scattering. These conditions, in turn, lead to the frequent occurrence of cycle slips, which manifests as persistent discrete changes in the bias of the carrier phase measurement. In order to effectively use the precise GNSS phase measurements under such conditions, we argue that a window of high-rate measurements must be used. In addition, we suggest that enforcing sparsity in the occurrence of detected cycle slips can aid in detection. We, therefore, develop a batch cycle-slip detection and estimation method that is effective and computationally tractable under harsh signal conditions. This work focuses in particular on strong ionosphere scintillation, which is among the most difficult scenarios for estimating cycle slips. We demonstrate the effectiveness of our method on both simulated and real GNSS scintillation datasets, showing around a 90% reduction of slips.","['Phase measurement', 'Ionosphere', 'Global navigation satellite system', 'Receivers', 'Estimation', 'Testing', 'Noise measurement']","['Cycle slips', 'global navigation satellite system (GNSS)', 'remote sensing']"
"Satellite-based microwave sensors that respond to the vertical distribution of hydrometeors have been continuously employed in the investigation of precipitation systems characteristics. Rain/no-rain classification (RNC) methods often are either applied before retrieving precipitation information from a number of algorithms based on passive microwave measurements or adopted to build the precipitation event-based databases. As a simple rain indicator, the polarized corrected temperature (PCT) at 89-GHz (PCT89) method using the global precipitation measurement (GPM) microwave imager (GMI) has been employed by many researchers, because it can estimate the scattering intensity while minimizing the effects of the surface emissivity at high resolution. This article presents a new consideration using the PCT89-based RNC method through statistical verification. Precipitating clouds were subdivided into 11 types (three stratiform types and four convective types) by the GPM dual frequency precipitation radar (DPR) precipitation classification algorithm. Quantitative comparison of verification results was performed in the tropics from January to December 2015 and major sources of uncertainty were analyzed from the perspective of the precipitation mechanism. Results showed a tendency of false identification for stratiform types except for those located near the convective core, and thus the method was susceptible to failure in the identification of convective types. Consequently, this method leads to an increase of 70% and 54% in the number of two significant stratiform types compared to DPR, while the convective types decreased by up to 53%. This article suggests that the precipitations identified by the PCT89 have features that enhance the bias toward the stratiform type.","['Clouds', 'Rain', 'Microwave measurement', 'Scattering', 'Microwave theory and techniques', 'Ice', 'Microwave imaging']","['Cloud type', 'hydrometeor', 'passive microwave', 'polarized corrected temperature (PCT)', 'rain/no-rain classification (RNC)']"
"Choosing how to encode a real-world problem as a machine learning task is an important design decision in machine learning. The task of the glacier calving front modeling has often been approached as a semantic segmentation task. Recent studies have shown that combining segmentation with edge detection can improve the accuracy of calving front detectors. Building on this observation, we completely rephrase the task as a contour tracing problem and propose a model for explicit contour detection that does not incorporate any dense predictions as intermediate steps. The proposed approach, called “Charting Outlines by Recurrent Adaptation” (COBRA), combines convolutional neural networks (CNNs) for feature extraction and active contour (AC) models for delineation. By training and evaluating several large-scale datasets of Greenland’s outlet glaciers, we show that this approach indeed outperforms the aforementioned methods based on segmentation and edge-detection. Finally, we demonstrate that explicit contour detection has benefits over pixel-wise methods when quantifying the models’ prediction uncertainties. The project page containing the code and animated model predictions can be found at https://khdlr.github.io/COBRA/ .","['Task analysis', 'Image edge detection', 'Active contours', 'Uncertainty', 'Predictive models', 'Computer architecture', 'Feature extraction']","['Active contours (ACs)', 'edge detection', 'glacier front', 'Greenland', 'uncertainty']"
"The observed phase in time series of interferometric synthetic aperture radar (InSAR) products is a superposition of various components. Differential topography, line-of-sight displacements, and differential atmospheric delays are the main contributions and need to be disentangled to derive accurate digital elevation model (DEM), deformation, or atmospherical products from InSAR. However, isolating the atmospheric component has been proven difficult as it is spatiotemporally highly dynamic and a superposition of two atmospheric states. Here, we propose an approach to parameterize the stochastic properties of the single-epoch atmospheric delay field as a way to define the atmospheric signal. We found that the atmospheric signal of a time series of interferograms can be characterized by structure functions, which can be used to isolate the single-epoch structure functions. Due to the scaling properties of the atmospheric signal, it is then possible to construct a parametric function per SAR acquisition, using two isotropic and three anisotropic parameters. In particular, the isotropic parameters for the short-distance variation and long-distance variation in atmospheric delay can be used to characterize the atmospheric signal. For a test set of 151 Sentinel-1 acquisitions, this results in an atmospheric energy range of about 10 for short-distance scales and about 50 for long-distance scales. Our parameterization demonstrates that we can describe the spatiotemporal variability of InSAR atmospheric delays, which provides a measure for atmospheric noise for individual epochs in deformation time series based on distance and azimuth.","['Delays', 'Atmospheric modeling', 'Atmospheric measurements', 'Deformation', 'Periodic structures', 'Meteorology', 'Liquids']","['Interferometric synthetic aperture radar (InSAR)', 'tropospheric delay', 'tropospheric scaling', 'turbulence']"
"Multisensor remote sensing applications require the registration of optical and synthetic aperture radar (SAR) images, which presents challenges due to significant radiometric and geometric differences resulting from distinct imaging mechanisms. Although various algorithms have been proposed, including hand-crafted features and deep learning networks, most of them focus on matching radiometric-invariant features while ignoring geometric differences. Furthermore, these algorithms often achieve promising results on datasets that use manually labeled ground truths that may be less reliable for high-resolution SAR images affected by speckle noise. To address these issues, we propose a robust global-to-local registration algorithm consisting of four modules: geocoding, global matching, local matching, and refinement. We generate a geometry-invariant mask in the geocoding module to help the local matching module focus on valid areas, introduce a fast global matching method to solve large offsets, and use matching confidence (MC) to guide subsequent local matching based on the accuracy of global matching. We propose a feature based on multidirectional anisotropic Gaussian derivatives (MAGDs) and embed it into the confidence-aware local matching with the geometry-invariant mask to reduce the effect of geometric differences. Finally, we refine correspondence positions and remove outliers. We also build a high-accuracy evaluation dataset with hundreds of image pairs, where the ground truth is obtained by meta poles, which have clear and reliable structures in both optical and SAR images. Experimental results on this dataset demonstrate the superiority of our proposed algorithm compared with several state-of-the-art methods.","['Optical imaging', 'Adaptive optics', 'Optical sensors', 'Radar polarimetry', 'Radiometry', 'Geometrical optics', 'Feature extraction']","['Dataset', 'image registration', 'multisensor', 'synthetic aperture radar (SAR)']"
"Although there are several methods described in the literature to accurately measure the complex permittivity of solid dielectrics at radio frequencies in a laboratory environment, none of these methods allow for a large-scale accurate characterization of natural ionic dielectrics. The work presented here reports results of the dielectric permittivity retrieved from in situ measurements in a Romanian salt mine. Measurements were performed in the 164-174-MHz bandwidth, over a propagation distance of 100 m. The characteristics of the layers of sedimentary salt are determined from measurements using a least mean square fitting algorithm, based on a detailed propagation model for a heterogeneous medium. The coupling of the instrumentation is also considered. The proposed approach demonstrates great promise for a large number of applications.","['Permittivity measurement', 'Permittivity', 'Antenna measurements', 'Frequency measurement', 'Dielectrics', 'Dipole antennas']","['Complex permittivity ionic crystals', 'radio wave propagation', 'salt mine']"
"Microwave scattering from forests generates pixel geolocation shifts in synthetic aperture radar (SAR) data that require an adequate representation within digital elevation models (DEMs) for preprocessing. We analyze the impact of DEM properties on the radiometry and geolocation of radiometric terrain corrected Copernicus Sentinel-1 imagery of forests to improve consistency in backscatter intensities for time series analyses. To account for the penetration depth of the C-band sensor, we approximate the structure of stands in a temperate deciduous forest using height percentiles from aerial laser scanning (ALS) point clouds in the Hainich National Park (HNP), Germany. Comparing the RTC results obtained using DEMs of SRTM, Copernicus, and ALS DEMs, the latter reduces topographically induced errors, resulting in visibly smaller effects from topography and spatially shifted information. Based on the P50 ALS vegetation elevation, the results show homogeneous intensities within the same orbit and reduce variance from 2.4 to 1.2 dB2 in the difference in mid-range data from ascending and descending azimuth directions. Over forest, we observe lower intensities on sensor-facing and increased intensities on away-facing slopes and correlations with the illuminated pixel area (IPA) and local incidence angle (LIA). We reduce this bias with linear regressions of intensity on IPA. ALS DEMs in RTC and the proposed regression correction increase the consistency of images across orbits, measured by the inter-orbit range (IOR), throughout the selected year at our study site. We suggest the proposed method applies to other areas, requiring further testing under different forest types and topography.","['Forestry', 'Radiometry', 'Orbits', 'Extraterrestrial measurements', 'Vegetation mapping', 'Backscatter', 'Synthetic aperture radar']","['Digital elevation model (DEM)', 'forest', 'radiometric terrain normalization', 'Sentinel-1', 'time series', 'viewing geometry']"
"An improved blackbody calibration procedure is developed, implemented, and tested for the cyclone global navigation satellite system (CYGNSS). Previously, CYGNSS calibrated its receivers once every minute to account for temperature-induced gain fluctuations. The time spent making calibration measurements limited the duty cycle of wind-speed measurements to approximately 90%. The analysis presented here shows that the 1-min cadence was overly conservative and can be increased to once every 10 min with minimal impact to data quality, thereby improving the wind-speed duty cycle to 98%. A permanent change to the blackbody cadence was made for the complete eight-satellite constellation during July 27, 2021–August 3, 2021, and subsequent analysis verifies that the new cadence improves duty cycle without impacting science data quality, as expected.","['Calibration', 'Ocean temperature', 'Temperature measurement', 'Sea measurements', 'Satellites', 'Extraterrestrial measurements', 'Space vehicles']","['Blackbody', 'calibration', 'cyclone global navigation satellite system (CYGNSS)', 'global navigation satellite system-reflectometry (GNSS-R)', 'remote sensing']"
"Based on the very-high-frequency (VHF) pulse signal detected by interferometer (INTF) antennas as the reference unit, a lightning location method based on pulse matching and peak extraction is proposed in this article. The first key step of this method is to optimize the raw VHF data of the INTF by using the ensemble empirical mode decomposition (EEMD) method to control the quality of the original signal by bandpass filtering to preserve only the relatively high-frequency components of 40–80 MHz. Then, through a combination of main and auxiliary windows, the matching of waveforms from different antennas is realized by means of generalized cross correlation. In a microscale window (11 ns), the pulse signals are further accurately matched, and the arrival time difference is calculated under threshold and similarity constraints. Finally, the 2-D coordinates of the matched pulsed radiation sources are obtained using the nonlinear least-squares method. Compared with the results of the traditional “centroid” approach, the number of radiation sources obtained with the proposed method is greatly increased. For lightning cases with different discharge intensities, the number of located radiation sources can be increased by a factor of 10–20, and for specific short-duration and rapidly changing discharge processes (such as dart leaders and K events), the number of located radiation sources can be increased by a factor of nearly 100. For INTF operation at 20–88 MHz with a sampling rate of 180 MHz, the analytical resolution of the lightning discharge process can be improved to 10 ns. When the radiation sources are fully located, the widths of the lightning channels can be successfully reconstructed from the density of no-size points representing the located radiation sources in space.","['Lightning', 'Optical interferometry', 'Broadband antennas', 'Broadband communication', 'Antennas', 'Uncertainty', 'Receiving antennas']","['Dart leader', 'ensemble empirical mode decomposition (EEMD)', 'K event', 'lightning observation', 'very-high-frequency (VHF) lightning interferometer (INTF)']"
"The thermal infrared brightness temperature (BT) of the eastern Tibetan Plateau (TP) was retrieved from the Moderate Resolution Imaging Spectroradiometer (MODIS) level-1B data. The multiyear averaged BT background field was subtracted from the punctual BT data to yield monthly BT spatial anomaly, and calculated time series of BT for the secondary blocks. Then, the spatial and temporal changes in the BT of the study area before the Menyuan M6.4, Zaduo M6.2, and Jiuzhaigou M7.0 earthquakes were investigated and analyzed based on the tectonic setting. The results show the following. The spatial BT radiation enhancement frequency rose remarkably before strong earthquakes; each of the three earthquakes was preceded by marked spatiotemporal continuous BT anomalies. The tectonic setting significantly influences the BT anomaly feature. The spatial BT anomaly was not notable in the Qaidam and Qilian block before the Menyuan earthquake; the spatial BT anomaly mainly appeared in the Qiangtang and Bayan Har blocks before the Zaduo and Jiuzhaigou earthquakes. The Qiangtang and Bayan Har block’s BT time series curves have similar features. The Qaidam and Qilian block’s BT time series curves have analogous shapes. The three earthquakes may be regarded as one seismic event induced by a stage of tectonic stress enhancement rather than three independent occasions. The spatial BT anomalous behavior before earthquakes is, to a great extent, like the result of the rock stress loading experiment; the rock compression and the lithosphere–atmosphere–ionosphere coupling (LAIC) may be the main reasons for the intensification of the BT radiation.","['Earthquakes', 'Rocks', 'MODIS', 'Satellites', 'Land surface', 'Spatiotemporal phenomena', 'Spatial resolution']","['Brightness temperature (BT)', 'Moderate Resolution Imaging Spectroradiometer (MODIS)', 'spatiotemporal BT anomaly', 'tectonic setting', 'Tibetan Plateau (TP)']"
"Wintertime Arctic surface emissivities are retrieved from Advanced Technology Microwave Sounder (ATMS) passive microwave measurements at 88.2, 165.5, and 183.31 GHz. Surface emitting layer temperatures are simultaneously retrieved at 183.31 GHz. Random errors in emissivities are estimated to be 2.0%, 2.0%, and 3.5% at 88.2, 165.5, and 183.31 GHz, respectively, and the random errors in surface emitting layer temperatures are 4.3 K. A series of tests on the retrieved products reveal that land and sea ice are Lambertian reflectors and ocean is a specular reflector. The retrieved emissivities show broad agreement with products from published databases, with differences partly due to the uncertainties in surface emitting layer temperatures. The geographical distribution of 165.5/183.31 GHz surface reflectance ratios over land and sea ice, which is important for the retrieval of microwave satellite water vapor column (WVC), is presented. Neglecting the geographical variations leads to random errors in retrieved wintertime Arctic WVCs of approximately 1.8% and 25% in the mid (1.5 - 9 kgm -2 ) and extended (8 - 15 kgm -2 ) slant column retrieval regimes, respectively. Choosing specular instead of Lambertian reflection in the surface emissivity retrievals over land and sea ice causes systematic WVC retrieval errors of up to -4.1%.","['Ocean temperature', 'Sea surface', 'Microwave measurement', 'Temperature measurement', 'Microwave FET integrated circuits', 'Microwave integrated circuits', 'Microwave radiometry']","['Atmospheric measurements', 'emission', 'microwave radiometry', 'millimeter wave radiometry', 'ocean', 'passive microwave remote sensing', 'reflectivity', 'sea ice', 'snow', 'surfaces']"
"Wave runup observations are key data for understanding coastal response to storms. Lidar scanners are capable of collecting swash elevation data at high spatial and temporal resolution in a range of environmental conditions. Efforts to develop automated algorithms that effectively separate returns off of the beach and the sea surface are complicated by environmental noise, thus requiring time-intensive data quality control or manual digitization. In this study, a fully convolutional neural network (FCNN) was trained and validated on 966 30-min lidar linescan time series of the beach and swash zone and tested on an additional 99 30-min linescan time series to improve the automated classification of lidar returns off of the beach and water, facilitating the extraction of a depth-defined wave runup time series. Lidar returns were classified as beach or water, and beach points at each cross-shore location were interpolated through time, creating a time-varying beach elevation surface that was used to calculate instantaneous swash depths. Runup was defined as the most landward position of 3-cm water depth through time. Overall, the runup time series determined using the manually and machine learning (ML)-digitized beach–water interface agreed well (0.02-m root-mean-square difference (RMSD) 3-cm contour location and 0.06-m RMSD 2% runup exceedance elevation R2%). The trained model was found to be robust to noise and moderate data gaps and applicable in a range of wave conditions. Results demonstrate the potential of the ML model to replace manual data processing steps and significantly reduce the time and effort required to extract the instantaneous runup location from lidar linescan time series.","['Laser radar', 'Sea measurements', 'Manuals', 'Data mining', 'Sea surface', 'Storms', 'Surface waves']","['Machine learning (ML) algorithms', 'oceans', 'remote sensing']"
"In seismic exploration methods, imperfect spatial sampling at the surface causes a lack of illumination at the target in the subsurface. The hampered image quality at the target area of interest causes uncertainties in reservoir monitoring and production, which can have a substantial economic impact. Especially in the case of a complex overburden, the impact of surface sampling on target illumination can be significant. The target-oriented acquisition analysis based on wavefield propagation and a known velocity model has been used to provide guidance for optimizing the acquisition parameters. Seismic acquisition design is usually a manual optimization process, with consideration of many aspects. In this study, we develop a methodology that automatically optimizes an irregular receiver geometry when the source geometry is fixed or vice versa. The methodology includes objective functions defined by two criteria: optimizing the image resolution and optimizing the angle-dependent illumination information. We use a two-step parameterization in order to make the problem more linear and, thereby, solve the acquisition design problem by using a gradient descent algorithm. With simple and complex velocity models, we demonstrate that the proposed method is effective, while the involved computational cost is acceptable. Interestingly, the optimization results in our examples show that the conventional uniform geometry already satisfies the resolution requirement, while optimizing for angle coverage can provide a large uplift and is strongly dependent on the velocity model.","['Receivers', 'Geometry', 'Lighting', 'Mathematical models', 'Acoustic beams', 'Computational modeling', 'Analytical models']","['Computational seismology', 'controlled source seismology', 'image processing', 'inverse theory', 'seismic instruments']"
"Current efforts for improving the hyperspectral optimization processing exemplar (HOPE) model include further testing of remote sensing reflectance ( $R_{\mathrm {rs}}$ ) features containing useful information for bathymetry retrieval via the minimization of the interference stemming from the variability in inherent optical properties and benthic reflectance. In this article, we detected a novel feature originating from the pure water absorption within the narrow spectral region of 570–600 nm. In most coastal regions of clear water in coral reefs, for example, in a coral reefs environment, pure water accounts for the majority of the total absorption in this spectral range. In addition to the depth variation, the spectral behavior of $R_{\mathrm {rs}}$ (570–600) is primarily dominated by a steep increase in pure water absorption with wavelength, whereas the influence of other optical properties, such as phytoplankton/colored dissolved organic matter (CDOM) absorption, particle backscattering, and benthic reflectance, can be simplified using the spectrally constant shape model. An HOPE pure water (HOPE-PW) algorithm using this feature was developed based on $R_{\mathrm {rs}}$ measurements with a spectral resolution of near 3.5 nm, in which only four uncertainties must be resolved. The validation from light detection and ranging (LiDAR) data and comparison with HOPE-bottom reflectance unmixing computation of the environment (BRUCE) using portable remote imaging radiometer (PRISM) data at 15 sites located in five distinct regions of Palau, Guam, Great Barrier Reef, Hawaiian Islands, and Florida Key confirmed that the HOPW-PW algorithm yielded a considerable performance and provided adequate transferability to other sites with varying bottom and water environments. Furthermore, the sensitivity analysis based on Hydrolight-simulated datasets was carried out and showed that HOPE-PW was less affected by variation of bottom types, but still had some limitations in retrieving water optical properties.","['Reflectivity', 'Hyperspectral imaging', 'Bathymetry', 'Laser radar', 'Data models', 'Water', 'Sea measurements']","['Airborne', 'bathymetry', 'hyperspectral', 'model inversion', 'pure water', 'spectrally constant model']"
"This paper presents an open-source canopy height profile (CHP) toolkit designed for processing small-footprint full-waveform LiDAR data to obtain the estimates of effective leaf area index (LAIe) and CHPs. The use of the toolkit is presented with a case study of LAIe estimation in discontinuous-canopy fruit plantations. The experiments are carried out in two study areas, namely, orange and almond plantations, with different percentages of canopy cover (48% and 40%, respectively). For comparison, two commonly used discrete-point LAIe estimation methods are also tested. The LiDAR LAIe values are first computed for each of the sites and each method as a whole, providing “apparent” site-level LAIe, which disregards the discontinuity of the plantations' canopies. Since the toolkit allows for the calculation of the study area LAIe at different spatial scales, between-tree-level clumping can be easily accounted for and is then used to illustrate the impact of the discontinuity of canopy cover on LAIe retrieval. The LiDAR LAIe estimates are therefore computed at smaller scales as a mean of LAIe in various grid-cell sizes, providing estimates of “actual” site-level LAIe. Subsequently, the LiDAR LAIe results are compared with theoretical models of “apparent” LAIe versus “actual” LAIe, based on known percent canopy cover in each site. The comparison of those models to LiDAR LAIe derived from the smallest grid-cell sizes against the estimates of LAIe for the whole site has shown that the LAIe estimates obtained from the CHP toolkit provided values that are closest to those of theoretical models.","['Vegetation', 'Laser radar', 'Estimation', 'Indexes', 'Photography', 'Cogeneration', 'Modeling']","['Aggregation', 'airborne full-waveform LiDAR', 'discontinuous-canopy cover', 'effective leaf area index (LAIe)', 'saturation', 'simulation', 'small-footprint']"
"Deep learning models have achieved remarkable success in many different fields and attracted many interests. Several researchers attempted to apply deep learning models to synthetic aperture radar (SAR) data processing, but it did not have the same breakthrough as the other fields, including optical remote sensing. SAR data are in complex domain by nature and processing them with real-valued (RV) networks neglects the phase component which conveys important and distinctive information. A complex-valued (CV) end-to-end deep network is developed in this study for the reconstruction and classification of CV-SAR data. Azimuth subaperture decomposition is utilized to incorporate physics-aware attributes of the CV-SAR into the deep model. Moreover, the correlation coefficient amplitude (coherence) of the CV-SAR images depends on the SAR system characteristics and physical properties of the target. This coherency should be considered and preserved in the processing chain of the CV-SAR data. The coherency preservation of the CV deep networks for CV-SAR images, which is mostly neglected in the literature, is evaluated in this study. Furthermore, a large-scale CV-SAR annotated dataset for the evaluation of the CV deep networks is lacking. A semantically annotated CV-SAR dataset from Sentinel-1 single look complex stripmap mode data [S1SLC_CVDL (complex-valued deep learning) dataset] is developed and introduced in this study. The experimental analysis demonstrated the better performance of the developed CV deep network for CV-SAR data classification and reconstruction in comparison with the equivalent RV model and more complicated RV architectures, as well as its coherency preservation and physics-aware capability.","['Synthetic aperture radar', 'Data models', 'Radar polarimetry', 'Image reconstruction', 'Deep learning', 'Decoding', 'Training']","['Annotated benchmark dataset', 'classification', 'coherency preservation', 'complex-valued (CV) neural network', 'deep learning', 'physics-aware network', 'reconstruction', 'synthetic aperture radar (SAR)']"
"The Advanced Topographic Laser Altimeter System (ATLAS) laser altimeter aboard the Ice, Cloud, and Land Elevation Satellite (ICESat-2) can measure the elevation of the Earth’s surface with unprecedented spatial detail. However, the quality of the derived signal and ground photons depends on the signal-to-noise ratio and canopy coverage. Current algorithms underperform for data collected during daytime over mountain areas with dense canopy. We demonstrate a novel procedure for signal photon detection and subsequent ground photon detection from ICESat-2 ATL03 data. We first introduce a gravity-based density model to characterize the anisotropic properties of photon distribution. Through jointly using the photon densities from the weak–strong beam pair, we are able to find key photons that have high probability being signals. A directional regional growing approach then takes these key photons as seeds to label all remaining signal photons. Finally, we introduce a weighted iterative median filter (WIMF) algorithm to identify ground photons whose height is closest to the estimated ground surface. A total of 36 ATL03 beams of two entire counties in USA are used for test and evaluation. Compared to the ATL03 and ATL08 algorithms, our signal photon finding method is more robust to the variation of topography, canopy coverage, and data collection time. Remarkably, the mislabeling caused by the after-pulsing effect does not present in our detected signal photons. Comparing current ATL03 and ATL08 products, the detected ground photons from our method are more consistent with reference to the 3DEP DEM, especially for strong beam data collected during daytime in dense canopy, high relief areas.","['Photonics', 'Measurement by laser beam', 'Laser beams', 'Signal to noise ratio', 'Surface topography', 'Sea surface', 'Laser radar']","['Advanced Topographic Laser Altimeter System (ATLAS)', 'after-pulsing effect', 'ATL03', 'ATL08', 'gravity-based density', 'Ice', 'Cloud', 'and Land Elevation Satellite (ICESat-2)', 'photon counting lidar']"
"The camera response function (CRF) that projects hyperspectral radiance to the corresponding RGB images is important for most hyperspectral image super-resolution (HSI-SR) models. In contrast to most studies that focus on improving HSI-SR performance through new architectures, we aim to prevent the model performance drop by learning the CRF of any given HSIs and RGB image from the same scene in an unsupervised manner, independent of the HSI-SR network. Accordingly, we first decompose the given RGB image into endmembers and an abundance map using the Dirichlet autoencoder architecture. Thereafter, a linear CRF learning network is optimized to project the reference HSIs to the RGB image that can be similarly decomposed like the given RGB, assuming that objects in both images share the same endmembers and abundance map. The quality of the RGB images generated from the learned CRFs is compared with that of the corresponding ground-truth images based on the true CRFs of two consumer-level cameras Nikon 700D and Canon 500D. We demonstrate that the effectively learned CRFs can prevent significant performance drop in three popular HSI-SR models on RGB images from different categories of standard datasets of CAVE, ICVL, Chikusei, Cuprite, Salinas, and KSC. The successfully learned CRF using the method proposed in this study would largely promote a wider implementation of HSI-SR models since tremendous performance drop can be prevented practically.","['Cameras', 'Hyperspectral imaging', 'Image reconstruction', 'Spatial resolution', 'Imaging', 'Superresolution', 'Image color analysis']","['Abundance map', 'camera response function (CRF)', 'endmember', 'hyperspectral image (HSI)', 'super-resolution', 'unsupervised deep learning']"
This article describes the development and validation of a statistical algorithm to retrieve global all-weather sea surface wind speed (GAWS) from microwave radiometers in operational environment. Measurements from the Advanced Microwave Scanning Radiometer-2 (AMSR2) are utilized to demonstrate the efficacy of the new all-weather wind speed data product. The GAWS algorithm exploits the linear combination of dual-polarized radiometer channels to significantly mitigate the effect of rain contamination while maintaining sensitivity to all wind speed regimes from global winds to tropical cyclone (TC) conditions. The GAWS algorithm was developed using≈1000AMSR2 orbits from 2013 to 2021 covering all possible variations of brightness temperatures and wind speeds. The Global Data Assimilation System (GDAS) and the Hurricane Weather Research and Forecasting Model (HWRF) were used as the assumed surface truth for training and validation. Results from comprehensive quantitative and qualitative analyses show that the GAWS retrievals are less susceptible to rain than standard microwave radiometer wind speeds and can reach hurricane force winds up to hurricane category 5 (>70 m/s).,"['Microwave radiometry', 'Rain', 'Sea surface', 'Wind speed', 'Ocean temperature', 'Pollution measurement', 'Sea measurements']","['Advanced Microwave Scanning Radiometer-2 (AMSR2)', 'all weather', 'microwave radiometers', 'ocean surface wind speed', 'rain', 'tropical cyclones (TCs)']"
"Unmanned aerial vehicles (UAVs) are now widely applied to data acquisition due to its low cost and fast mobility. With the increasing volume of aerial videos, the demand for automatically parsing these videos is surging. To achieve this, current research mainly focuses on extracting a holistic feature with convolutions along both spatial and temporal dimensions. However, these methods are limited by small temporal receptive fields and cannot adequately capture long-term temporal dependencies that are important for describing complicated dynamics. In this article, we propose a novel deep neural network, termed Fusing Temporal relations and Holistic features for aerial video classification (FuTH-Net), to model not only holistic features but also temporal relations for aerial video classification. Furthermore, the holistic features are refined by the multiscale temporal relations in a novel fusion module for yielding more discriminative video representations. More specially, FuTH-Net employs a two-pathway architecture: 1) a holistic representation pathway to learn a general feature of both frame appearances and short-term temporal variations and 2) a temporal relation pathway to capture multiscale temporal relations across arbitrary frames, providing long-term temporal dependencies. Afterward, a novel fusion module is proposed to spatiotemporally integrate the two features learned from the two pathways. Our model is evaluated on two aerial video classification datasets, ERA and Drone-Action, and achieves the state-of-the-art results. This demonstrates its effectiveness and good generalization capacity across different recognition tasks (event classification and human action recognition). To facilitate further research, we release the code at https://gitlab.lrz.de/ai4eo/reasoning/futh-net .","['Three-dimensional displays', 'Spatiotemporal phenomena', 'Feature extraction', 'Task analysis', 'Remote sensing', 'Convolution', 'Streaming media']","['Aerial video classification', 'convolutional neural networks (CNNs)', 'holistic features', 'temporal relations', 'two-pathway', 'unmanned aerial vehicle (UAV)']"
"From May 15 to October 11, 2019, six Saildrone uncrewed surface vehicles (USVs) were deployed for 150-day cruises collecting a suite of atmospheric and oceanographic measurements from Dutch Harbor, Alaska, transiting the Bering Strait into the Chukchi Sea and the Arctic Ocean. Two Saildrones funded by the National Aeronautics and Space Administration (NASA), SD-1036 and SD-1037, were equipped with infrared (IR) radiation pyrometers in a “unicorn” structure on the deck for the determination of the ocean sea surface skin temperature (SST _{\mathrm {skin}}). We present an algorithm to derive SSTskin from the downward- and upward-looking radiometers and estimate the main contributions to the inaccuracy of SSTskin. After stringent quality control of data and eliminating measurements influenced by sea ice and precipitation, and restricting the acceptable tilt angles of the USV based on radiative transfer simulations, SSTskin can be derived to an accuracy of approximately 0.12 K. The error budget of the derived SSTskin is developed, and the largest component comes from the instrumental uncertainties, assuming that the viewing geometry is adequately determined. Thus, Saildrones equipped with these sensors could provide sufficiently accurate SSTskin retrievals for studying the physics of the thermal skin effect, in conjunction with accurate subsurface thermometer measurements, and for validating satellite-derived SSTskin fields at high latitudes.","['Ocean temperature', 'Temperature measurement', 'Sea measurements', 'Skin', 'Sea surface', 'Atmospheric measurements', 'Sensors']","['Arctic', 'infrared (IR) radiometer', 'Saildrone', 'sea surface skin temperature (SSTskin)']"
"Satellite ocean color products derived from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi National Polar-orbiting Partnership (SNPP) and the National Oceanic and Atmospheric Administration (NOAA)-20, and the Ocean and Land Colour Instrument (OLCI) on the Sentinel-3A (S3A) and Sentinel-3B (S3B) have been widely used for surveillance of the ocean environment and research on ocean physical, biological, biogeochemical, and ecological processes. However, either VIIRS or OLCI daily ocean color images are often incomplete in spatial coverage due to cloud cover, contamination of high sun glint, narrow swath width, high sensor-zenith angle, high solar-zenith angle, and/or other unfavorable retrieval conditions. Although merging daily ocean color images from multiple satellite sensors can help reduce the number of invalid pixels, gap-filling methods, such as the data interpolating empirical orthogonal function (DINEOF), are often used to reconstruct invalid pixels and generate gap-free images. The 9-km spatial resolution global gap-free ocean color data have been routinely produced by the NOAA Ocean Color Science Team and distributed through NOAA CoastWatch ( https://coastwatch.noaa.gov/cw/index.html ). In this study, we aim to develop and produce improved spatial resolution gap-free products, including chlorophyll-a (Chl-a) concentration, diffuse attenuation coefficient at the wavelength of 490 nm [ K_{d}(490)], and suspended particulate matter (SPM) concentration for spatial resolutions of 0.5-, 1-, and 2-km. Two-sensor (VIIRS-SNPP and VIIRS-NOAA-20), three-sensor (two-sensor + OLCI-S3A), and four-sensor (three-sensor + OLCI-S3B) daily merged global Level-3 ocean color data are created and compared. It is found that, by merging data from the two VIIRS sensors, ~38% more valid ocean product data are retrieved compared with a single sensor from either SNPP or NOAA-20. Adding OLCI-S3A to the two-sensor merged data can increase the number of valid pixels by ~12%, and adding OLCI-S3B to the three-sensor merged data can further increase the number of valid pixels by ~8%. The DINEOF method is applied to daily two-, three-, and four-sensor merged data to generate global 2-km resolution gap-free images. Results show that 2-km resolution gap-free images are able to resolve fine ocean features, such as coastal eddies and filaments, which are not available in the 9-km resolution images. While adding OLCI-S3A data significantly improves the three-sensor-derived gap-free images over the two-sensor images, no significant enhancement is found in the four-sensor-derived gap-free images by adding OLCI-S3B data. The DINEOF method is also applied to 1- and 0.5-km resolution four-sensor merged Chl-a, K_{d}(490), and SPM images in the Gulf of Mexico and U.S. West Coast region. It is found that both 0.5- and 1-km resolution images show more detailed ocean structures and features in coastal regions.","['Oceans', 'Image color analysis', 'Sensors', 'Spatial resolution', 'Satellites', 'Sea measurements', 'US Government agencies']","['Data interpolating empirical orthogonal function (DINEOF)', 'global gap-free data', 'high-resolution', 'Ocean and Land Colour Instrument (OLCI)', 'ocean color', 'satellite remote sensing', 'Visible Infrared Imaging Radiometer Suite (VIIRS)']"
"Recently, deep learning methods have made significant progress in solving hyperspectral image (HSI) classification problems of high-dimensional features, band redundancy, and spectral mixture. However, the deep neural network is too complex, with a long training time and high energy consumption, making it difficult to deploy on edge computing devices. In order to solve the above problems, this article proposes a brain-inspired computing framework based on the spiking leaky integrate-and-fire neuron model for HSIs’ classification. Then, we design an approximate derivative algorithm to solve the nondifferentiable spike activity of the spiking neuron. The framework uses direct coding to generate spatiotemporal spikes for input HSI and achieves efficient extraction of spatial–spectral features through spiking standard convolution and spiking depthwise separable convolution. Extensive experiments are performed on four benchmark hyperspectral datasets and two public unmanned aerial vehicle-borne hyperspectral datasets. Experiments show that the proposed model has the advantages of high classification accuracy and fewer spiking time steps. The proposed model can save about ten times computational energy consumption compared with the CNN of the same architecture. This research has great significance for overcoming the technical bottleneck of HSI classification based on brain-inspired computing, solving the critical problems of mobile computing in unmanned autonomous systems, and realizing the engineering application of unmanned aerial vehicles and software-defined satellites. The source code will be made available at https://github.com/Katherine-Cao/HSI_SNN .","['Neurons', 'Feature extraction', 'Computational modeling', 'Training', 'Convolutional neural networks', 'Hyperspectral imaging', 'Brain modeling']","['Approximate derivative algorithm', 'brain-inspired computing', 'hyperspectral image~(HSI) classification', 'neuromorphic computing', 'spiking neural network (SNN)']"
"In infrared search and tracking (IRST) systems, small target detection is challenging because IR imaging lacks feature information and has a low signal-to-noise ratio. The recently studied small IR target detection methods have achieved high detection performance without considering execution time. We propose a fast and robust single-frame IR small target detection algorithm while maintaining excellent detection performance. The augmented infrared intensity map based on the standard deviation speeds up small target detection and improves detection accuracy. Density-based clustering helps to detect the shape of objects and makes it easy to identify centroid points. By incorporating these two approaches, the proposed method has a novel approach to the small target detection algorithm. We have self-built 300 images with various scenes and experimented with comparing other methods. Experimental results demonstrate that the proposed method is suitable for real-time detection and effective even when the target size is as small as 2 pixels.","['Object detection', 'Standards', 'Real-time systems', 'Shape', 'Neural networks', 'Target tracking', 'Sea measurements']","['Density-based spatial clustering', 'image gradient', 'infrared (IR) image', 'small target detection']"
"This article reports on the initial analysis of spectral smile calibration of the Hyperspectral Imager Suite (HISUI) onboard the International Space Station, which has been continuously acquiring data since September 4, 2020. HISUI is an optical hyperspectral imager consisting of two subsystems: VNIR covering 400–980 nm at intervals of 10 nm and SWIR covering 895–2481 nm at 12.5-nm intervals. Based on the atmospheric correction for actual observation images, we assessed cross-track dependences of the wavelength deviation (spectral smile) and the full-width at half-maximum (FWHM) of the HISUI response function. We found that significant spectral smile was observed, with maximum variations of 1.8 nm in VNIR and 4.3–4.5 nm in SWIR. In addition, the cross-track variation of FWHM was observed with maximum variations of 5.0 nm for VNIR and 2.5–3.5 nm for SWIR. We used the results to model the smile functions to update a smile correction table in the internal calibration system of HISUI. Then, we evaluated how the smile functions reduce the spectral smile in the data acquired after the update on September 27, 2021. We confirmed that VNIR showed a nearly flat profile within 0.25 nm with a nearly constant FWHM. For SWIR, although a slight amount of spectral smile and a variation of FWHM were still observed partly due to wavelength dependence in the spectral smile, the spectral smile was reduced to less than ~2.2 nm. This study demonstrated that wavelength calibration using actual observation images for ground surfaces is important for the characterization of hyperspectral sensors.","['Calibration', 'Hyperspectral imaging', 'Detectors', 'Absorption', 'Earth', 'Sensor phenomena and characterization', 'Optical surface waves']","['Full-width at half-maximum (FWHM)', 'Hyperspectral Imager Suite (HISUI)', 'hyperspectral sensor', 'response function', 'spectral smile', 'wavelength calibration']"
"Recent investigations using polarimetric decomposition and numerical models have helped to improve the understanding of how radar signals interact with lake ice. However, further research is needed on how radar signals are impacted by varying lake ice properties. Radiative transfer (RT) models provide one method of improving this understanding. These are the first published experiments using the snow microwave RT (SMRT) model to investigate the response of different frequencies (L-, C-, and X-band) at horizontal-horizontal (HH) and vertical-vertical (VV) polarizations using various incidence angles (20°, 30°, and 40°) to changes in ice thickness, porosity, bubble radius, and ice–water interface roughness. This is also the first use of SMRT in combination with a thermodynamic lake ice model. Experiments were for a lake with tubular bubbles and one without tubular bubbles under difference scenarios. An analysis of the backscatter response to different properties indicates that increasing ice thickness and layer porosity have little impact on backscatter from lake ice. X-band backscatter shows increased response to surface ice layer bubble radius; however, this was limited to other frequencies except at shallower incidence angles (40°). All three frequencies display the largest response to increasing root mean square (rms) height at the ice–water interface, which supports surface scattering at the ice–water interface as being the dominant scattering mechanism. These results demonstrate that the SMRT is a valuable tool for understanding the response of backscatter to changes in freshwater lake ice properties and could be used in the development of inversion models.","['Ice', 'Lakes', 'Backscatter', 'Microwave integrated circuits', 'Microwave FET integrated circuits', 'Mathematical models', 'Synthetic aperture radar']","['Lake ice', 'radar', 'radiative transfer (RT) model', 'synthetic aperture radar (SAR)']"
"Hoh Xil is an uninhabited extremity secluded on the Tibetan Plateau hinterland. A complete mapping of ground motion variation in Hoh Xil is essential for an in-depth understanding of the terrain’s responses to climate change on the Tibetan Plateau. However, the inaccessibility and extremely harsh environment impeded extensive field investigations on landform alteration and its formative process. Such difficulty can be resolved by interferometric synthetic aperture radar (InSAR), which enables a broad detection of subtle permafrost motions at millimeter precision. This study, for the first time, accomplished a multitemporal InSAR (MT-InSAR) deformation mapping from 2015 to 2020 in Hoh Xil, with a wide coverage of about 200 000 km2. 1592 Sentinel-1 images were processed based on the small baseline subset (SBAS) technique. The results show that Hoh Xil was experiencing dynamic permafrost disturbances. Thawing permafrost with both a linear subsidence rate higher than 2 mm/year and a periodic amplitude over 2 mm was primarily detected in areas of flat or gentle slopes. The InSAR cumulative deformation is highly correlated with permafrost thawing depth. Significant lag times were identified between seasonal oscillation of InSAR deformation and land surface temperature (LST). Thermokarst landforms of retrogressive thaw slumps and thermokarst lakes broadly formed and dynamically evolved as a consequence of permafrost degradation. Particularly, widespread thawing permafrost characterized by the spatial clustering of thermokarst lakes appeared to occur in areas adjacent to large lakes. The discovered dynamic permafrost disturbances in Hoh Xil manifested even the secluded Tibetan Plateau hinterland was facing the threat of climate change.","['Deformation', 'Lakes', 'Land surface temperature', 'Climate change', 'Degradation', 'Precipitation', 'Synthetic aperture radar']","['Climate change', 'Hoh Xil', 'interferometric synthetic aperture radar (InSAR)', 'permafrost', 'Tibetan Plateau', 'thermokarst']"
"Clouds play a key role in regulating climate change but are difficult to simulate within Earth system models (ESMs). Improving the representation of clouds is one of the key tasks toward more robust climate change projections. This study introduces a new machine-learning-based framework relying on satellite observations to improve understanding of the representation of clouds and their relevant processes in climate models. The proposed method is capable of assigning distributions of established cloud types to coarse data. It facilitates a more objective evaluation of clouds in ESMs and improves the consistency of cloud process analysis. The method is built on satellite data from the Moderate Resolution Imaging Spectroradiometer (MODIS) instrument labeled by deep neural networks with cloud types defined by the World Meteorological Organization (WMO), using cloud-type labels from CloudSat as ground truth. The method is applicable to datasets with information about physical cloud variables comparable to MODIS satellite data and at sufficiently high temporal resolution. We apply the method to alternative satellite data from the Cloud_cci project (ESA Climate Change Initiative), coarse-grained to typical resolutions of climate models. The resulting cloud-type distributions are physically consistent and the horizontal resolutions typical of ESMs are sufficient to apply our method. We recommend outputting crucial variables required by our method for future ESM data evaluation. This will enable the use of labeled satellite data for a more systematic evaluation of clouds in climate models.","['Climate change', 'Clouds', 'Machine learning', 'Modeling', 'Process control', 'Satellite communication', 'MODIS']","['Climate modeling', 'clouds', 'CloudSat', 'Cumulo dataset', 'ESA Cloud_cci', 'machine learning', 'Moderate Resolution Imaging Spectroradiometer (MODIS)', 'process-oriented model evaluation']"
"Some applications of data from the Clouds and the Earth's Radiant Energy System (CERES) scanning radiometer require the use of the point response function (PRF), which describes the influence of radiance from each point on the measurement. A radiance source for the measurement of the PRF of the CERES instruments was built and installed into the Radiometric Calibration Facility, in which the CERES instruments have been calibrated. The design and application of the PRF source and the computation of the PRF from these measurements are described. In order to compare the PRF based on measurements with the theoretical PRF, it is necessary to account for the finite size of the beam from the source. The use of the PRF source and the analysis of the data are demonstrated by application to the FM-5 instrument. The measured results compare well with theory for the CERES instruments and are presented for FM-5.","['Mirrors', 'Radiometers', 'Detectors', 'Extraterrestrial measurements', 'Temperature measurement', 'Earth']","['Aqua', 'calibration', ""Clouds and the Earth's Radiation Energy System (CERES)"", 'Earth radiation budget', 'National Polar-orbiting Partnersship (NPP)', 'point response function (PRF)', 'point spread function', 'radiometry', 'Terra']"
"CryoSat-2 (CS2) is the first mission equipped with a pulse-limited radar altimeter capable of operating in Synthetic Aperture Radar (SAR) Interferometric (SARIn) mode. Over ice sheets and ice caps, CS2 SARIn data have been used to retrieve surface elevations over an across-track ground “swath.” This work demonstrates that retracking multiple coherent peaks of CS2 SARIn waveforms, in combination with the interferometric phase, enables to obtain more than one valid height estimate from single SARIn waveforms over Arctic sea ice. For some SARIn waveforms, the scattering from sea ice at the satellite nadir is successfully separated from returns originating from off-nadir leads. An average bias of -1.8 cm is found for absolute sea ice elevations when using a 50% threshold retracker. It is shown that including multiple SARIn peaks and the associated phase difference in the processing does not introduce any bias on the average sea ice freeboard heights compared with the estimates from regular SAR processing schemes, while significantly increasing the number of valid sea surface height retrievals (+55%) and the number of freeboard estimates in the coastal domain and in multi-year ice regions (~3 times). This results in an average ~34% reduction of the gridded random freeboard uncertainty, corresponding to a ~20% reduction of the gridded total sea ice thickness uncertainty. The results of this work show that SARIn acquisitions over Arctic sea ice provide improved spatial coverage and denser sampling of sea level and sea ice freeboard compared with the SAR mode, with accuracy being largely driven by the retracking algorithm.","['Sea ice', 'Arctic', 'Sea measurements', 'Sea surface', 'Uncertainty', 'Spaceborne radar']","['CryoSat-2 (CS2)', 'interferometry', 'radar altimetry', 'retracking', 'sea ice', 'swath processing']"
"High-precision monitoring of infrastructure using artificial reflectors is possible with freely available Sentinel-1 data, but large reflectors are needed. We find that a triangular trihedral corner reflector should typically have at least 1-m inner leg length. As such large reflectors are often not feasible for use in urban areas for essential infrastructure monitoring, we designed a multiple corner-reflector array to replace a single corner reflector with an inner leg length of 1 m. In this case, we use four reflectors where each of them is a truncated triangular trihedral with an inner leg length of 0.33 m. We measured interferometric synthetic aperture radar (InSAR) amplitude, phase, and coherence of this reflector array with various configurations of alignments of the array. We find that as long as great care is taken in the relative positioning of the four corner reflectors, so that they constructively interfere, each horizontal or vertical configuration provides the expected amplitude, coherence, and phase stability. Applications of multiple small corner reflectors in urban areas range from essential infrastructure monitoring (e.g., bridges, overpasses, and tunnel constructions), through assessment of structural health of buildings, to monitoring highway and railway embankments. We show that the multiple corner array works when placed in a single InSAR resolution cell, but depending on the application, the number and projection of corner reflectors can be varied, as long as sufficient signal-to-clutter ratio is achieved in the area of interest.","['Monitoring', 'Synthetic aperture radar', 'Legged locomotion', 'Arrays', 'Clutter', 'Signal to noise ratio', 'C-band']","['Corner reflectors', 'infrastructure monitoring', 'interferometric synthetic aperture radar (InSAR)']"
"As a 2-D spatial filter, the unidirectional electronically steerable planar phased-array synthetic aperture radar (SAR) antenna of the radar constellation mission (RCM) samples the space–time process in azimuth and elevation, respectively, with spatial intervals and visible region defined by the corresponding interelement spacings and wavenumbers. Since position and wavenumber are conjugate Fourier transform variables, interelement spacings need to abide by the Nyquist–Shannon criterion. Furthermore, since the antenna’s response is a 2\pi- periodic function, it produces an infinite number of periodically spaced peaks that are equal in strength to the main lobe peak. If any of these peaks is shifted to the visible region, due to azimuth and/or elevation steering, it becomes a grating lobe that warrants attention. This article investigates grating lobe ambiguities due to elevation steering (E-GLAs) in ScanSAR RCM imagery. E-GLAs resemble legitimate targets and they pose a challenging source of false alarms (FAs) for operational ship detection. Using theoretical analysis, simulations, and a real-world Ship Detection dataset, it is demonstrated that E-GLAs become visible for off-nadir angles \ge 50.38^{\circ}. A novel algorithm is presented to automatically locate E-GLAs in repeat-pass images. Efficacy of the algorithm is demonstrated by tracking a verified E-GLA in a stack of 26 \times 2RCM images near Galápagos Islands, processed using downlinked and definitive satellite orbits, respectively. Results based on definitive orbits are shown to outperform their downlinked counterparts.","['Gratings', 'Azimuth', 'Antennas', 'Radar antennas', 'Radar imaging', 'Marine vehicles', 'Directive antennas']","['Elevation steering', 'false alarm (FA)', 'grating lobe', 'radar constellation mission (RCM)', 'range ambiguity', 'repeat pass', 'ScanSAR', 'ship detection', 'steep incident angle', 'synthetic aperture radar (SAR)']"
"The Doppler shift obtained from synthetic aperture radar (SAR) measurements comprises the combined contribution to the radial motion of the ocean surface induced by the sea state (wind waves and swell) and underlying surface currents. Hence, to obtain reliable estimates of the ocean surface current (OCS), the sea-state-induced Doppler shifts must be accurately estimated and eliminated. In this study, we use a semiempirical dual co-polarization Doppler velocity (DPDop) model, presented in the companion paper, to calculate sea-state-induced Doppler shifts using buoy-measured wind speed, wind direction, and ocean wave spectra. The DPDop model-simulated Doppler shifts are compared with the collocated Sentinel-1B SAR Wave (WV) mode observations at the 24° and 37° incidence angles, showing a bias of −0.24 Hz and a root-mean-square error (RMSE) of 5.55 Hz. This evaluation is also implemented on a simplified DPDop model at the same incidence angles. The model inputs include wind fields from the European Center for Medium-Range Weather Forecasts (ECMWF) and wave characteristic parameters (e.g., significant wave height (SWH), mean wave direction, and mean wavenumber) from WAVEWATCH III (WW3). The estimated Doppler shifts are validated using the ascending and descending observations of Sentinel-1B WV over the global ocean. Furthermore, the comparisons show that the bias and RMSE are −0.71 and 9.25 Hz, respectively. Based on accurate wave bias correction, we obtain the radial current speeds of the ocean surface from the Doppler shift measurements. The estimated current speeds are compared with the collocated high-frequency (HF) radar measurements, with a bias of −0.04 m/s and an RMSE of 0.15 m/s. These results suggest that the original and simplified DPDop models can be used to estimate sea-state-induced Doppler shifts and, thus, derive accurate surface current retrievals.","['Doppler shift', 'Sea surface', 'Surface waves', 'Sea measurements', 'Wind', 'Current measurement', 'Synthetic aperture radar']","['Doppler shift', 'dual co-polarization Doppler velocity (DPDop) model', 'synthetic aperture radar (SAR)']"
"This paper deals with precise absolute geolocation of point targets by means of a pair of high-resolution synthetic aperture radar (SAR) acquisitions, acquired from a satellite. Even though a single SAR image is a 2-D projection of the backscatter, some 3-D information can be extracted from a defocussing analysis, depending on the resolution, thanks to orbital curvature. A second acquisition, observing the same scene under a different look angle, adds stereogrammetric capability and can achieve geolocation accuracy at decimeter level. However, for the stereogrammetric analysis to work, it is necessary to match targets correctly in the two images. This task is particularly difficult if it has to be automatic and targets are dense. Unfortunately, the defocussing-based geolocation is not sufficient for reliable target matching: the limiting factor is the unknown tropospheric delay that can cause geolocation errors of several meters in the elevation direction. However, observing that the tropospheric phase screen displays a low-pass character, this paper shows how to identify statistically the local atmospheric disturbances, therefore dramatically improving the score of successful matching. All steps involved exploit peculiar radar image characteristics and, thanks to this, avoid generic point cloud matching algorithms. The proposed algorithm is shown at work on a pair of TerraSAR-X staring spotlight images.","['Delays', 'Geology', 'Synthetic aperture radar', 'Geometry', 'Spaceborne radar', 'Radar polarimetry', 'Orbits']","['3-D point cloud', 'crossing orbit', 'high resolution', 'satellite geodesy', 'stereogrammetry', 'synthetic aperture radar (SAR)', 'target matching', 'urban digital elevation model (DEM)', 'zenith path delay (ZPD) estimation']"
"Mixed pixels are a ubiquitous problem in remote sensing images. Spectral unmixing has been used widely for mixed pixel analysis. However, up to now, most spectral unmixing methods require endmembers and cannot consider fully intraclass spectral variation. The recently proposed spatiotemporal spectral unmixing (STSU) method copes with the aforementioned problems through exploitation of the available temporal information. However, this method requires coarse-to-fine spatial image pairs both before and after the prediction time and is, thus, not suitable for important real-time applications (i.e., where the fine spatial resolution data after the prediction time are unknown). In this article, we proposed a real-time STSU (RSTSU) method for real-time monitoring. RSTSU requires only a single coarse-to-fine spatial resolution image pair before, and temporally closest to, the prediction time, coupled with the coarse image at the prediction time, to extract samples automatically to train a learning model. By fully incorporating the multiscale spatiotemporal information, the RSTSU method inherits the key advantages of STSU; it does not need endmembers and can account for intraclass spectral variation. More importantly, RSTSU is suitable for real-time analysis and, thus, facilitates the timely monitoring of land cover changes. The effectiveness of the method was validated by experiments on four Moderate Resolution Imaging Spectroradiometer (MODIS) datasets. RSTSU utilizes and enriches the theory underpinning the advanced STSU method and enhances greatly the applicability of spectral unmixing for time-series data.","['MODIS', 'Real-time systems', 'Training', 'Remote sensing', 'Spatial resolution', 'Earth', 'Artificial satellites']","['Machine learning', 'real time', 'spatiotemporal spectral unmixing (STSU)', 'spectral unmixing']"
"Seismological data plays a crucial role in timely slope failure hazard assessments. However, identifying rockfall waveforms from seismic data poses challenges due to their high variability across different events and stations. To address this, we propose RockNet, a deep-learning-based multitask model capable of detecting both rockfall and earthquake events at both the single-station and local seismic network levels. RockNet consists of two submodels: the single-station model, which computes waveform masks for earthquake and rockfall signals and performs earthquakePandSphase picking simultaneously on single-station seismograms, and the association model, which determines the occurrences of local seismic events by aggregating hidden feature maps from the trained single-station model across all stations. Since the rockfall data is relatively scarce and may not be sufficient to train a deep-learning model effectively, we augment the dataset with abundant nonrockfall data and add additional tasks to promote shared interpretability and robustness. RockNet is trained and tested on a local dataset collected from the Luhu tribe in Miaoli, Taiwan, achieving macro F1-scores of 0.983 and 0.990 for the single-station model and the association model, respectively. Furthermore, we evaluate RockNet on an independent dataset collected from the Super-Sauze unstable slope region in France, and it demonstrates good generalization performance in discriminating earthquake, rockfall, and noise with a macro F1-score of 0.927. This study highlights the potential of deep learning in leveraging diverse types of inputs for seismic signal detection even with limited training data.","['Earthquakes', 'Data models', 'Recording', 'Spectrogram', 'Task analysis', 'Computational modeling', 'Monitoring']","['Multitask learning', 'rockfall seismic monitoring', 'transfer learning']"
"Two leaf optical property models, PROSPECT-D and ABM-B, were compared to determine their respective parameter sensitivities and to correlate their parameters. ABM-B was used to generate 150 leaf spectra with various input parameters, and the inversion of PROSPECT-D was used to estimate leaf parameters from these spectra. Wavelength-specific sensitivities were described, and correlations were developed between the leaf pigments and structure parameters of the two models. Of particular importance was the correlation of PROSPECTD's structure parameter (N) which is a generalized parameter integrating several leaf-level and cell-level characteristics. At the leaf-level, N showed correlations with the leaf thickness and the mesophyll percentage, and at the cell-level, N was affected by the cell cap aspect ratios defined in ABM-B. The estimated value of N also varied substantially with changes in the angle of incidence specified in ABM-B. All of these correlations were nonlinear, and it is unclear how these parameters are combined to affect the final value for N. The correlations developed in this article indicate that additional structural parameters (possibly separated into leaf-level and cell-level) should be considered in future model development that aims to maintain inversion potential while providing more information about the leaf.","['Pigments', 'Biological system modeling', 'Sensitivity', 'Correlation', 'Absorption', 'Structural engineering', 'Current measurement']","['ABM', 'leaf pigment absorption', 'mathematical leaf modeling', 'PROSPECT', 'reflectance', 'sensitivity', 'spectroscopy', 'transmittance']"
"The occurrence of West Nile virus (WNV) represents one of the most common mosquito-borne zoonosis viral infections. Its circulation is usually associated with climatic and environmental conditions suitable for vector proliferation and virus replication. On top of that, several statistical models have been developed to shape and forecast WNV circulation: in particular, the recent massive availability of Earth observation (EO) data coupled with the continuous advances in the field of artificial intelligence offer valuable opportunities. In this article, we seek to predict WNV circulation by feeding deep neural networks (DNNs) with satellite images, which have been extensively shown to hold environmental and climatic features. Notably, while previous approaches analyze each geographical site independently, we propose a spatial-aware approach that considers also the characteristics of close sites. Specifically, we build upon graph neural networks (GNNs) to aggregate features from neighboring places and further extend these modules to consider multiple relations, such as the difference in temperature and soil moisture between two sites, as well as the geographical distance. Moreover, we inject time-related information directly into the model to take into account the seasonality of virus spread. We design an experimental setting that combines satellite images—from Landsat and Sentinel missions—with ground-truth observations of WNV circulation in Italy. We show that our proposed multiadjacency graph attention network (MAGAT) consistently leads to higher performance when paired with an appropriate pretraining stage. Finally, we assess the importance of each component of MAGAT in our ablation studies.","['Viruses (medical)', 'Satellites', 'Diseases', 'Remote sensing', 'Graph neural networks', 'Feature extraction', 'Earth']","['Deep learning (DL)', 'graph neural network (GNN)', 'Landsat', 'remote sensing', 'satellite imagery', 'self-supervised learning', 'Sentinel', 'West Nile virus (WNV)']"
"The increasing use of deep learning techniques has reduced interpretation time and, ideally, reduced interpreter bias by automatically deriving geological maps from digital outcrop models. However, accurate validation of these automated mapping approaches is a significant challenge due to the subjective nature of geological mapping and the difficulty in collecting quantitative validation data. Additionally, many state-of-the-art deep learning methods are limited to 2-D image data, which is insufficient for 3-D digital outcrops, such as hyperclouds. To address these challenges, we present Tinto, a multisensor benchmark digital outcrop dataset designed to facilitate the development and validation of deep learning approaches for geological mapping, especially for nonstructured 3-D data like point clouds. Tinto comprises two complementary sets: 1) a real digital outcrop model from Corta Atalaya (Spain), with spectral attributes and ground-truth data and 2) a synthetic twin that uses latent features in the original datasets to reconstruct realistic spectral data (including sensor noise and processing artifacts) from the ground truth. The point cloud is dense and contains 3242964 labeled points. We used these datasets to explore the abilities of different deep learning approaches for automated geological mapping. By making Tinto publicly available, we hope to foster the development and adaptation of new deep learning tools for 3-D applications in Earth sciences. The dataset can be accessed through this link: https://doi.org/10.14278/rodare.2256 .","['Geology', 'Deep learning', 'Point cloud compression', 'Three-dimensional displays', 'Hyperspectral imaging', 'Benchmark testing', 'Feature extraction']","['Deep learning', 'digital outcrop', 'hypercloud', 'hyperspectral', 'point cloud', 'point cloud segmentation', 'remote sensing', 'synthetic data']"
"Antarctica’s ice shelves play a critical role in modulating ice loss to the ocean by buttressing grounded ice upstream. With the potential to impact ice-shelf stability, persistent polynyas (open-water areas surrounded by sea ice that occur across multiple years at the same location) at the edge of many ice-shelf fronts are maintained by winds and/or ocean heat and are locations of strong ice–ocean–atmosphere interactions. However, in situ observations of polynyas are sparse due to the logistical constraints of collecting Antarctic field measurements. Here, we used wintertime (May–August) temperature and salinity observations derived from seal-borne instruments deployed in 2014, 2019, and 2020, in conjunction with thermal imagery from the Moderate Resolution Imaging Spectroradiometer (MODIS) and the Landsat 8 Thermal Infrared Sensor (TIRS) to investigate spatial, temporal, and thermal structural variability of polynyas near Pine Island Glacier (PIG). Across the three winters considered, there were 176 anomalously warm (3σfrom background) seal dives near the PIG ice front, including 26 dives that coincided with MODIS images with minimal cloud cover that also showed a warm surface temperature anomaly. These warm surface temperatures correlated with ocean temperatures down to 150 m depth or deeper, depending on the year, suggesting that MODIS-derived surface thermal anomalies can be used for monitoring polynya presence and structure during polar night. The finer spatial resolution (100 m) of TIRS wintertime thermal imagery captures more detailed thermal structural variability within these polynyas, which may provide year-round insight into subice-shelf processes if this dataset is collected operationally.","['Ocean temperature', 'Sea surface', 'Temperature measurement', 'Oceans', 'Remote sensing', 'MODIS', 'Ice']","['Ice-ocean interaction', 'Landsat 8 Thermal Infrared Sensor (TIRS)', 'Moderate Resolution Imaging Spectroradiometer (MODIS)', 'persistent polynyas', 'thermal remote sensing']"
"Radar imaging algorithms generally exploit linear models of the electromagnetic scattering phenomenon. This assumption leads to qualitative and computationally effective data inversion schemes, which only account for direct scattering from targets, whereas multipath signal contributions are neglected. As a result, multipath ghosts, i.e., false targets reconstructed at positions where no real target exists, affect the radar images, thus preventing a reliable interpretation of the observed scene. This article proposes a fully data-driven deep learning (DL) approach based on a convolutional neural network (CNN) and microwave tomography to face this challenge. The approach achieves multipath ghost suppression for the case of small targets in terms of probing wavelength. In the proposed training scheme, the tomographic image affected by ghosts represents the input of the network while a ghost-free reconstruction is the output. Numerical simulations addressing the detection of metallic rebars via ground penetrating radar (GPR) are presented. As shown, the proposed ghost removal strategy is effective and robust to variations of the scenario parameters on which the network is trained. Finally, an experimental validation shows the effectiveness of the proposed strategy even in operative conditions.",[],[]
"The identification of optically thin cirrus is crucial for their accurate parameterization in climate and Earth’s system models. This study exploits the characteristics of the infrared atmospheric sounding interferometer—new generation (IASI-NG) to develop an algorithm for the detection of optically thin cirrus. IASI-NG has been designed for the European Organization for the Exploitation of Meteorological Satellites (EUMETSAT) polar system second-generation program to continue the service of its predecessor IASI from 2024 onward. A thin-cirrus detection algorithm (TCDA) is presented here, as developed for IASI-NG, but also in parallel for IASI to evaluate its performance on currently available real observations. TCDA uses a feedforward neural network (NN) approach to detect thin cirrus eventually misidentified as clear sky by a previously applied cloud detection algorithm. TCDA also estimates the uncertainty of “clear-sky” or “thin-cirrus” detection. NN is trained and tested on a dataset of IASI-NG (or IASI) simulations obtained by processing ECMWF 5-generation reanalysis (ERA5) data with theσ-IASI radiative transfer model. TCDA validation against an independent simulated dataset provides a quantitative statistical assessment of the improvements brought by IASI-NG with respect to IASI. In fact, IASI-NG TCDA outperforms IASI TCDA by 3% in probability of detection (POD), 1% in bias, and 2% in accuracy, and the false alarm ratio (FAR) passes from 0.02 to 0.01. Moreover, IASI TCDA validation against state-of-the-art cloud products from Cloudsat/CPR and CALIPSO/Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP) real observations reveals a tendency for IASI TCDA to underestimate the presence of thin cirrus (POD = 0.47) but with a low FAR (0.07), which drops to 0.0 for very thin cirrus.",[],[]
"Multispectral light detection and ranging (LiDAR) technology was recently invented to improve the capability of thematic mapping through incorporating visible/infrared spectral information. Similar to image processing, point cloud classification usually considers contextual features derived from surrounding points to improve the model accuracy. Some of the existing methods construct contextual features of point clouds by querying a fixed scale/number of neighbor points or selecting a variable size neighborhood based on some optimality criterion. Although these methods are able to collect neighbor points to derive contextual features, they may also in turn introduce heterogeneity from the local neighborhood or select insufficient neighbor points, hindering the performance of classification. Therefore, we propose an optimal neighbor selection method based on the maximum entropy (MaxEnt) principle. More specifically, the proposed method determines the homogeneity of local neighborhood of each point and constructs geometric and radiometric features based on the use of MaxEnt to determine optimal points nearby. The constructed contextual features are then served as input into various machine learning classifiers for point cloud classification. Extensive experiments are conducted to compare the performance of MaxEnt against six other neighbor selection methods. The experimental results demonstrate that MaxEnt is able to achieve better classification results on multispectral airborne LiDAR data collected by Optech Titan in terms of overall accuracy (OA) improvement by 7.3%–19.1%. Moreover, MaxEnt is proven to be more suitable for land cover scenarios with imbalanced classes caused by detailed and tiny objects, e.g., perimeter fencings and power lines, than other existing neighbor selection methods.",[],[]
"As a new generation of microwave temperature sensor independently developed by China, the microwave temperature sounder-III (MWTS-III) onboard the polar-orbiting satellite FY-3E has 17 channels that provide temperature information from multiple atmospheric layers, making it an indispensable instrument for meteorological research. However, due to the limb effect caused by its cross-track scanning mode, it is impossible to directly observe weather changes using the MWTS-III brightness temperature data. Currently, the limb correction algorithm is an important method to eliminate the limb effect. However, it is difficult to apply existing limb correction methods with fixed parameters due to the newly added channels of MWTS-III. The results of our study show that while the old algorithm can effectively correct the limb effect in most channels of MWTS-III, there are still some limb effects in the corrected results of channels 5–7. On the other hand, the new algorithm can significantly improve the shortcomings of the old method and more accurately eliminate limb effects. In addition, by comparing the limb correction results under different training data, we found that the correction results of the new algorithm have a certain dependence on the training data, and using the same month in the previous year can achieve better correction results. Finally, it has been proved to be applicable to all channels of MWTS-II onboard FY-3D. Specially, the new algorithm can improve the shortcomings of the old algorithm in correcting the limb effect for MWTS-II channel 7, including the lower average brightness temperature at the orbit edge, discontinuous spatial distribution of brightness temperature, and the existence of abnormally low values.",[],[]
"Microwave sounder observations are essential for numerical weather prediction (NWP) systems, but utilizing channels sensitive to surface over sea ice has been challenging due to difficulties in estimating the sea ice surface radiance. This study presents a preprocessing method to assimilate near-surface microwave-sounding observations over winter sea ice, including an estimation of a real-time surface emissivity from satellite radiance and a bias correction scheme to minimize the radiance discrepancy between observation and model simulation. Our results show that the radiance simulated using dynamic emissivity exhibits a much better agreement with the measured one, although a significant negative bias of about 0.61–1.18 K remains over the winter sea ice. Thus, a new bias correction procedure, based on the regression relationships between the residual bias and potential bias sources such as the surface temperature and surface emissivity, is added. When it is applied, the remained bias is successfully estimated. Moreover, the sea ice observations from all temperature-sounding channels have been better utilized in the Korean Integrated Model (KIM). The additional information on the polar regions has increased the analysis increment and reduced the ensemble spread. In addition, a neutral to slightly positive impact on temperature analysis errors in layers sensitive to surface radiance encourages further utilization of microwave sounder data over sea ice.",[],[]
"Mariculture is an important offshore economic activity, and excessive farming can lead to the deterioration of sea ecology. The concentration of nutrients [mainly dissolved inorganic nitrogen (DIN) and orthophosphate-phosphorous (PO4)] is the main factor characterizing the health condition of farmed seas. Conventional field monitoring methods are spatiotemporally limited, and remote sensing technology has the advantages of high spatial coverage and long time series monitoring. Thus, the Sentinel-3 reflectance data and the in situ measured data for the offshore waters of Wenzhou were matched simultaneously. Then, the matched dataset between the Sentinel-2 band and the in situ measured data were obtained through spectral correspondence conversion between Sentinel-2 and Sentinel-3, and a machine learning algorithm was used to build the inversion model with an independent validation process. The correlations between the concentration of nutrients, area of rafts, and precipitation were assessed, and a strong positive correlation was found between the concentration of nutrients and the area of rafts, and a weak negative correlation was found between the former and precipitation.",[],[]
"A new Cyclone Global Navigation Satellite System (CYGNSS) data product is described, which is generated from the raw intermediate frequency (IF) data. The product includes several established signal coherence detectors, including the power ratio {P}_{\mathrm {ratio}}, complex zero-Doppler delay waveform, full entropy {E}_{\mathrm {full}}, and a novel fast entropy detector {E}_{\mathrm {fast}}. Both entropy detectors are provided with two temporal resolutions: 2 and 50 ms. Coherence performance is characterized using the phase derivative of the reflected signal at the peak of the delay waveform {\varphi }_{\mathrm {peak}}. Threshold values of the full entropy detector are determined, which classify scattering into three regimes: incoherent, partially coherent, and coherent. Several scattered signal strength products are included: signal-to-noise ratio (SNR), reflected power {P}_{g}, reflectivity {\Gamma }, and normalized bistatic radar cross section (NBRCS). Each of these products is derived using a coherent integration time of {T}_{c}= 1 ms and incoherent integration times of {N}_{\mathrm {inc}}= 1000, 500, 250, 100, 50, and 2 ms. Signal strength time series at the shorter (2 and 50 ms) times provides excellent detection of land–water transitions in heterogeneous scenes. Delay Doppler maps (DDMs) are also generated with high delay ( {\Delta \tau }= 1/16 chip) and Doppler ( {\Delta f}= 50 Hz) resolution. The behavior of each signal strength product as a coherence detector is examined using the full entropy method as a reference. Performance is characterized using receiver operating characteristic (ROC) curves. The fast entropy method, which has a much lower computational cost, is similarly characterized. This suite of coherence detection methods can be used to detect the presence of small inland water bodies.",[],[]
"Monitoring the chlorophyll content changes in the plant via remote sensing is of great significance for understanding plant growth and monitoring vegetation pests and diseases, which is an important method to study the global climate change. However, the monitored information is often interfered by leaf specular reflection, resulting in reduced accuracy of chlorophyll content inversion. In this article, to eliminate the interference of specular reflection in vegetation remote sensing, a polarized multispectral imaging system (PMSIS) used in the different-light-level situation to observe vegetation was developed, and a new specular reflection removal vegetation index (NSRVI) was proposed to better detect the vegetation health condition under specular reflection interference. Based on previous studies, several vegetation indices (simple ratio (SR) index, normalized difference vegetation index (NDVI); modified simple ratio index (mSR), modified normalized difference vegetation index (mNDVI); polarization based simple ratio index (pSR), polarization based normalized difference vegetation index (pNDVI); and NSRVI) were established, and the impact of specular reflection on vegetation health detection was evaluated. Correlation analysis was done on relative chlorophyll content [soil and plant analyzer development (SPAD)], SR, NDVI, mSR, mNDVI, pSR, pNDVI, and NSRVI to understand their potential ability to eliminate specular interference. The results show that SR and NDVI have the highest sensitivity to specular reflection, and the other three methods can alleviate the adverse effects of specular reflection to varying degrees. It was observed that NSRVI was well-correlated with SPAD (coefficient of determination ( R^{2}) =0.899and root-mean-square error (RMSE) =6.16), highlighting the potential of NSRVI in eliminating specular reflection interference and identifying vegetation health condition. In summary, this method can effectively eliminate specular interference and improve the detection accuracy of vegetation health condition.",[],[]
"Point cloud registration is a crucial part of 3-D computer vision. Existing point cloud registration methods primarily concentrate on utilizing features such as points, lines, and planes, disregarding the valuable contour cues inherent in the scene. In this article, we propose a novel sketch-based framework for point cloud registration that incorporates contour cues to enhance the point cloud registration task. To fully exploit the abundant information provided by contour cues in the scene, the point cloud is first abstracted into a sketch consisting of contour cues obtained through the utilization of planar features, which greatly preserves the inherent contour information. Subsequently, a local contour geometric descriptor is introduced to encode the contour cues in the sketch. Finally, a voting-based contour point pair feature (CPPF) framework is employed to fuse planar features, local contour geometric features, and point pair geometric features, enabling precise estimation of the pose transformation between pairwise point clouds. Extensive experiments conducted on two large-scale outdoor point cloud datasets and two indoor point cloud datasets validate the effectiveness of the proposed sketch-based method. Our proposed method successfully suppresses rotation and translation errors, ultimately achieving state-of-the-art performance.",[],[]
"We deduce a novel interferometric tropospheric error (NITE) formula for ground-based global navigation satellite system interferometric reflectometry (GNSS-IR). This formula contains two parts: a geometric displacement error that accounts for the reflection point change due to the atmosphere and Earth’s curvature, and a path delay derived following the definition of the mapping function (with the small curve path effect included). We validate the NITE formula together with two previously used approaches: the bending angle correction and the mapping function path delay (MPF delay) using raytracing and radiosonde data. The raytracing results show that the NITE formula is more accurate than the previous approaches. Numerically, the geometric displacement error is < 5% of the path delay error for a GNSS antenna located 20 m above sea level. We further evaluate six tropospheric correction strategies for GNSS-IR sea-level monitoring through two sets of experiments. With an elevation angle range test, we show that applying no tropospheric correction and applying the bending angle correction plus the MPF delay both introduce large elevation-dependent biases. Analyzing the time series of differences between GNSS-IR and tide gauges sea level, we show that the bending angle correction with the widely used Bennett equation introduces long-term (4 h to several months) trends in the sea-level retrievals. We identify one station where the NITE formula produces better long-term (τ>4h) sea-level retrievals. Finally, we show that at low elevation angles, the bending angle correction can be reformulated as an MPF delay.",[],[]
"The signal-to-noise ratio of modern cameras under normal operating conditions tends to be limited by “photon noise” originating from the random arrival of photons. For best signal-to-noise ratio, it is desirable to collect as much light as possible, and to avoid losses internally in the camera. There is currently no widely accepted metric for the resulting net light collection, which depends on many aspects of camera design. The IEEE Standards Association P4001 group is developing a standard for hyperspectral imaging, including ways to fully specify camera performance in an efficient way. Motivated by P4001 requirements, this work proposes to specify the net light collection in a single quantity, denotedA∗, essentially defined as the product of nominal geometrical étendue, optics transmission, and detector quantum efficiency. It is shown howA∗can be physically interpreted as the detector pixel area of an equivalent lossless camera whose exit pupil subtends 1sr. This article discusses how this quantity can be used as a figure of merit applying to hyperspectral cameras as well as to conventional multispectral and broadband cameras and other sensing systems employing imaging optics.",[],[]
"With the increased use of high-spatial-resolution (HSR) images for vegetation monitoring in arid areas, more details of the low vegetation coverage and interference from the land “background” are captured in the corresponding images. From computational time and accuracy, the multiangle method (MAM) in the pixel dichotomy model is a potential algorithm to apply in arid areas, but MAM needs the multiangle vegetation index (VI) as the driver parameters. However, most HSR images are obtained in nadir mode, and the multiangle information of reflectance is difficult to obtain, which limits the estimation of multiangle VI from HSR images. To address this issue, this study used a “graphical method” to modify the radiation influence caused by the canopy structure and land “background.” We developed an inversion method of the linear kernel-driven model (KDM) and designed a random sampling method to estimate multiangle VI from HSR images. Then, we proposed a new pixel dichotomy coupled linear KDM (PDKDM), validated using simulated, field-measured, and reference data. The results showed that the FVC in arid areas estimated by PDKDM was highly consistent with “true” data, with root-mean-square error (RMSE) < 0.062, RMSE < 1.125, and RMSE < 0.027 for comparison with simulated, field-measured, and reference data, respectively. PDKDM addressed the issue with the previous MAMs to estimate FVC from HSR images in arid areas. This study provides a useful algorithm with high computational efficiency for producing HSR FVCs in arid areas.",[],[]
"Change detection in the hyperspectral imagery (HSI) detects the changed pixels or areas in bitemporal images. HSIs contain hundreds of spectral bands, including a large amount of spectral information. However, most of deep learning-based change detection methods did not focus on the spectral dependency of spectral information in the spectral dimension and just adopted the difference strategy to represent the correlation of learned features, which limited the improvement of the change detection performance. To address the above-mentioned problems, we propose an end-to-end change detection network for HSIs, named spectrum-aware transformer network (SATNet), which includes SETrans feature extraction module, the transformer-based correlation representation module, and the detection module. First, SETrans feature extraction module is employed to extract deep features of HSIs. Then, the transformer-based correlation representation module is presented to explore the spectral dependency of spectral information and capture the correlation of learned features of bitemporal HSIs from both the perspective of difference and dot-product operations, so as to obtain more discriminative features. Finally, the decision fusion strategy in the detection module is utilized to the learned discriminative features to generate the final change map for better change detection performance. Experimental results on three hyperspectral datasets show that the proposed SATNet is superior to the existing change detection methods.",[],[]
"The estimation of displacement vectors for (objects on) the Earth’s surface using satellite InSAR requires geometric transformations of the observables based on orbital viewing geometries. Usually, there are insufficient viewing geometries available for full 3-D reconstruction, leading to nonunique solutions. Currently, there is no standardized approach to deal with this problem, resulting in products that are based on haphazard and/or oversimplified assumptions with biased estimates and reduced interpretability. Here, we show that a clear definition of—and subsequent adherence to—enabling conditions guarantees the validity and quality of displacement vector estimates leading to standardized interferometric products with improved interpretability. We introduce the concept of the null line as a key metric for InSAR geometry and bias estimation, assess its impact and orientation for all positions on Earth, and propose a novel reference system that is inherently unbiased. We evaluate current operational practice, leading to a taxonomy of frequently encountered misconceptions and to recommendations for InSAR product generation and interpretation. We also propose new subscript notation to uniquely distinguish different projection and decomposition products. Our propositions contribute to further standardization of InSAR product definition, improved map annotation, and robust interpretability.",[],[]
"We have developed a fast and efficient algorithm for estimating the global positioning system (GPS) differential code biases (DCBs) and ionospheric vertical total electron content (VTEC) using a local GPS network. The algorithm does not simultaneously estimate the DCBs and VTEC parameters, it instead determines the parameters in a two-step estimation. At the first estimation step, the algorithm determines the DCBs parameters and, at the second estimation step, the algorithm determines the VTEC parameters. While the DCB estimations (the first step) are carried out by recursively accumulating the normal equation for each observation batch (or for each epoch), the VTEC estimations (the second step) are recursively executed every batch. By using the algorithm, the users who only need the DCBs estimates may stop when the first estimation step finishes, making the computational time efficient. In constructing the algorithm, we applied the orthogonal transformation to ensure that the algorithm is numerically reliable. We then fully utilized the structure of the problem to enable recursive calculations for each estimation step, making the algorithm fast and efficient. We found that the algorithm produces identical results to those calculated using the standard method, but the calculation time is tremendously reduced. Furthermore, the algorithm enables epoch-wise solutions of the VTEC estimates to be rapidly executed using an ordinary laptop computer. In addition, the algorithm is applicable to determine the DCBs and VTEC estimates both in a network mode and a single receiver mode.",[],[]
"This work shows that additional ScanSAR capabilities, with respect to those achievable through the current Synthetic Aperture Radar (SAR) system of the forthcoming ROSE-L mission, may be easily enabled through a proper shaping of the azimuth pattern of the TX radar antenna. In particular, we show that by properly acting only on the distribution of the input excitations of the elements of the currently designed TX antenna array, we can move from a single azimuth look ScanSAR configuration to a more attractive two-look one. This result is achieved without upsetting the current system architecture and without changing the current mission parameters (such as azimuth resolution, range swath and prf), all tailored to a one-look ScanSAR configuration. On the other hand, the proposed two-look mode requires to double the azimuth beam-width with respect to the one-look mode, thus leading to an unavoidable antenna gain decrease, whose amount is of approximately 3 dB. The presented analysis also shows that, by still acting on the distribution of the input excitations of the TX antenna array, the scalloping effect can be significantly mitigated with respect to that of the current system design, in both scenarios relevant to the original one-look ScanSAR configuration and to the proposed two-look one.",[],[]
"Methods based on statistical learning have become prevalent in various signal processing disciplines and have recently gained traction in atmospheric lidar studies. Nonetheless, such methods often require large quantities of annotated or resolved data. Such data are rare and require effort, especially when exploring evolving phenomena. Existing simulators and databases primarily focus on atmospheric vertical profiles. We propose the Atmospheric Lidar Data Augmentation (ALiDAn) framework to fill this gap. ALiDAn serves as an end-to-end generation and augmentation framework of spatiotemporal and multiwavelength resolved lidar simulated data. ALiDAn employs a hybrid approach of physical models, data statistics, and sampling processes. In addition, it takes into account geographical and seasonal characteristics of aerosols and meteorological conditions along with short- and long-term phenomena that affect lidar measurements. This approach can provide diversified data and robust benchmarks to assist in developing and validating new lidar processing algorithms. We demonstrate simulations compatible with a pulsed time-of-flight lidar. Our approach leverages a broader use of existing databases and can inspire similar data augmentation to other types of lidars and active sensors.",[],[]
"There are various binary semantic segmentation tasks in remote sensing (RS) that aim to extract the foreground areas of interest, such as buildings and roads, from the background in satellite images. In particular, semisupervised learning (SSL), which can use limited labeled data to guide a large amount of unlabeled data for model training, can significantly promote the fast applications of these tasks in practice. However, due to the predominance of the background in RS images, the foreground only accounts for a small proportion of the pixels. It poses a challenge: models are biased toward the majority class of the background, leading to poor performance on the minority class of the foreground. To address this issue, this article proposes a novel and effective SSL framework, adaptive matching (AdaptMatch), for RS binary segmentation. AdaptMatch calculates individual and adaptive thresholds of the foreground and background based on their convergence difficulty in an online manner at the training stage; the adaptive thresholds are then used to select the high-confidence pseudo-labeled data of the two classes for model self-training in turn. Extensive experiments are conducted on two widely studied RS binary segmentation tasks, building footprint extraction and road extraction, to demonstrate the effectiveness and generalizability of the proposed method. The results show that the proposed AdaptMatch achieves superior performance compared with some state-of-the-art semisupervised methods in RS binary segmentation tasks. The codes will be publicly available at https://github.com/zhu-xlab/AdaptMatch .",[],[]
"Undesired echo from a flying object (aerial clutter) significantly contaminates the received signal of a wind profiler radar (WPR) because it has high intensity and spreads over a wide Doppler velocity range. In this study, results of aerial clutter mitigation obtained by applying adaptive clutter suppression (ACS) to a 1.3-GHz WPR are shown. The 1.3-GHz WPR used in this study has 13 antenna subarrays that compose the main antenna (MSAs). Five-element Yagi–Uda antennas were also used as antenna subarrays for detecting clutters from low elevation angles (CSAs). The CSAs were used only in reception and installed so that they covered most of the horizontal directions and the horizontal and vertical polarizations. The directionally constrained minimization of power (DCMP) method was used as the adaptive signal processing to mitigate clutter. By the DCMP method, the weighted sum of the signals collected by 13 MSAs and 11 CSAs was computed so that the power of output signals was minimized under the constraint of constant gain in the antenna beam direction. Results of a case study for an aerial clutter from a low elevation angle at 17:04:37 on October 1, 2020, showed that an overlap of the aerial clutter over a desired echo (i.e., clear-air echo) was solved by decreasing the aerial clutter whose peak intensity was ~24 dB greater than that of the clear-air echo. In a case study at 09:30:27 on September 18, 2020, the effects of the DCMP method on the processed results were discussed.",[],[]
"Glacial algae blooms on the Greenland ice sheet darken the surface albedo, enhancing surface melt. Sentinel-3 data was the first to highlight the extent of these blooms, which are expected to become larger as climate changes. Here, we propose a novel use of solar-induced chlorophyll fluorescence (SIF) data from TROPOspheric monitoring instrument (TROPOMI) to confirm the Sentinel-based maps and extend the data to daily levels to understand year-to-year variations in algae blooms. We combine the SIF data with a random walker algorithm to identify algae-covered areas. The SIF-based maps are found to be highly correlated with the Sentinel-based ones. The extent and the timing of the algae bloom are consistent between 2018 and 2020. This work opens the door to a new, complementary method for the monitoring of algae blooms on ice sheets and improving our understanding of their impact on glacier melt rate.",[],[]
"The rapid evolution of the effects observed in various areas of our planet related to climate change poses urgent questions about the knowledge of the state of the polar area and requires satellite acquisitions with fine spatial resolution and high accuracy to develop advanced products. The Copernicus Imaging Microwave Radiometer (CIMR) mission, based on a multifrequency microwave radiometer and designed to observe the ocean, sea ice, and Arctic environment, requires brightness temperature measurements with a total absolute uncertainty of 0.5 K and a spatial resolution of 5 km. This constraint demands very large reflectors with a gain value of tens of decibels. Mechanical constraints will be attained by using a mesh reflector, which guarantees the required resolution but with the drawback of a radiation pattern characterized by many grating lobes that contaminate the value of the brightness temperature associated with the boresight position. In this article, an antenna pattern correction (APC) is proposed to correct these effects. The algorithm takes advantage of an iterative formulation based on the Jacobi Method, providing a suitable correction that depends on the chosen spatial resolution. The APC algorithm was tested at both K- and Ka-bands with similar performance. Here, only the results from the latter are shown, as its antenna pattern is the most challenging among CIMR.",[],[]
"This article presents the first detailed description of the innovative measurement setup of an indoor tropospheric microwave radiometer [TROpospheric WAter RAdiometer (TROWARA)] that avoids water films on radome. We discuss the performance of a commercial outdoor microwave radiometer [Humidity And Temperature PROfiler radiometer (HATPRO)] for measuring tropospheric water parameters in Bern, Switzerland. The HATPRO is less than 20 m from the TROWARA and has different instrument characteristics. Brightness temperatures measured by HATPRO are analyzed by comparing them with coincident measurements from TROWARA and Radiative Transfer Simulations based on the [European Centre for Medium-Range Weather Forecasts (ECMWF)] operational analysis data (denoted as RTSE). To find the source of brightness temperature bias, a gradient boosting decision tree is used to analyze the sensitivity of eight feature factors to bias. Data processing routines of the two radiometers use different algorithms to retrieve integrated water vapor (IWV) and integrated cloud liquid water (ILW), whereas the same physical algorithms based on the radiative transfer equation are applied to obtain the opacity and rain rate. Using 62 days of data with varied weather conditions, it was found that TROWARA brightness temperatures are in good agreement with RTSE. HATPRO brightness temperatures are significantly overestimated by about 5 K at 22 GHz, compared to TROWARA and RTSE. HATPRO brightness temperatures at 31 GHz agree well with TROWARA and RTSE (within about ±1 K). The overestimated brightness temperatures in the K-band and the HATPRO retrieval algorithm lead to an overestimation of IWV and ILW by HATPRO. The opacities at 31 GHz match very well for TROWARA and HATPRO during no rain with a verified $R^{2}$ of 0.96. However, liquid water floating or remaining water films on the radome of the outdoor HATPRO radiometer induce an overestimation of the rain rate. The physical reason for the overestimated 22-GHz brightness temperatures of the HATPRO is mainly the result of the combined effect of instrument calibration, the surrounding environment of the instrument, and the Sun elevation angle. This can be a problem with the Generation 2 HATPRO radiometer and this problem was resolved in the Generation 5 HATPRO radiometer.",[],[]
"We present an analysis of the capability for imageless ground scene classification using a subset of the Fourier domain information obtained with a rotationally dynamic millimeter-wave antenna array. The concept is based on the detection of signal artifacts generated by artificial objects in a scene, which manifests in the Fourier, or spatial frequency, domain. Man-made, artificial structures, such as buildings and roads, are generally characterized by sharp edges, which generate spatial frequency responses that are confined to a narrow angular range but extend over a broad spatial frequency bandwidth. These artifacts can be detected by generating a ring-shaped filter in the Fourier domain, which can be obtained through the novel design of a linear antenna array with rotational dynamics. We discuss the design of a millimeter-wave linear dynamic array for generating ring-filters and analyze the ability of such an array to classify ground scenes containing artificial structures from those without when mounted on an aerial platform, such as a drone. We compare ring filter designs and explore the use of a heuristic classifier and the K-nearest neighbor (K-NN) classifier on a large dataset of microwave ground scenes obtained from a database. Using a single ring filter that can be implemented with a two-element antenna array, small classification errors of 0.6%–3.2% were observed. Implementing multiple filters in a linear array consisting of four elements reduced the error to 0.3%.",[],[]
"Sea and soil surfaces exhibit power-law spectra over a wide range of spatial frequencies. An analytical formulation of the electromagnetic scattering from such surfaces can be obtained via the two-scale model (TSM). However, this approach requires the definition of a cutoff surface wavenumber, separating the low- and high-frequency parts of the surface spectrum. The final obtained normalized radar cross section (NRCS) value is dependent on the choice of this cutoff wavenumber, which is, to some extent, arbitrary. This problem can be avoided by describing power-law spectrum surfaces via the theory of fractional Brownian motion (fBm) two-dimensional (2-D) random processes. The bistatic NRCS of an fBm surface can be analytically evaluated by using the Kirchhoff approximation (KA) or the first-order small slope approximation (SSA-1): its expression is related to the probability density function (pdf) of an alpha-stable random process, and it can be efficiently evaluated by means of proper asymptotic series expansions. However, fBm surfaces are statistically isotropic, whereas natural surfaces are often anisotropic. Therefore, in this work, we first of all show that an anisotropic power-law spectrum surface can be considered as a generalized anisotropic fBm surface; then, we present an analytical formulation of its NRCS, based on SSA-1; and finally, we compare the obtained results with measured NRCSs of natural surfaces and with NRCS values obtained via more accurate but more computationally demanding methods that require the numerical evaluation of scattering integrals.",[],[]
"Accurate monitoring of albedo trends over snow is essential to evaluate the consequences of the global snow cover retreat on Earth’s energy budget. Satellite observations provide the best way to monitor these trends globally, but their uncertainty increases over snow. Besides, different products sometimes show diverging trends. A better assessment of the fitness of satellite products for monitoring snow albedo trends is needed. We analyze the consistency of black-sky albedo estimates from global long-term products over snow: advanced very-high-resolution radiometer (AVHRR)-based (CLARA-A2.1, GLASS-v4.2), moderate resolution imaging spectroradiometer (MODIS)-based (MCD43C3-v6.1/v6, GLASS-v4.2), multiangle imaging spectro radiometer (MISR)-based (MIL3MLSN-v4), and multisensor (C3S-v1/v2). We use MCD43C3-6.1 as the reference based on a previous comparison against in situ measurements. CLARA-A2.1 is the one most consistent with MCD43C3, but has a low coverage in high latitudes and an artificial albedo decrease since 2015. The study shows the limitations of MIL3MLSN, Global Land Surface Satellite (GLASS), and Copernicus Climate Change Service (C3S) multisensor products over snow. MIL3MLSN has a too-low coverage of albedo over snow. GLASS-AVHRR overestimates albedo in regions with seasonal snow due to delayed snowmelt and underestimates it in permanently snow-covered regions. GLASS-MODIS is more consistent with MCD43C3 at mid-latitudes, and also underestimates albedo in regions with permanent snow and has an increase in missing values after 2011. Both the GLASS datasets are temporally inconsistent with the other products. Despite the improvements from v1 to v2, C3S-v2 has the largest negative bias over snow and discontinuities in the transitions between sensors. The study evidences the difficulties of AVHRR products to provide stable snow albedo estimates in polar regions, particularly before 2000.",[],[]
"Hurricane top structures are not well captured by airborne or dropsonde observations. Total column ozone (TCO) observations provided by the Ozone Mapping and Profiler Suite (OMPS) Nadir Mapper (NM) onboard the Suomi National Polar-orbiting Partnership (S-NPP) satellite are employed in an investigation of hurricane top structures. We show that the OMPS NM TCO data can capture the top structures of Hurricane Maria (2017) over the Atlantic Ocean. An observed local maximum of TCO in the eye region reveals a strong upper tropospheric downward motion that lowers the tropopause above the hurricane eye. A rainband-like distribution of low TCO content reflects strong convection areas where the tropopause is raised and well correlates spatially with the high cloud top regions derived from the S-NPP Visible Infrared Imaging Radiometer Suite (VIIRS). A sixth-order even polynomial fitting is used to reveal the TCO radial structures by introducing two characteristic parameters. One is a radial distance parameter ( R_{\mathrm {TCO}}) representing the spatial range, and the other is an intensity parameter defined as the TCO difference from the hurricane center to 600-km radial distance from the hurricane center ( \Delta {\mathrm {TCO}}_{600\,{\mathrm {km}}}). Based on an analysis of ten hurricanes over the Northern Atlantic Ocean in 2017, we show that before a hurricane reaches its maximum strength, there is always a decrease of R_{\mathrm {TCO}}and an increase of \Delta {\mathrm {TCO}}_{600\,{\mathrm {km}}}. It is anticipated that more accurate initial hurricanes could be produced if the TCO structures were combined with other surface, near surface, and tropospheric information in vortex initialization.",[],[]
"Now it is still a challenge to compress high-throughput hyperspectral tensor image data on lightweight air-carried/spaceborne remote sensing systems, primarily due to insufficient computational resources and limited transmission bandwidth. To address this challenge, we propose a bit-level tensor data compression network (BTC-Net) that provides higher compression performance by leveraging a data-driven lightweight quantized neural encoder with two-stage bit compression. The BTC-Net achieves semantic near-lossless high reconstruction quality at low compression bit rates thanks to its optimized decoder, which uses a channelwise attention-based enhancement module to recover hyperspectral tensor data. Experimental results on different hyperspectral datasets show that the BTC-Net could achieve an extremely low compression bit rate of fewer than 0.04 bits per pixel per band (bpppb) with the state-of-the-art (SOTA) reconstruction performances. The demo of BTC-Net will be publicly available online at: https://github.com/zx20173646/BTCNet .",[],[]
"Hyperspectral (HS) pansharpening has received a growing interest in the last few years as testified by a large number of research papers and challenges. It consists in a pixel-level fusion between a lower resolution HS datacube and a higher resolution single-band image, the panchromatic (PAN) image, with the goal of providing an HS datacube at PAN resolution. Due to their powerful representational capabilities, deep learning models have succeeded to provide unprecedented results on many general-purpose image processing tasks. However, when moving to domain-specific problems, as in this case, the advantages with respect to traditional model-based approaches are much lesser clear-cut due to several contextual reasons. Scarcity of training data, lack of ground truth (GT), and data shape variability are some such factors that limit the generalization capacity of the state-of-the-art deep learning networks for HS pansharpening. To cope with these limitations, in this work, we propose a new deep learning method, which inherits a simple single-band unsupervised pansharpening model nested in a sequential band-wise adaptive scheme, where each band is pansharpened refining the model tuned on the preceding one. By doing so, a simple model is propagated along the wavelength dimension, adaptively and flexibly, with no need to have a fixed number of spectral bands, and, with no need to dispose of large, expensive, and labeled training datasets. The proposed method achieves very good results on our datasets, outperforming both traditional and deep learning reference methods. The implementation of the proposed method can be found at https://github.com/giu-guarino/R-PNN .",[],[]
"We investigate the use of Bayesian methods for hyperspectral subpixel target detection, where the uncertainty associated with the target fill factor is “probabilized” by a suitable prior. Specifically, we present a general framework for Bayesian target detection by employing different models for the background distribution, comparing different choices for the Bayesian prior, and investigating different numerical schemes for evaluating the Bayesian integral. The Bayesian methods are furthermore compared to their generalized likelihood ratio test (GLRT)-based counterparts. Experiments performed over real hyperspectral imagery, with both real and implanted subpixel targets, show that incorporating prior knowledge by means of nonuniform priors emphasizing smaller target fill factors outperforms usage of the “noninformative” uniform prior and enhances Bayes performance beyond the GLRT, a result observed for both parametric and nonparametric background models. We find that even “rough” priors can successfully leverage the context-based information by emphasizing target sizes that are of most interest. We further observe that the Gauss–Legendre numerical integration scheme provides efficient integral approximation while maintaining the desirable admissibility property of Bayesian methods.",[],[]
"In desert steppe regions with sparse vegetation, there are discrepancies between vertical and oblique observations made by satellite-based sensors. In this study, we developed and deployed an online multiangle spectrometer in the desert steppe area of Inner Mongolia, China, to calibrate satellite-based vegetation indices. One of the key components of the device is a specially designed quarter-arc iron track that holds the fixed view angles of 30°, 45°, 60°, 75°, and 90° that are fixed. These observation positions equipped with high-efficiency multichannel sensors can capture the reflectance of ground objects at visible and near-infrared wavelengths. Real-time experiments were conducted with multiple observation angles and an error-based view angle correction model was constructed to reconcile the differences between angular and vertical observations. The calculated results were subsequently applied to the bias-correction process of the Sentinel-2 vegetation index. Across all view angles, the daily distributions of the normalized difference vegetation index (NDVI) and ratio vegetation index (RVI) exhibited a U-shaped pattern with the nadir occurring at noon. Among the data, RVI demonstrated superior overall stability compared to NDVI. However, with vegetation growth, the NDVI showed less sensitivity, resulting in a decrease in its coefficient of variation (CV) from 32.3% to 18.2%. To correct the bias in Sentinel-2 products, we initially applied path length correction (PLC) to eliminate the topographic influence on the Band4 and Band8 band reflectance. Our findings revealed that Band8 performed better than Band4 in mitigating the effects of topography, as evidenced by a decrease in the determination coefficient from 0.448 to 0.09. In addition, we corrected the view angle error of the NDVI of Sentinel-2 by constructing a view angle correction model ( $R^{2}$ = 0.84 and RMSE = 0.03). The corrected images exhibited significant variation characteristics in areas with relatively large view angles. The results of this study provide valuable scientific support for the correction of satellite image products and the accurate detection of vegetation through remote sensing in regions with low vegetation coverage.",[],[]
"Space-based ground-imaging lidar has become increasingly feasible with recent technological advances. Compact fiber-optic lasers and single-photon-sensitive Geiger-mode detector arrays push designs toward low pulse energies and high pulse rates. A challenge in implementing such a system is imperfect pointing knowledge caused by angular jitter, exacerbated by long distances between satellite and ground. Without mitigation, angular jitter would cause significant blurring of the 3-D data products. Reducing the error in pointing knowledge to avoid such problems might require extreme mechanical isolation, advanced inertial measurement units (IMUs), star trackers, or auxiliary passive optical sensors. These mitigations can increase cost and size, weight, and power considerably. An alternative approach is demonstrated, in which the two-axis jitter time series is estimated using only the lidar data. Simultaneously, a single-surface model of the ground is estimated as nuisance parameters. Expectation-maximization is used to separate signal and background detections while maximizing the joint posterior probability density of the jitter and surface states. The resulting estimated jitter, when used in coincidence processing or image reconstruction, can reduce the blurring effect of jitter to an amount comparable to the optical diffraction limit.","['Jitter', 'Laser radar', 'Satellites', 'Surface reconstruction', 'Measurement by laser beam', 'Optical sensors', 'Optical variables measurement']","['Angle-jitter', 'Bayesian', 'expectation–maximization (EM)', 'Geiger-mode detector', 'gradient descent', 'image correction', 'imaging', 'lidar', 'satellite', 'surface estimation']"
"We introduce the term loss-of-lock to describe a specific form of coherence loss that results in the breakage of a synthetic aperture radar interferometric (InSAR) time series. Loss-of-lock creates a specific pattern in the coherence matrix of a multilooked distributed scatterer (DS) by which it may be detected. Along with identification, we introduce a new DS processing methodology that is designed to mitigate the effects of loss-of-lock by introducing contextual data to assist in the time-series processing. This methodology is of particular relevance to regions that suffer from severe temporal decorrelation, such as northern peatlands. We apply our new method to two subsiding cultivated peatland regions in The Netherlands which previously proved impossible to monitor using DS InSAR techniques. Our results show a very good agreement with in situ validation data as well as spatial correlation between regions and the natural terrain.",[],[]
"Monostatic pursuit refers to the operating mode formed by two monostatic synthetic aperture radar (SAR) systems that follow an identical orbit with a separation in a time of several seconds. The detected changes between SAR scenes with several seconds of time difference are most likely the changes caused by ground moving targets. Hence, this operating mode opens an opportunity to detect ground moving targets by SAR change detection methods. This article investigates this possibility to detect ground moving targets using change detection and to combine change detection and ground moving target indication (GMTI) for GMTI. In this combination, a GMTI method will help to classify the detected changes obtained with a change detection method. Some GMTI results are provided in the article based on the measurements in the monostatic pursuit mode with deployed targets, conducted by TerraSAR-X and TanDEM-X in Sweden in early 2015.",[],[]
"Predisaster information storage is crucial for effective disaster response. The discussion regarding deep learning-based light detection and ranging (Lidar) semantic segmentation technology for indoor small items has been ongoing in recent years. However, the methods applicable to large-scale outdoor Lidar datasets for predisaster information storage remain limited. This study aims to propose a novel deep learning-based network for city-scale Lidar semantic segmentation to support predisaster information storage, called channel attention and normal-based local feature aggregation network (CNLNet). This network is designed to segment common urban land cover objects, including buildings and vegetation. This network incorporates surface normal information and the channel attention (CA) mechanism into the RandLA-Net backbone. Ablation studies have been devised to assess the performance of these two features. During the preprocessing step, color information from optical images is fused with Lidar data. The findings demonstrate that CNLNet can enhance the accuracy of the RandLA-Net backbone by improving mean intersection over union (mIoU) by at least 1%–2%. Including one of these two features also contributes to the backbone’s improved accuracy. Notably, CNLNet outperforms other well-known networks in terms of accuracy with the test of the public Sementic3D dataset. This study further reveals that the proposed network excels in building segmentation, a crucial facet of predisaster information storage. Moreover, the results show that spatial resolution, whether at 0.5 or 10 m per pixel for optical images, has limited influence on outcomes. One theoretical contribution of this study is the demonstration of the advantages of integrating either surface normal information or a CA mechanism to enhance large-scale outdoor Lidar semantic segmentation. Labeled Lidar datasets have been created for training. The practical contribution is that it can optimize disaster response by efficiently facilitating predisaster information storage.",[],[]
"This article introduces a method to correct intersensor calibration convolution errors that occur in the convolution of spectral response functions (SRFs) between narrow-band and broad-band instruments. By using the intersensor calibration analysis between Ozone Mapping and Profiler Suite (OMPS) Nadir Mapper (NM) and Global Ozone Monitoring Experiment-2 (GOME-2) as an example, the root cause of convolution errors in the intersensor calibration is addressed through direct comparison of OMPS NM SRF and convolved OMPS SRF with GOME-2 SRF. The results reveal that distorted SRF of the narrow-band instrument is the major cause, which appears for GOME-2 at a wide range of channels. The convolution errors in reflectance, which were ignored in previous studies, can be greater than 2% for wavelength shorter than 320 nm and∼0.5% for wavelengths between 320 and 330 nm. This study thus presents a hybrid convolution error correction method that consists of theoretical approximation of the convolution errors and empirical estimates of residuals due to the deviation of the theoretical approximation from the actual convolution errors. According to the validation through simulation, after applying convolution error correction, the mean convolution errors are less than 0.02%, while the root mean square errors are reduced from more than 0.5% to less than 0.1%. In addition, the correction method is applied to the intersensor calibration radiometric bias assessment between the Meteorological Operational satellite–B (Metop-B) GOME-2 and the Suomi National Polar-orbiting Partnership (S-NPP) OMPS NM. The averaged intersensor calibration reflectance differences are decreased by more than 16% after convolution error correction.",[],[]
"The distributions of cloud phases play an important role in influencing the weather and climate system. The characteristics of clouds above the Tibetan Plateau (TP) can profoundly affect regional and global atmospheric circulation. To research the distributions of cloud phases in the TP region, a retrieval algorithm was developed based on the combination of polarization lidar and millimeter cloud radar measurements and applied to the data from a comprehensive field campaign on the central TP in the summer of 2014. The structure and phase of four different types of clouds were retrieved accordingly, which validates the reliability of the algorithm. The result shows that the occurrence frequency of low clouds remains around 50%, which is very high throughout the whole day in Nagqu, Tibetan in summer. The liquid and mixed cloud frequencies are higher in the morning and afternoon, while ice cloud mainly occurs from the afternoon to midnight. Liquid and ice phase distributions show an inverse relationship in the atmospheric layer from 2 to 8 km in height. Meanwhile, the proportion of the liquid phase to the cloud top is significantly higher than that to the cloud body, which indicates that the supercooled water is more likely to appear at the cloud top than in the cloud. The fractional probabilities of the ice phase and liquid phase in the total cloud top phase intersect at about−26.7∘C.",[],[]
"Information derived from full-waveform (FW) light detection and ranging (lidar) data has already been shown to be relevant for point cloud analysis tasks. Relevant waveform attributes to populate the corresponding point’s feature vector are typically provided through a post-processing FW analysis (FWA) technique based on fitting the echo waveform with a parametric function describing the shape and location of the echo pulse in the waveform. Samples of the digitized echo are the primary source for any waveform analysis using parametric functions. On the other hand, for some FW lidar scanning systems, describing the complex system response model using a simple parametric function seems challenging or impractical. Earlier studies have shown the potential of a waveform’s digital samples as relevant waveform attributes for point cloud classification. The main goal of this study is to extend earlier experiments on direct exploitation of returned waveform signals collected by a FW terrestrial laser scanning (TLS) system to multireturn waveform signals for point cloud classification in a built environment. Furthermore, the classification performance on feature vectors containing calibrated waveform attributes, derived from a waveform processing approach performed in real-time by the FW TLS system, is evaluated on multiple-echo waveforms and compared with the classification performance derived from the proposed FW data classification technique via deep learning. Classification performance derived through the proposed technique demonstrates high information content of raw digitized waveform samples. Results show that feature vectors containing samples of digitized echoes carry more information about the physical properties of the target than those containing calibrated waveform attributes.",[],[]
"We evaluated the performance of unmanned aerial systems (UAS) airborne light detection and ranging (lidar) data in the species classification of pine, spruce, and broadleaf trees. Classifications were conducted with three machine learning (ML) approaches (multinomial logistic regression, random forest, and multilayer perceptron) using features computed from automatically segmented point clouds that represent individual trees. Trees were segmented from the point cloud using a marker-controlled watershed algorithm, and two types of features were computed for each segment: intensity and texture. Textural features were computed from gray-level co-occurrence matrices built from horizontal cross-sections of the point cloud. Intensity features were computed as the average intensity values within voxels. The classification accuracies were validated on 39 rectangular 30 m x 30 m field plots using leave-one-plot out cross-validation. The results showed only very small differences in the classification performance between the different ML approaches. Intensity features provided greater classification accuracy (kappa 0.73-0.77) than textural features (kappa 0.60-0.64). However, the best classification results (kappa 0.81) were achieved when both intensity and textural features were used. Feature importance in the different ML approaches was also similar. We conclude that the accurate classification of the three tree species considered in this study is possible using single sensor UAS lidar data.",[],[]
"Passive microwave radiative transfer models are strongly influenced by the cloud and precipitation hydrometeor properties. Particularly, they can sensitively interact with frozen hydrometeors through multiple high-frequency channels. However, frozen hydrometeors are one of the most difficult parameters to comprehend due to the lack of in situ data. Until recently, studies have attempted to describe more reasonable hydrometeor distributions using various microphysics parameterizations coupled with the weather research and forecasting (WRF) models. Herein, we aim to apply the proposed methods to passive microwave radiative transfer simulations. We implemented a passive microwave radiative transfer simulation that considers various microphysical assumptions by creating a new Mie scattering lookup table. Furthermore, we evaluated the bulk microphysics parameterizations [WDM6, Morrison (MORR), Thompson (THOM), and P3 schemes] for the tropical cyclone Krosa (2019) that were observed by the global precipitation measurement microwave imager instrument, specifically concentrating on the rimed and aggregated ice categories (snow, graupel, and P3 ice). Based on the evaluation results, we concluded the following: WDM6 graupel and MORR snow afford excessive scattering signals at 37 GHz. However, at 166 GHz, none of the parameterizations produces sufficient scattering signals for comparison with the observations. The P3 ice affords significantly underestimated scattering signals at 89 GHz and above, despite its sophisticated assumptions. On the contrary, THOM snow affords scattering signals similar to the observations, despite a shape-related error. In summary, this study introduced a method for implementing a microphysical-consistent radiative transfer computation and successfully showed how various microphysical assumptions of clouds can change the passive microwave radiative signatures.",[],[]
"Tensor decompositions are a powerful tool for multidimensional data analysis, interpretation, and signal processing. This work develops a constrained tensor decomposition framework for complex multidimensional synthetic aperture radar (SAR) data. The framework generalizes the canonical polyadic (CP) decomposition by formulating it as an optimization problem and allows precise control over the shape and properties of the output factors. The implementation supports complex tensors, automatic differentiation, and different loss functions and optimizers. We discuss the importance of constraints for physical validity, interpretability, and uniqueness of the decomposition results. To illustrate the framework, we formulate a polarimetric time series decomposition and apply it to data acquired over agricultural areas to analyze the development of four crop types at the X-, C-, and L-bands over the period of 12 weeks. The obtained temporal factors describe the changes in the crops in a compact way and show a correlation to certain crop parameters. We extend the existing polarimetric time series change analysis with the decomposition to show the changes in more detail and provide an interpretation through the polarimetric factors. The decomposition framework is extensible and promising for joint information extraction from multidimensional SAR data.",[],[]
"Laser scanning technology is becoming ubiquitous in studies involving 3-D characterizations of natural scenes, e.g., for geomorphological or archeological interpretations. Setting the point density in such scanning campaigns is usually dictated by the objects of interest within the site yet is applied to the entire scene. Such campaigns result in large data volumes, which are difficult to analyze and where the objects of interest may be hidden in the redundant data. To reduce these excessive volumes, existing simplification strategies maintain smoothness and preserve discontinuities in the point cloud but disregard the need to preserve detail at the regions of interest (ROIs). To address that, this article proposes a new, context-aware, subsampling approach that retains the high resolution of objects of interest while reducing the data load of less important regions. To do so, we identify the ROI by means of visual saliency measures and reduce the data volume only at the nonsalient regions. To facilitate progressive subsampling, the reduction is based on a hierarchical data structure that is surficial in nature. In this way, the retained representative points describe the underlying surface rather than an interpolation of it. We demonstrate our proposed model on datasets originating from different scanners that feature a variety of scenes. We compare our results to three common simplification approaches. Our results show a reduced point cloud that is similar to the original and allows analysis of ROI at the required point resolution, regardless of the level of simplification.",[],[]
"Contrails (condensation trails) are line-shaped ice clouds caused by aircraft and are a substantial contributor to aviation-induced climate change. Contrail avoidance is potentially an inexpensive way to significantly reduce the climate impact of aviation. An automated contrail detection system is an essential tool to develop and evaluate contrail avoidance systems. In this paper, we present a human-labeled dataset named OpenContrails to train and evaluate contrail detection models based on GOES-16 Advanced Baseline Imager (ABI) data. We propose and evaluate a contrail detection model that incorporates temporal context for improved detection accuracy. The human labeled dataset and the contrail detection outputs are publicly available on Google Cloud Storage at gs://goes_contrails_dataset.",[],[]
"In the perspective of long and dense time-series analyses for environmental monitoring applications, this article discusses a cross-comparison analysis between the different instruments of Landsat and Sentinel missions [thematic mapper (TM), enhanced thematic mapper plus (ETM+), operational land imager (OLI), and multispectral instrument (MSI)]. The level-2 surface reflectance (SR) products were considered (in particular, the reprocessed Collection-2 for Landsat). The calibration coefficients for four of the most popular vegetation indexes [normalized difference vegetation index (NDVI), enhanced vegetation index (EVI), soil adjusted vegetation index (SAVI), and normalized difference moisture index (NDMI)] were estimated, with the aim of harmonizing and minimizing radiometric differences for the combined use of these sensors. For this purpose, more than 20000 pairs of images almost simultaneously acquired (±1-day tolerance window) were selected over a period of several years (depending on the lifespan overlap of every sensor pair). Vegetation indices (VIs) were computed for each image, and for each cross comparison, 100 random extractions of 300 000 sample pixels were performed all over the European continent. Linear transformation functions for each VI and between each sensor couple were computed by regression analyses, also assessing the repeatability of the estimation. Furthermore, the stability over time of the obtained coefficients was assessed when enough years of corresponding observations are available.",[],[]
"The uncontrolled growing number of resident space objects (RSOs) threatens the safe operation of space-related activities. Since the beginning of the Space Age, outer space is getting populated by objects emerging after breakup events; spare components through launching, orbiting, and aging of satellite missions; collisions between functional or defunct RSOs; or by missions that either completed or began their life cycle. One potential measure toward the sustainable use of outer space may start by preventing collisions between existing RSOs. Collisions between existing RSOs will only exacerbate the current situation, which could lead to a cascade effect known as the Kessler syndrome. In the context of such collisions, the enabling of optical daylight tracking has the potential to reduce the uncertainty of the estimated state vector for each RSO, thus aiding the planning and execution of efficient avoidance maneuvers when a collision is foreseeable, as well as benefiting just-in-time collision avoidance strategies in the future. This study starts by analyzing the impact of optical daylight observations, within the domain of defunct RSOs, with respect to the currently restricted nighttime observation windows, the type of observable acquired by the observing station, and the relative geometry between the Sun, the RSO, and the ground station. We highlight the role of key hardware components on each observing system deemed critical for current observing optical ground stations to enable daylight measurement acquisition. Once we have inspected all factors deemed crucial for daylight observations in our system, we present successful daylight observations, from which we derived angular observables, ranges, and apparent brightness. We additionally provide an example where the combination of measurements acquired by the different systems, operating in the optical regime only, contributed to partial disambiguation of the tumbling motion of a selected rocket body. All observations were conducted using a scientific complementary-metal-oxide-semiconductor (CMOS) sensor and a geodetic laser ranging system. Both systems make use of the 1-m Zimmerwald laser and astrometry telescope (ZIMLAT) for measurement acquisition and target RSO tracking at the Swiss Optical Ground Station and Geodynamics Observatory Zimmerwald (SwissOGS), operated by the Astronomical Institute of the University of Bern, Switzerland.",[],[]
"Subtle volcanic deformations point to volcanic activities, and monitoring them helps predict eruptions. Today, it is possible to remotely detect volcanic deformation in mm/year scale thanks to advances in interferometric synthetic aperture radar (InSAR). This article proposes a framework based on a deep learning model to automatically discriminate subtle volcanic deformations from other deformation types in five-year-long InSAR stacks. Models are trained on a synthetic training set. To better understand and improve the models, explainable artificial intelligence (AI) analyses are performed. In initial models, Gradient-weighted Class Activation Mapping (Grad-CAM) linked new-found patterns of slope processes and salt lake deformations to false-positive detections. The models are then improved by fine-tuning (FT) with a hybrid synthetic-real data, and additional performance is extracted by low-pass spatial filtering (LSF) of the real test set. The t-distributed stochastic neighbor embedding (t-SNE) latent feature visualization confirmed the similarity and shortcomings of the FT set, highlighting the problem of elevation components in residual tropospheric noise. After fine-tuning, all the volcanic deformations are detected, including the smallest one, Lazufre, deforming 5 mm/year. The first time confirmed deformation of Cerro El Condor is observed, deforming 9.9–17.5 mm/year. Finally, sensitivity analysis uncovered the model’s minimal detectable deformation of 2 mm/year.",[],[]
"Groundwater is Afghanistan’s main water supply resource, but the insufficient information and mismanagement of the surface and groundwater system have resulted in an alarming shortage of this precious resource. The monthly groundwater storage variation (ΔGWS) has been calculated in millimeter (mm) with a trend (mm/month) for the time interval of April 2002–October 2021 using the Gravity Recovery and Climate Experiment (GRACE) dataset for five major river basins of Afghanistan. A maximum and a minimum deseasonalizedΔGWS are observed at Amu Darya (246 to −253 mm) and Hari Rud (89 to −102 mm) basins, respectively. Subsequently, the GWS deficit (GWSD) and GWSD index (GWSDI) were calculated, and a negative GWSDI value signified the groundwater drought. Analysis of the GWS abstraction (GWSabs) has also been carried out for the entire country. The estimated GWSabs trend gives a maximum value of 12.60 mm/year in the northeast and southwest parts of the country. A spatiotemporal analysis showed the maximum GWSabs variation up to 23.12 mm in 2021. Two phases of land deformation were determined in Kabul City using the interferometric synthetic aperture radar (InSAR) technique. In phase-I (2015–2017), there is a gentle negative trend [−20.66 mm/year in Upper Kabul (UKBL) and −18.54 mm/year in Lower Kabul (LKBL)], but in phase-II (2018–2020), there is a high negative trend (−151.34 mm/year in UKBL and −145.32 mm/year in LKBL). Overall, the entire country is experiencing a severe groundwater decline, apparently from the interplay of hydroclimatic and anthropogenic factors, which are most dominant in the Southern and Western parts of Afghanistan.",[],[]
"New radar remote sensing measurements of the turbulent hurricane boundary layer (HBL) are examined through analysis of airborne (Imaging Wind and Rain Airborne Profiler (IWRAP)) and spaceborne (synthetic aperture radar (SAR)) data from Hurricanes Dorian (2019) and Rita (2005). These two systems provide a wide range of storm intensities and intensity trends to examine the turbulent HBL. The central objective of the work is to document the characteristics of coherent turbulent structures (CTSs) found in the eyewall region of the HBL. Examination of the IWRAP data in Dorian shows that the peak, localized wind speeds are found inside the CTSs near the eye–eyewall interface. The peak winds are typically located at lower levels (0.15–0.50) km but sometimes are found at higher levels (1.0–1.5 km) when the CTSs are stretched vertically. A SAR overpass of Dorian’s eyewall showed ocean surface backscatter perturbations at the eye–eyewall interface that have connections to the CTSs identified in the IWRAP data. Wavelet analysis, including detailed significance testing, was performed on the IWRAP and SAR data to study the CTS wavelengths and power characteristics. Both datasets showed a multiscale structure in the wavelet power spectrum with peaks at ~10 km (eyewall), ~4–5 km (merger of small-scale eddies), and ~2 km (native scale of the CTSs). The ~2-km native scale of the CTSs is robust across intensity trends (rapid intensification, weakening, and steady state), storm cases, and region of the storm. This information is useful for turbulence parameterization schemes used in numerical models that require the specification of a turbulent length scale.",[],[]
"Along with vessel detection, vessel recognition in high-resolution SAR images was necessary in order to monitor marine vessels effectively. However, lack of target data and phase defocusing of target from its velocity limited the recognition performance, especially when using detectors based on artificial intelligence. This study accordingly proposed effective vessel recognition in high-resolution ICEYE spotlight SAR images consecutively utilizing (i) vessel detector robust to defocused moving vessels and (ii) mitigation of moving target phase distortion. In order to apply quantitative and qualitative training data enhancement, a target velocity SAR phase refocusing function was developed. The proposed target velocity SAR phase refocusing function generated defocused SLC image with respect to different target azimuth velocity, which can be utilized for both training data augmentation and refocusing of velocity-induced phase distortion. Achievement of stable vessel recognition performance was enabled from (i) robust vessel detection on defocused moving vessels and (ii) well-focused detected vessel targets, both of which were consecutively applied using the proposed target velocity SAR phase refocusing function. Vessel detection results demonstrated robust performance regardless of vessel motion and vessel recognition results significantly improved after phase refocusing, both of which were subject to quantitative and qualitative training data enhancement. Performance of the proposed algorithm was analyzed both in terms of phase focusing and velocity estimation. Refocusing performance outperformed that of conventional state-of-the-art autofocusing algorithm, modified Phase Gradient Autofocusing, while azimuth velocity estimation derived the average offset of 0.68 m/s, which was regarded more accurate than previous azimuth velocity estimators based on single-channel SAR image.",[],[]
"In the framework of passive multistatic radars, formation-flying synthetic aperture radar (FF-SAR) represents an intriguing remote sensing solution due to the enhanced imaging capabilities with respect to the conventional SAR. In this article, we focus on FF-SAR systems operating in a far-from-transmitter geometry, developing a signal model suited to this specific configuration. Additionally, we present an efficient processing scheme that, by properly combining the received raw echoes in a coherent fashion, enables two peculiar imaging modes of FF-SAR, namely, signal-to-noise ratio (SNR) improvement and high-resolution wide swath (HRWS). Simulation results show that achieved radiometric and geometric imaging performances are in line with those offered by an equivalent monostatic SAR. In HRWS imaging mode, azimuth ambiguity suppression and SNR improve as the number of satellites increases and are maximized by specific receivers along-track baselines, whose expression is provided. Finally, we present a statistical analysis of the processing performance parameters for random receivers’ positions, both assuming a fixed pulse repetition frequency (PRF) and allowing for an adaptive PRF tuning. This last analysis is also directly applicable to clusters of SAR receivers not far from the transmitter.",[],[]
"Recent development of elastic least-squares reverse time migration (ELSRTM) focuses on producing high-resolution images of elastic model perturbations such as elastic impedance images. There is a clear gap between such development and conventional elastic reverse time migration (ERTM) that generates images of PP and PS reflectors. We develop a novel ELSRTM method for iteratively enhancing PP and PS images. To achieve this purpose, we introduce a vector reflection demigration operator by combining reflection demigration theory with decoupled velocity–stress equations. This vector ELSRTM (VELSRTM) method is practical without introducing any phase distortions of sources, adjoint, or demigration wavefields and performs well with kinematically accurate migration velocity models. We demonstrate the effectiveness and advantages of our VELSRTM method using layer, Marmousi2, SEG Advanced Modeling (SEAM) models, and a field data example. Numerical results show that our VELSRTM method with only a few iterations successfully improves both PP and PS images with higher spatial resolution and more balanced imaging amplitudes than ERTM images.",[],[]
"The acquisition of wide geophysical data of vast oceans by satellites can be impeded by clouds, which may result in gaps in the data acquired in the infrared and visual bands, such as sea surface temperature (SST) data, leading to limited data usage. To address this issue, we proposed a general and robust method, named the empirical function (EF) method, which involves expanding an ocean field with space-time-separation functions and determining the functions by minimizing the expansion’s residual on the observation data while considering the prior-knowledge constraint that an ocean field varies smoothly in space and time. To test the effectiveness of the EF method, we applied it to reconstruct the 14-year cloud-free SST data in the Gulf Stream region spanning from 24.5°N to 44°N and from 82.5°W to 54.5°W. The original data consist of the 0.025∘×0.025∘gridded daily composite daytime SST products of the Moderate-Resolution Imaging Spectroradiometer on the Aqua Sun-synchronous satellite, with an annual data-missing rate fluctuating around 78% in the region. In addition, we validated the reconstructed data against in situ buoy measurements. The reconstructed SST data’s accuracies are−0.11∘C±0.91 °C (bias ± standard deviation of error) and−0.12∘C±0.67 °C in the areas without and with satellite observations, respectively, which are slightly lower and higher than the gappy satellite SST products’ accuracy of−0.14∘C±0.77 °C.",[],[]
"Quantitative estimation of regional leaf area index (LAI) is an important basis for large-scale crop growth monitoring and yield estimation. With the development of deep learning, theoretically, the use of neural networks can effectively improve the accuracy of LAI estimation, but sufficient training samples are often required due to a large number of network parameters. In an actual regional LAI quantitative estimation, there are only a few samples, which is difficult to train in networks. Therefore, a crop dual-learning generative adversarial network (CROP-DualGAN) was proposed in this article for data enhancement of small samples to estimate regional LAI. The method uses dual learning to generate hyperspectral reflectance and corresponding LAI, including two groups of generative adversarial networks, in which the generator is used to generate data that conforms to the distribution of the training set, and the discriminator is used to judge the true or false generated samples. The generators and discriminators are constantly optimized in the confrontation so that the distribution of generated data is closer to that of training samples. In single crop type experiments, 30 training samples with enhanced in VGG16 achieved the R^{2}of cereal, maize, and rape seed as 0.921, 0.990, and 0.956, and in SSLLAI-Net achieved the R^{2}of cereal, maize, and rape seed as 0.971, 0.991 and 0.962. In multiple crop types experiments, the result is lower than individual crop estimation, but higher than that without enhancement. Finally, a non-parametric test is used to prove that most improvement in LAI estimation is significant, and the accuracy would not decrease when improvement is not significant. In all, the proposed method is universal and can effectively help benchmark models to improve regional LAI estimation accuracy with neural networks.",[],[]
"Information on snow depth on sea ice and bulk sea ice density is required to convert CryoSat-2 radar freeboard ( F_{r} ) into sea ice thickness (SIT). It is difficult to obtain their information on an Arctic basin scale; therefore, most CryoSat-2 SIT products largely rely on the distributions of snow depth and bulk sea ice density derived from parameterizations, which are based on sea ice type and climatological values. Several observational studies have found that the distributions of parameterized variables are inaccurate compared to the actual distributions. This study aims to develop a new type of retrieval algorithm for snow depth, SIT and bulk density, and ice freeboard in the Arctic winter by synergizing active CryoSat-2 with passive microwave and infrared measurements. Two parameterizations for the snow–ice thickness ratio and bulk sea ice density were combined with the hydrostatic balance and radar wave speed correction equations. Consequently, solutions for the four target variables were obtained and applied to different CryoSat- 2~F_{r} , derived from empirical and waveform-fitting (WF) retracker algorithms. The retrieved thickness-related parameters based on F_{r} from the Lognormal WF retracker algorithm showed good agreement with the airborne snow depth, total freeboard, and mooring ice draft measurements. The retrieved multiyear sea ice bulk density was significantly higher than the value of 882 kg \cdot ~\text{m}^{-3} , which was used in the previous density parameterization, showing a higher agreement with values from in situ measurements. The spatial and interannual variabilities of SIT increased when the results from this study were compared with those based on previous parameterizations.",[],[]
"Six Clouds and the Earth’s Radiant Energy System (CERES) instruments on four satellites are used to produce a global continuous multidecadal record of Earth’s radiation budget (ERB) at the top-of-atmosphere (TOA). Each CERES instrument was calibrated and characterized on the ground before launch, while postlaunch calibration was conducted using onboard calibration sources. The performance of the CERES instruments is verified using vicarious approaches involving both Earth and celestial targets. In this article, we describe the calibration and validation approach and demonstrate the performance of the CERES instruments on the Terra and Aqua spacecraft over the 20-year period since launch. Validation results demonstrate that after applying the appropriate calibration corrections, all four instruments are stable and perform consistently with each other. Comparisons of observations between instruments on the two spacecraft during orbital crossings further confirm the consistent performance across all instruments over the 20-year period.",[],[]
"Retrieval of rain from Passive Microwave radiometers data has been a challenge ever since the launch of the first Defense Meteorological Satellite Program in the late 1980s. Enormous progress has been made since the launch of the Tropical Rainfall Measuring Mission (TRMM) in 1997 but until recently the data were processed pixel-by-pixel or taking a few neighboring pixels into account. Deep learning has obtained remarkable improvement in the computer vision field, and offers a whole new way to tackle the rain retrieval problem. The Global Precipitation Measurement (GPM) Core satellite carries similarly to TRMM, a passive microwave radiometer and a radar that share part of their swath. The brightness temperatures measured in the 37 and 89 GHz channels are used like the RGB components of a regular image while rain rate from Dual Frequency radar provides the surface rain. A U-net is then trained on these data to develop a retrieval algorithm: Deep-learning RAIN (DRAIN). Using only the brightness temperatures from four channels as input and no other a priori information, DRAIN is offering similar or slightly better performances than GPROF, the GPM official algorithm, in most situations. These performances are assumed to be due to the fact that DRAIN works on an image basis instead of the classical pixel-by-pixel basis.",[],[]
"Passive microwave (PM) and synthetic aperture radar (SAR) observations are essential tools for providing long time series of sea-ice cover information, including sea-ice concentration (SIC) and sea-ice extent (SIE). Large uncertainties have been revealed in PM SIC/SIE products in the marginal ice zone (MIZ) and during the melting season, where fusion with SAR data could be effective for improving accuracy due to its high spatial resolution and ability to preserve detailed ice distributions. A comprehensive comparison of PM and SAR ice cover products is needed for better data fusion. This study evaluates one of the PM SIE products, the advanced microwave scanning radiometer 2 (AMSR2) SIE product retrieved with the arctic radiation and turbulence interaction study (ARTIST) sea ice (ASI) algorithm, using a neural-network-based SAR SIE product throughout the year 2019. First, we present key results of three assessment parameters, including the overall accuracy (OA), error-of-ice (EI), and ice edge location distance (LD), and then estimate the optimal SIC segmentation threshold for AMSR2 ASI SIE. Based on OA and EI, the annual average SIC threshold of 12.24%, winter average of 9.25%, and summer average of 16.43% are obtained and regarded as optimal by excluding cases with large uncertainties. Second, the AMSR2 ASI SIE product is found to perform better in identifying thin ice and melt ponds, while the SAR NN SIE product has better detection of brash ice and frazil ice. We introduce a parameter of sea-ice fragmentation fraction (IFF) to analyze the primary impact factors behind the different performances. It is found that the ratio of LD to IFF could distinguish the aforementioned different ice conditions, thus providing hints for combining the complementary advantages of the two SIE products during data fusion.",[],[]
"Maritime surveillance using synthetic aperture radar (SAR) calls for both wide swath and high resolution. This allows frequent monitoring of large areas with high detection probability and low false alarm rate. Conventional SAR modes are, however, limited in that a wide swath can only be imaged at the expense of a reduced azimuth resolution. Ambiguous SAR modes, based on low pulse repetition frequency (PRF) or continuous variation of short pulse repetition intervals (PRIs) (staggered ambiguous mode), overcome this limitation and allow imaging a wide swath with high resolution for specific ship monitoring applications without the need for digital beamforming (DBF) or multiple receive apertures. This article reports on the demonstration of the staggered ambiguous mode via an experimental acquisition with the TerraSAR-X satellite over the North Sea. Despite technical limitations in the SAR instrument, a ground range swath of 110 km was imaged with an azimuth resolution of 2.2 m, i.e., with a resolution improvement of a factor of 8 with respect to TerraSAR-X ScanSAR mode. Despite the higher disturbance level resulting from the presence of range ambiguities of the sea clutter, a detection probability higher than 0.8 was achieved for small ships of 21 m×6m size. Range ambiguities of the ships were furthermore identified based on their position and signature. The detected ships were validated using maritime positioning data from their automatic identification system (AIS). These results motivate the adoption of ambiguous SAR modes in existing and future SAR systems and missions.",[],[]
"The Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) employs a unique multibeam photon counting approach to acquire a near-continuously sampled profile and provides more precise technology for mapping the leaf area index (LAI) at the global scale. The inversion accuracy of LAI is affected by the clumping effect, which has been an open question for spaceborne laser scanning (SLS). Here, we present a segmented method based on the path length distribution model to calculate the clumping-corrected LAI independently using ICESat-2 data. The results showed that the LAI derived by the proposed method with a 200 m segment was consistent with the airborne laser scanning (ALS)-derived LAI, with a root mean squared error (RMSE) of 0.37. A satisfactory agreement (RMSE =1.03 ) was also shown between moderate resolution imaging spectroradiometer (MODIS) LAI and ICESat-2 LAI. Moreover, the LAI derived by the proposed method was on average 31.72% higher than the LAIe derived by Beer’s law, which indicated that the proposed method achieved the purpose of correcting the clumping effect. The gap probability was calculated by the 200 m moving window and the path length distribution was obtained by the 1 m moving window as the model input had the highest accuracy. In addition, the limitation of the point cloud data and the time lag of ICESat-2 acquisitions and ALS observations may affect the inversion accuracy of LAI. This study proposed a feasible way to correct the clumping effect and invert LAI independently using ICESat-2 data, which has the potential to characterize vegetation structure precisely at regional and global scales.",[],[]
"We investigate the feasibility of a method of estimating liquid water path (LWP, the sum of LWPs of cloud and rain) over land using satellite-based Ka-band passive microwave measurements. Specifically, we utilize brightness temperatures at 36.5 GHz (TB36) from the Advanced Microwave Scanning Radiometer for the Earth Observing System (AMSR-E). TB36 is appropriate for liquid-only estimation as it is less affected by ice scattering compared to higher frequency measurements. However, estimating LWP over land using TB36 is challenging due to weak cloud signals and strong and heterogeneous land radiation in TB36. To address this, our method (Le36) dynamically estimates land emissivity using lower frequency (6.9 and 10.7 GHz) measurements from AMSR-E, minimizing land radiation errors. Synthetic simulations indicated that Le36 has high performance in cases of a wide range of LWPs (~8 kg/m 2 ) for cloud-only cases and a range of about 1.5~8.0 kg/m 2 for cloud-plus-rain cases if the cloud top (CT) height is appropriately set. For cases without suitable CT settings, emphasis should be placed on high-CT clouds (>8000 m). In real-case applications, Le36 estimates showed reasonable agreement with independent cloud radar products, even over land. Furthermore, a comparison between Le36 and a method using 89.0 GHz (Le89) reveals that Le36 outperforms Le89 for liquid-only estimation, while estimates from Le89 include ice scattering effects. This study highlights the promising performance of Ka-band LWP estimation over land by mitigating the ice scattering effect and suggests the potential for more detailed cloud water content estimation using the TB36-TB89 difference.",[],[]
"Few-shot aerial image semantic segmentation is a challenging task that requires precisely parsing unseen-category objects in query aerial images with limited annotated support aerial images. Formally, category prototypes would be extracted from support samples to segment query images in a pixel-to-pixel matching manner. However, aerial objects in aerial images are often distributed with arbitrary orientations, and varying orientations could cause a dramatic feature change. This unique property of aerial images renders conventional matching manner without consideration of orientations fails to activate same-category objects with different orientations. Furthermore, the oscillation of the confidence scores in existing rotation-insensitive algorithms, engendered by the striking changes of object orientations, often leads to false recognition of lower scored rotated semantic objects. To tackle these challenges, inspired by the intrinsic rotation invariance in aerial images, we propose a novel few-shot rotation-invariant aerial semantic segmentation network (FRINet) to efficiently segment aerial semantic objects with diverse orientations. Specifically, through extracting orientation-varying yet category-consistent support information, FRINet provides rotation-adaptive matching for each query feature in a feature-aggregation manner. Meanwhile, to encourage consistent predictions for aerial objects with arbitrary orientations, segmentation predictions from different orientations are supervised by the same label and further fused to obtain the final rotation-invariant prediction in a complementary manner. Moreover, aiming at providing a better solution searching space, the backbones are newly pretrained in the base category to basically boost the segmentation performance. Extensive experiments on the few-shot aerial image semantic segmentation benchmark demonstrate that the proposed FRINet achieves a new state-of-the-art performance. The code is available at https://github.com/caoql98/FRINet .",[],[]
"Synthetic aperture radar (SAR) satellite images are used increasingly more for Earth observation. While SAR images are useable in most conditions, they occasionally experience image degradation due to interfering signals from external radars, called radio frequency interference (RFI). RFI-affected images are often discarded in further analysis or preprocessed to remove the RFI. However, few on-ground radars can cause RFI in SAR images and such information can thus increase domain awareness greatly over both land and sea, where, e.g., localizing and characterizing RFI signals in the ocean could help classify otherwise overlooked ships. The aim of the current study is to detect and localize RFI signals automatically in Sentinel-1 level-1 images and further characterize the on-ground radar. The spatial structure of RFI signals vary greatly. A convolutional autoencoder (CAE) was therefore developed to reconstruct RFI-free Sentinel-1 images. Conversely, RFI-affected images could not be well reconstructed. Anomalous heatmaps were then developed to automatically detect and localize RFI anomalies in the images under varying environmental and geographical conditions, whereafter the external radar characteristics were extracted manually from Sentinel-1 level-0 data. We could consequently classify and localize RFI signals believed to originate from both stationary radars and ship-borne radars. We further argue that the calculated ship-borne radar characteristics correspond to those of air-surveillance radars. Empirically, the method showed better detection results than those of previous studies. Our study shows that more information can be extracted from certain detected objects, such as ships, from SAR images.",[],[]
"With the development of deep learning (DL), research on ship classification in synthetic aperture radar (SAR) images has made remarkable progress. However, such research has primarily focused on classifying large ships with distinct features, such as cargo ships, containers, and tankers. The classification of SAR fishing vessels is extremely challenging because of two main reasons: 1) the small size and minor interclass differences of fishing vessels make learning fine-grained features difficult and 2) determining fishing vessel types is difficult, resulting in a lack of labeled data. Hence, after designing a process framework for vessel tagging, we construct a high-resolution fine-grained fishing vessel classification dataset (FishingVesselSAR), which contains 116 gillnetters, 72 seiners, and 181 trawlers. We then propose a novel DL model (FishNet) that aims to strengthen feature extraction and utilization. In FishNet, we introduce four innovative modules to ensure superior performance in SAR fishing vessel classification: a multipath feature extraction (MUL) module, a feature fusion (FF) module, a multilevel feature aggregation (MFA) module, and a parallel channel and spatial attention (PCSA) module. Furthermore, we design an adaptive loss function to achieve better classification performance by mitigating the effects of class imbalance. In this article, we report extensive ablation studies conducted to confirm the efficacy of the five improvements listed above. Sufficient comparisons with 33 advanced methods from the DL and SAR target classification communities demonstrate that FishNet achieves an SAR fishing vessel classification accuracy of 89.79%, which is 6.77% higher than that of the second-best method.",[],[]
"The normalized difference index (NDI) originates from NDVI, which has been widely used in remote sensing applications and to guide the development of NDI in other fields due to its excellent performance; however, injective mapping from the original bands to NDI leads to information loss in land cover classification. When NDI is represented as a simple form, it is, furthermore, prone to premature saturation in specific change detection and variable inversion tasks. In this study, we first propose the radius index (RI), a new index to represent illumination variations by using the missing band information from NDI. Based on RI, we develop a generalized NDI (GND) by adding four positive scaling coefficients to NDI foundation, and the value range and sensitivity of GND are adjusted by these four coefficients, which are derived from the statistical information of the study area. The derivation of these four coefficients is, moreover, reversible, making it possible to interpret the applicable range of the derived set of coefficients. Our experiments demonstrate that: 1) GND is more effective in terms of improving saturation than the traditional indices and 2) mapping the original bands to GND-RI (GND combined with RI) can guide classifiers to learn more generalized features based on spectral information and thus achieve higher classification accuracy both in machine learning and the latest deep learning semantic segmentation models. The data and code for the article can be found at https://github.com/Zoulinx/GND .",[],[]
"This work develops around the problem of modeling statistics and correlation properties of the Global Navigation Satellite System (GNSS) L-band signal scattered by the moving ocean surface, presented in the companion article by Principe et al. (2021). The modeling work is here completed by deriving an effective method for simulation of the received signal as well as by making a validation study based on simulated and real data. The simulation of the stochastic process embedding the design statistical properties, the temporal correlation model and the system and surface parameters is intricate, in general, and provides a useful tool for researchers interested in modeling and simulation of GNSS reflected signals from a time-evolving sea surface. The GNSS received waveforms after 1 ms coherent integration are simulated and the correlation properties of the specular and near-specular points are compared to the theoretical model and to results from real data. Real raw data were collected during an aircraft mission in the Gulf of Finland and in a spaceborne scenario with Cyclone Global Navigation Satellite System (CYGNSS) satellites. Comparisons show good agreement among simulations, model, and real data, and demonstrate that the estimated correlation time of the GNSS-R signal fits real data with higher accuracy with respect to the case in which sea correlation is not accounted, especially when the receiving platform velocity is moderate.",[],[]
"We propose a geodesic distance (GD)-based scattering power decomposition for compact polarimetric (CP) synthetic aperture radar (SAR) data acquired over agricultural landscapes. The proposed technique decomposes the polarized portion of the total backscattered power in proportion to the normalized target similarity measures. The measures are derived from the GDs, which are computed between the Kennaugh matrices of observed and canonical targets (dihedral or trihedral). We observed a pseudo-power component in the double bounce power, which can be attributed to target irregularities. To compensate for the pseudo-power component, we proposed a compensation strategy by utilizing the CP radar vegetation index (CpRVI). The compensation factor assists in readjusting the polarized power components. The proposed approach was tested with real (RADARSAT Constellation Mission: RCM) and simulated (RADARSAT-2: RS2) hybrid CP data over agricultural sites in Canada. The effectiveness of the approach was demonstrated by comparing the decomposed powers with a recently proposed CP scattering power decomposition.",[],[]
"Technological and computational advances continuously drive forward the field of deep learning in remote sensing. In recent years, the derivation of quantities describing the uncertainty in the prediction—which naturally accompanies the modeling process—has sparked interest in the remote sensing community. Often neglected in the machine learning setting is the human uncertainty that influences numerous labeling processes. As the core of this work, the task of local climate zone (LCZ) classification is studied by means of a dataset that contains multiple label votes by domain experts for each image. The inherent label uncertainty describes the ambiguity among the domain experts and is explicitly embedded into the training process via distributional labels. We show that incorporating the label uncertainty helps the model to generalize better to the test data and increases model performance. Similar to existing calibration methods, the distributional labels lead to better-calibrated probabilities, which in turn yield more certain and trustworthy predictions. For reproducibility, we provide our code here https://github.com/ChrisKo94/LCZ_LDL and here https://gitlab.lrz.de/ai4eo/WG_Uncertainty/lcz_ldl .",[],[]
"Hyperspectral unmixing (HSU) is an effective tool to ascertain the material composition of each pixel in a hyperspectral image with typically hundreds of spectral channels. In this article, we propose two graph-based semisupervised unmixing methods. The first one directly applies graph learning to the unmixing problem, while the second one solves an optimization problem that combines the linear unmixing model and a graph-based regularization term. Following a semisupervised framework, our methods require a very small number of training pixels that can be selected by a graph-based active learning method. We assume to obtain the ground-truth information at these selected pixels, which can be either the exact (EXT) abundance value or the one-hot (OH) pseudo-label. In practice, the latter is much easier to obtain, which can be achieved by minimally involving a human in the loop. Compared with other popular blind unmixing methods, our methods significantly improve performance with minimal supervision. Specifically, the experiments demonstrate that the proposed methods improve the state-of-the-art blind unmixing approaches by 50% or more using only 0.4% of training pixels.",[],[]
"Three-dimensional geoinformation is of great significance for understanding the living environment; however, 3-D perception from remote sensing data, especially on a large scale, is restricted, mainly due to the high costs of 3-D sensors such as light detection and ranging (LiDAR). To tackle this problem, we propose a method for monocular height estimation from optical imagery, which is currently one of the richest sources of remote sensing data. As an ill-posed problem, monocular height estimation requires well-designed networks for enhanced representations to improve the performance. Moreover, the distribution of height values is long-tailed with the low-height pixels, e.g., the background (BG), as the head, and thus, trained networks are usually biased and tend to underestimate building heights. To solve the problems, instead of formalizing the problem as a regression task, we propose HTC-DC Net following the classification–regression paradigm, with the head-tail cut (HTC) and the distribution-based constraints (DCs) as the main contributions. HTC-DC Net is composed of the backbone network as the feature extractor, the HTC-AdaBins module, and the hybrid regression process. The HTC-AdaBins module serves as the classification phase to determine bins adaptive to each input image. It is equipped with a vision transformer (ViT) encoder to incorporate local context with holistic information and involves an HTC to address the long-tailed problem in monocular height estimation for balancing the performances of foreground (FG) and BG pixels. The hybrid regression process does the regression via the smoothing of bins from the classification phase, which is trained via DCs. The proposed network is tested on three datasets of different resolutions, namely ISPRS Vaihingen (0.09 m), Data Fusion Contest 19 (DFC19) (1.3 m), and Global Building Height (GBH) (3 m). The experimental results show the superiority of the proposed network over existing methods by large margins. Extensive ablation studies demonstrate the effectiveness of each design component. The codes and trained models are published at https://github.com/zhu-xlab/HTC-DC-Net .",[],[]
"The precise attitude determination and imagery positioning is crucial for remote sensing satellites to accomplish diverse observation missions. However, the complex fluctuations of environmental conditions result in severe and time-variant camera misalignment, which degrades the inertial attitude determination precision of optical payloads greatly and further hinders the improvement of positioning accuracy. Here, we develop a high-accuracy real-time attitude determination and imagery positioning system. With multiple laser sources integrated on the camera focal plane and a retroreflector reflecting the laser into a star tracker, the active optical monitoring path between the camera and the star tracker is constructed and provides the ability to monitor the camera misalignment. Using a dichroic mirror, the star tracker can detect the stars and the laser simultaneously. Then, the camera attitude determination and imagery positioning model based on the “star–laser” joint detection is established, which can precisely monitor the camera misalignment by the laser imaging and determine the inertial camera attitude combined with the star imaging. The ground simulation experiment is conducted and the results demonstrate the accuracy and effectiveness of the proposed method in monitoring the camera misalignment. Therefore, the proposed method provides an innovative idea for high-accuracy positioning.",[],[]
"A continuous and consistent fundamental climate data record (FCDR) from satellite observations is an essential source for climate research. In this study, a highly consistent multichannel brightness temperature (TB) FCDR during 1991–present has been developed using measurements from two Special Sensor Microwave Imagers (SSM/I) onboard F11 and F13 satellites and one Special Sensor Microwave Imager/Sounder (SSMIS) onboard F17 satellite from the U.S. Defense Meteorological Satellite Program (DMSP). The hardware differences between these instruments were corrected by a combination of several technologies including the principal component analysis (PCA), use of the third instrument as an intermediate, and the weighted average approach which takes into account interchannel covariability and observation matchup issues. After intercalibration, all imagers were homogenized with SSMIS, which was used as an observation reference. The mean biases of the recalibrated TBs for almost all channels between any two instruments are less than 0.2 K globally with the standard deviations (STDs) less than 1.2 K. This resulted in a 30-year long continuous and stable FCDR. Based on this FCDR, a long time series of column water vapor (CWV) over the global oceans was retrieved. Validation of this retrieved moisture product against reanalysis data and site measurements from radiosonde and Global Navigation Satellite System (GNSS) resulted in reasonably good accuracy, suggesting that the presented FCDR has high application potential for climate research.",[],[]
"Ice cores of polar regions (ice sheets) are one of the most prominent natural archives that can reveal essential historical information from the past environment of our planet. The ice-core microstructure is a key feature in determining the principal properties of ice such as pore close-off, albedo, and melt events. Microcomputer tomography (CT) scans can provide valuable information about the microstructure of materials, although achieving a high-quality automated segmentation of porous materials, especially with phase/density changes is still a challenge. This work proposes a new method for improving the segmentation of porous microstructures where a weak segmentation [Gaussian mixture model (GMM)] on high-resolution (30 μm) data is used as ground truth to train a deep-learning model (U-net) for segmentation of low-resolution (60 μm) data. This approach has reached high segmentation accuracy in terms of quantitative metrics having the F1-score of 92.5% and an intersection over the union (IoU) of 91%, with a considerable improvement compared to thresholding and unsupervised methods. Also, the segmentation results of U-net are closer to the real weight, density, and specific surface area (SSA) of the specimen.",[],[]
"Clouds play an important role in the radiative energy balance of the Earth–atmosphere system. Compared with traditional optical satellite sensors, polarimetric sensors combine multiangle, multipolarization, and multispectral information, displaying the advantages of high spatial and temporal resolutions and global coverage. Such remote sensing measurements improve the accuracy of cloud properties retrieval. Due to the observation characteristics of passive satellites, even a tiny variation in position will result in a great change in the observation geometry. A large number of studies have shown that the scattering angle is very crucial for the polarization characteristics retrieval of reflected light. In this study, we analyze the dependence of the remote sensing retrieval implementation of different cloud characteristics on the observed scattering angle coverage, considering both ice and water clouds. Three satellite sensors—POLarization and Directionality of the Earth’s Reflectance-3/Polarization and Anisotropy of Reflectance for Atmospheric Sciences coupled with Observations from a Lidar (POLDER-3/PARASOL), Directional Polarimetric Camera/GaoFen-5 spacecraft (DPC/GF-5), and DPC/GF-5(02)—were selected to compare their scattering angle coverages and the number of angular measurements at equatorial, middle, and high latitudes. The requirements for angular polarized and nonpolarized observations varied depending on the retrieval of cloud properties. The impact of orbital characteristics and viewing settings was investigated for cloud detection (C-Det), cloud phase classification, and cloud microphysical properties retrieval. Finally, an analytical model to comprehensively evaluate the effective angular measurements according to the orbital characteristics and viewing settings was developed to facilitate the future design of similar sensors for cloud remote sensing.",[],[]
"Satellite-based synthetic aperture radar (SAR) images can be used as a source of remote sensed imagery regardless of cloud cover and day–night cycle. However, the speckle noise and varying image acquisition conditions pose a challenge for change detection classifiers. This article proposes a new method of improving SAR image processing to produce higher quality difference images (DIs) for the classification algorithms. The method is built on a neural network-based mapping transformation function that produces artificial SAR images from a location in the requested acquisition conditions. The inputs for the model are: previous SAR images from the location, imaging angle information from the SAR images, digital elevation model, and weather conditions. The method was tested with data from a location in North-East Finland by using Sentinel-1 SAR images from the European Space Agency (ESA), weather data from Finnish Meteorological Institute (FMI), and a digital elevation model from National Land Survey of Finland (NLS). In order to verify the method, changes to the SAR images were simulated, and the performance of the proposed method was measured using experimentation where it gave substantial improvements to performance when compared to a more conventional method of creating DIs.",[],[]
"Incidence angle normalization is used to reduce the radiometric ambiguity within or between synthetic aperture radar (SAR) images. For sea ice, incidence angle normalization is typically constrained to winter months because of the difficulty of capturing the rapidly changing backscatter values during the melt season. Here, we make use of high-temporal-resolution RADARSAT Constellation Mission (RCM) SAR images to quantify incidence angle dependencies (slopes) for first-year ice (FYI), second-year ice (SYI), and multi-year ice (MYI) during several stages of melt. We apply a new successive image differencing method to mitigate the rapid changes in backscatter during the melt season. Slopes for SYI are shown, for the first time, for winter, and for most melt season periods. Time series of slopes are shown, also for the first time, at intervals as short as 30 min. Slopes for the early melt period (FYI: −0.230; SYI: −0.191; and MYI: −0.175 dB/1°) are similar to those for winter (FYI: −0.235; SYI: −0.208; and MYI: −0.167 dB/1°). During the snowmelt period, slopes remain similar to winter for FYI (−0.235 dB/1°) but become steeper for SYI (−0.241 dB/1°) and MYI (−0.240 dB/1°). All ice types reach their maximum slope steepness during the ponding period (FYI: −0.308; SYI: −0.283; and MYI: −0.289 dB/1°) and then become shallower again during the drainage period (FYI: −0.198; SYI: −0.207; and MYI: −0.240 dB/1°). We show that the melt-season-specific slopes provide important improvements for the visual interpretation of SAR imagery and in backscatter consistency for automated classification algorithms.",[],[]
"In this study, a method for estimating two-scale roughness influences on the ocean surface emissivity is developed by solving a simplified two-scale ocean emissivity model equation. In this model, scatterings by small-scale roughness are described by the Kirchhoff approximation. For large-scale roughness, the mean local incidence angle (LIA) is introduced to describe slanted surface slope deviation from flat surface. This study focuses on the ocean state under low/moderate wind conditions in order to preclude foam and anisotropic influences within the model. Consequently, a unique pair of two-scale roughness parameters are estimated from the equation using observed ocean emissivities from AMSR2-measured radiances. The results show that the estimated small-scale roughness at 6.925 and 10.65 GHz is linearly correlated with the 10-m height wind speedU10. As the frequency reaches 36.5 GHz, however, the scatters between small-scale roughness andU10are increased, which suggests that the Kirchhoff bistatic scattering function is not fully suitable to describe the small-scale roughness at this frequency. The linear relationships between mean LIA andU10are found with high correlation coefficients. In addition, the estimated mean LIA corresponds well with associated roughness calculated from both observed and modeled ocean wave height spectra. This evidence demonstrates that the proposed large-scale roughness parameterization is physically meaningful and, therefore, the mean LIA has a physical basis in large-scale roughness. In addition, the strong correlations between the roughness parameters andU10demonstrate the possibility to estimateU10from the AMSR2 data using intermediate parameters that are physically based on ocean surface characteristics.",[],[]
"Vector Polygons are valuable survey data, serving as crucial outputs of national geographical censuses and a fundamental data source for detecting changes in geographical conditions. Current remote-sensing image change detection methods rely on comparing images but overlook abundant historical vector results, struggle with model generalization, and lack adequate samples. Consequently, change detection remains a manual process primarily, unable to meet the requirements for automated and efficient monitoring of standardized geographical conditions. Hence, this paper proposes a change detection method for land cover vector polygons based on high-resolution remote sensing images and deep learning. Initially, the enhanced simple linear iterative clustering (SLIC) algorithm is applied to segment dual-temporal images from identical regions. Subsequently, an annotated dataset is generated using a multi-scale extraction, cropping-with-inpainting approach. Next, datasets derived from pre- and post-temporal images are used for training and testing, respectively, and the training set is purified by using two-classifier cross-validation. Finally, an improved object-oriented convolutional neural network (CNN) model performs fine-grained scene classification. The change rules and post-processing method are then integrated to identify changed vector polygons. To validate the effectiveness and superiority of the proposed method, we conducted experiments on land cover change detection using datasets from two study areas. The results indicate that the proposed method achieves precision and recall rates of 91.89% and 94.44% on dataset-1, respectively. Similarly, in dataset-2, the precision and recall rates reach 87.59% and 91.41%, respectively. These findings demonstrate the method’s efficacy in detecting changed vector polygons, reducing manual intervention, and enhancing detection efficiency.",[],[]
"Floods in Pekalongan, Indonesia, often occur due to river water overflowing during heavy monsoon rain. Simultaneously, the northern coastal area of Pekalongan, located adjacent to the Java Sea, has been affected by coastal floods due to sea level rise. The flood conditions in this area were exacerbated by land subsidence, leading to coastal inundation. Monitoring land subsidence in Pekalongan has become essential in predicting other possible land subsidence occurrence areas and mitigating the possible hazards caused by land subsidence. The analysis of land subsidence has been much easier since the introduction of radar satellites. In this study, 124 synthetic aperture radar (SAR) datasets from the Sentinel-1 radar satellite between 2017 and 2022 in descending tracks were used. The data were processed through a time-series interferometry SAR (InSAR) method based on the improved combined scatterers interferometry with optimized point scatterers (ICOPS) algorithm to provide accurate measurements over large areas by improving the selection of measurement points (MPs) from persistent scatterer (PS) and distributed scatterer (DS) points using a deep learning algorithm based on a convolutional neural network (CNN), and the resulting optimized MPs were then spatially clustered using optimized hot spot analysis (OHSA) to estimate significant points statistically and define them as hot spot points. The results of time-series deformation in Pekalongan were compared with the GPS station measurements. From the comparison, a good correlation in terms of deformation patterns between time-series InSAR and GPS measurements was observed. Our study revealed that land subsidence in Pekalongan has occurred mostly in settlement areas under the young alluvium soil, which cannot support many buildings’ maximum compression. Another cause of land subsidence in Pekalongan is excessive groundwater extraction in settlement areas. Thus, compaction in the aquifer areas may occur as a result of the reduced effective stress of the pore pressure. Further analysis of this study would involve monitoring groundwater activity using data from the Gravity Recovery and Climate Experiment (GRACE) satellite and comparing them with weather station data. The analysis of the two datasets aims to understand the relationships between groundwater storage data and the monthly precipitation in Pekalongan. Finally, the potential outcomes of land subsidence in Pekalongan will be assessed using the geographic information system (GIS) method based on susceptibility mapping.",[],[]
"This study assessed the long-term radar reflectivity ( Z) biases of collocated S- and C-band dual-polarization radars. The systematic bias, wet-radome effect (WRE), and attenuation effect were investigated. The algorithm of self-consistency utilizes Z, differential reflectivity ( Z_{\mathrm{ dr}}), and a specific differential phase ( K_{\mathrm{ dp}}) to estimate the systematic bias and WRE of both radars. Eleven years of disdrometer data in northern Taiwan were used to obtain the self-consistency and K_{\mathrm{ dp}}-based attenuation correction relation coefficients. Subsequently, a series of sensitivity tests were conducted to examine the influence of these coefficients on bias and attenuation corrections. The K_{\mathrm{ dp}}(Z,Z_{\mathrm{ dr}})relationship outperformed that of K_{\mathrm{ dp}}(Z). The K_{\mathrm{ dp}}(Z,Z_{\mathrm{ dr}})relationship with seasonal coefficients and systematic bias-corrected Z_{\mathrm{ dr}}constituted the optimal procedure. The corrected Zof collocated radars was in good agreement, lending further validity to the correction schemes. The results demonstrated that the stable systematic bias values of two radars were −1.89 to −1.14 dB and −2.46 to −1.87 dB. During the WRE period, additional underestimations of Zby nearly 4 and 7 dB were recorded for S- and C-band radars, respectively. The mean value of radar reflectivity near radar ( Z_{\mathrm{ nr}}) was obtained to identify the WRE period. In this study, an innovative quadratic polynomial fitting equation was proposed to investigate the systematic and WRE biases using Z_{\mathrm{ nr}}. Moreover, a pronounced wind intensity dependency of the WRE could be observed in the quadratic polynomial fitting equation.",[],[]
"The Earth’s interior consists of multiscale structures that range from micrometer-scale mineral assemblages to 1000-km-scale heterogeneities. Mantle plumes are one such mega-scale structure that connects the core–mantle boundary with Earth’s surface. Reconstructing these structures can provide insights into mantle material and energy convection, as well as Earth’s long-term evolution. However, mantle plume has not yet been convincingly reconstructed by electromagnetic (EM) induction; even they have significantly high electrical conductivity compared with the surrounding mantle. Here, we numerically reconstruct mantle plumes by employing a deep Earth EM induction method—geomagnetic depth sounding (GDS). We build the electrical structure of mantle plumes and conduct inversion tests to investigate how different station coverage areas, station spacings, noise levels, and response period ranges influence the construction. The test results indicate that the reconstruction of a broad 10° diameter plume head near the mantle transition zone (MTZ) requires a station coverage area of at least 10∘×10∘and a 2° station spacing; the station spacing can be increased to 5° for a 20∘×20∘coverage area. A continuous two-year record with ~5% noise is sufficient to recover the electrical structure of the plume head. Plumes with different types and roots can be distinguished by images near the MTZ, while reconstruction of the narrow tail in the deep lower mantle seems to be difficult due to the limited resolution of GDS. A mantle plume beneath South China is discovered by GDS from the field geomagnetic data. GDS is expected to be used for reconstructing the mantle plume beneath important locations and contributing to the study of Earth’s dynamics.",[],[]
"In the field of maritime surveillance, the global navigation satellite system (GNSS)-based passive radar has proven its potential for moving target detection (MTD), localization, and velocity estimation. The next stage is to investigate the possibility of obtaining the radar image of the moving ship for target recognition. However, the limited signal power budget of GNSS prevents the conventional inverse synthetic aperture radar technique that is based on target rotational motion and short observation time for GNSS-based passive radar imaging moving target. In this article, a two-stage imaging processing method relying on the target translational motion over a long observation time is proposed. The first stage confirms the presence of the target by a long-time MTD processing technique. In the second stage, based on the analysis of the Doppler history of the target signal in the slow-time domain, short-time Fourier transform and modified random sample consensus are combined to robustly estimate target velocity with reduced computation complexity. To obtain the focused bistatic image, azimuth compression is conducted by using the estimated target velocity. Finally, an image fusion operation is implemented to combine the bistatic images achievable from multiple satellites so that a multistatic image with high quality can be created. The effectiveness of the proposed method is confirmed by the real experimental results of three cargo ships illuminated by several satellites.",[],[]
"Cloud removal (CR) is a significant and challenging problem in remote sensing, and in recent years, there have been notable advancements in this area. However, two major issues remain hindering the development of CR: the unavailability of high-resolution imagery for existing datasets and the absence of evaluation regarding the semantic meaningfulness of the generated structures. In this article, we introduce M3R-CR, a benchmark dataset for high-resolution CR with multimodal and multiresolution data fusion. M3R-CR is the first public dataset for CR to feature globally sampled high-resolution optical observations, paired with radar measurements and pixel-level land-cover annotations. With this dataset, we consider the problem of CR in high-resolution optical remote-sensing imagery by integrating multimodal and multiresolution information. In this context, we have to take into account the alignment errors caused by the multiresolution nature, along with the more pronounced misalignment issues in high-resolution images due to inherent imaging mechanism differences and other factors. Existing multimodal data fusion-based methods, which assume the image pairs are aligned accurately at the pixel level, are thus not appropriate for this problem. To this end, we design a new baseline named Align-CR to perform the low-resolution synthetic aperture radar (SAR) image-guided high-resolution optical image CR. It gradually warps and fuses the features of the multimodal and multiresolution data during the reconstruction process, effectively mitigating concerns associated with misalignment. In the experiments, we evaluate the performance of CR by analyzing the quality of visually pleasing textures using image reconstruction (IR) metrics and further analyze the generation of semantically meaningful structures using a well-established semantic segmentation task. The proposed Align-CR method is superior to other baseline methods in both areas. The project is available at https://gitlab.lrz.de/ai4eo/M3R-CR .",[],[]
"We have established a workflow for a multiorder sequential joint inversion (MOSJI) of gravity and gravity gradients, that aims at modeling vertically stacked sources in various geological scenarios. We consider the joint inversion of the gravity data and one of thehth-order derivatives of the gravity data. The first step involves separate inversions, which are fundamental to fully exploit the different wavelength-content of the two quantities to invert. The joint inversion is warranted by using the scheme of a sequential joint inversion with a cross-gradient constraint. The algorithm is able to exploit different types of a priori information, such as compactness and inhomogeneous model-weighting function. First, we test this approach on a realistic synthetic model from the SEg Advanced Modeling (SEAM) Phase I model, involving salt and mother salt structures. Then, we consider a synthetic model containing either shallower or deeper karst cavities. These tests produced a better modeling of both shallower and deeper sources, when compared to the separate unconstrained inversions. Thanks to these good results, we apply our method to a real case for cavity detection in Southern Spain. The method shows an accurate modeling of the expected sources. In all the aforementioned tests, we obtain a strong decrease of the cross-gradient values and a meaningful linearization in the scatter plots of physical parameters, both indicating the good performance of the joint inversion.",[],[]
"With the huge variety of Earth observation satellite missions available nowadays, the collection of multisensor remote sensing information depicting the same geographical area has become systematic in practice, paving the way to the further breakthroughs in automatic land cover mapping with the aim to support decision makers in a variety of land management applications. In this context, along with the increase in the volume of data available, the availability of ground-truth (GT) data to train supervised models, which is usually time-consuming and costly, may even be more critical. In this scenario, the possibility to transfer a model learned on a particular time span (source domain) to a different period of time (target domain), over the same geographical area, can be advantageous in terms of both cost and time efforts. However, such model transfer is challenging due to different climate, weather, or environmental conditions affecting remote sensing data collected at different time periods, resulting in possible distribution shifts between the source and target domains. With the aim to cope with the multisensor temporal transfer scenario in the context of land cover mapping, where multitemporal and multiscale information are used jointly, we propose a Multisensor, Multitemporal, and Multiscale SPatially Aware Domain Adaptation (M3SPADA) framework, a deep learning methodology that jointly exploits self-training and adversarial learning to transfer a multisensor land cover classifier (MSLCC) from a time period (year) to a different one on the same geographical area. Here, we consider the case in which each domain (source and target) is described by a pair of remote sensing datasets: a satellite image time series (SITS) of optical images and a single very high spatial resolution (VHR) scene. Experimental evaluation on a real-world study case located in Burkina Faso and characterized by operational constraints shows the quality of our proposal to deal with the temporal multisensor transfer in the context of land cover mapping.",[],[]
"We present the expected performance for a ground-based terahertz (THz) radiometer, a plan to be launched on the TERahertz EXplore-1 (TEREX-1) Mars exploration microspacecraft. The small THz passive radiometer has been developed for the TEREX series of future microspacecrafts. This spacecraft is an opportunity for organizations with limited resources and technology to conduct frequent missions to Mars well suited for resource exploration in contrast to all of the current and past Mars missions of large/giant class missions with fully government lead. The observation frequencies of the TEREX-1 radiometer are 474.64–475.64 and 486.64–487.64 GHz with a 100-kHz resolution, and the double-sideband noise temperature less than 3000 K. A theoretical error analysis is performed with the instrument characteristics to assess for the first time up-looking observations of atmospheric oxygen molecules (O 2 ) and water vapor (H 2 O). Measurement errors for O 2 and H 2 O are 7%–22% and 14%–25% with 8–17- and 5–10-km vertical resolution in the vertical ranges 0–55 and 0–25 km, respectively. TEREX-1 is also capable to measure minor species, O 3 and H 2 O 2 , with a precision better than 30% within two independent layers. We used the integration time of 1 h for all simulations. Our theoretical simulation showed the instrument characteristics of the TEREX-1 sensor are able to observe vertical profiles of O 2 and H 2 O abundances with the same level of the large class missions.",[],[]
"This article studies the orbital perturbation effects on the azimuthal resolution in lunar-based synthetic aperture radar (LBSAR). We derive explicit expressions for the Doppler frequency modulation rate (DFMR) and beam-crossing velocity using the antenna beam pointing and orbit models. Following that, the azimuthal resolution is expressed in line with orbital elements and synthetic aperture radar (SAR) configurations. The results show that the long-term orbital variations caused by accumulated perturbation effects significantly affect the azimuthal resolution, which, in effect, produces aperiodic variations in the azimuthal resolution. Such a phenomenon is most distinguished for a large LBSAR look angle, leading to a fluctuation of over 30% or even larger in the azimuthal resolution across different cycles. In addition, the errors given rise by short-term orbital perturbations could impact azimuthal resolution to a lesser extent, with corresponding fluctuations consistently below 3%. The findings reveal that it is imperative to consider the irregular variability of azimuthal resolution due to orbital perturbations in the LBSAR.",[],[]
"Single-pass InSAR elevation measurements of dry snow, firn, and ice are known to be substantially biased downward due to a partial penetration of the radar signals into the medium, resulting in a phase center location within the volume. The so-called penetration bias, i.e., the elevation difference between surface and InSAR phase center, can be estimated from the contribution of the volume to the interferometric coherence and may be used to retrieve the surface elevation. In this paper, we show that both an additional elevation bias and a horizontal shift occur in the InSAR processing for natural media with a dielectric constant different to the one of air, originating from an uncompensated stretch of the vertical wavenumber in the medium and refraction effects at the surface. This geolocation error depends on the magnitude of the penetration bias, the dielectric constant, and the acquisition geometry. It may reach up to few meters for X- and C-band frequencies and more for lower frequencies and therefore may significantly affect cryospheric elevation products from past (SRTM), current (TanDEM-X), and future (e.g., Harmony, Tandem-L) SAR interferometers. In this paper, the geolocation error is assessed and an adapted interferometric processing allowing for an accurate geolocation (i.e., surface elevation measurement) is presented.",[],[]
"This study aims at relating the stickiness parameter (τ) of the dense media radiative transfer theory in quasi-crystalline approximation of Mie scattering of densely packed sticky spheres (DMRT-QMS), to the physical parameters of the layered snowpack. A relationship has been derived to expressτ, which modulates the attractive contact force between ice spheres, as a function of ice volume fraction (ϕ) and coordination number (nc). Sinceτis not a measurable parameter, this is a step forward with respect to what is commonly made in the literature, whereτis assumed as an arbitrary parameter, generally ranging between 0.1 and 0.3, to fit simulated backscattering data with those measured. As a first validation, DMRT-QMS was integrated with the SNOWPACK model to simulate backscattering at X-band (9.6 GHz) driven by nivo-meteorological data acquired on a test area located in Monti Alti di Ornella, Italy. The simulations were compared with Synthetic Aperture Radar COSMO-SkyMed (CSK) satellite observations. The results show a significant agreement (R2=0.68), although for a limited dataset of eight points in a unique winter season.",[],[]
"Human modification of the landscape affects total suspended solid (TSS) concentrations in water. The quantitative extent of these changes remains poorly understood, partly because of the challenges associated with observing TSS dynamics in inland waters over large scales. While many current missions and sensors provide usable data to estimate inland water quality (e.g. Landsat series, VIIRS, and Sentinel-2), future missions present the opportunity to increase transferability and accuracy of TSS estimation. Here, we degrade assumed ideal spectral data to evaluate the optimal data quality for TSS retrieval using an optical sensor configuration. We also perform wavelet analysis and a river size distribution analysis to study temporal and spatial data quantity requirements, respectively. We find that while the highest resolution data always gives the best retrieval accuracy, some factors are more essential in TSS estimation than others and can simplify mission design. Specifically, fine hyperspectral resolution is key in improving retrieval accuracy and a finer spatial resolution allows exponentially more river surface area to be observed. A revisit period of approximately five days or less best captures TSS pulse events, such as floods. Understanding the optimal mission specifications for observing inland water quality, especially TSS, will assist in developing and proposing future optical satellite missions.",[],[]
"Precipitation observations from a ground-based gauge provide a reliable data source for hydrological and climatological studies. However, these data are sparse in many regions of the world, particularly the Mekong River Basin (MRB). Satellite-based precipitation products (SPPs) are the sole data source available with worldwide coverage. Despite this, there is a mismatch between SPPs and gauge-based observations, and the correct procedures should be utilized to minimize systematic bias in SPPs. This study aimed to benchmark the efficacy of four state-of-the-art bias-correcting deep learning models (DLMs) for the tropical rainfall measuring mission-based precipitation product named TRMM_3B42 (hereafter TRMM) over the entire MRB. These models were designed mainly based on convolutional neural network (CNN) and encoder–decoder (ENDE) architectures, including ConvENDE, ConvUNET, ConvINCE, and ConvLSTM. The bias-corrected dataset by DLMs was then confirmed against the gauge-based dataset (Asian precipitation-highly resolved observational data integration toward evaluation of water resources, APHRODITE). From the results obtained, all four DLMs effectively minimized the bias of the TRMM product. Among them, ConvENDE and ConvUNET had a higher consistency and performance level compared to ConvINCE and ConvLSTM. Additionally, the complexity of DLMs did not enhance their efficiency, as is the case with ConvINCE and ConvLSTM, despite using many computing resources. Given the observed data shortage for the MRB since 2016, the application of DLMs, such as ConvENDE and ConvUNET, can serve to improve the reliability of existing rainfall datasets and provide valuable input for various research purposes in the MRB.",[],[]
"The high-resolution imaging camera (HiRIC) onboard China’s Tianwen-1 Mars probe aims to acquire detailed imagery of the Martian surface to comprehensively investigate its topography and geomorphology. The HiRIC is a pushbroom camera comprising three CCDs to simultaneously achieve submeter resolution and a large swath. However, processing HiRIC images using the conventional photogrammetric workflow is difficult due to the large shifts and narrow overlapping among the CCD lines. This article presents a novel approach for photogrammetric processing of HiRIC images for precision topographic mapping that incorporates: 1) the fitting of the initial rational polynomial coefficients (RPCs) of images from the HiRIC position and pointing data; 2) a deep-learning-based method for tie-point matching between adjacent CCD images and cross-orbit images; 3) the bundle adjustment of multiple CCD images for tripled-epipolar image generation to ensure inner orbit consistency; 4) the block adjustment of multiple orbit images to ensure cross-orbit consistency; and 5) dense image matching and space intersection based on the refined RPCs to generate digital elevation models (DEMs). Experimental analyses were conducted using HiRIC images covering the landing region of the Zhurong rover. The results revealed that subpixel accuracy was achieved for image residuals among multiple-CCD or multiple-orbit images. Comparison with the reference data (HiRISE and MOLA DEMs) revealed a mean deviation of less than 7 m in terms of the geometric accuracy and the subtle topographic details of the HiRIC DEM. The presented approach offers a reliable solution for using the new dataset of HiRIC imagery for Mars topographic mapping.",[],[]
"The vertical distribution profiles of NO 2 are essential for understanding the mechanisms, detecting near-surface emissions, and tracking pollutant transportation at high altitude. However, most of the published NO 2 studies are based on the surface 2-D measurements. The ground-based 3-D remote-sensing stations were recently built to measure vertical distribution profiles of NO 2 . However, the stations were spatially sparse due to the high cost and could not make the measurements without sunlight. In this study, we first developed a multimodel fusion network (MF-net) based on the sparse vertical observations from the Jing-Jin-Ji region. We achieved the 3-D profile prediction of NO 2 in the range of 39.005–41.405N and 115.005–117.905E with 24-h coverage. The MF-net significantly surpassed the conventional WRF-CHEM model and provided a more accurate evaluation of the NO 2 transmission between Beijing and the neighboring cities. Besides, the MF-net covers the monitoring of NO 2 to the whole study area and extends the monitoring time to the entire day (24 h), making it serviceable for continuous spatial-temporal estimation of NO 2 and its transmission in pollution events. The MF-net provides more robust data support to formulate reasonable and effective pollution prevention and control measures.",[],[]
"Large-scale characterization of water table depth in shallow aquifers in hyperarid areas provides crucial insights into groundwater dynamics under increasing anthropogenic discharge and climatic fluctuations. Due to their penetration capabilities into arid soils, airborne very-high-frequency (VHF) sounding radars can achieve this objective under specific system design, topographic and geophysical constraints, superseding sporadic well logs, and ground-based surveys that provide compromised assessments of the distribution and depth of these water bodies. One of the least constrained ambiguities limiting the design of such systems, however, is the maximum penetration depth in desiccated sandy soils, which covers a sizeable fraction of desert landscapes. To constrain the latter, we perform a ground survey using 50- and 80-MHz GPRs with effective dynamic ranges of ~80-dB at the surface to probe the unconfined aquifer under desiccated linear dunes in the Wahiba Sands in Oman. Our survey resolves the water table down to at least 69 m depth, the deepest achieved at VHF frequencies in hyperarid terrains. We observe the average two-way plane-wave subsurface radar attenuation, accounting for both dielectric and scattering losses, to range from 0.1 to 1.4 dB/m through these sandy formations. Dielectric and scattering losses can be of equal magnitude depending on the sounding frequency and stratigraphic setting of the subsurface. Penetration depths to the water table are validated with time-domain electromagnetic (TDEM) measurements and well-log data. In addition, we identify shallow paleochannels from L-band synthetic aperture radar (SAR) observations that suggest modern meteoritic recharge of the probed aquifer, creating shallow localized anomalous losses in the radar signal in the first few meters. We conclude that the minimum requirements for an airborne VHF sounding radar to probe shallow aquifers at depths of tens of meters in sandy formations in hyperarid areas are a signal-to-noise ratio (SNR) of 55 dB at the surface, a bandwidth of 10 MHz, and a surface $h_{\mathrm {rms}}$ not exceeding 2 m.",[],[]
"Semantic segmentation is a fundamental and crucial task that is of great importance to real-world satellite image-based applications. Yet a widely acknowledged issue that occurs when applying the semantic segmentation models to unseen scenery is that the model will perform much poorer than when it was applied to scenery similar to the training data. This phenomenon is usually termed as the domain shift problem. To tackle it, this article presents a self-training-based unsupervised domain adaptation (UDA) method. Different from the previous self-training approaches which focus on rectifying and improving the quality of the pseudo labels, we instead seek to exploit feature-level relation among neighboring pixels to structure and regularize the prediction of the adapted model. Based on the assumption that spatial topological relation is maintained despite the impact of the domain shift, we propose a novel self-training mechanism to perform DA by exploiting local relation in the feature space spanned by the teacher model, from which the pseudo labels are generated. Quantitative experiments on four different public benchmarks demonstrate that the proposed method can outperform the other UDA methods. Besides, analytical experiments also intuitively verify the proposed assumption. Codes will be publicly available at https://github.com/zhu-xlab/PFST .",[],[]
"Waveform decomposition is needed as a first step in the extraction of various types of geometric and spectral information from hyperspectral full-waveform light detection and ranging (LiDAR) echoes. We present a new approach to deal with the “pseudo-monopulse” waveform formed by the overlapped waveforms from multitargets when they are very close. We use one single skew-normal distribution (SND) model to fit waveforms of all spectral channels first and count the geometric center position distribution of the echoes to decide whether it contains multitargets. The geometric center position distribution of the “pseudo-monopulse” presents aggregation and asymmetry with the change of wavelength, while such an asymmetric phenomenon cannot be found from the echoes of the single target. Both theoretical and experimental data verify the point. Based on such observation, we further propose a hyperspectral waveform decomposition method utilizing the SND mixture model with: 1) initializing new waveform component parameters and their ranges based on the distinction of the three characteristics (geometric center position, pulsewidth, and skew coefficient) between the echo and fit SND waveform; 2) conducting single-channel waveform decomposition (SCWD) for all channels; 3) setting thresholds to find outlier channels based on statistical parameters of all single-channel decomposition results (the standard deviation and the means of geometric center position); and 4) reconducting SCWD for these outlier channels. The proposed method significantly improves the range resolution from 60 to 5 cm at most for a 4-ns width laser pulse and represents the state-of-the-art in “pseudo-monopulse” waveform decomposition.",[],[]
"Advanced Microwave Scanning Radiometer-2 (AMSR2) is a successor of AMSR for Earth-Observation System (AMSR-E), while the third generation of AMSR (AMSR3) will be launched in the near future. The AMSR2 soil moisture (SM) product is also an important component of the SM operational products system (SMOPS) datasets that are operationally produced by National Oceanic and Atmospheric Administration (NOAA). The refinement of the NOAA AMSR2 SM data product can not only benefit the past AMSR-E and the upcoming AMSR3 but also improve the SMOPS data quality. In this second article of the two-part series, the extreme gradient boosting (XGB) model was trained using the AMSR2 6.925-, 10.65-, 18.7-, and 36.5-GHz brightness temperature (Tb) measurements in dual polarizations, ancillary maps, and the vegetation index datasets and in turn used to predict the daily global AMSR2 SM retrievals from 2012 to 2021. Validation results show that the refined AMSR2 SM retrievals (AMSRr) show an overwhelming advantage in data accuracy over the currently operational AMSR2 (AMSRc) product. Compared to the AMSRc, the developed AMSRr presents a significant improvement on data availability. Results also indicate that the refined AMSR2 datasets are comparable with the latest version SM active passive (SMAP) SM product. Based on this study, higher quality AMSRr SM data product will be operationally produced in the NOAA and will eventually benefit the NOAA SMOPS blended SM product and its users.",[],[]
"Vehicle-mounted multichannel ground penetrating radar (MC-GPR) is a revolutionary technology that facilitates the acquisition of volume images by arranging multiple antennas; however, its images are highly affected by noise due to different antenna characteristics. This study proposes reflectivity-consistent sparse blind deconvolution (RC-SBD) for appropriate denoising of ground penetrating radar (GPR) volume images. RC-SBD interprets the observed waveform as the convolution of the emitted wavelets and reflectivity, plus stationary clutter such as reflections from the vehicle itself. The method obtains denoised reflectivity by estimating the wavelets and clutter. The key feature of RC-SBD is that it extends the existing SBD method to 3-D, and introduces an assumption of reflectivity smoothness in the horizontal direction, expressed by the total variation (TV) regularization term. The estimation is formulated as a minimization problem involving \ell _{2}and \ell _{1}norms and is optimized using the Split–Bregman algorithm. Trade-off hyperparameters of the objective function are optimized via Bayesian optimization, maximizing the kurtosis of the calibrated volume image. Validation with synthetic data demonstrates accurate wavelet estimation and significant denoising of the volume image. Real-world data application further reveals considerable improvements in the channel-depth cross section, providing a clear visualization of structures like rebar and steel plates. Notably, the calibrated image remains stable across diverse datasets, including earthwork and bridge sections, showcasing the versatility and reliability of the proposed methodology.",[],[]
"The reflection characteristics of electromagnetic waves can be used to estimate certain environmental properties of the Earth surfaces. In this study, a low-cost and low-complexity dual circularly polarized reception (DCPR) system is used to measure the relative permittivity of land and seawater. This system consists of a custom, phase stable, dual circularly polarized antenna (DCPA) and off-the-shelf receivers for simultaneous reception of direct and reflected Global Navigation Satellite System (GNSS) signals. DCPR offers two distinct improvements in comparison with the dual linear polarization reception. First, the DCPR system enables two low-correlation data streams due to the utilization of GNSS polarization and its orthogonal counterpart. Secondly, it enables easy observation of the surface scattering characteristics through the behavior of the reflected cross-polarized carrier-to-noise density ratio (C/N0). The right-hand (RHCP) and left-hand circularly polarized (LHCP) reflection coefficients were extracted from the received C/N0 streams using the GNSS-Interference Pattern Technique. The extracted signal peaks were fitted with low-order curve functions for a better approximation of the Brewster angles. The measured RHCP and LHCP reflection coefficients enabled the determination of the Brewster angle position, leading to the relative permittivity estimate for the reflecting surfaces. The measured mean relative permittivity for the sea was 84.5 and for the land 13.57. The corresponding standard deviations were 10.48 and 8.05, and the standard error of mean were, 0.23 and 0.17, respectively. The main factors for the uncertainties were the topography of sea and land surfaces, randomly distributed vegetation growth on land, and the receiver quantization process.",[],[]
"Spatiotemporal distribution of soil moisture is important for hydrometeorological and agricultural applications. There is growing interest in monitoring soil moisture in relation to soil- and land-based natural flood management (NFM), to understand the soil’s ability, via land-use and management changes, and to delay the arrival of flood peaks in nearby watercourses. This article monitors relative surface soil moisture (rSSM) across the Thames Valley, U.K., using Sentinel-1 data, and the Vienna University of Technology (TU-Wien) Change Detection Algorithm, with a novel exploration of monthly and annual normalization factors and spatial averaging. Two pairs of normalization factors are introduced to remove impacts from varying local incidence angles through direct and multiple regression slopes. The spatiotemporal distribution of rSSM values at various spatial resolutions (1000, 500, 250, and 100 m) is assessed. Comparisons with in situ soil moisture data from the COSMOS-UK network show that, while general temporal trends agree, the difference in effective depth of measurements, coupled with vegetation impacts during the growing season, makes comparison with soil moisture observations difficult. Temporal rSSM trends can be retrieved at spatial resolutions down to 100 m, and the rSSM RMSE was found to decrease as the spatial resolution increases. The vegetation effects upon the rSSM are further explored by comparing the two dominant land cover types: Arable and Horticulture, and Improved Grassland. It was found that, while the rSSM retrieval for these land covers was possible, and the general soil moisture trend is clear, overlying vegetation during the summer artificially increased the rSSM values.","['Soil moisture', 'Backscatter', 'Radar', 'Satellite broadcasting', 'Moisture', 'Remote sensing', 'Vegetation mapping']","['Change detection algorithm', 'River Thames', 'Sentinel-1', 'soil moisture', 'synthetic aperture radar (SAR)']"
"The presence of outliers (anomalous values) in synthetic aperture radar (SAR) data and the misspecification in statistical image models may result in inaccurate inferences. To avoid such issues, the Rayleigh regression model based on a robust estimation process is proposed as a more realistic approach to model this type of data. This article aims at obtaining Rayleigh regression model parameter estimators robust to the presence of outliers. The proposed approach considered the weighted maximum likelihood method and was submitted to numerical experiments using simulated and measured SAR images. Monte Carlo simulations were employed for the numerical assessment of the proposed robust estimator performance in finite signal lengths, their sensitivity to outliers, and the breakdown point. For instance, the nonrobust estimators show a relative bias value 65-fold larger than the results provided by the robust approach in corrupted signals. In terms of sensitivity analysis and break down point, the robust scheme resulted in a reduction of about 96% and 10%, respectively, in the mean absolute value of both measures, in compassion to the nonrobust estimators. Moreover, two SAR datasets were used to compare the ground type and anomaly detection results of the proposed robust scheme with competing methods in the literature.",[],[]
"Microwave vegetation optical depth (VOD) and soil moisture (SM) can be simultaneously retrieved based on L-band radiometry with polarization information. VOD is indicative of the vegetation water content (VWC) because it captures the extinction of land surface emission. If the connectivity of VOD to VWC is robust, the pair of VWC-SM observations can be viable bases for understanding soil–plant–atmosphere water relations, providing new perspectives on ecosystem science. Simultaneous SM–VOD retrievals are feasible by inverting theτ−ωmodel with two independent datasets in dual-channel algorithms. However, given correlated satellite vertical and horizontal brightness temperatures (TBs; TB v andTBh), an ill-posed inverse problem arises where TB errors result in high uncertainties of retrievals. In this study, we apply the degrees-of-information (DoI) metric and propose a signal-to-noise ratio (SNR) metric to assess the “retrievability” of VOD given the Soil Moisture Active Passive (SMAP) TB v –TB h linear dependence. The application of these metrics allows determining where the VOD retrievals are robust and reliable. This is a necessary step in supporting the applications of VOD in ecology and hydrology. Results show that regions with mainly nonwoody vegetation have the best potential for VOD retrievals, though regularization is necessary. We then assess VOD time variations from two regularization products that reduce the impact of underdetermined inversions: the L3 dual-channel algorithm (L3-DCA) and the multitemporal dual-channel algorithm (MTDCA), which constrain VOD time dynamics with and without using a priori VOD climatology, respectively. Though they both reduce noise, especially in the VOD retrievals, they result in differences in VOD seasonal amplitude and coupling to SM at high frequencies as we outline here.","['Vegetation mapping', 'Measurement', 'Soil moisture', 'Robustness', 'Ocean temperature', 'Surface roughness', 'Sea surface']","['Microwave retrieval algorithms', 'regularization', 'soil moisture (SM)', 'Soil Moisture Active Passive (SMAP)', 'vegetation optical depth (VOD) robustness']"
"We present a sampling-free probabilistic inversion of latent target property based on the principles of expectation propagation where we estimate the joint distribution of the target variable in a local region. The prior model matches the prior distribution in the local-focused region but integrates our model parameters outside the focus region using approximate distributions. The approximate distribution includes large spatial structure information while maintaining the dimension of the inversion small. In addition, we map and solve the inversion into a new feature space where we can exclude components where the data have little influence, thereby decreasing the dimensionality of the inversion, and therefore, the inversion runtime. We test the method on seismic amplitude-versus-offset (AVO) inversion examples for the prediction of facies classes, as well as on the estimation of vuggy porosity in computed tomography (CT) images of core from a carbonate reservoir. We demonstrate that our method achieves good-quality predictions while significantly reducing the computational demand, making it particularly interesting to run large-scale inversion studies.",[],[]
"With the growing threat of unmanned aerial vehicle (UAV) intrusions, the topic of anti-UAV tracking has received widespread attention from the community. Traditional Siamese trackers struggle with small UAV targets and are plagued by model degradation issues. To mitigate this, we propose a novel searching region-free and template-free Siamese network (SiamSRT) to track UAV targets in thermal infrared (TIR) videos. The proposed tracker builds a two-stage Siamese architecture with the former providing detection of the first-frame ground truth by using a cross-correlated region proposal network (C-C RPN) and the latter providing detection of previous-frame predictions via a similarity-learning region convolutional neural network (S-L RCNN). In both stage, global proposals are acquired by region of interest (ROI) alignment operation to break the limitation of searching region. Then, a spatial location consistency function is introduced to suppress background thermal distractors and a temporal memory bank (TMB) is utilized to avoid template update degradation problem. Further, a single-category foreground detector (SCFD) is designed to independently predict the position of the UAV target. SCFD can re-initialize the tracker without the given target in the first frame, which can help to recover the tracking failures. Comprehensive experiments demonstrate that SiamSRT achieves the best performance compared to the most advanced algorithms in the anti-UAV tracking missions.",[],[]
"In supervised learning, deep learning models demand a large corpus of annotated data for object detection and classification tasks. This constrains their utility in humanitarian emergency response. To overcome this problem, we have proposed an unsupervised dwelling counting from very high-resolution satellite imagery by combining a Variational Autoencoder(VAE) with an anomaly detection approach. When VAEs are applied in earth observation for dwelling localization and counting, we observed two critical limitations (1) the balance between reconstruction and good latent code, where in-favour of good reconstruction of dwellings leads to weak anomaly score maps that fail to properly localize dwellings (2) limited spatiotemporal invariance of the learned latent code. When the model is trained with datasets obtained from different geography and time, it fails to properly localize dwellings. For the first problem, we introduced self-supervision by creating synthetic anomalies. For the second problem, we introduced latent space conditioning. The approach is tested on 9 very high-resolution images obtained from six Forcibly Displaced People settlement areas. Results indicate that combining VAE with an anomaly detection approach has reached an AUC value ranging from 0.70 at complex settlements towards 0.98 at relatively less complex settlement areas. Similarly, an MAE value of 56.67 towards 5.03 is achieved for dwelling counting. Joint training of combined datasets with latent space conditioning and self-supervision enabled the achievement of results better than classical VAE, with improved spatiotemporal transferability of the model with more crisp and strong anomaly maps. Overall implementation code will be available at https://github.com/getch-geohum/SSL-VAE.",[],[]
"Acid-saline sediments in shallow-lake environments in southwestern Australia host complex mineralogical suites representing long-term weathering and modern extreme acid-saline chemistry. It is not known whether large-scale reflectance spectroscopy datasets from watersheds across the Yilgarn Craton show regional mineralogical variability. This study assesses over 2500 spectra from datasets of shallow subsurface sediments by using automatic minima detections for spectral features of interest. Integrating microscopic and spectral observations show that proportionally similar mineral assemblages exist in a variety of recognizable textures. Spectral results indicate that iron oxide and phyllosilicates are the most detected mineral types, alongside less common alunite, jarosite, and gypsum. The difference in detection abundance by percentage for mineral groups, particularly iron oxides, accounts for the largest regional variation observed in spectra. Spectral feature characteristics show significant regional variation for iron oxide, alunite, and hydrated mineral absorptions. The spectral observations align with previous mineralogical observations of these landscapes and the expected weathering evolution for granitoids, where Al-phyllosilicates and iron oxides are most prevalent. Observations of these mineral suites enhance understanding of regional differences and similarities in chemical precipitates and products of water–rock interaction in extreme acid-saline environments and have implications for studying sediments and rocks on Mars.",[],[]
"One-bit synthetic aperture radar (SAR) imaging has garnered significant interest due to its ability to lower the cost of storing enormous amounts of data during sampling and transmission, as well as the expense of analog-to-digital converters (ADCs). However, existing one-bit SAR imaging methods suffer from high computational complexity and artifacts in the resulting images. To address these problems, the sparse logistic regression (SLR) model solved by iterative hard thresholding (IHT) is applied to one-bit SAR imaging, and a new SLR-IHT imaging method is proposed. The SLR-IHT method models the one-bit SAR imaging problem as an SLR task and optimizes the solution using the IHT framework. By leveraging the joint sparsity of the real and imaginary components, the proposed method enhances imaging quality while effectively suppressing artifacts. To accelerate computation, the Armijo step size criterion is employed to adjust the step size and support set during the iterative procedure. Moreover, a theoretical investigation into the convergence properties of the proposed method was conducted. Extensive simulations and real data experiments are conducted to evaluate the performance of the SLR-IHT method. The results demonstrate its superiority over existing one-bit SAR imaging techniques in terms of imaging quality and computational efficiency.",[],[]
"A new algorithm is proposed, which estimates two parameters of the particle size distribution (PSD) at each range bin from the global precipitation measurement’s (GPM’s) dual-frequency precipitation radar (DPR) data. The equation that expresses the relationship of the PSD parameters between adjacent range bins is derived. By including the attenuation effect within the bin in the discretized equation, the new algorithm alleviates the double-solution problem when attenuation within the bin is sufficiently large. The stability of the solutions to the equation depends on the value of the mean diameterDmand its gradient with respect to the range in the case of liquid precipitation. There is a critical diameter above which the backward processing of the equation provides a stable or moderately diverging solution, unlike the forward processing that often gives unstable solutions. To provide a set of initial conditions without using the surface reference technique (SRT) in the backward processing, an initialization method using the Hitschfeld–Bordan (HB) attenuation correction method is proposed and tested. The proposed algorithm may provide a tool for investigating the assumptions used in various algorithms.",[],[]
"Detecting the evolution of large-area landscape patterns using long-term remote-sensing images is helpful in supporting research on the relationship between landscape patterns and ecological processes, as well as the development of ecological process simulations and spatiotemporal interaction models. However, detection methods have generally been developed as separate applications, each with a separate type of landscape pattern change; remote-sensing images are acquired at epochal timesteps. Consequently, in practical applications, many omission changes for some types of pattern changes and inaccurate evolution time are presented in the detected map. In this article, state-and-evolution detection models (SEDMs) are promoted to obtain complete information about the evolution of landscape patterns based on yearly land cover data. In the proposed framework, we first define the major categories of landscape pattern changes to comprehensively reveal the characteristics of landscape pattern changes associated with real change cases. Next, a morphological rule-based pattern recognition approach is proposed for quantitative discrimination among these categories. This approach is then applied in annual land cover data to continuously detect landscape pattern evolution processes and evolution time. Finally, the detected evolution time in different evolution processes is applied to measure the timestep between two disparate types. The performances of the SEDMs are presented by Landsat-derived land cover evolution in Shanxi, China. The detected results are indirectly verified by the land cover conversion matrix and connect index, indicating strong robustness and generalization ability of the SEDMs.",[],[]
"This study regards the assessment of surface gravity surveying for CO2 plume monitoring in the deep Johansen saline aquifer, a potential offshore site for CO2 geologic storage. We used the available benchmark model and geological information to simulate the injection and postinjection phases. We calculated the gravity response at the surface from the estimated models of reservoir density and saturation at different time intervals and for different injection rates. The forward calculation is achieved by assuming a tetrahedral mesh discretization such as to ensure an accurate and detailed reconstruction of the complex reservoir. The results show a gravity anomaly extending radially around the well position and reaching a peak of about−15μGal for an injection rate of 60 kg/s. During the postinjection period, the gravity maps clearly show an increase in brine saturation around the injection well and the migration of the CO2 plume toward shallower portions of the reservoir. We also show that time-lapse gravity data can be used to successfully estimate the CO2 stored mass by means of depth from extreme point (DEXP) multiscale analysis, even when the anomaly is incompletely defined due to a not proper areal coverage of the survey. The DEXP method has proven to be very stable with respect to noise and to be an efficient technique for simultaneously determining the CO2 plume depth, its geometrical features, and stored mass.",[],[]
"Land uses (e.g., commercial, residential, and industrial lands) and functional spaces (e.g., living, productive, and ecological spaces) are two-level landscape patches and totally work as basic units for urban planning. The two-level patches are interrelated and mutually binding, but existing mapping methods extracted them separately, leading to substantial conflicts and errors in their mapping results. Accordingly, this study proposes a synergistic classification of multilevel land patches (SC-MLPs). It considers a multitask learning strategy and proposes a novel correlation loss function to measure the correlations between land uses and functional spaces, which is expected to resolve conflicts and improve the accuracy of the two-level land patch mapping results. Consequently, land-use and functional-space maps of three major Chinese cities are generated, which generally have a high resolution of 2 m and high overall accuracies of 90.1% for land uses and 93.8% for functional spaces. Compared to state-of-the-art land-use and functional-space mapping methods, our results have not only higher accuracies but also a better consistency which is improved by 36%. Accordingly, the proposed SC-MLP can generate not only accurate but also consistent maps of land uses and functional spaces, which plays a fundamental role in land system research and urban planning.",[],[]
"Our awareness of ice caps’ and mountain glaciers’ sensitivity to climate change has driven major advances in the application of remote sensing techniques during the past decade. Regarding ESA’s SARIn altimeter CryoSat-2, processing the full waveform to generate swaths of elevation estimates has become standard practice in regions of complex topographies. This technique provides information on areas where we would be blind otherwise. In this article, we discuss systematic errors and analyze their impact on surface elevation measurements and change rates of two test areas. In particular, we focus on periodically occurring errors in elevation swaths, caused by the superposition of coherent signals from range-ambiguous surfaces. They can lead to measurement errors in excess of 10 m, affect most measurements in mountainous regions, are difficult to exclude with established post-processing techniques, and occur repeatedly for satellite revisits introducing a 369-day periodicity—difficult to distinguish from the annual cycle. We show a correlation between derived elevation swaths and the sensor view angle and explore the influence of common data exclusion choices on higher level products. Our results indicate that these systematic errors hold a substantial share of the error budget and that the choice of thresholds impacts higher level products. We conclude that error correlations need to be considered to characterize the data accuracy. With the established data editing strategies, systematic errors prevent resolving seasonal mass changes of single mountain glacier basins and impact aggregates over larger areas or longer periods.",[],[]
"Generating 3-D city models rapidly is crucial for many applications. Monocular height estimation (MHE) is one of the most efficient and timely ways to obtain large-scale geometric information. However, existing works focus primarily on training and testing models using unbiased datasets, which does not align well with real-world applications. Therefore, we propose a new benchmark dataset to study the transferability of height estimation models in a cross-dataset setting. To this end, we first design and construct a large-scale benchmark dataset for cross-dataset transfer learning on the height estimation task. This benchmark dataset includes a newly proposed large-scale synthetic dataset, a newly collected real-world dataset, and four existing datasets from different cities. Next, a new experimental protocol, few-shot cross-dataset transfer, is designed. Furthermore, in this article, we propose a scale-deformable convolution (SDC) module to enhance the window-based Transformer for handling the scale-variation problem in the height estimation task. Experimental results have demonstrated the effectiveness of the proposed methods in traditional and cross-dataset transfer settings. The datasets and codes are publicly available at https://mediatum.ub.tum.de/1662763 and https://thebenchmarkh.github.io/ .",[],[]
"In this study, the potential of raw samples of digitized echo waveforms collected by full-waveform (FW) terrestrial laser scanning (TLS) for point cloud classification is investigated. Two different TLS systems are employed, both equipped with a waveform digitizer for access to the raw waveform and online waveform processing which assigns calibrated waveform attributes to each point measurement. Point cloud classification based on samples of the raw single-peak echo waveform is compared with point cloud classification based on the calibrated online waveform attributes. A deep convolutional neural network (DCNN) is designed for the supervised classification. Random forest classifier is used as a benchmark to evaluate the performance of the proposed DCNN model. In addition, feature importance and temporal stability of the raw waveform samples versus the calibrated waveform attributes for point cloud classification are reported. Classification results are evaluated at two study sites, a built environment on a university campus and a coastal wetland environment. Results show that direct classification of the raw waveform samples outperforms classification based on the set of waveform attributes at both study sites. Results also show that the contribution of the range, as the only geometric attribute in the raw waveform feature vector, significantly increases the classification performance. Finally, the performance of the DCNN for filtering ground points to generate a digital terrain model (DTM) based on classification of the raw waveform samples is assessed and compared to a DTM generated from a progressive morphological filter and to real-time kinematic (RTK) GNSS survey data.","['Laser radar', 'Feature extraction', 'Measurement by laser beam', 'Task analysis', 'Point cloud compression', 'Laser modes', 'Laser beams']","['Deep learning', 'full-waveform analysis (FWA)', 'light detection and ranging (lidar)', 'machine learning', 'point cloud classification', 'remote sensing (RS)']"
"One of the main goals of SIMBIO-SYS, the Spectrometer and Imagers for Mercury Planet Orbiter (MPO) BepiColombo (BC)-Integrated Observatory SYStem, on board ESA’s BC mission is to image the entire surface of Mercury. During the first year of operations, the visible and infrared hyperspectral imager (VIHI), one of the three optical channels of SIMBIO-SYS, will be used in accordance with the scientific requirements of the mission, to build a global map of the planet in the spectral range 0.4–2.0 μmwith a spectral sampling of 12.5 nm/band and a spatial resolution of 400 m/px. This map will be employed to derive mineralogy and surface regolith physical properties by means of spectrophotometric analyses. For the purpose of reaching this goal, a deterministic method able to predict the best GM strategy for VIHI in terms of time sequences, coverage, spatial sampling, integration/repetition times, signal-to-noise ratio (SNR) optimization, and data redundancy is discussed. Starting from these criteria, the determination of the timeline containing the sequence of the instrumental telecommands is derived from the orbital kernels of the spacecraft and within the limitation of the allocated data volume which poses strict operational constraints on the data generation and redundancy.",[],[]
"The Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi National Polar-orbiting Partnership (Suomi-NPP) satellite has been used for the early detection and daily monitoring of active wildfires. How to effectively segment the active fire (AF) pixels from VIIRS image time-series in a reliable manner remains a challenge because of the low precision associated with high recall using automatic methods. For AF detection, multicriteria thresholding is often applied to both low-resolution and mid-resolution Earth observation images. Deep learning approaches based on convolutional neural networks (ConvNets) are also well-studied on mid-resolution images. However, ConvNet-based approaches have poor performance on low-resolution images because of the coarse spatial features. On the other hand, the high temporal resolution of VIIRS images highlights the potential of using sequential models for AF detection. Transformer networks, a recent deep learning architecture based on self-attention, offer hope as they have shown strong performance on image segmentation and sequential modeling tasks within computer vision. In this research, we propose a transformer-based solution to segment AF pixels from the VIIRS time-series. The solution feeds a time-series of tokenized pixels into a transformer network to identify AF pixels at each timestamp and achieves a significantly higher F1-score than prior approaches for AFs within the study areas in California, New Mexico, and Oregon in the U.S., and in British Columbia and Alberta in Canada, as well as in Australia, and Sweden.",[],[]
"Climate-driven sea ice loss has exposed the Arctic to increased human activity, which comes along with a higher risk of oil spills. As a result, we investigated the ability of C-band polarimetric parameters in a controlled mesocosm to accurately identify and discriminate between oil-contaminated and uncontaminated newly formed sea ice (NI). Parameters, such as total power, copolarization ratio, copolarization correlation coefficient, and others, were derived from the normalized radar cross section and covariance matrix to characterize the temporal evolution of NI before and after oil spill events. For separation purposes, entropy (H) and mean-alpha (α) were extracted from eigen decomposition of the coherency matrix. TheHversusαscatterplot revealed that a threshold classifier of 0.3-Hand 18°-αcould distinguish oil-contaminated NI from its oil-free surroundings. From the temporal evolution of the polarimetric parameters, the results demonstrate that the copolarization correlation coefficient is the most reliable polarimetric parameter for oil spill detection, as it provides information on a variety of oil spill scenarios, including oil encapsulated within ice and oil spreading on top of ice. Overall, these findings will be used to support existing and future C-band polarimetric radar satellites for resolving ambiguities associated with Arctic oil spill events, particularly during freeze-up seasons.",[],[]
"Satellite image time series in the optical and infrared spectrum suffer from frequent data gaps due to cloud cover, cloud shadows, and temporary sensor outages. It has been a long-standing problem of remote sensing research how to best reconstruct the missing pixel values and obtain complete, cloud-free image sequences. We approach that problem from the perspective of representation learning and develop U-TILISE, an efficient neural model that is able to implicitly capture spatio-temporal patterns of the spectral intensities, and that can therefore be trained to map a cloud-masked input sequence to a cloud-free output sequence. The model consists of a convolutional spatial encoder that maps each individual frame of the input sequence to a latent encoding; an attention-based temporal encoder that captures dependencies between those per-frame encodings and lets them exchange information along the time dimension; and a convolutional spatial decoder that decodes the latent embeddings back into multi-spectral images. We experimentally evaluate the proposed model on EarthNet2021, a dataset of Sentinel-2 time series acquired all over Europe, and demonstrate its superior ability to reconstruct the missing pixels. Compared to a standard interpolation baseline, it increases the PSNR by 1.8 dB at previously seen locations and by 1.3 dB at unseen locations.",[],[]
"With the development of photogrammetry, digital city, and metaverse, the 3-D representation of urban buildings has attracted more and more attention. As the main form of the 3-D urban building model, the triangular mesh model has deficiencies such as high complexity, high-data volume, and low-structural information, which seriously restrict its application in spatial analysis and urban planning. This article proposes a hybrid modeling strategy geared toward the mesh model generated from oblique images to obtain building models that are compact, manifold, watertight, and have certain structural and semantic information. First of all, when the planar region topology graph has been established, a topology decoupling strategy is designed to obtain a set of relatively independent topology subgraphs which form a hierarchical structure. After that, to improve model quality, topology optimization of parallel planes has also been studied systematically. Then, we adopt a divide-and-conquer strategy to perform data-driven and model-driven building modeling for the primary and ancillary structures. Finally, a component-level simple polygon model combination is generated. Experiments prove that the proposed method has excellent visual authenticity, structural completeness advantages, and decent LoD3 ability. As a mesh simplification method, the data is compressed to 0.11%–0.75% in a Hausdorff metric around 0.3 m, which further proves that this method is state-of-the-art.",[],[]
"A physics-based approach called waveform parsimonious refraction interferometry (WPRI), which interpolates head or refraction waveform, is introduced in this work. WPRI yields a significant advantage in exploration and engineering applications, as it mitigates the excessive time and labor cost in 2-D field acquisitions that require dense receivers and shots while improving coverage in refraction seismic imaging processes. Our proposed method generates the virtual seismic refraction waveform, which involves kinematic and dynamic information with near-perfect accuracy for near-surface seismic waveform inversion and migration. To achieve this, we record data from two shot locations situated at opposite ends of the seismic profile, as well as a handful of near-offset traces along the profile. The virtual head or refraction wavefield is then determined through the convolution and cross correlation of the recorded wavefields with the objective of eliminating common wavepath. Furthermore, we introduce a source wavelet deconvolution step to correct dynamic discrepancies present in the virtual waveform. By implementing this type of technique, we are able to produce virtual seismic data that are highly accurate and can be effectively employed in near-surface seismic imaging applications.",[],[]
"In the above article [1] , Fig. 4 was incorrectly provided. The correct figure and full caption appear here.",[],[]
"Printing sensors and electronics over flexible substrates are an area of significant interest due to low-cost fabrication and possibility of obtaining multifunctional electronics over large areas. Over the years, a number of printing technologies have been developed to pattern a wide range of electronic materials on diverse substrates. As further expansion of printed technologies is expected in future for sensors and electronics, it is opportune to review the common features, the complementarities, and the challenges associated with various printing technologies. This paper presents a comprehensive review of various printing technologies, commonly used substrates and electronic materials. Various solution/dry printing and contact/noncontact printing technologies have been assessed on the basis of technological, materials, and process-related developments in the field. Critical challenges in various printing techniques and potential research directions have been highlighted. Possibilities of merging various printing methodologies have been explored to extend the lab developed standalone systems to high-speed roll-to-roll production lines for system level integration.","['Printing', 'Substrates', 'Sensors', 'Polymers', 'Flexible electronics', 'Thermal stability']","['Printed Sensors', 'Printed Electronics', 'Flexible Electronics', 'Large Area Electronics', 'Roll-to-Roll', 'Dispersion Solutions']"
"Sensing technologies for objective evaluation such as the discrimination and quantification of tastes have been developed since around 1990, before the discovery of taste receptors. Electronic tongues aim to discriminate and analyze foods and beverages and are well known as sensing technologies that greatly contribute to quality management. A taste sensor, i.e., an electronic tongue with global selectivity, is developed to realize a sensor that responds to taste chemical substances and can be used to quantify the type of taste focusing on the fact that humans discriminate the taste of foods and beverages on the tongue with the five basic tastes. In this paper, we focus on the taste sensor and describe its sensing principle, its difference from general electronic tongues that do not aim to quantify tastes, examples of its use, and the recent trend of research of electronic tongues.",[],[]
"Hierarchical routing in wireless sensor networks (WSNs) is a very important topic that has been attracting the research community in the last decade. Typical hierarchical routing is called clustering routing, in which the network is divided into multiple clusters. Recently, some types of atypical hierarchical routing arise, including chain-based, tree-based, grid-based routing, and area-based routing. There are several survey papers that present and compare the hierarchical routing protocols from various perspectives, but a survey on atypical hierarchical routing is still missing. This paper makes a first attempt to provide a comprehensive review on atypical hierarchical routing. We offer a classification of atypical hierarchical routing of WSNs, and give detailed analysis of different logical topologies. The most representative atypical hierarchical routing protocols are described, discussed, and qualitatively compared. In particular, the advantages and disadvantages of different atypical hierarchical routing protocols are analyzed with respect to their significant performances and application scenarios. Finally, we put forward some open issues concerning the design of hierarchical WSNs. This survey aims to provide useful guidance for system designers on how to evaluate and select appropriate logical topologies and hierarchical routing protocols for specific applications.","['Routing', 'Wireless sensor networks', 'Topology', 'Routing protocols', 'Data communication', 'Sensors', 'Energy consumption']","['Wireless sensor networks', 'atypical hierarchical routing', 'chain-based', 'tree-based', 'grid-based', 'area-based']"
"This paper presents and compares two different types of screen-printed flexible and conformable pressure sensors arrays. In both variants, the flexible pressure sensors are in the form of segmental arrays of parallel plate structure-sandwiching the piezoelectric polymer polyvinylidene fluoride trifluoroethylene [P(VDF-TrFE)] between two printed metal layers of silver (Ag) in one case and the piezoresistive [multiwall carbon nanotube (MWCNT) mixed with poly(dimethylsiloxane (PDMS)] layer in the other. Each sensor module consists of 4 × 4 sensors array with 1-mm × 1-mm sensitive area of each sensor. The screen-printed piezoelectric sensors array exploits the change in polarization level of P(VDF-TrFE) to detect dynamic tactile parameter such as contact force. Similarly, the piezoresistive sensors array exploits the change in resistance of the bulk printed layer of MWCNT/PDMS composite. The two variants are compared on the basis of fabrication by printing on plastic substrate, ease of processing and handling of the materials, compatibility of the dissimilar materials in multilayers structure, adhesion, and finally according to the response to the normal compressive forces. The foldable pressure sensors arrays are completely realized using screen-printing technology and are targeted toward realizing low-cost electronic skin.","['Printing', 'Sensor arrays', 'Substrates', 'Electrodes', 'Piezoresistance']","['Screen Printing', 'Flexible Electronics', 'Tactile Sensors', 'Piezoelectric Sensors', 'Piezoresistive Sensors']"
"If a wireless sensor network (WSN) is integrated into the Internet as a part of the Internet of things (IoT), there will appear new security challenges, such as setup of a secure channel between a sensor node and an Internet host. In this paper, we propose a heterogeneous online and offline signcryption scheme to secure communication between a sensor node and an Internet host. We prove that this scheme is indistinguishable against adaptive chosen ciphertext attacks under the bilinear Diffie-Hellman inversion problem and existential unforgeability against adaptive chosen messages attacks under theq-strong Diffie-Hellman problem in the random oracle model. Our scheme has the following advantages. First, it achieves confidentiality, integrity, authentication, and non-repudiation in a logical single step. Second, it allows a sensor node in an identity-based cryptography to send a message to an Internet host in a public key infrastructure. Third, it splits the signcryption into two phases: i) offline phase; and ii) online phase. In the offline phase, most heavy computations are done without the knowledge of a message. In the online phase, only light computations are done when a message is available. Our scheme is very suitable to provide security solution for integrating WSN into the IoT.",[],[]
"In wireless sensor networks, exploiting the sink mobility has been considered as a good strategy to balance the nodes energy dissipation. Despite its numerous advantages, the data dissemination to the mobile sink is a challenging task for the resource constrained sensor nodes due to the dynamic network topology caused by the sink mobility. For efficient data delivery, nodes need to reconstruct their routes toward the latest location of the mobile sink, which undermines the energy conservation goal. In this paper, we present a virtual grid-based dynamic routes adjustment (VGDRA) scheme that aims to minimize the routes reconstruction cost of the sensor nodes while maintaining nearly optimal routes to the latest location of the mobile sink. We propose a set of communication rules that governs the routes reconstruction process thereby requiring only a limited number of nodes to readjust their data delivery routes toward the mobile sink. Simulation results demonstrate reduced routes reconstruction cost and improved network lifetime of the VGDRA scheme when compared with existing work.","['Mobile communication', 'Sensors', 'Wireless sensor networks', 'Data collection', 'Mobile computing', 'Routing', 'Energy consumption']","['Routes reconstruction', 'energy efficiency', 'mobile sink', 'wireless sensor networks']"
"Recognition of human movements with radar for ambient activity monitoring is a developed area of research that yet presents outstanding challenges to address. In real environments, activities and movements are performed with seamless motion, with continuous transitions between activities of different duration and a large range of dynamic motions, compared with discrete activities of fixed-time lengths which are typically analysed in the literature. This paper proposes a novel approach based on recurrent LSTM and Bi-LSTM network architectures for continuous activity monitoring and classification. This approach uses radar data in the form of a continuous temporal sequence of micro-Doppler or range-time information, differently from from other conventional approaches based on convolutional networks that interpret the radar data as images. Experimental radar data involving 15 participants and different sequences of 6 actions are used to validate the proposed approach. It is demonstrated that using the Doppler-domain data together with the Bi-LSTM network and an optimal learning rate can achieve over 90% mean accuracy, whereas range-domain data only achieved approximately 76%. The details of the network architectures, insights in their behaviour as a function of key hyper-parameters such as the learning rate, and a discussion on their performance across are provided in the paper.","['Radar imaging', 'Monitoring', 'Doppler radar', 'Spectrogram', 'Wearable sensors']","['FMCW radar', 'micro-Doppler', 'remote activity monitoring', 'classification', 'LSTM and Bi-LSTM networks']"
"Temperature sensing is an important parameter needed to be measured by the eSkin during the physical interaction of robots with real-world objects. Yet, most of the work on sensors in eSkin has focused on pressure sensing. Here we present a skin conformable printed temperature sensor with poly(3,4-ethylenedioxythiophene): poly (styr-enesulfonate) (PEDOT:PSS)-graphene oxide (GO) as a temperature sensitive layer and silver (Ag) as contact electrodes. The demonstration of PEDOT:PSS/GO as a highly temperature sensitive layer is the distinct feature of the work. The response of presented sensor observed over ~25 °C (room temperature (RT)) to 100°C, by measuring the variation in resistance across the GO/PEDOT:PSS layer showed ~80% decrease in resistance. The sensitivity of the sensor was found to be 1.09% per °C. The sensor's response was also observed under static and dynamic bending (for 1000 cycles) conditions. The stable and repeatable response of sensor, in both cases, signifies strong adhesion of the layers with negligible delamination or debonding. In comparison to the commercial thermistor, the printed GO/PEDOT:PSS sensor is faster (~73% superior) with response and recovery times of 18 s and 32 s respectively. Finally, the sensor was attached to a robotic hand to allow the robot to act by using temperature feedback.","['Temperature sensors', 'Temperature measurement', 'Resistance', 'Robot sensing systems', 'Sensor phenomena and characterization']","['Temperature sensor', 'eSkin', 'printed electronics', 'tactile skin', 'robotics']"
"Nutrition-related diseases are nowadays a main threat to human health and pose great challenges to medical care. A crucial step to solve the problems is to monitor the daily food intake of a person precisely and conveniently. For this purpose, we present AutoDietary, a wearable system to monitor and recognize food intakes in daily life. An embedded hardware prototype is developed to collect food intake sensor data, which is highlighted by a high-fidelity microphone worn on the subject's neck to precisely record acoustic signals during eating in a noninvasive manner. The acoustic data are preprocessed and then sent to a smartphone via Bluetooth, where food types are recognized. In particular, we use hidden Markov models to identify chewing or swallowing events, which are then processed to extract their time/frequency-domain and nonlinear features. A lightweight decision-tree-based algorithm is adopted to recognize the type of food. We also developed an application on the smartphone, which aggregates the food intake recognition results in a user-friendly way and provides suggestions on healthier eating, such as better eating habits or nutrition balance. Experiments show that the accuracy of food-type recognition by AutoDietary is 84.9%, and those to classify liquid and solid food intakes are up to 97.6% and 99.7%, respectively. To evaluate real-life user experience, we conducted a survey, which collects rating from 53 participants on wear comfort and functionalities of AutoDietary. Results show that the current design is acceptable to most of the users.","['Feature extraction', 'Sensors', 'Hidden Markov models', 'Microphones', 'Acoustics', 'Monitoring', 'Accuracy']","['Food Intake Recognition', 'Wearable Sensor', 'Acoustic Signal Processing', 'Embedded System']"
"This paper reports on results obtained from monitoring the respiration and cardiac activity of a patient during a magnetic resonance imaging (MRI) survey using an optical strain sensor based on a fiber Bragg grating. The sensor is proposed specifically to acquire ballistocardiographic signals from a patient exposed to high intensity electromagnetic radiation. A Bland-Altman analysis shows the measurements that have a satisfactory accuracy for monitoring purposes, and the relative error is . The method is both noninvasive and safe for the patient. In addition, the sensor does not affect the MRI imaging quality.","['Magnetic resonance imaging', 'Heart rate', 'Optical fiber sensors', 'Monitoring', 'Strain', 'Fiber gratings']","['Ballistocardiography (BCG)', 'fiber Bragg gratings (FBGs)', 'heart rate (HR)', 'magnetic resonance imaging (MRI)', 'respiration rate (RR)', 'strain sensors']"
"The essential human gait parameters are briefly reviewed, followed by a detailed review of the state of the art in deep learning for the human gait analysis. The modalities for capturing the gait data are grouped according to the sensing technology: video sequences, wearable sensors, and floor sensors, as well as the publicly available datasets. The established artificial neural network architectures for deep learning are reviewed for each group, and their performance are compared with particular emphasis on the spatiotemporal character of gait data and the motivation for multi-sensor, multi-modality fusion. It is shown that by most of the essential metrics, deep learning convolutional neural networks typically outperform shallow learning models. In the light of the discussed character of gait data, this is attributed to the possibility to extract the gait features automatically in deep learning as opposed to the shallow learning from the handcrafted gait features.","['Deep learning', 'Foot', 'Wearable sensors', 'Knee', 'Legged locomotion', 'Force']","['Deep learning', 'floor sensor', 'gait', 'neural network', 'sensor fusion', 'video sequence', 'wearable sensor']"
"Road surface quality is essential for improving driving experience and reducing traffic accidents. Traditional road condition monitoring systems are limited in their temporal (speed) and spatial (coverage) responses needed for maintaining overall road quality. Several alternative systems have been proposed that utilize sensors mounted on vehicles. In particular, with the ubiquitous use of smartphones for navigation, smartphone-based road condition assessment has emerged as a promising new approach. In this paper, we propose to analyze different multiclass supervised machine learning techniques to effectively classify road surface conditions using accelerometer, gyroscope and GPS data collected from smartphones. Our work focuses on classification of three main class labels- smooth road, potholes, and deep transverse cracks. We hypothesize that using features from all three axes of the sensors provides more accurate results as compared to using features from only one axis. We also investigate the performance of deep neural networks to classify road conditions with and without explicit manual feature extraction. Our results indicate that models trained with features from all axes of the smartphone sensors outperform models that use only one axis. We also observe that the use of neural networks provides a significantly improved data classification. The machine learning approach discussed here can be implemented on a larger scale to monitor roads for defects that present a safety risk to commuters as well as to provide maintenance information to relevant authorities.","['Roads', 'Sensors', 'Accelerometers', 'Inspection', 'Vibrations', 'Machine learning', 'Manuals']","['Support vector machines', 'neural network', 'multilayer perceptron', 'decision tree', 'road condition', 'pavement condition', 'pothole', 'crack', 'smartphone sensor', 'accelerometer']"
"This paper aims to describe a novel smart textile that uses a single-mode hetero-core optical fiber sensor for monitoring heartbeat and respiration. The smart textile was designed by weaving hetero-core optical fibers together with the wool fabric. This novel textile that can detect variations in shapes can be incorporated into clothes. Such clothes can offer comfort to the wearer. To simultaneously monitor heartbeat and respiration, the proposed textile is sewn on to the clothes to be able to sense minute load changes produced by chest movements. The vital signs, which were calculated from the heartbeat and respiration frequency of a healthy adult, while sitting, were in agreement with those verified using commercial monitoring devices. In addition, to confirm the capability of monitoring vital signs, seven monitoring trials were performed in which the subjects were asked to wear and take off the smart cloth. The vital signs rates were successfully extracted within the four beats per minute error by the fiber optic sensor system. The proposed smart textile in clothes can monitor vital signs in daily life activities, such as sitting and standing.","['Optical sensors', 'Optical device fabrication', 'Textiles', 'Intelligent sensors', 'Adaptive optics']","['Smart textile', 'fiber optic sensor', 'hetero-core fiber optics', 'plain weave structure', 'vital signs monitoring']"
"Positioning accurately and safely a train is nowadays a great challenge. That includes currently available railway sensors and new candidate sensors for data fusion. Global Navigation Satellite System and Inertial Measurement Unit sensors arise as prominent technologies to incorporate in railways. Although satellite-based train localization tests can be found in the scientific literature, there are no common criteria to evaluate the performance of the positioning achieved. In this paper, a series of criteria is defined and justified in order to be able to evaluate the most recent and relevant works related to train positioning. The results of this comparative analysis are gathered in tables, where the criteria defined are applied to the works compiled. According to the results obtained, a research gap in safety related applications is found. It is concluded that the economic viability of given solutions should be explored, so as to design an on-board train-integrated positioning system.","['Rail transportation', 'Europe', 'Radar tracking', 'Sensor fusion', 'Sensor phenomena and characterization', 'Data integration']","['Inertial sensors', 'satellite positioning', 'train navigation', 'data fusion', 'ground truth']"
"Specifically conceived for applications related to face analytics and tracking, scene segmentation, hand/finger tracking, gaming, augmented reality, and RGB-D cameras are nowadays used even as 3-D scanners. Despite depth cameras' accuracy and precision are not comparable with professional 3-D scanners, they still constitute a promising device for reverse engineering (RE) applications in the close range, due to their low cost. This is particularly true for more recent devices, such as, for instance, the RealSense SR300, which promises to be among the best performing close range depth cameras in the market. Given the potentiality of this new device, and since to date a deep investigation on its performances has not been assessed in scientific literature, the main aim of this paper is to characterize and to provide metrological considerations on the Intel RealSense SR300 depth sensor when this is used as a 3-D scanner. To this end, the device sensor performances are first assessed by applying the existing normative guidelines (i.e. the one published by the Association of German Engineers - Verein Deutscher Ingenieure - VDI/VDE 2634) both to a set of raw captured depth data and to a set acquired with optimized setting of the camera. Then, further assessment of the device performances is carried out by applying some strategies proposed in the literature using optimized sensor setting, to reproduce “real life” conditions for the use as a 3-D scanner. Finally, the performance of the device is critically compared against the performance of latest short-range sensors, thus providing a useful guide, for researchers and practitioners, in an informed choice of the optimal device for their own RE application.","['Three-dimensional displays', 'Cameras', 'Performance evaluation', 'Sensor phenomena and characterization', 'Standards', 'Guidelines']","['SR300', 'depth camera', 'device characterization', 'VDI/VDE normative', 'structured light', 'temporal multiplexing']"
"Castable elastomers have been used to fabricate soft robotic devices and it has been shown that the technique scales well from prototyping to mass manufacturing. However, similarly scalable techniques for integrating strain or curvature sensors into such devices are still lacking. In this paper, we show that screen-printed silver conductors serve well as curvature sensors for soft robotic devices. The sensors are produced onto elastomer substrates in a single printing step and integrated into soft pneumatic actuators. We characterized the resistance-curvature relationship of the sensors, which allows the curvature of the actuators to be estimated from the sensor measurements. Hysteresis was observed, which does limit the absolute accuracy of the sensors. However, temperature characterizations showed that the sensor measurements are not significantly affected by temperature fluctuations during normal operation. Dynamic experiments showed that the bandwidth of the sensors is larger than the bandwidth of the actuators. We experimentally validated that these sensors can be used to detect whether the motion of an actuator has been blocked, clearing the way toward simple-to-fabricate soft robots that react to their surroundings. Finally, we demonstrate a three-fingered soft robotic gripper with integrated sensors. We conclude that screen-printing is a promising way to integrate curvature sensors into soft robots.","['Actuators', 'Silver', 'Sensor phenomena and characterization', 'Soft robotics', 'Electrical resistance measurement', 'Temperature measurement']","['Mechanical sensors', 'flexible printed circuit', 'soft robotics', 'strain measurement']"
"Dry electrodes are a promising solution for prolonged EEG signal acquisition, whereas wet electrodes may lose their signal quality in the same situation and require skin preparation for set-up. Here, we review the impedance and noise of passive and active dry EEG electrodes. In addition, we compare noise and input impedance of the EEG amplifiers. As there are multiple definitions of impedance in each EEG system, they are all first defined. Electrodes must be compatible with amplifiers to accurately record EEG signals. This implies that their impedance plays a significant role in amplifier compatibility and affects total input-referred noise. Therefore, we review the impedance and noise of state-of-the-art amplifiers and electrodes. Furthermore, we compare the various structures and materials used and their final impedance to that of wet electrodes. Finally, we compare state-of-the-art electrodes and amplifiers to the standards of the IFCN and IEC80601-2-26. We investigate bottlenecks and propose a guideline for future work on passive and active dry electrodes, as well as EEG amplifiers.","['Electrodes', 'Impedance', 'Electroencephalography', 'Sensors', 'Epidermis', 'Scalp']","['Electroencephalogram', 'dry electrode', 'active electrode', 'impedance', 'noise']"
"This paper reports on a low-cost turbidity sensor design for continuous on-line water quality monitoring applications. The measurement of turbidity by agricultural and environmental scientists is restricted by the current cost and functionality of available commercial instruments. Although there are a number of low-cost turbidity sensors exploited within domestic `white-goods', such as dishwashers, the lack of sensitivity, and power-usage of these devices make them unsuitable for fresh-water quality monitoring purposes. The recent introduction of wireless protocols and hardware, associated with the `Internet-of-Things' concept for machine-to-machine autonomous sensing and control, has enabled the large-scale networked intelligent water turbidity monitoring system that implements relatively low-cost sensors to be developed. The proposed sensor uses both transmitted light and orthogonal (90 degrees) scattered light detection principles, and is 2-3 orders of magnitude lower in cost compared to the existing commercial turbidity sensors. With an 850-nm infrared LED, and dual orthogonal photodetectors, the proposed design is capable of measuring turbidity within the range of 0-1000 Nephelometric Turbidity Unit (NTU) with improved accuracy and robustness as compared with the existing low cost turbidity sensors. The combination of orthogonal and transmitted light detection unit provides both 0-200 NTU high resolution and accuracy sensing and 0-1000 NTU lower resolution and accuracy sensing capability. Results from calibration experiment are presented, which proved that the proposed sensor design produced comparable turbidity readings as that of a commercial turbidity sensor.","['Monitoring', 'Wireless sensor networks', 'Sensor systems', 'Calibration', 'Phototransistors', 'Light sources']","['Turbidity measurement', 'turbidity sensors', 'fresh water quality', 'NTU', 'low cost', 'low power', 'Internet-of-Things (IoT)', 'wireless']"
"This paper presents the development of air quality low-cost sensors (LCS) with improved accuracy features. The LCS features integrate machine learning based calibration models and virtual sensors. LCS performances are analyzed and some LCS variables with low performance are improved through intelligent field-calibrations. Meteorological variables are calibrated using linear dynamic models.While, due to the non-linear relationship to reference instruments, fine particulate matter (PM 2.5 ) are calibrated using non-linear machine learning models. However, due to sensor drifts or faults, carbon dioxide (CO 2 ) does not present correlation to reference instrument. As a result, the LCS for CO 2 is not feasible to be calibrated. Hence, to estimate the CO 2 concentration,mathematicalmodels are developed to be integrated in the calibrated LCS, known as a virtual sensor. In addition, another virtual sensor is developed to demonstrate the capability of estimating air pollutant concentrations, e.g. black carbon, when the physical sensor devices are not available. In our paper, calibration models and virtual sensors are established using corresponding reference instruments that are installed on two reference stations. This strategy generalizes the models of calibration and virtual sensing which then allows LCS to be deployed in field independently with a high accuracy. Our proposed methodology enables scaling-up accurate air pollution mapping appropriate for smart cities.","['Calibration', 'Air pollution', 'Pollution measurement', 'Instruments', 'Intelligent sensors']","['Air quality', 'low-cost sensors', 'calibration', 'virtual sensors', 'machine learning']"
"This paper presents multifunctional electronic skin (e-Skin) with a stack of pressure and temperature sensors arrays. The pressure sensor layer comprises of an 8x 8 array of capacitive sensors using soft elastomers as the dielectric medium and the temperature sensing layer comprises of 4 x4 array of conductive polymers based resistive sensors. Three variants of capacitive pressure sensors were developed using two different dielectric materials (PDMS and Ecoflex) to find the best combination of performance and softness. The Ecoflex-based pressure sensor showed high sensitivity (~4.11 kPa −1 ) at a low-pressure regime (< 1 kPa) and the 7.5:1 PDMS based pressure sensor showed high sensitivity (~2.32 kPa −1 ) in the high-pressure regime (>1 kPa). Two variants of temperature sensors were fabricated using CNT and CNT & PEDOT:PSS conducting polymer composite and their performance compared. Finally, a highly sensitive CNT+PEDOT:PSS based resistive temperature sensors layer was integrated on top of 7.5:1 PDMS based capacitive pressure sensors layer to realize the e-Skin prototype. The developed e-Skin is capable of sensing pressures greater than 10 kPa with a high sensitivity of ~2.32 kPa −1 at 1 kPa and temperatures with the sensitivity of ~0.64 (%)/(°C) up to 80°C, thus demonstrating high potential for use in robotics and touch based interactive systems.","['Sensors', 'Temperature sensors', 'Sensor arrays', 'Pressure sensors', 'Sensor phenomena and characterization', 'Robot sensing systems', 'Capacitive sensors']","['Touch sensors', 'e-Skin', 'temperature sensors', 'pressure sensors', 'flexible electronics', 'sensor stack']"
"Bioimpedance spectroscopy is used in a wide range of biomedical applications. This paper presents an integrated analog readout, which employs synchronous detection to perform galvanostatic multi-channel, multi-frequency bioimpedance measurements. The circuit was fabricated in a 0.35-μm CMOS technology and occupies an area of 1.52 mm 2 . The effect of random dc offsets is investigated, along with the use of chopping to minimize them. Impedance measurements of a known RC load and skin (using commercially available electrodes) demonstrate the operation of the system over a frequency range up to 1 MHz. The circuit operates from a ±2.5 V power supply and has a power consumption of 3.4-mW per channel.","['Frequency measurement', 'Frequency modulation', 'Impedance', 'Demodulation', 'Impedance measurement', 'Semiconductor device measurement', 'Current measurement']","['Analog circuit design', 'bioimpedance', 'impedance spectroscopy', 'multi-frequency', 'readout', 'synchronous detection']"
"The problem of optimally placing sensors under a cost constraint arises naturally in the design of industrial and commercial products, as well as in scientific experiments. We consider a relaxation of the full optimization formulation of this problem and then extend a well-established greedy algorithm for the optimal sensor placement problem without cost constraints. We demonstrate the effectiveness of this algorithm on the datasets related to facial recognition, climate science, and fluid mechanics. This algorithm is scalable and often identifies sparse sensors with near-optimal reconstruction performance, while dramatically reducing the overall cost of the sensors. We find that the cost-error landscape varies by application, with intuitive connections to the underlying physics. In addition, we include experiments for various pre-processing techniques and find that a popular technique based on the singular value decomposition is often suboptimal.","['Sensors', 'Interpolation', 'Greedy algorithms', 'Matrix decomposition', 'Approximation algorithms', 'Optimization', 'Standards']","['Sensor phenomena and characterization', 'computational and artificial intelligence', 'computation theory', 'greedy algorithms', 'data systems', 'data processing', 'data preprocessing']"
"Flexible sensors that can be integrated into clothing to measure everyday functional performance is an emerging concept. It aims to improve the patient's quality of life by obtaining rich, real-life data sets. One clinical area of interest is the use of these sensors to accurately measure knee motion in, e.g., osteoarthritic patients. Currently, various methods are used to formally calculate joint motion outside of the laboratory and they include electrogoniometers and inertial measurement units. The use of these technologies, however, tends to be restricted, since they are often bulky and obtrusive. This directly influences their clinical utility, as patients and clinicians can be reluctant to adopt them. The goal of this paper is to present the development process of a patient centered, clinically driven design for an attachable clothing sensor (ACS) system that can be used to assess knee motion. A pilot study using 10 volunteers was conducted to determine the relationship between the ACS system and a gold standard apparatus. The comparison yielded an average root mean square error of∼1∘, a mean absolute error of∼3∘, and coefficient of determination above(R2)0.99 between the two systems. These initial results show potential of the ACS in terms of unobtrusive long-term monitoring.",[],[]
"Amorphous selenium (a-Se) is a photoconductive material that has been intensively investigated from its early application in xerography to its present application in flat panel X-ray imagers. It can be deposited up to a few millimeters thick over a large area. Its high vapor pressure yields uniform coverage in novel device structures for low-cost and large-area applications. The evidence of avalanche multiplication in a-Se and application of a-Se in high-gain avalanche rushing photoconductor video-tubes goes back to the early 1980s. Over the past decade there has been increasing research interest in novel detector structures and integration of a-Se with new materials to leverage the avalanche properties. We summarize some of the shortcomings of a-Se such as low charge carrier mobility, low charge conversion efficiency, depth dependence, and high dark current at high electric fields. We then highlight recent developments in a-Se-based devices to address these shortcomings and enable picosecond timing performance and high detection efficiency.","['Detectors', 'X-ray imaging', 'Dark current', 'Photoconducting materials', 'Electrodes', 'Electric fields']","['Amorphous selenium', 'X-ray image sensor', 'direct conversion', 'indirect conversion']"
"Due to the influence of the complex marine environment, the marine target detection based on statistical theory is difficult to achieve high-performance. Moreover, due to various targets' motion characteristics, only using a single feature for detection is unreliable. In this paper, from the perspective of feature extraction and classification, marine target and sea clutter are classified by deep learning methods. To achieve the required false alarm rate, the dual-channel convolutional neural networks (DCCNN) and false-alarm-controllable classifier (FACC)-based marine target detection method is proposed. Firstly, the measured sea clutter and the target signal are preprocessed to obtain the time-Doppler spectrum and amplitude information. The Marine-DCCNN (MDCCNN) is then constructed for features extraction and fusion, and the feature vectors of the signals are obtained. The performance of different feature extraction models is tested and compared. Finally, the FACC is used as a detector to classify the feature vectors into two categories and the control of the false alarm rate is realized. The detection performances were verified by two popular public radar datasets, i.e., IPIX radar dataset (floating target) and CSIR dataset (maneuvering marine target). The results show that compared with single-channel CNN and histogram of oriented gradient support vector machine (Hog-SVM) classification, a combination of MDCCNN feature extraction model and softmax classifier can achieve higher performance and controllable false alarm rate. Moreover, HH polarization and mixed training datasets under different sea states can help improve detection performance.","['Feature extraction', 'Radar', 'Convolution', 'Clutter', 'Radar detection', 'Object detection', 'Radar imaging']","['Radar target detection', 'marine target', 'feature extraction', 'dual-channel convolutional neural network (DCCNN)', 'false alarm controllable classifier (FACC)']"
"An ideal hand prosthesis should provide satisfying functionality based on reliable decoding of the user's intentions and deliver tactile feedback in a natural manner. The absence of tactile feedback impedes the functionality and efficiency of dexterous hand prostheses, which leads to a high rejection rate from prostheses users. Thus, it is expected that integration of tactile feedback with hand prostheses will improve the manipulation performance and enhance perceptual embodiment for users. This paper reviews the state-of-the-art of non-invasive stimulation-based tactile sensation for upper-extremity prostheses, from the physiology of the human skin, to tactile sensing techniques, non-invasive tactile stimulation, and an emphasis on electrotactile feedback. The paper concludes with a detailed discussion of recent applications, challenging issues, and future developments.","['Strain', 'Skin', 'Tactile sensors', 'Prosthetics', 'Force']","['Prosthetic hand', 'tactile transduction techniques', 'non-invasive stimulation feedback', 'electrotactile stimulation']"
"PThis paper presents the advanced version of novel piezoelectric oxide semiconductor field effect transistor (POSFET) devices-based tactile sensing chip. The new version of the tactile sensing chip presented here comprises of a 4 × 4 array of POSFET touch sensing devices and integrated interface electronics (i.e., multiplexers, high compliance current sinks, and voltage output buffers). The chip also includes four temperature diodes for the measurement of contact temperature. Various components on the chip have been characterized systematically and the overall operation of the tactile sensing system has been evaluated. With new design, the POSFET devices have improved performance [i.e., linear response in the dynamic contact forces range of 0.01-3 N and sensitivity (without amplification) of 102.4 mV/N], which is more than twice the performance of their previous implementations. The integrated interface electronics result in reduced interconnections which otherwise would be needed to connect the POSFET array with off-chip interface electronic circuitry. This paper paves the way for CMOS implementation of full on-chip tactile sensing systems based on POSFETs.","['Logic gates', 'Robot sensing systems', 'System-on-chip', 'Capacitance', 'Temperature measurement', 'Arrays']","['POSFET', 'CMOS', 'tactile sensing']"
"We present a platform to allow up to 50000 students to simultaneously collect and learn from their personal activity, transportation, and environmental data. The main goals that we met during the design of our sensor platform were to: 1) be low cost; 2) remain powered for the duration of the data collection campaign; 3) robustly sense a wide range of environmental parameters; and 4) be packaged in a form factor conducive to wide-spread adoption and ease of use. We describe and generalize the design methods we applied on the hardware and firmware. Our sensors employ Wi-Fi communication to move data as well as to localize themselves using a radio-map of Singapore. Our system uses embedded as well as server-based machine learning algorithms to perform on-sensor transportation mode identification and state inference. The testing and validation methods that we applied ensured that over 98% of the deployed sensors successfully met all of their design goals. In addition, we summarize the results of a large-scale deployment of our system for a nation-wide experiment in Singapore in 2015, and describe three sample applications of the collected data. We publish sample data sets and algorithm code for researchers to analyze.","['IEEE 802.11 Standard', 'Testing', 'Hardware', 'Wireless sensor networks', 'Sensor systems', 'Temperature sensors']","['Internet of things', 'wearable sensors', 'state estimation', 'machine learning', 'engineering education']"
Colon Cancer is the fourth most common form of cancer and the second-leading cause of cancer related deaths in the United States. A large part of the reason the number of fatalities is so high is late detection as when detected in the early stages of development 9 out of 10 cases of colon cancer are non-fatal 5 years after detection. Currently the main method of colon cancer detection are colonoscopies which are highly invasive and time intensive procedures. In this work we propose a new diagnostic technique for colon cancer using an InSb based device that works by a change in the spectral response of the metamaterial when exposed to electromagnetic waves in the terahertz regime and combined with cancerous colon tissue as compared to the response when combined with healthy colon tissue. We attribute this change in spectral response is due to the differing optical properties of the healthy and cancerous colon tissue as well as the creation of surface plasmon polaritons within our device.,"['Cancer', 'Colon', 'Metamaterials', 'Optical sensors', 'Optical buffering', 'Biomedical optical imaging', 'Permittivity']","['Colon cancer', 'cancer detection', 'THz metamaterials', 'perfect optical absorber', 'water-based metamaterial', 'semiconductor devices']"
"Energy efficiency is a key concern for wireless sensor nodes, especially for wireless body area network (WBAN) in which sensors operate in close vicinity to, on or even inside a human body. In this paper, we first present a system-level energy consumption model associated with transmission distance d and transmission data rate over on-body wireless communication link. Then, based on the analysis of tradeoff between circuit energy and transmission energy on distance, a threshold distance d th which is responsible for the proportion of transmission energy and circuit energy is derived for energy saving in WBAN. With the case of d ≤ d th , since circuit energy is comparable with transmission energy consumption, the total energy consumption can be saved by optimizing the transmission data rate R. Simulation results show that a 59.77% or even more energy saving is achievable using the optimized scheme, compared with baseline scheme. With d > d th , since the total energy consumption is monotonically decreasing with respect to time t, an offline algorithm is applied to energy saving by prolonging transmission time within the deadline time. In addition, on the basis of the offline algorithm, a battery-aware transmission approach is presented for WBAN using battery electrochemical property. Experimental results show that, using the presented battery-aware approach, 71.05% and 60.81% energy saving can be obtained, in comparison with the baseline and offline schemes, respectively.","['Energy consumption', 'Batteries', 'Biosensors', 'Wireless communication', 'Simulation', 'Wireless sensor networks']","['Energy efficiency', 'wireless body area network (WBAN)', 'threshold distance', 'battery', 'recovery effect']"
"This paper describes a flexible and soft tactile sensor that measures the tri-axis force based on inductance measurement. The proposed sensor has four spiral inductors printed on a flexible circuit board and a mounted cylindrical elastomer (silicon rubber). A disk-shaped magnetorheological elastomer (ferromagnetic marker) is embedded in the cylindrical elastomer and its 3-D displacement is estimated by monitoring the inductance changes of the four inductors. In this paper, we investigated the relationship between the applied tri-axis force and inductance changes. Our results can be summarized as follows: 1) the inductance changes of the four inductors were monotonic and linear against the applied normal and shear force; 2) the applied tri-axis force could be estimated well with linear functions of the sum and difference of the measured inductances; and 3) the estimation error of the tri-axis force increased when a larger force was applied and/or faster contact speeds were used.","['Inductors', 'Force', 'Tactile sensors', 'Inductance', 'Rubber', 'Silicon']","['Force and tactile sensing', 'tactile sensor', 'flexible sensor', 'magnetorheological elastomer', 'inductance measurement']"
"We propose a time-frequency processing method that localizes and enhances a target sound by exploiting spectral and spatial characteristics of the ego-noise captured by a microphone array mounted on a multi-rotor micro aerial vehicle. We first exploit the time-frequency sparsity of the acoustic signal to estimate at each individual time-frequency bin the local direction of arrival (DOA) of the sound and formulate spatial filters pointing at a set of candidate directions. Then, we combine a kurtosis measure based on the spatial filtering outputs and a histogram measure based on the local DOA estimation to calculate a spatial likelihood function for source localization. Finally, we enhance the target sound by formulating a time-frequency spatial filter pointing at the estimated direction. As the ego-noise generally originates from specific directions, we propose a DOA-weighted spatial likelihood function that improves source localization performance by identifying noiseless sectors in the DOA circle. The DOA weighting scheme localizes the target sound even in extremely low signal-to-noise conditions when the target sound comes from a noiseless sector. We experimentally validate the performance of the proposed method with two array placements.","['Time-frequency analysis', 'Sensor arrays', 'Microphone arrays', 'Direction-of-arrival estimation', 'Acoustics']","['Acoustic sensing', 'ego-noise reduction', 'micro aerial vehicle', 'microphone array', 'source localization']"
"This paper presents a comprehensive evaluation of fabrication techniques for the integration of coils into textiles, for the purpose of enabling low-power wireless power transfer; for example, the powering of on-body monitoring devices, such as heart-rate monitors. Key electrical parameters of the coils required to maximize power transfer efficiency are identified from theory. Flexible coils have been fabricated using standard processes widely used in the textile industry, such as screen printing and embroidery. The screen printed coils were fabricated with a silver-polymer ink on a printed interface layer; the embroidered coils were fabricated using a variety of conductive threads formed by coating textile fibers and through the use of copper fibers. These coils have been experimentally characterized and evaluated for use in wireless power transfer applications. The effects of coil geometry and separation on the dc–dc power transfer efficiency using Qi standard compliant driver and receiver circuits are reported.","['Coils', 'Textiles', 'Receivers', 'Resistance', 'Q-factor', 'Transmitters', 'Fabrication']","['Flexible coils', 'inductive power transfer', 'smart textiles', 'wireless power transfer']"
"A low complexity range-azimuth frequency-modulated continuous-waveform (FMCW) radar sensor using joint angle and delay estimation method without singular value decomposition (SVD) and eigenvalue decomposition (EVD) is presented in this paper. Conventional joint angle and delay estimation techniques exploit the dual-shift-invariant structure of received signals through matrix decompositions, such as SVD and EVD, which increases the computational burden. The proposed method utilizes the dual-shift-invariant structure through matrix inversion and performs angle and delay estimation using extended one-dimensional pseudospectrum searching instead of two-dimensional pseudospectrum searching to reduce the computational complexity. We demonstrate the effectiveness of the proposed method through Monte-Carlo simulations. The proposed algorithm is also verified by processing real FMCW data collected in an anechoic chamber.","['Delay estimation', 'Multiple signal classification', 'Estimation', 'Noise', 'Radar', 'Sensors', 'Joints']","['FMCW', 'low complexity', 'range', 'azimuth', 'radar detection']"
"Smart meters have been installed to report users' real-time electricity consumption data to the utility supplier periodically, which enables fine-grained energy supply, as the utility supplier can adjust its supplement based on users' consumptions. However, these real-time electricity data can also reveal the behaviors of the inhabitants; for example, the real-time electricity consumption data can reveal if the inhabitant is at home, if the television is working, and so on. People are reluctant to disclose these kinds of personal information. In this paper, we come up with a smart meter data aggregation scheme based on the Paillier homomorphic cryptosystem, this aggregation scheme enables a utility supplier to get the total consumption of all the smart meters, while the utility supplier is unable to get the consumption data of a single smart meter. In addition, the proposed scheme enables the smart meter to report multiple types of data in one reporting message, which makes it possible for the supplier to conduct the variance analysis and the one-way analysis of variance on the data. The formal security analysis shows that the proposed scheme is semantically secure. The experiment results show that the proposed scheme can reduce the computation cost both on the smart meter side and on the aggregator side.","['Smart meters', 'Data aggregation', 'Public key', 'Real-time systems', 'Smart grids', 'Analysis of variance']","['Homomorphic cryptosystem', 'smart grid', 'privacy-preserving', 'multidimensional aggregation']"
"This paper presents different information fusion approaches to classify human gait patterns and falls in a radar sensors network. The human gaits classified in this work are both individual and sequential, continuous gait collected by a FMCW radar and three UWB pulse radar placed at different spatial locations. Sequential gaits are those containing multiple gait styles performed one after the other, with natural transitions in between, including fall events developing from walking gait in some cases. The proposed information fusion approaches operate at signal and decision level. For the signal level combination, a simple trilateration algorithm is implemented on the range data from the 3 UWB radar sensors, achieving good classification results with the proposed Bi-LSTM (Bidirectional LSTM neural network) as classifier, without exploiting conventional micro-Doppler information. For the decision level fusion, the classification results of individual radars using the Bi-LSTM network are combined with a robust Naive Bayes Combiner (NBC), and this showed subsequent improvement compared to the single radar case thanks to multi-perspective views of the subjects. Compared to conventional SVM and Random Forest classifiers, the proposed approach yields +20% and +17% improvement in the classification accuracy of individual gaits for the range-only trilateration method and NBC decision fusion method, respectively. When classifying sequential gaits, the overall accuracy for the two proposed methods reaches 93% and 90%, with validation via a 'leaving one participant out' approach to test the robustness with subjects unknown to the network.","['Radar', 'Sensors', 'Doppler radar', 'Ultra wideband radar', 'Legged locomotion', 'Sensor fusion', 'Sensor phenomena and characterization']","['RF sensing', 'radar', 'machine learning', 'sensor fusion', 'gait analysis', 'fall detection']"
"An optically tunable perfect light absorber as a refractive index (RI) metamaterial (MM) nanobiosensor (NBS) is designed for sensing chemicals, monitoring the concentration of water-soluble glucose, and detecting viruses. This plasmon induced tunable metasurface works based on multiband super-absorption in the infrared frequency regime. It consists of a metal mirror that facilitates the MM to work as an absorber where the metal pattern at the top layer creates an enhanced evanescent wave that facilitates the metasurface to work as a RI optical sensor. The modelling and numerical analysis are carried out using Finite Difference Time Domain (FDTD) method-based software, CST microwave studio where a genetic algorithm (GA) is used to optimize the geometric parameters. We demonstrate multiband super-absorption spectra having maximum absorption of more than 99%. Furthermore, we show how the multiband super-absorber nanostructure can be used as a RI NBS, where the resonance frequency shifts with the RI of the surrounding medium. The achieved opto-chemical sensitivity is approximately 65 nm / RIU RIU , the bio-optical sensitivity to detect viruses is approximately 76 nm / RIU; and the optical sensitivity of the water-soluble glucose concentration is about 300 nm / RIU ; all sensitivities are comparable in comparison with the reported values in the literature.","['Metals', 'Absorption', 'Optical sensors', 'Sensors', 'Optical surface waves', 'Biomedical optical imaging', 'Optical reflection']","['Optical nano sensor', 'optical virus detection', 'metamaterials (MMs)', 'nano plasmonics', 'thin film sensors', 'refractive index (RI) Sensor', 'multi-band light absorber', 'perfect absorber']"
"With the growing interest in wireless sensor networks (WSNs), minimizing network delay and maximizing sensor (node) lifetime are important challenges. Since the sensor battery is one of the most precious resources in a WSN, efficient utilization of the energy to prolong the network lifetime has been the focus of much of the research on WSNs. For that reason, many previous research efforts have tried to achieve tradeoffs in terms of network delay and energy cost for such data aggregation tasks. Recently, duty-cycling technique, i.e., periodically switching ON and OFF communication and sensing capabilities, has been considered to significantly reduce the active time of sensor nodes and thus extend network lifetime. However, this technique causes challenges for data aggregation. In this paper, we present a distributed approach, named distributed delay efficient data aggregation scheduling (DEDAS-D) to solve the aggregation-scheduling problem in duty-cycled WSNs. The analysis indicates that our solution is a better approach to solve this problem. We conduct extensive simulations to corroborate our analysis and show that DEDAS-D outperforms other distributed schemes and achieves an asymptotic performance compared with centralized scheme in terms of data aggregation delay.","['Wireless sensor networks', 'Data aggregation', 'Delays', 'Sensors', 'Schedules', 'Job shop scheduling']","['DEDAS-D', 'distributed aggregation scheduling', 'duty-cycle', 'WSNs', 'wireless sensor networks']"
"This paper presents the results from the experimental application of smartwatch sensors to predict occupants' thermal comfort under varying environmental conditions. The goal is to investigate the measurement accuracy of smartwatches when used as thermal comfort sensors to be integrated into Heating, Ventilation and Air Conditioning (HVAC) control loops. Ten participants were exposed to various environmental conditions as well as warm - induced and cold-induced discomfort tests and 13 participants were exposed to a transient-condition while a network of sensors and a smartwatch collected both environmental parameters and heart rate variability (HRV). HRV features were used as input to Machine Learning (ML) classification algorithms to establish whether a user was in discomfort, providing an average accuracy of 92.2 %. ML and Deep Learning regression algorithms were trained to predict the thermal sensation vote (TSV) in a transient environment and the results show that the aggregation of environmental and physiological quantities provide a better TSV prediction in terms of Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE), 1.2 and 20% respectively, than just the HRV features used for the prediction. In conclusion, this experiment supports the assumption that physiological quantities related to thermal comfort can improve TSV prediction when combined with environmental quantities.","['Heart rate variability', 'Sensors', 'Electrocardiography', 'Biomedical measurement', 'Physiology', 'Electroencephalography', 'Sensor phenomena and characterization']","['Thermal comfort', 'environmental control', 'human perception', 'thermal sensation vote', 'wearable sensors', 'heart rate variability']"
"This paper presents an innovative wrist-worn device with machine learning capabilities and a wearable pressure sensor array. The device is used for monitoring different hand gestures by tracking tendon movements around the wrist. Thus, an array of PDMS-encapsulated capacitive pressure sensors is attached to the user to capture wrist movement. The sensors are embedded on a flexible substrate and their readout requires a reliable approach for measuring small changes in capacitance. This challenge was addressed by measuring the capacitance via the switched capacitor method. The values were processed using a programme on LabVIEW to visually reconstruct the gestures on a computer. In addition, to overcome limitations of tendo’s uncertainty when the wristband is re-worn, or the user is changed, a calibration step based on the support vector machine (SVM) learning technique is implemented. Sequential minimal optimization algorithm is also applied in the system to generate SVM classifiers efficiently in real-time. The working principle and the performance of the SVM algorithms demonstrate through experiments. Three discriminated gestures have been clearly separated by SVM hyperplane and correctly classified with high accuracy (>90%) during real-time gesture recognition.","['Hidden Markov models', 'Licenses', 'Sensor phenomena and characterization', 'Erbium', 'Radio frequency']","['Support vector machine', 'sequential minimal optimization', 'gesture recognition', 'wearable intelligence', 'capacitance measurement']"
"Wireless body area networks (WBANs) are expected to act as an important role in monitoring the health information and creating a highly reliable ubiquitous healthcare system. Since the data collected by the WBANs are used to diagnose and treat, only authorized users can access these data. Therefore, it is important to design an access control scheme that can authorize, authenticate, and revoke a user to access the WBANs. In this paper, we first give an efficient certificateless signcryption scheme and then design an access control scheme for the WBANs using the given signcryption. Our scheme achieves confidentiality, integrity, authentication, non-repudiation, public verifiability, and ciphertext authenticity. Compared with existing three access control schemes using signcryption, our scheme has the least computational cost and energy consumption for the controller. In addition, our scheme has neither key escrow nor public key certificates, since it is based on certificateless cryptography.","['Access control', 'Body area networks', 'Wireless communication', 'Sensors', 'Public key', 'Authentication']","['Wireless body area networks', 'security', 'access control', 'signcryption', 'certificateless cryptography']"
"The human olfactory system remains one of the most challenging biological systems to replicate. Humans use it without thinking, where it can measure offer protection from harm and bring enjoyment in equal measure. It is the system's real-time ability to detect and analyze complex odors that makes it difficult to replicate. The field of artificial olfaction has recruited and stimulated interdisciplinary research and commercial development for several applications that include malodor measurement, medical diagnostics, food and beverage quality, environment and security. Over the last century, innovative engineers and scientists have been focused on solving a range of problems associated with measurement and control of odor. The IEEE Sensors Journal has published Special Issues on olfaction in 2002 and 2012. Here we continue that coverage. In this article, we summarize early work in the 20 th Century that served as the foundation upon which we have been building our odor-monitoring instrumental and measurement systems. We then examine the current state of the art that has been achieved over the last two decades as we have transitioned into the 21 st Century. Much has been accomplished, but great progress is needed in sensor technology, system design, product manufacture and performance standards. In the final section, we predict levels of performance and ubiquitous applications that will be realized during in the mid to late 21 st Century.","['Sensors', 'Instruments', 'Sensor phenomena and characterization', 'Olfactory', 'Monitoring', 'Electrodes', 'Temperature sensors']","['Artificial olfaction', 'electronic nose', 'machine olfaction', 'odor detection', 'machine learning', 'headspace sampling', 'VOC analysis']"
"This paper presents a Chitosan-Graphene Oxide (CS-GO)-based array of ultra-thin biosensors with gold (Au)-based microgap (60 μm) electrode. The cross-linked GO is shown to improve the stability of chitosan substrate in aqueous medium and compatibility with microfabrication steps. The sensor patch has been evaluated for label-free monitoring by immobilizing the CS-GO surface with human dermal fibroblast (HDF) cells. The cyclic voltammetry (CV) of HDF cell immobilized CS-GO surface show quasi-reversible nature with a characteristic cathodic peak at +300 mV and anodic peak at -300 mV. Both peaks are stable and repeatable up to 50-scan cycle without any potential shift. The device shows a steady-state peak enhancement (1.923-11.195nA) during the DHF cell growth period (0-96h). The redox peak enhancement correlates with the cell proliferation rates over time, indicating that it could be employed for investigation of the cyto-physiological state against any endo and exogenous stimulation. In addition, the developed sensor-patch was used to detect a wide range of glucose from 1 μM to 20mM in vitro with a sensitivity of 0.17 μA/mM. Considering these, the presented sensor-patch has a great potential for the detection of glucose level, cell-health proliferation rate at the wound site, and diabetic wound monitoring applications.","['Electrodes', 'Substrates', 'Wounds', 'Gold', 'Monitoring', 'Biosensors']","['Chitosan', 'graphene oxide', 'HDF cell', 'cyclic voltammetry', 'linear swift voltammetry', 'wound monitoring']"
"Indoor/outdoor localization topic has gained a significant research interest due to the wide range of potential applications. Commonly, the Fingerprinting methods for spatial characterization of the environments monitored are employed in deterministic/statistical estimation. However, there are Fingerprint parameters that are generally neglected and can seriously affect the performance yielding to low accurate location. Nowadays, machine and deep learning (DL) methods are employed in this topic due to its ability to approximate complex non-linear models being capable of mitigating the undesirable effects of wireless propagation. In this paper, a complete overview of most influential aspects in Fingerprinting and indoor tracking methods is presented. Furthermore, a novel multi-modal complete tracking system, called SWiBluX, based on statistic and DL techniques is presented. The system relies on relevant feature extraction from available data sources to estimate user's/target indoor position using a multi-phase statistical Fingerprint and DL disruptive approach. In addition, a Gaussian outlier filter is applied to the position estimation model output to further reduce the error in the estimation. The set of experiments performed shows that Fingerprint positioning accuracy estimation can be improved up to 45% resulting in a final estimation error that outperforms related literature.","['Fingerprint recognition', 'Sensors', 'Estimation', 'Wireless sensor networks', 'Wireless communication', 'Monitoring', 'Deep learning']","['Indoor positioning', 'tracking', 'orientation', 'fingerprinting', 'particle filter', 'wireless', 'RSSI', 'IMU', 'feature vector', 'machine learning', 'deep learning', 'neural network']"
"This paper evaluates the use of accelerometers for continuous monitoring of respiratory rate (RR), which is an important vital sign in post-intensive care patients or those inside the intensive care unit (ICU). The respiratory rate can be estimated from accelerometer and photoplethysmography (PPG) signals for patients following ICU discharge. Due to sensor faults, sensor detachment, and various artifacts arising from motion, RR estimates derived from accelerometry and PPG may not be sufficiently reliable for use with existing algorithms. This paper described a case study of 10 selected patients, for which fewer RR estimates have been obtained from PPG signals in comparison to those from accelerometry. We describe an algorithm for which we show a maximum mean absolute error between estimates derived from PPG and accelerometer of 2.56 breaths/min. Our results obtained using the 10 selected patients are highly promising for estimation of RR from accelerometers, where significant agreements have been observed with the PPG-based RR estimates in many segments and across various patients. We present this research as a step towards producing reliable RR monitoring systems using low-cost mobile accelerometers for monitoring patients inside the ICU or on the ward (post-ICU).","['Accelerometers', 'Estimation', 'Monitoring', 'Modulation', 'Sensors', 'Electrocardiography', 'Acceleration']","['Respiratory rate (RR)', 'accelerometer', 'photoplethysmography (PPG)', 'adaptive line enhancer (ALE)', 'auto-regressive (AR)']"
"This paper presents a drowsiness detection model that is capable of sensing the entire range of stages of drowsiness, from weak to strong. The key assumption underlying our approach is that the sitting posture-related index can indicate weak drowsiness that drivers themselves do not notice. We first determined the sensitivity of the posture index and conventional indices for the stages of drowsiness. Then, we designed a drowsiness detection model combining several indices sensitive to weak drowsiness and to strong drowsiness, to cover all drowsiness stages. Subsequently, the model was trained and evaluated on a dataset comprised of data collected from approximately 50 drivers in simulated driving experiments. The results indicated that posture information improved the accuracy of weak drowsiness detection, and our proposed model using the driver's blink and posture information covered all stages of drowsiness (F1-score 53.6%, root mean square error 0.620). Future applications of this model include not only warning systems for dangerously drowsy drivers but also systems which can take action before their drivers become drowsy. Since measuring the information requires no restrictive equipment such as on-body electrodes, the model presented here based on blink and posture information can be used in several practical applications.","['Vehicles', 'Indexes', 'Physiology', 'Adaptation models', 'Frequency conversion', 'Time-frequency analysis', 'Sensors']","['Driver fatigue', 'driving performance', 'drowsiness detection', 'multi-modal sensing', 'slight drowsiness']"
"The ability to capture joint kinematics in outside-laboratory environments is clinically relevant. In order to estimate kinematics, inertial measurement units can be attached to body segments and their absolute orientations can be estimated. However, the heading part of such orientation estimates is known to drift over time, resulting in drifting joint kinematics. This study proposes a novel joint kinematic estimation method that tightly incorporates the connection between adjacent segments within a sensor fusion algorithm, to obtain drift-free joint kinematics. Drift in the joint kinematics is eliminated solely by utilizing common information in the accelerometer and gyroscope measurements of sensors placed on connecting segments. Both an optimization-based smoothing and a filtering approach were implemented. Validity was assessed on a robotic manipulator under varying measurement durations and movement excitations. Standard deviations of the estimated relative sensor orientations were below 0.89° in an optimization-based smoothing implementation for all robot trials. The filtering implementation yielded similar results after convergence. The method is proven to be applicable in biomechanics, with a prolonged gait trial of 7 minutes on 11 healthy subjects. Three-dimensional knee joint angles were estimated, with mean RMS errors of 2.14°, 1.85°, 3.66° in an optimization-based smoothing implementation and mean RMS errors of 3.08°, 2.42°, 4.47° in a filtering implementation, with respect to a golden standard optical motion capture reference system.","['Kinematics', 'Motion segmentation', 'Acceleration', 'Gyroscopes', 'Robot sensing systems']","['Body sensor networks', 'gait', 'inertial-sensor drift', 'motion analysis', 'sensor fusion', 'wearable sensors']"
"When a micro aerial vehicle (MAV) captures sounds emitted by a ground or aerial source, its motors and propellers are much closer to the microphone(s) than the sound source, thus leading to extremely low signal-to-noise ratios (SNR), e.g., -15 dB. While microphone-array techniques have been investigated intensively, their application to MAV-based ego-noise reduction has been rarely reported in the literature. To fill this gap, we implement and compare three types of microphone-array algorithms to enhance the target sound captured by an MAV. These algorithms include a recently emerged technique, time-frequency spatial filtering, and two well-known techniques, beamforming and blind source separation. In particular, based on the observation that the target sound and the ego-noise usually have concentrated energy at sparsely isolated time-frequency bins, we propose to use the time-frequency processing approach, which formulates a spatial filter that can enhance a target direction based on local direction of arrival estimates at individual time-frequency bins. By exploiting the time-frequency sparsity of the acoustic signal, this spatial filter works robustly for sound enhancement in the presence of strong ego-noise. We analyze in details the three techniques and conduct a comparative evaluation with real-recorded MAV sounds. Experimental results show the superiority of blind source separation and time-frequency filtering in low-SNR scenarios.","['Time-frequency analysis', 'Correlation', 'Sensors', 'Array signal processing', 'Microphone arrays', 'Acoustics']","['Acoustic sensing', 'ego-noise reduction', 'micro aerial vehicles', 'microphone array']"
"A 16 × 256 element single-photon avalanche diode array with a 256-channel, 3-bit on-chip time-to-digital converter (TDC) has been developed for fluorescence-suppressed Raman spectroscopy. The circuit is fabricated in 0.35 μm high-voltage CMOS technology and it allows a measurement rate of 400 kframe/s. In order to be able to separate the Raman and fluorescence photons even in the presence of the unavoidable timing skew of the timing signals of the TDC, the time-of-arrival of every detected photon is recorded with high time resolution at each spectral point with respect to the emitted short and intensive laser pulse (~150 ps). The dynamic range of the TDC is set so that no Raman photon is lost due to the timing skew, and thus the complete time history of the detected photons is available at each spectral point. The resolution of the TDC was designed to be adjustable from 50 ps to 100 ps. The error caused by the timing skew and the residual variation in the resolution of the TDC along the spectral points is mitigated utilizing a calibration measurement from reference sample with known smooth fluorescence spectrum. As a proof of concept, the Raman spectrum of sesame seed oil, having a high fluorescence-to-Raman ratio and a short fluorescence lifetime of 1.9 ns, was successfully recorded.","['Fluorescence', 'Raman scattering', 'Photonics', 'Timing', 'Logic gates', 'Measurement by laser beam']","['SPAD detector', 'Raman spectroscopy', 'time gating']"
"An integrated 0.35-μmHV-CMOS driver has been designed for a gain-switched quantum well laser diode to generate short, energetic (100 ps/∼0.5nJ) optical pulses for a pulsed time-of-flight (TOF) laser radar, operating on the single-photon detection principle. The driver can produce a current pulse with a peak amplitude of∼2A, a pulsewidth of 1 ns, and a pulsing rate of 500 kHz. The peak optical power and pulsewidth of the transmitter output with these parameters are 3 W and 100 ps, respectively. A single-shot precision of 2–3 cm is achieved in the TOF measurements of a non-cooperative target at 25 m with a reflectivity of 8% and a receiver aperture of 18 mm using a CMOS SPAD as the receiver.","['Diode lasers', 'Receivers', 'Transistors', 'Optical transmitters', 'Switching circuits', 'CMOS integrated circuits', 'Laser radar']","['CMOS driver', 'laser radar', 'pulsed time-offlight', 'single photon detection']"
"This paper presents a smart sensor patch with flexible strain sensor and a printed temperature sensor integrated with a Near Field Communication (NFC) tag to detect strain or temperature in a semi-quantitative way. The strain sensor is fabricated using conductive polymer poly (3,4-ethylenedioxythiophene) polystyrene sulfonate (PEDOT: PSS) in a polymer Polydimethylsiloxane microchannel. The temperature sensor is fabricated by printing silver electrodes and PEDOT:PSS on a flexible polyvinyl chloride (PVC) substrate. A custom-developed battery-less NFC tag with an LED indicator is used to visually detect the strain or temperature by modulating the LED light intensity. The LED shows maximum brightness for relaxed or no strain condition, and also in the case of maximum temperature. In contrast, the LED is virtually off for the maximum strain condition and for room temperature. Both these could be related to food spoilage. Swollen food packages can be detected with the strain sensor, serving as beacons of microbial contamination. Temperature deviations can result in the growth or survival of food-spoilage bacteria. Based on this, the potential application of the sensor system for smart food packaging is presented.","['Temperature sensors', 'Sensors', 'Strain', 'Capacitive sensors', 'Intelligent sensors', 'Radiofrequency identification', 'Light emitting diodes']","['Flexible electronics', 'NFC tag', 'strain sensor', 'smart food packaging', 'temperature sensor', 'wireless communication']"
"Distributed optical vibration sensors (DOVS) have attracted much attention recently since it can be used to monitor mechanical vibrations or acoustic waves with long reach and high sensitivity. Phase-sensitive optical time domain reflectometry (Φ-OTDR) is one of the most commonly used DOVS schemes. For Φ-OTDR, the whole length of fiber under test (FUT) works as the sensing instrument and continuously generates sensing data during measurement. Researchers have made great efforts to try to extract external intrusions from the redundant data. High signal-to-noise ratio (SNR) is necessary in order to accurately locate and identify external intrusions in Φ-OTDR systems. Improvement in SNR is normally limited by the properties of light source, photodetector and FUT. But this limitation can also be overcome by post-processing of the received optical signals. In this context, detailed methodologies of SNR enhancement post-processing algorithms in Φ-OTDR systems have been described in this paper. Furthermore, after successfully locating the external vibrations, it is also important to identify the types of source of the vibrations. Pattern classification is a powerful tool in recognizing the intrusion types from the vibration signals in practical applications. Recent reports of Φ-OTDR systems employed with pattern classification algorithms are subsequently reviewed and discussed. This thorough review will provide a design pathway for improving the performance of Φ-OTDR while maintaining the cost of the system as no additional hardware is required.","['Signal to noise ratio', 'Optical fiber sensors', 'Vibrations', 'Backscatter', 'Optical fibers']","['Fiber optics sensors', 'optical time domain reflectometry', 'phase-sensitive optical time domain reflectometry', 'scattering measurement']"
"Employing dual-modality tomography inherently involves data from multiple dimensions, and thus a coherent approach is required to fully exploit the information from various dimensions. This paper describes a novel approach for dual-modality electrical resistance and capacitance tomography (ERT-ECT) to visualize gas-oil-water flow in horizontal pipeline. Compared with the conventional methods with dual-modality tomographic systems, the approach based on thresholding takes the account of multi-dimensional data, which therefore is capable of providing insights into investigated flow in both spatial and temporal terms. The experimental results demonstrate the feasibility of the approach, by which six common flow regimes in horizontal pipeline flow are visualized based on the multi-dimensional data with ERT-ECT systems, including (wavy) stratified flow, plug flow, slug flow, annular flow, and bubbly flow. Although the present approach is proposed for data acquired with an ERT-ECT system, it is potentially adaptable to other dual-modality tomographic systems that use concentration tomograms as inputs.","['Data integration', 'Tomography', 'Image reconstruction', 'Permittivity', 'Data visualization', 'Conductivity', 'Pipelines']","['Dual-modality electrical tomography', 'multiphase flow measurement and visualization', 'multi-dimensional data fusion', 'flow regimes visualization']"
"Every year, for ten years now, the IPIN competition has aimed at evaluating real-world indoor localisation systems by testing them in a realistic environment, with realistic movement, using the EvAAL framework. The competition provided a unique overview of the state-of-the-art of systems, technologies, and methods for indoor positioning and navigation purposes. Through fair comparison of the performance achieved by each system, the competition was able to identify the most promising approaches and to pinpoint the most critical working conditions. In 2020, the competition included 5 diverse off-site off-site Tracks, each resembling real use cases and challenges for indoor positioning. The results in terms of participation and accuracy of the proposed systems have been encouraging. The best performing competitors obtained a third quartile of error of 1 m for the Smartphone Track and 0.5 m for the Foot-mounted IMU Track. While not running on physical systems, but only as algorithms, these results represent impressive achievements.","['Sensor phenomena and characterization', 'Indoor navigation', 'Testing', 'Standards', 'Satellite broadcasting', 'Recurrent neural networks', 'Received signal strength indicator']","['Indoor positioning and navigation', 'evaluation', 'smartphone-based positioning', 'foot-mounted IMU', 'positioning in industrial scenarios and factories', 'vehicle-positioning']"
"The main objective of this study is the realization of advanced methods of automatic evaluation and continuous monitoring tools for the support to the medical evaluation of Parkinson's disease (PD) and to the follow up of the patient also through the presentation and classification of data through a universally recognized clinical scale. We have developed an embedded wearable device integrating a tri-axial accelerometer, a tri-axial gyroscope, and a tri-axial magnetometer, realizing a sensor with embedded data fusion and aggregation algorithms. Two different classification and detection algorithms are implemented to provide a comprehensive description and objective quantification of the two most investigated PD's symptoms as tremor and freezing of gait (FoG). The system is usable in clinical and diagnostic settings showing its effectiveness above all in patients home monitoring through the upload in Cloud and the real-time processing of the acquired data. In the latter case, it allows the detection and objectification of PD symptoms manifested by the patient in everyday life, thereby providing the doctor with essential information for the dosage and effective personalization of drug therapy. For the system validation, a series of tests were carried out according to a defined experimental protocol on a sample of PD patients and healthy subjects with the presence of a video recording system and under the supervision of a neurologists team. The results obtained show a system's accuracy of 97.7% for tremor classification and 99.7% for FoG events detection.","['Monitoring', 'Real-time systems', 'Diseases', 'Biomedical monitoring', 'Wearable sensors', 'Drugs']","['Cloud computing', 'freezing of gait', 'inertial measurement system', 'IoT', 'Parkinson disease', 'tremor', 'wearable sensors']"
"For oil fields producing high fractions of water, it is critical to be able to accurately meter the water volumetric flowrate for production allocation and field life optimization. This paper reports the measurement principles and experimental results of a multiphase flow meter prototype, capable of non-intrusively measuring the water volumetric flowrate in a multiphase flow. This research prototype is based on a novel concept of combining two tomographic systems-magnetic induction tomography and electromagnetic velocity tomography-for measuring the cross-sectional volumetric fraction and the local axial velocity of the water phase, respectively. The fundamental principles and imaging capability of each technique are shown. First impressions of the prototype performance are demonstrated using experimental data from an in-house flow loop. The challenges and potential improvements are also addressed.","['Velocity measurement', 'Mathematical model', 'Volume measurement', 'Phase measurement', 'Tomography', 'Electromagnetics']","['Multiphase flow', 'magnetic induction tomography', 'electromagnetic velocity tomography']"
"Hyperspectral sensors record radiances in a large number of wavelengths of the electromagnetic spectrum and can be used to distinguish different tree species based on their characteristic reflectance signatures. Reflectance spectra were measured from airborne hyperspectral AISA Eagle/Hawk imagery in order to identify different Mediterranean tree species at a coastal test site in Portugal. A spectral range from 400 to 2450 nm was recorded at 2-m spatial resolution. The hyperspectral data are divided into five spectral data ranges. The chosen ranges for segmentation are based on statistical properties as well as on their wavelengths, as radiances of a particular wavelength may overlap with neighboring wavelengths. Principal component analysis (PCA) is applied individually to each spectral range. The first three principal components (PCs) of each range are chosen and are fused into a new data segment of reduced dimensionality. The resulting 15 PCs contain 99.42% of the information content of the original hyperspectral image. These PCs were used for a maximum likelihood classification (MLC). Spectral signatures were also analyzed for the hyperspectral data, and were validated with ground data collected in the field by a handheld spectro-radiometer. Different RGB combinations of PC bands of segmented PC image provide distinct feature identification. A comparison with other classification approaches (spectral angle mapper and MLC of the original hyperspectral imagery) shows that the MLC of the segmented PCA achieves the highest accuracy, due to its ability to reduce the Hughes phenomenon.","['Hyperspectral imaging', 'Principal component analysis', 'Image segmentation', 'Atmospheric modeling', 'Sensors', 'Vegetation']","['Hyperspectral remote sensing', 'segmented principal component analysis', 'maximum likelihood classification', 'spectral angle mapper', 'forest mapping', 'ground data', 'coastal vegetation']"
"An unexpected and unwanted influx of gas or “kick” into the wellbore during hydrocarbon drilling can cause catastrophic blowout incidents, resulting in human casualties, ecological damage, and asset losses. The ability of the oil and gas industry to control gas kick depends on our ability to accurately detect and monitor gas migration in a borehole in real-time. This study demonstrates the application of optical fiber-based Distributed Acoustic Sensors (DAS) for early detection and monitoring of gas in wellbore. Multiphase flow experiments conducted in a 5000 ft. deep test-well are analyzed for different injection, circulation, and pressure conditions. In each case, the low-frequency component of DAS demonstrates a superior capability to detect gas signatures both inside the tubing and the annulus of the well, even at small gas volumes. In comparison, the high-frequency DAS data seems limited in detail. The gas influx velocity was calculated using the frequency-wavenumber analysis of the gradient of the low-frequency DAS phase with respect to time, which shows good agreement with theoretical velocity estimates using flow models and surface gauge measurements. This study demonstrates a novel workflow to analyze low-frequency DAS to qualitatively and quantitatively map gas influx in a wellbore.","['Optical fiber sensors', 'Optical fibers', 'Sensors', 'Monitoring', 'Temperature measurement', 'Optical fiber cables', 'Temperature sensors']","['Distributed acoustic sensing', 'gas detection', 'fiber optic sensing', 'low-frequency DAS', 'wellbore monitoring']"
"With the increased life expectancy and rise in health conditions related to aging, there is a need for new technologies that can routinely monitor vulnerable people, identify their daily pattern of activities and any anomaly or critical events such as falls. This paper aims to evaluate magnetic and radar sensors as suitable technologies for remote health monitoring purpose, both individually and fusing their information. After experiments and collecting data from 20 volunteers, numerical features has been extracted in both time and frequency domains. In order to analyze and verify the validation of fusion method for different classifiers, a support vector machine with a quadratic kernel, and an artificial neural network with one and multiple hidden layers have been implemented. Furthermore, for both classifiers, feature selection has been performed to obtain salient features. Using this technique along with fusion, both classifiers can detect 10 different activities with an accuracy rate of approximately 96%. In cases where the user is unknown to the classifier, an accuracy of approximately 92% is maintained.","['Radar', 'Magnetic sensors', 'Radar antennas', 'Wearable sensors', 'Feature extraction', 'Monitoring']","['Magnetic sensor', 'radar sensing', 'assisted living', 'feature selection', 'neural networks', 'machine learning']"
"In this paper, we assess the capability of a unique unobtrusive footprint imaging sensor system, based on plastic optical fiber technology, to allow efficient gait analysis from time domain sensor data by pattern recognition techniques. Trial gait classification experiments are executed as ten manners of walking, affecting the amplitude and frequency characteristics of the temporal signals. The data analysis involves the design of five temporal features, subsequently analyzed in 14 different machine learning models, representing linear, non-linear, ensemble, and deep learning models. The model performance is presented as cross-validated accuracy scores for the best model-feature combinations, along with the optimal hyper-parameters for each of them. The best classification performance was observed for a random forest model with the adjacent mean feature, yielding a mean validation score of 90.84% ± 2.46%. We conclude that the floor sensor system is capable of detecting changes in gait by means of pattern recognition techniques applied in the time domain. This suggests that the footprint imaging sensor system is suitable for gait analysis applications ranging from healthcare to security.","['Sensor systems', 'Legged locomotion', 'Imaging', 'Intelligent sensors', 'Floors', 'Optical fibers']","['Floor sensor', 'sensor fusion', 'gait analysis', 'pattern recognition', 'machine learning']"
"This study proposes a magnetic sensor design and application for monitoring the health of rotor magnets in permanent magnet (PM) electrical machines through in-situ observation of the air-gap magnetic flux density. The reported device employs the concept of Fibre Bragg Grating (FBG) strain sensing fusion with magnetostrictive material to deliver a machine stator slot wedge integrated sensor that allows straightforward installation and retrofit with no invasive action to core elements of the machine. The sensing theory, design, prototyping, calibration and installation of the proposed magnetic sensing scheme are detailed in the paper. The sensor was installed into an inverter driven surface mount PM synchronous machine (SPMSM) and its performance for in-situ observation of rotor PM magnetization conditions validated in a range of healthy and demagnetised PM conditions tests. The obtained experimental data demonstrate the reported device's capability to enable recognition of rotor PMs' magnetisation level and thus their health monitoring. Finally, a fault index is proposed and experimentally validated that allows the application of in-situ magnetic sensor measurements for relative quantification of PM demagnetization fault severity.","['Magnetic sensors', 'Magnetostriction', 'Magnetic hysteresis', 'Magnetostatics', 'Monitoring', 'Strain']","['PM machines', 'magnetic sensing', 'demagnetisation diagnosis', 'FBG strain sensor', 'magnetostriction']"
"Recent advances in the development of multimodal wearable sensors enable us to gather richer contexts of mobile user activities. The combination of foot force sensor (FF) and GPS is able to afford fine-grained mobility activity recognition. We derive and identify 12 (out of 31) maximally informative FF features, and the minimal most effective insole positions (two per foot) for sensing, to improve the use of FF + GPS methods for mobility activity recognition. We tested the improved FF + GPS method using over 7000 samples collected from ten volunteers in a natural, unconstrained, environment. The results show that the improved FF + GPS can achieve an average accuracy of over 90% when detecting five different mobility activities, including walking, cycling, bus-passenger, car-passenger, and car-driver.","['Force', 'Foot', 'Sensors', 'Monitoring', 'Global Positioning System', 'Accuracy', 'Legged locomotion']","['Foot force sensors', 'activity recognition', 'mobile phone sensing']"
"The emergence of planar metamaterials (PrMMs) has opened a gateway to unprecedented electromagnetic (EM) properties and functionality unattainable from naturally occurring materials, thus enabling a family of PrMM based devices. In this paper, a novel class of superconducting (SC) PrMM is presented and a series of THz reflectance spectral responses simulations reveals that these SC PrMM structures portend applications in a variety of temperature sensors, thermo-optical modulators, and magnetic switch devices.","['Metamaterials', 'Magnetic fields', 'Sensors', 'Optical switches', 'Conductivity', 'Temperature measurement', 'Magnetic materials']","['Superconducting metamaterial', 'plasmonic waveguide', 'temperature sensing', 'magnetic switching application', 'thermo-optical modulating application']"
"Parkinson's disease (PD) is a progressive and neurodegenerative condition causing motor impairments. One of the major motor related impairments that present biggest challenge is freezing of gait (FOG) in Parkinson's patients. In FOG episode, the patient is unable to initiate, control or sustain a gait that consequently affects the Activities of Daily Livings (ADLs) and increases the occurrence of critical events such as falls. This paper presents continuous monitoring ADLs and classification freezing of gait episodes using Wi-Fi and radar imaging. The idea is to exploit the multi-resolution scalograms generated by channel state information (CSI) imprint and micro-Doppler signatures produced by reflected radar signal. A total of 120 volunteers took part in experimental campaign and were asked to perform different activities including walking fast, walking slow, voluntary stop, sitting down & stand up and freezing of gait. Two neural networks namely Autoencoder and a proposed enhanced Autoencoder were used classify ADLs and FOG episodes using data fusion process by combining the images acquired from both sensing techniques. The Autoencoder provided overall classification accuracy of ~87% for combined datasets. The proposed algorithm provided significantly better results by presenting an overall accuracy of ~98% using data fusion.","[""Parkinson's disease"", 'Sensor fusion', 'OFDM', 'Wireless fidelity', 'Deep learning', 'Legged locomotion']","['Radar sensing', 'Wi-Fi sensing', 'deep learning', 'FOG detection']"
"Less attention has been given to the inspection using the first longitudinal guided wave mode due to its attenuative and dispersive properties at commonly used ultrasonic guided waves (UGWs) operating frequency region (20-100 kHz). However, the first longitudinal guided wave mode has higher flaw sensitivity due to having a shorter wave length and having higher number of non-axisymmetric wave modes at a given frequency. This enhances the capabilities of advanced UGW techniques which require higher number of non-axisymmetric modes. This study has been performed to investigate the potential of mode purity and flaw sensitivity of the first longitudinal guided wave mode compared with other axisymmetric modes in the UGW operating frequency region. Numerical and experimental investigation have been conducted to investigate pure excitation and flaw sensitivity of the first longitudinal guided wave mode. It has been validated that the first longitudinal guided wave mode can be used in the UGW inspection effectively in isolation by adopting transducers with out-of-plane vibration. This reduces the cost and the weight of the UGW inspection tooling. The flaw sensitivity of the first longitudinal guided wave mode has been investigated by aid of an empirically validated UGW focusing technique. Under the studied conditions in this paper, the first longitudinal guided wave mode has∼5times higher flaw sensitivity compared with the second longitudinal guided wave mode and∼2.5times higher than the first torsional guided wave mode. This enhances the capability of UGW flaw detection and sizing.","['Inspection', 'Vibrations', 'Dispersion', 'Transducers', 'Finite element analysis', 'Sensitivity', 'Pipelines']","['pipeline inspection', 'ultrasonic guided waves', 'compression transducers', 'first longitudinal guided wave mode', 'ultrasonic guided wave focusing']"
"Detection of winding faults in permanent magnet synchronous machines (PMSMs) with stranded winding designs remains a challenging task for conventional diagnostic techniques. This paper proposes a new sensing approach to this problem by investigating the application of dedicated electrically non-conductive and electromagnetic interference immune fiber Bragg grating (FBG) temperature sensors embedded in PMSM windings to enable winding open-circuit fault diagnosis based on observing the fault thermal signature. The final element analysis thermal and electromagnetic models of the examined practical PMSM design are first developed and used to enable the understanding of open-circuit winding fault-induced signature that can be used for effective diagnostic purposes, indicating in situ thermal excitation as an optimal diagnostic measurand. A purpose build test rig with an inverter-driven commercial PMSM instrumented with in situ FBG sensors monitoring phase winding hot spots is then used to evaluate the efficacy of the proposed diagnostic scheme. It is shown that unambiguous diagnosis and severity trending of winding open-circuit faults is enabled by the use of in situ FBG sensors. A comparison with conventional fault diagnostic technique utilizing current signal sensing and analysis is also reported, indicating the considerable advantages of the proposed monitoring scheme employing FBG sensors.","['Windings', 'Circuit faults', 'Thermal sensors', 'Monitoring', 'Fiber gratings']","['Fiber Bragg grating sensors (FBG)', 'embedded thermal monitoring', 'open circuit fault', 'PM machines', 'windings']"
"Multiple-input multiple-output-synthetic aperture radar (MIMO-SAR) can realize high-resolution imaging by its predominance of parallel sampling. However, the motion compensation and large data amount are the inevitable problems to be solved. In this paper, a novel motion compensating method for MIMO-SAR imaging with the under-sampled echo signal is put forward. First, the echo signal model of MIMO-SAR system with motion error is analyzed and a compensating method with the Nyquist-sampled echo data is proposed. Using the technique of two-step compensation, the motion error is compensated and the imaging result is obtained. Second, to compensate the motion error with the under-sampled echo signal, based on the above compensating method, the transform operator and the CS-based imaging scheme are constructed, which could implement the first-step compensation, range compression, and range cell migration correction simultaneously. At last, the imaging result is obtained by the second-step compensation and azimuth compression. Using the proposed method, just a small amount of imaging data is required for MIMO-SAR imaging. Finally, the effectiveness of the proposed method is proved by the imaging simulations.","['Imaging', 'Trajectory', 'Radar imaging', 'Azimuth', 'Sensors', 'Discrete Fourier transforms']","['MIMO-SAR', 'Motion compensation', 'Two-step compensation', 'Compressed Sensing', 'Transform operator']"
"Nowadays, detection of trace concentration gases is still challenging for portable sensors, especially for the low-cost and easily operated metal-oxide-semiconductor (MOX) gas sensors. In this paper, a widely applicable amplification circuit is designed and fabricated to evidently enhance the signal of the MOX sensors by adding a field effect transistor (FET) into the conventional circuits. By optimizing the FET parameters and the loading resistance, this amplification circuit enables the commercial Figaro TGS2602 toluene sensors response effectively to the highest permissive limit (0.26 ppm) of toluene in indoor air of cars, with the detection limit of ~0.1 ppm. Furthermore, this circuit can also make the commercial Hanwei MP502 acetone sensors and MQ3 ethanol sensors response to the 1-2-ppm acetone in breath of diabetes and 2-ppm ethanol for fast and effectively drinker driver screening. The mechanism is investigated to be the gate voltage induced resistance change of the FET, with the highest theoretically estimated and experimentally measured magnification factor of 5-6. This FET amplifier can effectively enable the ppm level commercial MOX sensors response to sub-ppm level gases, promising for MOX gas sensor integration and also for other kind of resistive sensors.","['Field effect transistors', 'Gas detectors', 'Resistance', 'Gases', 'Ethanol', 'Logic gates']","['Environment and health', 'field effect transistor', 'amplifier', 'low concentration', 'metal oxide semiconductor sensor']"
"In vivo and in vitro studies of differential continuous wave photoacoustic spectroscopy (DCW-PAS) for non-invasive blood glucose monitoring were performed. The DCW-PAS technique utilizes amplitude modulation of dual wavelengths of light to determine changes in glucose concentration. The study compared DCW-PAS measurements with results from invasive blood glucose sensor measurements during oral glucose tolerance tests (OGTTs) of healthy people. The trends in blood glucose levels (BGLs) obtained from invasive sensors and from the photoacoustic signal have good agreement, with the standard error and correlation coefficient against BGLs of 48 mg/dL or less and 0.80, respectively. Our proposed photoacoustic spectroscopy (PAS) method shows high potential for use in a non-invasive BGL sensor.","['Glucose', 'Temperature sensors', 'Temperature measurement', 'Blood', 'Monitoring', 'Optical sensors']","['Photoacoustic', 'continuous-wave', 'non-invasive', 'blood glucose levels', 'in vivo measurement']"
"This paper presents printed thick film Ag|AgCl|KCl reference electrodes (RE) for electrochemical sensors. The screenprinted REs with 10-μm thick glass-KCl salt matrix layer exhibit a stable potential of 5 mV. Cyclic voltammetric analysis shows that the anodic and cathodic peak current of the RE increases with the scan rate in the range of 25-150 mVs -1 . The analytical performance of the REs shows a stable open circuit potential for the NaCl concentrations in the range of 30-100 mM. Testing the presented REs for electrochemical pH sensor application (with RuO 2 -based sensitive electrode) the sensitivity of 55 mV/pH was noted in the pH range of 4.5-9. Evaluating the effect of temperature on the performance of REs, a potential variation of -3.8 mV/ °C was observed. Finally, a LabVIEW interface was developed to store, analyze, and calculate the sensitivity of the sensor under different temperature conditions. The LabVIEW interface can also be used to calculate the pH-value/temperature of unknown solutions under known temperature/pH conditions.","['Electrodes', 'Thick films', 'Temperature sensors', 'Temperature measurement', 'Glass', 'Electric potential']","['Reference electrodes', 'pH sensor', 'thick film sensors', 'electrochemical studies', 'screen printing']"
"Highly sensitive capacitive pressure sensors with wide detection range are needed for applications such as human-machine interfaces, electronic skin in robotics, and health monitoring. However, it is challenging to achieve high sensitivity and wide detection range at the same time. Herein, we present an innovative approach to obtain a highly sensitive capacitive pressure sensor by introducing a zinc oxide nanowire (ZnO NW) interlayer at the polydimethylsiloxane (PDMS)/electrodes interface in the conventional metal-insulator-metal architecture. The ZnO NW interlayer significantly enhanced the performance with ~7 times higher sensitivity (from 0.81%kPa −1 to 5.6452%kPa −1 at a low-pressure range (0-10 kPa)) with respect to conventional capacitive sensors having PDMS only as the dielectric. The improvement in sensitivity is attributed to the enhanced charge separation and electric dipole generation due to the displacement of Zn + and O − under applied pressure. Further, the orientation of ZnO NWs and their placement between the electrodes were investigated which includes either vertical or horizontal NWs near the electrodes, placing a third ZnO NW interlayer in the middle of dielectric PDMS and introducing an air gap between the ZnO NWs/electrode. Among various combinations, the introduction of air gap between the electrode and ZnO NW interlayer revealed a significant improvement in the device performance with ~50 times enhancement at a low-pressure range (0-10 kPa) and more than 200 times increase at a high-pressure range (10-200 kPa), in comparison with the conventional PDMS-based pressure sensor.","['Zinc oxide', 'II-VI semiconductor materials', 'Electrodes', 'Pressure sensors', 'Sensitivity', 'Capacitance', 'Sensor phenomena and characterization']","['Flexible pressure sensors', 'capacitive sensors', 'ZnO NW', 'electronic skin', 'PDMS', 'soft touch sensor']"
"We design a low-noise, large-dynamic-range amplifier based on transimpedance amplifier (TIA) for detecting shot noise of 1064-nm laser in quantum optical experiments. Compared with a single-junction field-effect transistor (JFET) buffered TIA, the electronic noise is effectively 2.8 dB suppressed at the analysis of 2 MHz, and 4 dB suppressed at 4 MHz by means of a JFET bootstrap structure. Under the transimpedance gain of 200 kΩ and 0.5 pF, the measured shot noise at the injected laser power of 51 μW is 12.5 dB above the electronic noise at 2 MHz, and 9.8 dB at 4 MHz. With adjustment of bias condition of the bootstrap structure and application of a L-C (inductance and capacitance) structure, the dynamic range is largely boosted from the original 1.52 to 11.22 mW. The amplifier meets the requirement of SNR for Bell-state detection in quantum optical experiments.","['Capacitance', 'JFETs', 'Laser noise', 'Optical amplifiers', 'Optical buffering', 'Stimulated emission']","['TIA', 'low noise', 'large dynamic range', 'JFET buffering', 'JFET bootstrap', 'L-C structure', 'Bell-state detection', 'quantum optical experiments']"
"Structural Health Monitoring (SHM) of aircraft structures, which are manufactured more and more using composite material, is a well-established technique for increasing reliability and reducing maintenance costs. Implementing damage inspection algorithms that run in dedicated electronic systems helps reduce big data processing time with improved results. RAPID (Reconstruction Algorithm for Probabilistic Inspection of Damage) algorithm, developed some years ago, provides a technique to identify and characterize damages both in metallic and composite materials. However, this algorithm sometimes provides inaccurate predictions of the damage location, caused mainly by the influence of path intersection points among transducers, that can mask the current location of damages. This work presents a geometrical modification of standard RAPID algorithm (RAPID-G) that mitigates the influence of the intersection point between sensor paths. The results highlight that the location of damages improves significantly with this modification of the algorithm.","['Transducers', 'Composite materials', 'Sensor phenomena and characterization', 'Monitoring', 'Inspection', 'Standards']","['Structural health monitoring', 'composite', 'RAPID', 'Lamb waves', 'guided waves', 'damage detection', 'damage characterization']"
"In this paper, an instantaneously recorded baseline method is proposed using piezoelectric transducers for damage localization under varying temperature. This method eliminates need for baselines required when operating at different temperatures by mapping a baseline area onto the interrogation area. Instantaneously recorded baselines and current interrogation signals are calibrated based on the sensor mapping. This allows the extraction of damage scatter signal which is used to localize damage. The proposed method is used to localize actual impact damage on a composite plate under varying temperatures. The method is also applied to a stiffened fuselage panel to accurately localize impact damage.","['Transducers', 'Temperature sensors', 'Sensor phenomena and characterization', 'Propagation', 'Temperature distribution']","['Ultrasonic', 'lamb waves', 'piezoelectric', 'non-destructive testing', 'structural health monitoring', 'impact damage', 'composites', 'BVID']"
"We report on the photonic variant of the previously introduced guided-path tomography (GPT), by demonstrating a system for footstep imaging using plastic optical fiber (POF) sensors. The 1 m × 2 m sensor head is manufactured by attaching 80 POF sensors on a standard commercial carpet underlay. The sensing principle relies on the sensitivity of POF to bending, quantified by measuring light transmission. The photonic GPT system, comprising the sensor head with processing hardware and software, covered by a mass-production general-purpose carpet top, successfully performs footstep imaging and correctly displays the position and footfall of a person walking on the carpet in real time. We also present the implementation of fast footprint center of mass calculations, suitable for recording gait and footfall. A split-screen movie, showing the frame-by-frame camera-captured action next to the reproduced footprints, can be downloaded at http://ieeexplore.ieee.org.","['Optical fibers', 'Intelligent sensors', 'Sensitivity', 'Floors', 'Imaging', 'Optical fiber sensors']","['Footstep imaging', 'gait', '“intelligent carpet”', 'parallel center of mass algorithm (PCoMA)', 'photonic guided-path tomography (PGPT)', 'plastic optical fiber (POF)']"
"Ultrasonic guided waves (UGW) can be used to inspect and monitor a structure from a single test location. Piezoelectric transducers are commonly dry-coupled with force to the surface of the waveguide in order to excite UGWs. These UGWs propagating within the waveguide will interact and reflect from known features, thus possible damage could be detected. In this paper, the interaction of UGWs with piezoelectric transducers is reported and investigated. A Finite Element Analysis (FEA) approach has been used to conduct a parametric study in order to quantify the effect of the waveguide diameter on the guided wave response. Laboratory experiments are carried out to measure the effect of the force on the dry-coupled piezoelectric transducers and the corresponding guided wave response, including reflections and mode conversions. A test rig is used to apply and measure the force on the piezoelectric transducers. For verification, a 3D Laser Doppler Vibrometry (3D-LDV) scan is performed on the waveguide in order to quantitatively identify the modes of interest. The conclusions reached this paper, particularly with respect to the quantification of the wave mode properties, lead to useful recommendations which may contribute to field inspection scenarios.","['Transducers', 'Force', 'Dispersion', 'Finite element analysis', 'Reflection', 'Couplings', 'Aluminum']","['Piezoelectric transducers', 'Ultrasonic Guided Wave (UGW)', 'Mode conversion', 'Transducer coupling Force']"
"Acetone vapor sensing is important for environmental monitoring and non-invasive screening of diabetes mellitus (DM). Inhaling higher than 176 parts per million (ppm) acetone concentrations affects the respiratory system, while acetone in exhaled breath correlates with blood glucose and exhaling more than 1.8 ppm indicates the person is in danger of DM. DM is currently diagnosed invasively by measuring glucose level in blood, which is painful, and therefore inconvenient. This paper reports MEMS sensor device functionalized with blend of Chitosan/Polyethylene glycol polymers for acetone vapor sensing for possible non-invasive screening of diabetes. The sensor was experimentally tested using synthetic acetone vapor, and found to give linear response for 0.05-5 ppm acetone in air, with a sensitivity of 21 mV/ppm, good repeatability, response, and reversibility. Cross-sensitivity for 2-propanol and methanol was examined, where the responses of the sensor to 1 ppm concentration in air of these two analytes were found to be 24% and 33%, respectively, less compared to its response to the same concentration of acetone.","['Diabetes', 'Sugar', 'Blood', 'Insulin', 'Temperature measurement', 'Temperature sensors']","['MEMS sensor device', 'Chitosan/Polyethylene glycol', 'micro-device', 'Acetone vapor', 'diabetes screening']"
"As Global Navigation Satellite System (GNSS) spoofing techniques are highly stealthy and pose a tremendous risk to targets using GNSS technology, studies on GNSS spoofing techniques have been in the spotlight. If the accurate position and velocity of the target receiver can be obtained, the target receiver can be covertly spoofed during the signal tracking stage using synchronous lift-off spoofing. However, it is often difficult to accurately obtain the position and velocity of a target in real GNSS spoofing scenarios. To address this problem, To study the effects of spoofing signals’ power (relative to the real signal), code pulling rate, carrier Doppler shift, initial code phase difference, and carrier phase difference on the efficacy of spoofing, the intrusion of receiver’s signal tracking loop by spoofing signals is mathematically modeled. Based on the model, an asynchronous lift-off spoofing for GNSS receivers in the signal tracking stage is proposed. Theoretical analysis and experimental results show that the new method resulted in stable Doppler frequency variations, short fluctuations in carrier-to-noise ratio (C/N) and signal lock time, and gentle changes to the receiver’s 3D Earth-Centered Earth Fixed (ECEF) coordinates, when the target’s position and velocity were approximately known during the intrusion period. The proposed spoofing method is highly feasible and could expand the scope of applicability of lift-off spoofing.","['Receivers', 'Global navigation satellite system', 'Target tracking', 'Sensors', 'Doppler effect', 'Satellites', 'Correlation']","['Asynchronous', 'satellite navigation', 'lift-off spoofing', 'signal tracking stage', 'spoofing signal', 'CLC number: TN972 Document code: A']"
"With the rise of advanced driver assistance systems (ADAS), range sensors and their data processing methods are becoming more and more important. Light detection and ranging (LiDAR) sensors are attracting attention due to their unique advantages in terms of radial distance resolution and detection range. However, the study of LiDAR data processing is usually divorced from the LiDAR sensor measurement process itself. This leads to critical measurement information being overlooked. This paper seeks a breakthrough to improve the performance of single-photon-avalanche-diode-based direct time-of-flight LiDAR systems by reviewing the data processing stages and corresponding processing approaches for LiDAR measurements, starting from photon incidence and ending with high-level feature recognition. Firstly, we propose a LiDAR system model based on data generation and transfer. The data forms in such a LiDAR system are mainly classified into timestamps, time-correlated histograms, point cloud data, and high-level properties. Subsequently, data processing methods applied to each of these data forms are analyzed. A number of hardware solutions closely related to data transmission and control are also included in the discussion. The principles, limitations, and challenges of these methods are discussed in detail and the criteria for evaluation of time-correlated histograms in ADAS are proposed. Finally, the research gaps in data processing are summarized, and future directions for research development are presented.","['Laser radar', 'Sensors', 'Data processing', 'Photonics', 'Single-photon avalanche diodes', 'Measurement by laser beam', 'Sensor phenomena and characterization']","['ADAS', 'data processing', 'LiDAR', 'point cloud', 'time of flight', 'time-correlated histograms']"
"A distributed dynamic strain measurement is demonstrated using small gain stimulated Brillouin scattering (SBS) in Brillouin optical time domain reflectometry based on the short-time Fourier transform algorithm. The input power limits, frequency uncertainties for given pulse durations, fiber lengths, and the number of averaging are calculated. The output signal power and the signal-to-noise ratio of the system output are enhanced by SBS. It is found that the signal processing is faster and requires fewer averaging to achieve dynamic sensing performance along the fiber under test. A 60-Hz vibration on a 6-m fiber section at the end of a 935-m fiber is detected with the spatial resolution of 4 m with a sampling rate of 2.5 kS/s.","['Optical fibers', 'Sensors', 'Signal to noise ratio', 'Optical scattering', 'Optical pulses']","['Brillouin scattering', 'distributed fiber optic sensors', 'strain measurement']"
"This paper investigates the key design and operational features of embedded fiber Bragg grating (FBG) sensing for thermal hot spot monitoring in random wound coils, such as used in low voltage electrical machines. To this end thermal experiments are performed on test wound coils embedded with FBG sensors to examine the vital application features of embedded sensor design, such as the sensor packaging material choice, in-situ calibration, sensitivity to vibration, and thermal response time. Measurement error rates are examined and quantified in representative practical tests. The reported results enable a much improved understanding of the performance implications of embedded FBG sensor design features and the attainable in-situ hot spot thermal monitoring performance in random wound coils.","['Coils', 'Sensor phenomena and characterization', 'Temperature measurement', 'Monitoring', 'Temperature sensors']","['FBG thermal sensor', 'embedded sensing', 'random wound coils', 'winding thermal hot spot monitoring']"
"Isolated current sensing is fundamental in several contexts, including power electronics, automotive, and smart buildings. In order to meet the requirements of modern applications, current sensors should feature ever larger bandwidth and dynamic range, as well as reduced power consumption and dimension. Among the available current sensing technologies, Hall-based current sensors have gained an increased popularity owing to their advantages in terms of size, economic feasibility, low power consumption, high dynamic range, and integrability with standard CMOS technologies. This tutorial aims at providing a comprehensive insight into the interdisciplinary world of Hall-effect current sensors, encompassing the fundamental principles of operation, the design of the semiconductor device, the implementation techniques, as well as the methods for sensor modeling and characterization. In particular, this manuscript focuses on Hall-effect sensors realized on standard silicon technologies, and it reviews some typical architectures for the transduction of the measurand current into a magnetic field, as well as the electronic front-end. While this tutorial is mainly addressed to students and non-expert readers, specific design aspects and dispersion effects due to temperature and other external phenomena are also discussed.","['Sensors', 'Magnetic sensors', 'Voltage measurement', 'Sensor phenomena and characterization', 'Magnetic hysteresis', 'Current measurement', 'Perpendicular magnetic anisotropy']","['Current sensing', 'Hall effect', 'galvanic isolation', 'instrumentation amplifier', 'analog front-end', 'current-to-magnetic field transduction', 'sensor characterization', 'sensor modeling']"
"Zero velocity update is a common and efficient approach to bound the accumulated error growth for foot-mounted inertial navigation system. Thus a robust zero velocity detector (ZVD) for all kinds of locomotion is needed for high accuracy pedestrian navigation systems. In this paper, we investigate two machine learning-based ZVDs: Histogram-based Gradient Boosting (HGB) and Random Forest (RF), aiming at adapting to different motion types while reducing the computation costs compared to the deep learning-based detectors. A complete data pre-processing procedure, including a feature engineering study and data augmentation techniques, is proposed. A motion classifier based on HGB is used to distinguish “single support” and “double float” motions. This concept is different from the traditional locomotion classification (walking, running, stair climbing) since it merges similar motions into the same class. The proposed ZVDs are evaluated with inertial data collected by two subjects over a 1.8 km indoor/outdoor path with different motions and speeds. The results show that without huge training dataset, these two machine learning-based ZVDs achieve better performances (55 cm positioning accuracy) and lower computational costs than the two deep learning-based Long Short-Term Memory methods (1.21 m positioning accuracy).","['Detectors', 'Legged locomotion', 'Training', 'Foot', 'Navigation']","['Pedestrian navigation', 'inertial sensors', 'IMU', 'machine learning', 'zero-velocity detection']"
"Smart IoT solutions integrated into power grid stations are important due to their high economic and social value. Power over fiber technology to remotely feeding sensors and control electronics is a good choice in these environments of high electromagnetic interference. A sensing system design for magnetic field monitoring, fire and temperature/presence detection, and remotely fed by optical means is discussed. This design includes two types of nodes, smart and passive. Smart remote nodes have an energy manager to provide power on demand. Asymmetric splitting is proposed to optimize power distribution. Some tests on remote node power consumption, feeding, sensing, and centralized monitoring in one type of those nodes are successfully performed and reported.","['Optical transmitters', 'Optical fiber sensors', 'Monitoring', 'Intelligent sensors', 'Optical fibers']","['Electromagnetic compatibility', 'multimode optical fiber', 'power-over-fiber (PoF)', 'power distribution', 'sensor network', 'smart remote node', 'Internet of Things (IoT)']"
"Producing food in a sustainable way is becoming very challenging today due to the lack of skilled labor, the unaffordable costs of labor when available, and the limited returns for growers as a result of low produce prices demanded by big supermarket chains in contrast to ever-increasing costs of inputs such as fuel, chemicals, seeds, or water. Robotics emerges as a technological advance that can counterweight some of these challenges, mainly in industrialized countries. However, the deployment of autonomous machines in open environments exposed to uncertainty and harsh ambient conditions poses an important defiance to reliability and safety. Consequently, a deep parametrization of the working environment in real time is necessary to achieve autonomous navigation. This article proposes a navigation strategy for guiding a robot along vineyard rows for field monitoring. Given that global positioning cannot be granted permanently in any vineyard, the strategy is based on local perception, and results from fusing three complementary technologies: 3D vision, lidar, and ultrasonics. Several perception-based navigation algorithms were developed between 2015 and 2019. After their comparison in real environments and conditions, results showed that the augmented perception derived from combining these three technologies provides a consistent basis for outlining the intelligent behavior of agricultural robots operating within orchards.","['Robot sensing systems', 'Three-dimensional displays', 'Agriculture', 'Navigation', 'Cameras']","['3D Vision', 'field robotics', 'autonomous navigation', 'digital farming', 'local perception', 'sensor fusion']"
"In this paper, a new technique which enables communication data to be embedded into multiple-input multiple-output (MIMO) radar waveforms is presented. A linear frequency modulated (LFM) signal is used for radar sensing, and multiple phase-shift keying (PSK) symbols or bit sequences are embedded into the LFM signal for communications. Such waveforms are subsequently used for radar sensing with MIMO beamforming. Orthogonality between transmitters is ensured using either a time-division multiplexing (TDM) or code-division multiplexing (CDM) approach. The performance of these novel techniques is demonstrated through both simulation and experimentation.","['Phase shift keying', 'Frequency modulation', 'Transmitters', 'MIMO radar', 'Radar', 'Time division multiplexing', 'MIMO']","['Dual function radar and communication', 'MIMO', 'PSK-LFM waveform']"
"The introduction of sensor technology in our daily lives has brought comfort, convenience, and improved health over the past few decades. Technological advances further expanded the use of medical sensors by reducing their size and costs. Medical sensors improve the intelligence and capabilities of healthcare services including, remote health monitoring, surgical procedures, therapy, and rehabilitation. We present a comprehensive review of medical sensors in the last 50 years focusing on their deployment in healthcare applications. The review also discusses the role of Internet of Things (IoT) technology in enhancing the capabilities of sensor technologies for the healthcare domain. Moreover, we also investigate the benefits and challenges of various integrated architectures which have been proposed recently to seamlessly integrate heterogeneous medical sensors with emerging technologies and paradigms that include edge, Mobile Cloud Computing (MCC), fog, and cloud computing technologies. Finally, we identify future challenges that must be addressed to achieve the maximum potential and benefits of medical sensor technologies and ultimately provide robust, scalable, reliable, and cost-effective healthcare delivery.","['Sensors', 'Medical services', 'Temperature sensors', 'Sensor phenomena and characterization', 'Monitoring', 'Wireless communication', 'Medical diagnostic imaging']","['Body area network', 'cloud', 'edge', 'fog', 'healthcare', 'Internet of Things (IoT)', 'sensing', 'sensor']"
"Organic electrochemical transistors (OECTs) have been recognized as a major emerging technology in the area of flexible electronics in the last decade. Although they have yet to be incorporated in common electronic fabrication technologies, they have considerably advanced as an emerging platform for biosensing applications. The paper provides a comprehensive and critical review of the most important advances in the field of OECT-based biosensors. A brief description of the device physics is given with the most important equations and a comparison has been made with the conventional MOSFET devices and characteristic equations. The use of OECTs as an emerging biosensing platform has been explored and their application as biomolecule, enzyme, bacteria, viruses, cells, nucleotide detectors as well as electrophysiological and wearable sensors has been reported. Furthermore, trends have been extracted and described in the paper in terms of fabrication technologies, electrode materials and most importantly, the semiconducting polymer. Additionally, future perspectives on the development and fabrication technologies of these devices have been further explored.","['Ions', 'Logic gates', 'MOSFET', 'Biosensors', 'Electrodes']","['Biocompatibility', 'biosensors', 'high transconductance', 'Ion sensors', 'organic electrochemical transistors', 'OECTs']"
"Even as modern indoor positioning systems become more precise and computationally lightweight, most rely on specific infrastructure to be installed, leading to increased setup and maintenance costs. As such, multiple infrastructure-free solutions were devised relying on signals such as magnetic field, ambient light, and movement. In this paper, we propose a framework for determining the user's location through the sound recorded by the user's device. With this goal, we present two algorithms: SoundSignature and SoundSimilarity. With SoundSignature, we extract acoustic fingerprints from the recorded audio and employ them in a support vector machine classifier. With SoundSimilarity, where we employ a novel audio similarity measure to detect if users are in the same location as other users or microphone equipped devices. Both of these algorithms require no infrastructure and are computationally lightweight, thus allowing their use either in conjunction with other infrastructure-free technologies or standalone. The training of these algorithms requires nothing more than a smartphone or a similar device under normal usage conditions, eliminating the need of any dedicated equipment.","['Feature extraction', 'Sensors', 'Spectrogram', 'Mel frequency cepstral coefficient', 'Support vector machines', 'Data mining']","['Indoor location', 'sound analysis', 'infrastructure-free', 'support vector machine', 'feature selection', 'SMOTE']"
"The one-step modification of a commercial RFID sensing tag is demonstrated using polydimethylsiloxane-based thin-film chemistry to construct reusable passive RFID sensors for changes in the dielectric properties of electrolyte solutions as a function of concentration. The effects of PDMS film thickness were characterized as a function of RFID sensor code value. The output sensor code of the RFMicron RFM2100-AER wireless flexible moisture sensor (taken between 800 and 860 MHz) was compared with the readings taken when the tag was dry and when the tag had a water deposition on the sensor area. The effect of the direct application of liquid water on the tag was to alter the capacitance presented to the integrated chip which auto-tunes to correct for the reactance. By varying the thickness of the PDMS film between the interdigitated sensor and deposited liquid, the sensitivity of the tag to a high dielectric medium could be controlled. Aqueous salt solutions were tested on a 500-μm-thick film. It was found that the sensing platform could be used as a means of measuring the concentration of various salt solutions within the range 0-2 M and in turn could be used as a passive UHF RFID dielectric measuring tool. The measurement capability of the platform was subsequently demonstrated using a reduced frequency range (845-865 MHz).","['Electrodes', 'Radiofrequency identification', 'Sensor phenomena and characterization', 'Antennas', 'Liquids', 'Monitoring']","['Radio frequency identification', 'RFID tags', 'chemical sensor', 'aqueous electrolyte']"
"Long term sensors drift is a challenging problem to solve for instruments like an Electronic Nose System (ENS). These electronic instruments rely on Machine Learning (ML) algorithms for recognizing the sensed odors. The effect of long-term drift influences the performance of ML algorithms and the models those are trained on drift free data fail to perform on the drifted data. Moreover, the response of an electronic nose system depends on the variable response of the sensors and a delay is expected in reaching a steady state by the sensors. In this paper, these two problems of ‘sensors long term drift’ and ‘delayed response’ are solved simultaneously to propose a robust and fast electronic nose system, with following merits: (i) only initial transient state features are used in the proposed system without waiting for the sensors to reach a steady state, (ii) a modified boxplot approach is used to handle noisy/drifted data points as a preprocessing step before the classification setup, (iii) a heuristic tree classification approach with optimized transient features is proposed, (iv) the proposed approach only relies on adapted ML methods contrary to the traditional approaches like system recalibration or sensors replacement for handling sensors drift, and (v) the proposed ML model does not require any target domain data and uses only the source domain data for learning the classifier, opposed to the other ML solutions available in the existing literature. The proposed method is tested using a large scale gas sensors drift benchmark dataset available freely on UCI Machine Learning repository and is found better than the existing state-of-the art approaches with an overall accuracy of 87.34%.","['Sensors', 'Transient analysis', 'Sensor systems', 'Sensor phenomena and characterization', 'Gas detectors', 'Steady-state', 'Time factors']","['Artificial olfaction', 'electronic nose', 'heuristic optimization', 'industrial gases', 'sensors drift']"
"The in situ wireless sensing of dielectric properties for organic aqueous solutions with a wide range of relative permittivities is presented. The use of an ultra-high frequency passive label antenna design attached to either clear borosilicate glass bottle or Petri plate is proposed, which allows for the unobtrusive, safe monitoring of the liquid solutions. The meandered dipole antenna (with a parasitic loop matching component) frequency is highly reliant on the chosen container as well as on the liquid present within, and adjusts with shifting dielectric properties. Tested solutions of high-relative permittivity (such as water) along with low permittivity, lossy liquids (such as xylene) presented distinctive frequency characteristics with read distances of up to 7 m for each type of container tested. The sensor was also able to detect `unknown' solutions and determine the dielectric properties by utilizing standard curve analysis with an accuracy of ±0.834 relative permittivity and ±0.050 S · m -1 conductivity (compared to a standard dielectric measurement system available on the market). With the accuracy known, tuning the design to fit any necessary frequency is possible as a means to detect specific changes in any one liquid system. This sensor is a possible candidate for discreet real-time monitoring of liquid storage containers and an alternative for low-cost bulk liquid dielectric property identification, which could be implemented in areas requiring, constant, or remote monitoring as needed.","['Liquids', 'Sensors', 'Dielectrics', 'Permittivity', 'Glass', 'Resonant frequency', 'Dipole antennas']","['Antenna', 'battery-free', 'passive sensing', 'radio frequency identification (RFID)', 'ultra-high frequency (UHF)', 'wireless sensing']"
"This paper presents a tilt sensor comprising of 3D printed capacitive sensors located at the four ends of a ‘+’ shaped channel to provide the orientation of objects by using the capacitive fluid level measurement concept. The interdigitated capacitive sensors were developed by 3D printing and the channel was filled with ecoflex and silicone oil to obtain two variants of tilt sensor. The results show a change in the capacitance of ~11.5% and ~9.53% for ecoflex and silicone oil-based sensors respectively. A drift of ~2.6% is observed for ecoflex and ~0.16% for silicone oil. Considering the lower viscosity and the lower drift, the silicone oil-based tilt sensors were further investigated and two tilt sensors with varying silicone volumes (1 ml and 1.5 ml ) were fabricated and compared for tilt angles ranging from 0° to 30°. The result from all four interdigitated capacitive sensors in the tilt sensing structure show similar rate of change in capacitance (~0.67% per degree increase in the tilt angle) with a standard deviation of ~±0.1%. However, the sensor with higher volume of silicone oil (1.5 ml ) saturated at a tilt angle of ~20° which is ~10° smaller than the response of the sensor fabricated with 1 ml of silicone oil (saturated at 30°). We also demonstrate the possibility of extending the sensor range by optimizing the volume of fluid and the channel’s fluid capacity. With integration of fabricated tilt sensor with a robots’ body, white cane or smart objects etc., it will be possible to obtain the information about orientation.","['Sensors', 'Fluids', 'Capacitive sensors', 'Sensor phenomena and characterization', 'Oils', 'Capacitance', 'Robot sensing systems']","['3D printing', 'additive manufacturing', 'capacitive sensor', 'tilt sensor', 'robotics', 'rehabilitation']"
"During the bidirectional exchange of electricity between electric vehicles (EVs) and smart grid, plenty of sensors have been deployed to sense the EVs' battery status and monitor the electricity regulation requirements. When electricity consumption at power grid demand-side sharply increases/decreases, charging points (CPs) access these sensors by multicast technology to aggregate the EVs' battery status, publish electricity regulation requests and assign the state of charge (SOC) transition instructs in vehicle-to-grid (V2G). However, intermittent connections arising from distributed and mobile energy storage bring many challenging problems on multicast scheduling. First, in smart grid, peak shaving and load shifting require V2G to sense and regulate the EVs' SOC according to their battery status precisely, while the traditional multicast only considers their mutable network locations (e.g., WLAN, IP, and MAC). Second, as the number of EVs that participate into V2G increases, V2G needs to schedule the massive multicast traffic with priority to satisfy the dynamic and real-time regulation requirements. To address these problems, in this paper, we first aim to demonstrate that the appreciable battery status is more adaptive than network location to act as a multicast primitive in V2G. We propose a battery status sensing software-defined multicast (BSS-SDM) scheme to reduce the latency of V2G regulation services. In the BSS-SDM scheme, the battery status of each EV is identified during SOC transitions and maintained by a centralized controller. Besides, we propose a battery-status-based multicast scheduling algorithm to implement the V2G regulation optimization. Simulation results verify the effectiveness of proposed schemes.","['Batteries', 'Sensors', 'State of charge', 'IEC Standards']","['Smart grid', 'battery status', 'vehicle-to-grid (V2G)', 'state of charge (SOC)', 'software-defined multicast']"
"The increasing bandwidth requirement of new wireless applications has lead to standardization of the millimeter wave spectrum for high-speed wireless communication. The millimeter wave spectrum is part of 5G and covers frequencies between 30 and 300 GHz that correspond to wavelengths ranging from 10 to 1 mm. Although millimeter wave is often considered as a communication medium, it has also proved to be an excellent `sensor', thanks to its narrow beams, operation across a wide bandwidth, and interaction with atmospheric constituents. In this paper, which is to the best of our knowledge the first review that completely covers millimeter wave sensing application pipelines, we provide a comprehensive overview and analysis of different basic application pipeline building blocks, including hardware, algorithms, analytical models, and model evaluation techniques. The review also provides a taxonomy that highlights different millimeter wave sensing application domains. By performing a thorough analysis, complying with the systematic literature review methodology and reviewing 165 papers, we not only extend previous investigations focused only on communication aspects of the millimeter wave technology and using millimeter wave technology for active imaging, but also highlight scientific and technological challenges and trends, and provide a future perspective for applications of millimeter wave as a sensing technology.","['Sensors', 'Millimeter wave radar', 'Millimeter wave communication', 'Pipelines', 'Analytical models', 'Data models', 'Millimeter wave measurements']","['5G', 'analytical modeling', 'millimeter wave', 'millimeter wave sensing application pipeline', 'radar', 'systematic literature review']"
"A novel pedestrian dead reckoning method conceived to be used with sensors freely positioned not too far from the waist level is presented. Attitude and heading reference systems already built in in nowadays inertial measurements units (IMUs) are exploited to cast the sampled data into a global reference coordinate system, where human gait analysis can be used to figure out the motion related to each single step. In particular, vertical accelerations are processed by means of a phase locked loop to detect the pace and the steps, and then the step length is computed exploiting an empirical piecewise linear relationship with the pace, while the geometrical features of the planar acceleration are used to estimate the stride heading, based on the waist kinematics. Experiments show the good results of the proposed algorithm when using both a low-cost IMU embedded in a smartphone and a more expensive stand-alone device, highlighting the method robustness with respect to the implementing hardware.","['Sensors', 'Acceleration', 'Legged locomotion', 'Estimation', 'Principal component analysis', 'Dead reckoning', 'Kinematics']","['Dead reckoning', 'pedestrian navigation', 'phase locked loops', 'wearable sensors']"
"The health status of an elderly person can be identified by examining the additive effects of aging along disease linked to it and can lead to the 'unstable incapacity'. This health status is essentially determined by the apparent decline of independence in Activities of Daily Living (ADLs). Detecting ADLs provide possibilities of improving the home life of elderly people as it can be applied to fall detection systems.. This article looks at Radar images to detect large scale body movements. Using a publicly available Radar spectogram dataset, Deep Learning and Machine Learning techniques are used for image classification of Walking, Sitting, Standing, Picking up Object, Drinking Water and Falling Radar spectograms. The Machine Learning algorithm used were Random Forest, K Nearest Neighbours and Support Vector Machine. The Deep Learning algorithms used in this article were Long Short Term Memory, Bi-directional Long Short-Term Memory and Convolutional Neural Network. In addition to using Machine Learning and Deep Learning on the spectograms, data processing techniques such as Principal Component Analysis and Data Augmentation is applied to the spectogram images. The work done in this article is divided into 4 experiments. The first experiment applies Machine and Deep Learning to the the Raw images data, the second experiment applies Principal Component Analysis to the Raw image Data, the third experiment applies Data Augmentation to the Raw image data and the fourth and final experiment applies Principal Component Analysis and Data Augmentation to the Raw image data. The results obtained in these experiments found that the best results were obtained using the CNN algorithm with Principal Component Analysis and Data Augmentation together to obtain a result of 95.30 % accuracy. Results also showed how Principal Component Analysis was most beneficial when the training data was expanded by augmentation of the available data.","['Machine learning', 'Sensors', 'Senior citizens', 'Principal component analysis', 'Radar imaging', 'Gray-scale']","['Wandering behavior', 'wireless sensing', 'machine learning', 'human activity', 'patient monitoring']"
"Advances in sensing technology raise the possibility of creating neural interfaces that can more effectively restore or repair neural function and reveal fundamental properties of neural information processing. To realize the potential of these bioelectronic devices, it is necessary to understand the capabilities of emerging technologies and identify the best strategies to translate these technologies into products and therapies that will improve the lives of patients with neurological and other disorders. Here, we discuss emerging technologies for sensing brain activity, anticipated challenges for translation, and perspectives for how to best transition these technologies from academic research labs to useful products for neuroscience researchers and human patients.","['Electrodes', 'Neurons', 'Capacitance', 'Optical sensors', 'Impedance', 'Surface topography']","['Neural engineering', 'sensors']"
"In this work, we present an optical space imaging dataset using a range of event-based neuromorphic vision sensors. The unique method of operation of event-based sensors makes them ideal for space situational awareness (SSA) applications due to the sparseness inherent in space imaging data. These sensors offer significantly lower bandwidth and power requirements making them particularly well suited for use in remote locations and space-based platforms. We present the first publicly-accessible event-based space imaging dataset including recordings using sensors from multiple providers, greatly lowering the barrier to entry for other researchers given the scarcity of such sensors and the expertise required to operate them for SSA applications. The dataset contains both day time and night time recordings, including simultaneous co-collections from different event-based sensors. Recorded at a remote site, and containing 572 labeled targets with a wide range of sizes, trajectories, and signal-to-noise ratios, this real-world event-based dataset represents a challenging detection and tracking task that is not readily solved using previously proposed methods. We propose a highly optimized and robust feature-based detection and tracking method, designed specifically for SSA applications, and implemented via a cascade of increasingly selective event filters. These filters rapidly isolate events associated with space objects, maintaining the high temporal resolution of the sensors. The results from this simple yet highly optimized algorithm on the space imaging dataset demonstrate robust high-speed event-based detection and tracking which can readily be implemented on sensor platforms in space as well as terrestrial environments.","['Sensors', 'Space vehicles', 'Cameras', 'Telescopes', 'Image sensors', 'Radar tracking']","['Space situational awareness', 'event-based detection', 'event-based features', 'event-based tracking', 'event-based processors']"
"In recent years additive manufacturing technologies have become widely popular. For complex functional components or low volume production of workpieces, laser powder bed fusion can be used. High safety requirements, e.g. in the aerospace sector, demand extensive quality control. Therefore, offline non-destructive testing methods like computed tomography are used after manufacturing. Recently, for enhanced profitability and practicality online non-destructive testing methods, like optical tomography have been developed. This paper discusses the applicability of eddy current testing with magnetoresistive sensors for laser powder bed fusion parts. For this purpose, high spatial resolution giant magnetoresistance arrays are utilized for testing in combination with a single wire excitation coil. A heterodyne principle minimizes metrology efforts. This principle is compared to conventional signal processing in an eddy current testing setup using an aluminum test sample with artificial surface defects. To evaluate the influence of the powder used in the manufacturing process on eddy current testing and vice versa, a laser powder bed fusion mock-up made from stainless steel powder (316L) is used with artificial surface defects down to100 μm. This laser powder bed fusion specimen was then examined using eddy current testing and the underlying principles.","['Frequency measurement', 'Powders', 'Testing', 'Bridge circuits', 'Sensors', 'Eddy current testing', 'Laser fusion']","['Eddy current testing', 'giant magnetoresistance', 'additive manufacturing', 'laser powder bed fusion', '316L', 'heterodyning']"
"Multiple leak sources may occur in a large pressure vessel that contains corrosive materials or has been in use for a long period of time. Although, a variety of leak localization methods have been proposed in previous studies, they are capable of locating only a single leak source. Methods for simultaneous localization of multiple leak sources are desirable in practical applications. To address this issue, a novel method using acoustic emission (AE) sensors in conjunction with MUltiple SIgnal Classification (MUSIC) algorithm and wavelet packet analysis is proposed and experimentally assessed. High-frequency AE sensors are assembled into a linear array to acquire signals from multiple leak sources. Characteristics of the leak signals are analyzed in the frequency domain. Wavelet packet analysis is deployed to extract useful information about the signals from the frequency band of 50–400 kHz. The MUSIC algorithm is applied to identify the directions of the leak sources through a space spectrum function. Leak sources are located based on the directions identified by the AE sensor array placed at different locations. The performance of the proposed method is evaluated through experimental tests on a stainless steel flat plate of 100 cm×100cm×0.4cm. The results demonstrate that the method is capable of locating two leak holes. In addition, the localization accuracy depends on the leaking pressure. It is demonstrated that the two leak holes are located within two small areas, respectively, which are 25.12 cm 2 for leak hole 1 and 1.96 cm 2 for leak hole 2.","['Sensor arrays', 'Wavelet packets', 'Multiple signal classification', 'Wavelet analysis', 'Covariance matrices', 'Pressure vessels']","['Acoustic emission', 'localization', 'multiple leak sources', 'MUSIC', 'wavelet packet']"
"In this paper, the 2f/1f tunable diode laser wavelength modulation spectroscopy is used for the simultaneous measurement of concentration and temperature of CO 2 in the exhaust plume of an aero engine. The suitability of the R48 spectral feature of CO 2 at 1997.2 nm is discussed for this project and for further application of CO 2 tomographic imaging on large-scale aero-engines. To ensure accurate recovery of gas parameters at the high exhaust temperatures, a full spectral characterization of the absorption feature is presented using a direct spectroscopy. The 2f/1f method is validated in the laboratory for controlled gas mixtures and temperatures to recover concentration and temperature. Good agreement with the actual temperature and concentration values is demonstrated. Finally, single path measurements are presented for an aero engine exhaust showing good correlation with the measured engine conditions.","['Temperature measurement', 'Engines', 'Wavelength measurement', 'Measurement by laser beam', 'Temperature dependence', 'Temperature sensors', 'Optical fibers']","['Optical sensor', 'high temperature spectroscopy', 'aerospace engineering', 'wavelength modulation spectroscopy', 'optical tomography']"
"The market size of civilian drones is tremendously increasing and is expected to reach 1.66 million by the end of 2023. The increase in number of civilian drones poses several privacy and security threats. To safeguard critical assets and infrastructure and to protect privacy of people from the illegitimate uses of commercial drones, a drone detection system is inevitable. In particular, there is a need for a drone detection system that is efficient, accurate, robust, cost-effective and scalable. Recognizing the importance of the problem, several drone detection approaches have been proposed over time. However, none of these provides sufficient performance due to the inherited limitations of the underlying detection technology. More specifically, there are trade-offs among various performance metrics e.g., accuracy, detection range, and robustness against environmental conditions etc. This motivates an in-depth study and critical analysis of the existing approaches, highlighting their potential benefits and limitations. In this paper, we provide a rigorous overview of the existing drone detection techniques and a critical review of the state-of-the-art. Based on the review, we provide key insights on the future drone detection systems. We believe these insights will provide researchers and practicing engineers a holistic view to understand the broader context of the drone detection problem.","['Drones', 'Sensors', 'Privacy', 'Autonomous aerial vehicles', 'Sensor phenomena and characterization', 'Wireless sensor networks', 'Wireless communication']","['Unmanned aerial vehicles', 'drone detection', 'privacy', 'radio', 'radar', 'security', 'visual']"
"Considering an ultra-reliable low latency communication scenario, we assess the trade-off in terms of energy consumption between achieving time diversity through retransmissions and having to communicate at a higher rate due to latency constraints. Our analysis considers Nakagami-m block-fading channels with Chase combining hybrid automatic repeat request. We derive a fixed-point equation to determine the best number of allowed transmission attempts considering the maximum possible energy spent, which yields insights into the system behavior. Furthermore, we compare the energy consumption of the proposed approach against direct transmission with frequency diversity. Results show substantial energy savings using retransmissions when selecting the maximum number of transmission attempts according to our approach. For instance, considering a Rayleigh channel and smart grid teleprotection applications, our approach uses around 8 times less energy per bit compared with a direct transmission with frequency diversity.","['Signal to noise ratio', 'Channel estimation', 'Receivers', 'Energy consumption', 'Diversity reception', 'Fading channels', 'Reliability']","['URLLC', 'energy efficiency', 'CC-HARQ']"
"This paper presents a complete architecture of an innovative wireless sensor network able to real-time monitor and control complex lighting systems such as the newly installation at the Scrovegni Chapel, Padua, Italy, representing the first world example in cultural heritage field. The realization effectively creates the Internet of Things paradigm, since all its components are directly connected to the Internet and each other thanks to global IPv6 addresses. The new IoT lighting system represents a “restoration of perception” project because the luminaries can be controlled in intensity and color temperature based on natural light and the desired rendering. In the initial phase, specifically designed sensors measure indoor lighting variations and this monitoring will be extended for a sufficient time to determine the proper control actions. Furthermore, besides adjusting the illuminance and color temperature for aesthetic and perceptual purposes, the system can also real-time monitor and control a variety of environmental parameters, fundamental for cultural heritage conservation, providing long-term studies of correlations between artificial light, natural light, and lighting performance, and also to evaluate over time the conservative aspects correlating them with any other critical environmental condition. The control of exposure to artificial light is also important in other contexts because light can interfere with biological processes controlled by endogenous circadian rhythms, with possible negative consequences for health results. Finally, this architectural solution can be extended to a complete Smart City system that integrates cultural heritage as one of the countless elements to be monitored and controlled in the city.","['Lighting', 'Cultural differences', 'Sensors', 'Monitoring', 'Real-time systems', 'Internet of Things', 'Temperature measurement']","['Circadian rhythms', 'cultural heritage', 'Internet of Things', 'light of things', 'Smart Cities', 'wireless sensor network', 'LED lighting system']"
"Elevated cholesterol levels are associated with a greater risk of developing cardiovascular disease and other illnesses, making it a prime candidate for detection on a disposable biosensor for rapid point of care diagnostics. One of the methods to quantify cholesterol levels in human blood serum uses an optically mediated enzyme assay and a bench top spectrophotometer. The bulkiness and power hungry nature of the equipment limits its usage to laboratories. Here, we present a new disposable sensing platform that is based on a complementary metal oxide semiconductor process for total cholesterol quantification in pure blood serum. The platform that we implemented comprises readily mass-manufacturable components that exploit the colorimetric changes of cholesterol oxidase and cholesterol esterase reactions. We have shown that our quantification results are comparable to that obtained by a bench top spectrophotometer. Using the implemented device, we have measured cholesterol concentration in human blood serum as low as 29 μM with a limit of detection at 13 μM, which is approximately 400 times lower than average physiological range, implying that our device also has the potential to be used for applications that require greater sensitivity.","['Biochemistry', 'Photodiodes', 'Light emitting diodes', 'Semiconductor device measurement', 'Optical sensors', 'Biosensors']","['Biosensor', 'CMOS', 'colorimetric', 'diagnostics', 'enzyme', 'photodiode']"
"This paper reports a step-wise protocol to immobilize corresponding antibody on gold electrodes for electrochemical measurement of Osteocalcin (Oc), a bone turnover marker, levels in human serum. To our knowledge this is the first such sensor developed for Oc measurement. The magnitude of electrochemical response current was linearly dependent on Oc concentration with an R square of 0.99. The detection limit was calculated to be 0.65 ng/mL. The sensor was capable to assess antigen levels of 2.5-90 ng/mL. The correlation between our sensor results and that of electrochemiluminescence (ECLIA), the current state-of-art, was also investigated and the results showed a high correlation (slope of 0.97 and R 2 of 0.98) between the two systems. This is while the total assay time for the Oc sensor is about 5 min and ECLIA needs several hours to be performed.","['Gold', 'Electrodes', 'Proteins', 'Nanoparticles', 'Biosensors', 'Surface treatment']","['Osteocalcin', 'osteoporosis', 'biosensor', 'electrochemistry']"
"Electrical bioimpedance entails the measurement of the electrical properties of tissues as a function of frequency. It is thus a spectroscopic technique. It has been applied in a plethora of biomedical applications for diagnostic and monitoring purposes. In this tutorial, the basics of electrical bioimpedance sensor design will be discussed. The electrode/electrolyte interface is thoroughly described, as well as methods for its modelling with equivalent circuits and computational tools. The design optimization and modelling of bipolar and tetrapolar bioimpedance sensors is presented in detail, based on the sensitivity theorem. Analytical and numerical modelling approaches for electric field simulations based on conformal mapping, point electrode approximations and the finite element method (FEM) are also elaborated. Finally, current trends on bioimpedance sensors are discussed followed by an overview of instrumentation methods for bioimpedance measurements, covering aspects of voltage signal excitations, current sources, voltage measurement front-end topologies and methods for computing the electrical impedance.","['Electrodes', 'Impedance', 'Bioimpedance', 'Biosensors', 'Voltage measurement', 'Impedance measurement', 'Frequency measurement', 'Tutorials']","['Bipolar impedance sensor', 'electrical bioimpedance', 'instrumentation', 'tetrapolar impedance sensor', 'tissue impedance']"
"On-line vibration monitoring plays an important role in the fault diagnosis and prognosis of industrial belt drive systems. This paper presents a novel measurement technique based on electrostatic sensing to monitor the transverse vibration of power transmission belts in an on-line, continuous, and non-contact manner. The measurement system works on the principle that variations in the distance between a strip-shaped electrode and the naturally electrified dielectric belt give rise to a fluctuating current output. The response of the sensor to a belt moving both axially and transversely is numerically calculated through finite-element modeling. Based on the sensing characteristics of the sensor, the transverse velocity of the belt is characterized through the spectral analysis of the sensor signal. Experiments were conducted on a two-pulley belt drive system to verify the validity of the sensing technique. The belt vibration at different axial speeds was measured and analyzed. The results show that the belt vibrates at well-separated modal frequencies that increase with the axial speed. A closer distance between the electrode and the belt makes higher order vibration modes identifiable, but also leads to severer signal distortion that produces higher order harmonics in the signal.","['Belts', 'Sensors', 'Vibrations', 'Electrostatics', 'Vibration measurement', 'Monitoring', 'Electrostatic measurements']","['Belt drive', 'vibration monitoring', 'electrostatic sensor', 'finite element modelling', 'sensing characteristics']"
"An automatic stress detection system that uses unobtrusive smart bands will contribute to human health and well-being by alleviating the effects of high stress levels. However, there are a number of challenges for detecting stress in unrestricted daily life which results in lower performances of such systems when compared to semi-restricted and laboratory environment studies. The addition of contextual information such as physical activity level, activity type and weather to the physiological signals can improve the classification accuracies of these systems. We developed an automatic stress detection system that employs smart bands for physiological data collection. In this study, we monitored the stress levels of 16 participants of an EU project training every day throughout the eight days long event by using our system. We collected 1440 hours of physiological data and 2780 self-report questions from the participants who are from diverse countries. The project midterm presentations (see Figure 3) in front of a jury at the end of the event were the source of significant real stress. Different types of contextual information, along with the physiological data, were recorded to determine the perceived stress levels of individuals. We further analyze the physiological signals in this event to infer long term perceived stress levels which we obtained from baseline PSS-14 questionnaires. Session-based, daily and long-term perceived stress levels could be identified by using the proposed system successfully.","['Stress', 'Feature extraction', 'Biomedical monitoring', 'Physiology', 'Heart rate variability', 'Tools', 'Skin']","['Commercial smartwatch', 'mental stress', 'psychophysiological', 'emotion regulation', 'heart rate variability', 'electrodermal activity']"
"In this paper, we introduce a 3D convolutional neural network (CNN)-based method to segment point clouds obtained by mobile laser scanning (MLS) sensors into nine different semantic classes, which can be used for high definition city map generation. The main purpose of semantic point labeling is to provide a detailed and reliable background map for self-driving vehicles (SDV), which indicates the roads and various landmark objects for navigation and decision support of SDVs. Our approach considers several practical aspects of raw MLS sensor data processing, including the presence of diverse urban objects, varying point density, and strong measurement noise of phantom effects caused by objects moving concurrently with the scanning platform. We also provide a new manually annotated MLS benchmark set called SZTAKI CityMLS, which is used to evaluate the proposed approach, and to compare our solution to various reference techniques proposed for semantic point cloud segmentation. Apart from point level validation we also present a case study on Lidar-based accurate self-localization of SDVs in the segmented MLS map.","['Three-dimensional displays', 'Sensors', 'Semantics', 'Urban areas', 'Phantoms', 'Labeling', 'Roads']","['Semantic point cloud segmentation', 'deep learning', 'mobile laser scanning']"
"This paper presents the characterization of an algorithm aimed at performing accurate fiber optic-based shape sensing. The measurement of the shape relies on the evaluation of the strains applied to an optic fiber in order to identify relevant spatial parameters, such as the curvature radii and bending direction, which define its shape. The measurement system is based on a 7-core multicore fiber, containing up to 9 triplets of fiber Bragg grating sensors (FBGs) organized around a central core used as reference. The proposed study aims at comparing the widely used Frenet-Serret equations with an algorithm based on the homogeneous transformation matrices that are normally used in robotics to express the position of a point in different frames, i.e. from local to global coordinates. The numerical results of the performed experiments (with different multicore fibers and setups) extensively prove the superiority of the alternative method over the Frenet-Serret equations in terms of finding a trade-off between accuracy and execution time.","['Shape', 'Optical fiber sensors', 'Fiber gratings', 'Strain', 'Optical fibers']","['Shape sensing', 'fiber Bragg grating', 'multicore optical fiber', 'homogeneous transformation matrix', 'Frenet-Serret', 'performance', 'three-dimensional']"
"The automatic recognition of a metal component or workpiece currently relies on optical techniques and image matching. It is not possible to distinguish workpieces with different materials. In this paper, a novel electromagnetic inductive sensor array similar to those used in the electromagnetic tomography has been designed to address this problem. Furthermore, instead of reconstructing the full magnetic polarizability tensor, we have proposed a partial tensor approach, which shows that a 2-D tensor is capable of distinguishing the material difference and recognising the geometric dominance of workpieces with experimental data. In addition, it has been found that the phase of the tensor is strongly linked to the materials properties while the magnitude of the tensor eigenvalues implies the basic geometry of workpiece.","['Tensile stress', 'Magnetic sensors', 'Magnetic resonance imaging', 'Mathematical model', 'Magnetic moments', 'Electromagnetic interference']","['Workpiece recognition', 'electromagnetic induction', '2-D magnetic polarizability tensor']"
"Timely diagnosis of cardiovascular diseases (CVD) is crucial to prevent morbidity and mortality. Atrial fibrillation (AFib) and heart failure (HF) are two prevalent cardiac disorders that are associated with a high risk of morbidity and mortality, especially if they are concurrently present. Current approaches fail to screen many at-risk individuals who would benefit from preventive treatment; while others receive unnecessary interventions. An effective approach to the detection of CVDs is mechanocardiography (MCG) by which translational and rotational precordial chest movements are monitored. In this study, we collected MCG data from a study sample of 300 hospitalized cardiac patients using multidimensional built-in inertial sensors of a smartphone. Our main objective was to detect concurrent AFib and acute decompensated HF (ADHF) using smartphone MCG (or sMCG). To this end, we adopted a supervised machine learning classification using multi-label and hierarchical classification. Logistic regression, random forest, and extreme gradient boosting were used as candidate classifiers. The results of the analysis showed the area under the receiver operating characteristic curve values of 0.98 and 0.85 for AFib and ADHF, respectively. The highest percentages of positive and negative predictive values for AFib were 91.9 and 100; while for ADHF, they were 56.9 and 88.4 for the multi-label classification and 69.9 and 68.8 for the hierarchical classification, respectively. We conclude that using a single sMCG measurement, AFib can be detected accurately whereas ADHF can be detected with moderate certainty.","['Heart', 'Feature extraction', 'Sensor phenomena and characterization', 'Electrocardiography', 'Hospitals', 'Hafnium']","['Acute decompensated heart failure', 'atrial fibrillation', 'gyrocardiography', 'machine learning', 'seismocardiography', 'smartphone mechanocardiography']"
"Atrial fibrillation (AFib) is the most common sustained heart arrhythmia and is characterized by irregular and excessively frequent ventricular contractions. Early diagnosis of AFib is a key step in the prevention of stroke and heart failure. In this paper, we present a comprehensive time-frequency pattern analysis approach for automated detection of AFib from smartphone-derived seismocardiography (SCG) and gyrocardiography (GCG) signals. We sought to assess the diagnostic performance of a smartphone mechanocardiogram (MCG) by considering joint SCG-GCG recordings from 435 subjects including 190 AFib and 245 sinus rhythm cases. A fully automated AFib detection algorithm consisting of various signal processing and multidisciplinary feature engineering techniques was developed and evaluated through a large set of cross-validation (CV) data including 300 (AFib = 150) cardiac patients. The trained model was further tested on an unseen set of recordings including 135 (AFib = 40) subjects considered as cross-database (CD). The experimental results showed accuracy, sensitivity, and specificity of approximately 97%, 99% and 95% for the CV study and up to 95%, 93% and 97% for the CD test, respectively. The F1 scores were 97% and 96% for the CV and CD, respectively. A positive predictive value of approximately 95% and 92% was obtained, respectively, for the validation and test sets suggesting high reproducibility and repeatability for mobile AFib detection. Moreover, the kappa coefficient of the method was 0.94 indicating a near-perfect agreement in rhythm classification between the smartphone algorithm and visual interpretation of telemetry recordings. The results support the feasibility of self-monitoring via easy-to-use and accessible MCGs.","['Heart', 'Sensors', 'Electrocardiography', 'Rhythm', 'Feature extraction', 'Vibrations', 'Machine learning']","['Atrial fibrillation', 'seismocardiography', 'gyrocardiography', 'smartphone mechanocardiography', 'machine learning']"
"The development of techniques for monitoring finger movement is becoming increasingly important in areas, such as robotics, virtual reality, and rehabilitation. To date, various techniques have been proposed for tracking hand movements, but the majority suffer from poor accuracy and repeatability. Inspired by the articulated structure of finger joints, we propose a novel 3-D printed optical sensor with a compact hinged configuration for tracking finger flexion. This sensor exploits Malus’ law using the attenuation of light transmitted through crossed polarizers. The sensor consists of a single LED, two pieces of linear polarizing film, and a photodetector that detects the changes in polarized light intensity proportional to the angle of finger flexion. This paper presents the characterization of the proposed optical sensor and compares it with a commonly used commercial bend sensor. Results show that the bend sensor exhibits hysteresis error, low sensitivity at small angles, and significant temporal drift. In contrast, the optical sensor is more accurate (±0.5°) in the measuring range from 0° to 90°, and exhibits high repeatability and stability, as well as a fast dynamic response. Overall, the optical sensor outperforms the commercial bend sensor, and shows excellent potential for monitoring hand movements in real time.","['Optical sensors', 'Sensor phenomena and characterization', 'Magnetic sensors', 'Light emitting diodes', 'Tracking', 'Immune system']","['Angle measurement', 'bend sensor', 'glove-based system', 'hinged configuration', 'hand motion tracking', 'optical sensor', '3-D printing']"
"This paper reports a distributed thermal monitoring scheme for power electronic modules (PEMs) in wind turbine converters. The sensing system is based on utilizing electrically non-conductive and electromagnetic interference immune fiber Bragg Grating (FBG) sensing technology embedded in the PEM baseplate. The design and implementation features of the proposed scheme are presented first. The scheme is then applied in a commercial PEM operating within an inverter bridge, equipped also with a conventional distributed thermal monitoring system using a multiple point thermo-couple (TC) sensor suite. A range of tests are performed to evaluate the performance of the FBG distributed thermal monitoring system and correlate it to TC measurements under steady-state and transient operating conditions representative of PEM operation in an actual wind turbine application. It is shown that the proposed FBG monitoring system can offer practical operational improvements in establishment of distributed thermal sensing schemes for wind turbine PEM.","['Fiber gratings', 'Temperature sensors', 'Temperature measurement', 'Monitoring']","['Power electronic module', 'temperature monitoring', 'distributing thermal sensing', 'fibre Bragg grating temperature sensors (FBG)']"
"Millimeter-wave radar and machine vision are both important means for intelligent vehicles to perceive the surrounding environment. Aiming at the problem of multi-sensor fusion, this paper proposes the object detection method of millimeter-wave radar and vision fusion. Radar and camera complement each other, and radar data fusion in machine vision network can effectively reduce the rate of missed detection under insufficient light conditions, and improve the accuracy of remote small object detection. The radar information is processed by mapping transformation neural network to obtain the mask map, so that radar information and visual information in the same scale. A multi-data source deep learning object detection network (MS-YOLO) based on millimeter-wave radar and vision fusion was proposed. Homemade datasets were used for training and testing. This maximized the use of sensor information and improved the detection accuracy under the premise of ensuring the detection speed. Compared with the original YOLOv5 (the fifth version of the You Only Look Once) network, the results show that the MS-YOLO network meets the accuracy requirements better. Among the models, the large model of MS-YOLO has the highest accuracy with an mAP reaching 0.888. The small model of MS-YOLO has good accuracy and speed, and the mAP reaches 0.841 while maintaining a high frame rate of 65 fps.","['Sensors', 'Object detection', 'Radar detection', 'Radar imaging', 'Millimeter wave radar', 'Sensor phenomena and characterization', 'Intelligent sensors']","['MS-YOLO', 'object detection', 'multi-sensor fusion', 'deep learning']"
"IoT sensors are becoming increasingly important supplement to traditional monitoring systems, particularly for in-situ based monitoring. Data collected using IoT sensors are often plagued with missing values occurring as a result of sensor faults, network failures, drifts and other operational issues. Missing data can have substantial impact on in-field sensor calibration methods. The goal of this research is to achieve effective calibration of sensors in the context of such missing data. To this end, two objectives are presented in this paper. 1) Identify and examine effective imputation strategy for missing data in IoT sensors. 2) Determine sensor calibration performance using calibration techniques on data set with imputed values. Specifically, this paper examines the performance of Variational Autoencoder (VAE), Neural Network with Random Weights (NNRW), Multiple Imputation by Chain Equations (MICE), Random Forest-based Imputation (missForest) and K-Nearest Neighbour (KNN) for imputation of missing values on IoT sensors. Furthermore, the performance of sensor calibration via different supervised algorithms trained on the imputed dataset were evaluated. The analysis showed VAE technique to outperform the other methods in imputing the missing values at different proportions of missingness on two real-world datasets. Experimental results also showed improved calibration performance with imputed dataset.","['Sensors', 'Calibration', 'Artificial neural networks', 'Mice', 'Task analysis', 'Sensor phenomena and characterization', 'Measurement uncertainty']","['Calibration', 'imputation', 'Internet of Things (IoT)', 'missing data', 'neural network', 'regression', 'sensors', 'variational autoencoder', 'XGBoost']"
"The resilience of indoor localization systems is a main concern of their industrial application. A combination of different techniques can enhance the overall robustness of such systems. In this work, we present fusion possibilities of coarse Bluetooth Low Energy localization based on the received signal strength indicator and the finer ultrasound time difference of arrival (TDOA) technique. This approach offers the advantage to robustify the high-accuracy ultrasonic localization in areas with non-optimal coverage. Moreover, the data fusion enables to enhance the overall localization area in a cost effective manner. This contribution proposes and evaluates (i) novel methods of how the ultrasonic system can be extended to a constrained area and (ii) a novel possibility to incorporate available Bluetooth signal strength information in the TDOA algorithm to improve accuracy.","['Location awareness', 'Acoustics', 'Bluetooth', 'Sensors', 'Costs', 'Wireless fidelity', 'Received signal strength indicator']","['Bluetooth low energy', 'indoor localization', 'received signal strength indicator', 'resilient localization', 'time-difference-of-arrival', 'ultrasonic localization']"
"In this paper, we demonstrate a simple square-wave electrical modulation scheme for imaging with laser feedback interferometry (LFI). Distinct advantages of this scheme include: 1) the straightforward creation of the modulating signal, even for high-current lasers and 2) its natural suitability for lock-in detection. We compare this simple scheme against two established imaging modalities for LFI: 1) mechanical modulation using an optical chopper and 2) the swept-frequency feedback interferometry approach. The proposed scheme lends itself to high-frequency modulation, which paves the way for high frame-rate LFI imaging with no motion artefacts using off-the-shelf equipment.","['Optical feedback', 'Imaging', 'Optical modulation', 'Optical sensors', 'Choppers (circuits)', 'Frequency modulation']","['Laser feedback', 'interferometry', 'semiconductor lasers']"
"Activity Recognition is important in assisted living applications to monitor people at home. Over the past, inertial sensors have been used to recognize different activities, spanning from physical activities to eating ones. Over the last years, supervised methods have been widely used, but they require an extensive labeled data set to train the algorithms and this may represent a limitation of concrete approaches. This paper presents a comparison of unsupervised and supervised methods in recognizing nine gestures by means of two inertial sensors placed on the index finger and on the wrist. Three supervised classification techniques, namely random forest, support vector machine, and multilayer perceptron, as well as three unsupervised classification techniques, namely k-Means, hierarchical clustering, and self-organized maps, were compared with the recognition of gestures made by 20 subjects. The obtained results show that the support vector machine classifier provided the best performances (0.94 accuracy) compared with the other supervised algorithms. However, the outcomes show that even in an unsupervised context, the system is able to recognize the gestures with an average accuracy of ~0.81. The proposed system may be therefore involved in future telecare services that could monitor the activities of daily living, allowing an unsupervised approach that does not require labelled data.","['Wearable sensors', 'Support vector machines', 'Accelerometers', 'Gesture recognition', 'Unsupervised learning']","['Gesture recognition', 'unsupervised analysis', 'wearable sensors', 'accelerometers']"
"A wireless medium access control protocol that uses a topology-dependent slot schedule is vulnerable to link failures. This becomes severer in industrial fields due to node mobility and various obstructions unfriendly to wireless communication. The proposed protocol allocates a big sharable slot (SS) to each tree level, producing a topology-independent schedule that makes the protocol highly responsive to the changes of topology. Then, the nodes at the same level use carrier sense multiple access (CSMA)/CA for data transmission and channel hopping to cope with the varying condition of a channel. The use of CSMA/CA and channel hoping makes data transmission reliable against internal and external interferences. Another feature is that every node gets its SS using a slot generation function independently of other nodes. Simulations and experiments verify that the proposed protocol achieves good performance in terms of packet delivery ratio and energy efficiency against the variation of multipath fading, various external sources of interference, and node mobility.","['Data communication', 'Reliability', 'Wireless sensor networks', 'Protocols', 'Schedules', 'Interference', 'Sensors']","['Real time', 'reliability', 'sharable time slot', 'slot allocation', 'wireless sensor networks']"
"Automotive radars have become an important part of sensing systems in vehicles and other traffic applications due to their accuracy, compact design, and robustness under severe light and weather conditions. The increased use of radars in various traffic applications has given rise to the problem of mutual interference, which needs to be mitigated. In this paper, we investigate interference mitigation in chirp sequence (CS) automotive radars via signal reconstruction based on autoregressive (AR) models in fast- and slow-time. The interference is mitigated by replacing the disturbed baseband signal samples with samples predicted using the estimated AR models. Measurements from 77GHz frequency modulated continuous wave (FMCW) static and moving radars are used to evaluate the signal reconstruction performance in terms of the signal-to-interference-plus-noise ratio (SINR), peak side-lobe level (PSLL), and mean squared error (MSE). The results show that the interference is suppressed down to the general noise floor, leading to an improvement in the SINR. Additionally, enhanced side-lobe suppression is achieved via AR signal reconstruction, which is compared to a commonly used inverse-cosine method. Furthermore, the paper notes that the slow-time signal reconstruction can be more beneficial for interference suppression in certain scenarios.","['Interference', 'Radar', 'Chirp', 'Automotive engineering', 'Signal reconstruction', 'Baseband', 'Bandwidth']","['Automotive radar', 'autoregressive (AR) modeling', 'chirp sequence (CS)', 'frequency modulated continuous wave (FMCW)', 'interference mitigation', 'signal reconstruction']"
"This work proposes the use of femtosecond laser-written fiber Bragg grating (FBG) sensors for internal temperature monitoring of tumors undergoing gold nanorod (AuNR)-mediated photothermal therapy (PTT). Arrays of sub-millimetric FBGs enabled an accurate and quasi-distributed temperature measurement within subcutaneous breast tumors in mice. Furthermore, FBGs permitted to investigate the laser-tissue interaction and AuNR-assisted photothermal enhancement on cancerous tissue exposed to 940 nm and 1064 nm radiations. The introduction of the tumor-localized AuNRs resulted in an overall increase of 13 °C of the mean temperature change, compared to control, in case of 1064 nm, while ~6 °C in case of 940 nm. This sensing solution allows the minimally invasive measurement of the internal tumor temperature under AuNR-assisted PTT. This feasibility study sets the basis for the evaluation of the thermal outcome mediated by nanoparticles under different laser sources.","['Sensors', 'Temperature sensors', 'Temperature measurement', 'Fiber gratings', 'Optical fiber sensors', 'Tumors', 'Sensor phenomena and characterization']","['Fiber Bragg grating sensors', 'temperature sensors', 'femtosecond laser inscription', 'temperature measurement', 'gold nanorods', 'photothermal therapy', 'laser']"
"Flexural ultrasonic transducers are a widely available type of ultrasonic sensor used for flow measurement, proximity, and industrial metrology applications. The flexural ultrasonic transducer is commonly operated in one of the axisymmetric modes of vibration in the low-kilohertz range, under 50 kHz, but there is an increasing demand for higher frequency operation, towards 300 kHz. At present, there are no reports of the measurement of high-frequency vibrations using flexural ultrasonic transducers. This research reports on the measurement of high-frequency vibration in flexural ultrasonic transducers, utilizing electrical impedance and phase measurement, laser Doppler vibrometry, and response spectrum analysis through the adoption of two flexural ultrasonic transducers in a transmit-receive configuration. The outcomes of this research demonstrate the ability of flexural ultrasonic transducers to measure high-frequency ultrasound in air, vital for industrial metrology.","['Ultrasonic transducers', 'Frequency measurement', 'Ultrasonic variables measurement', 'Vibrations', 'Ultrasonic imaging', 'Acoustics']","['Acoustic transducers', 'frequency measurement', 'ultrasonic sensors', 'air-coupled ultrasound']"
"Walking underwater reduces joint impacts, enhances stability and lowers the net body weight of the patient during rehabilitation. It is a recent rehabilitation method and few suitable methods exist to study underwater gait kinematics. We propose an underwater inertial measurement (IMU) system analogous to those used in land-based rehabilitation to investigate gait kinematics. The objective of this study was to test and validate the proposed system in two human trials by evaluating the knee angle during the gait. In the first trial, a three-way performance analysis was carried out between the IMU, optoelectronic and motion-capture systems in a traditional rehabilitation setting on land. In the second trial, the proposed underwater IMU is compared with camera-based motion-capture both inside and outside the water environment, using the same subjects in both phases of the trial. This allows for an evaluation of the walking gait in air and underwater as well as a cross-comparison of IMU-based knee angle estimates before and after Gaussian Process Regression. The major finding of this work is that the proposed underwater wearable IMU system provides reliable and repeatable measurements of the knee angle during the gait, both in air and underwater.","['Knee', 'Cameras', 'Sensors', 'Kinematics', 'Legged locomotion', 'Sensor phenomena and characterization', 'Protocols']","['Gait analysis', 'inertial measurement unit (IMU)', 'kinematics', 'rehabilitation', 'underwater', 'optoelectronic tracking', 'motion-capture']"
"We investigated and compared various algorithms in machine learning for anomaly assessment with different feature analyses on ultrasonic signals recorded by sensor networks. The following methods were used and compared in anomaly detection modeling: hidden Markov models (HMM), support vector machines (SVM), isolation forest (IF), and reconstruction autoencoders (AEC). They were trained exclusively on sensor signals of the intact state of structures commonly used in various industries, like aerospace and automotive. The signals obtained on artificially introduced damage states were used for performance evaluation. Anomaly assessment was evaluated and compared using various classifiers and feature analysis methods. We introduced novel methodologies for two processes. The first was the dataset preparation with anomalies. The second was the detection and damage severity assessment utilizing the intact object state exclusively. The experiments proved that robust anomaly detection is practically feasible. We were able to train accurate classifiers which had a considerable safety margin. Precise quantitative analysis of damage severity will also be possible when calibration data become available during exploitation or by using expert knowledge.","['Sensor phenomena and characterization', 'Machine learning', 'Acoustics', 'Hidden Markov models', 'Support vector machines', 'Aluminum', 'Transducers']","['Machine learning', 'non-destructive testing', 'ultrasonic transducers']"
"Vision systems capable of acquiring both two-dimensional and three-dimensional information through Light Detection And Ranging are assuming ever-increasing importance, being this market driven by the push from automotive companies to develop systems to be integrated in self-driving vehicles. Among others, candidate sensors for these systems are avalanche photodiodes, single-photon avalanche diodes, and silicon photomultipliers. Avalanche Photodiodes provide a good robustness to high background light at the cost of requiring an analog readout, instead Single-Photon Avalanche Diodes offer the possibility to implement digital readout and single-photon sensitivity, but are prone to saturation at extremely high background levels. We compare these three single- and multi-photon detector topologies, operated either in linear or digital regime, aiming at identifying the best suited detector to achieve the highest performance in Light Detection And Ranging applications at the lowest optical power active illumination and in presence of intense background (e.g. 100 klux). We present Matlab modelling and simulations and their experimental validation. Eventually, we propose a nomogram (referred to 100 m target distance) for identifying the most suited sensor topology across different operating areas and constraints, in order to achieve at least 70% success ratio.","['Photonics', 'Detectors', 'Avalanche photodiodes', 'Histograms', 'Distance measurement', 'Optical sensors']","['Single photon detectors', 'avalanche photodiode (APD)', 'single-photon avalanche diode (SPAD)', 'silicon photomultiplier (SiPM)', '3D ranging', 'light detection and ranging (LiDAR)']"
"This paper presents tactile sensor devices based on flexible aluminum nitride (AlN) piezocapacitor coupled with the metal-oxide-semiconductor field-effect transistor (MOSFET). The AlN exhibits piezoelectric behavior without the typical requirement of high voltage for poling, and this makes it an ideal candidate for sensor, where the transducer layer is integrated with MOSFET. The AlN film used here was deposited on a polyimide substrate by room temperature RF sputtering to obtain flexible piezocapacitor. The film properties, such as orientation, roughness, elemental composition, and thickness, were investigated by the X-ray diffraction (XRD), atomic force microscopy (AFM), energy-dispersive X-ray spectroscopy (EDX), and scanning electron microscope (SEM), respectively. The tactile sensor developed by connecting the flexible AlN piezocapacitor in an extended gate configuration exhibited a sensitivity of 2.64N−1for a force range 0.5-3.5 N. The developed sensor demonstrates a promising route toward the development of a complete CMOS compatible process for the development of tactile sensors.","['Aluminum nitride', 'III-V semiconductor materials', 'Sensors', 'Substrates', 'MOSFET', 'X-ray scattering']","['AlN', 'piezocapacitor', 'tactile sensing', 'flexible electronics']"
"An unscented Kalman filter (UKF) is derived for integrating vision with inertial measurements from gyros and accelerometers sensors based on three-view geometry. The main goal of the proposed method is to provide better estimations compared to the implicit extended Kalman filter introduced by Indelman . The UKF uses a selected set of points to more accurately map the probability distribution of the measurement model than the linearization of the extended Kalman filter, leading to faster convergence from inaccurate initial conditions in estimation problems. The proposed method is validated using a statistical study based on simulated navigation and synthetic images data.",[],[]
"The influence of custom microphone housings on the acoustic directionality and frequency response of a multiband bio-inspired MEMS microphone is presented. The 3.2 mm by 1.7 mm piezoelectric MEMS microphone, fabricated by a cost-effective multi-user process, has four frequency bands of operation below 10 kHz, with a desired first-order directionality for all four bands. 7×7×2.5 mm 3 3-D-printed bespoke housings with varying acoustic access to the backside of the microphone membrane are investigated through simulation and experiment with respect to their influence on the directionality and frequency response to sound stimulus. Results show a clear link between directionality and acoustic access to the back cavity of the microphone. Furthermore, there was a change in direction of the first-order directionality with reduced height in this back cavity acoustic access. The required configuration for creating an identical directionality for all four frequency bands is investigated along with the influence of reducing the symmetry of the acoustic back cavity access. This paper highlights the overall requirement of considering housing geometries and their influence on acoustic behavior for bio-inspired directional microphones.","['Microphones', 'Micromechanical devices', 'Acoustics', 'Biomembranes', 'Atmospheric modeling', 'Substrates', 'Sensors']","['3D-printing', 'acoustic response', 'bio-inspired directional microphones', 'MEMS', 'microphone housings']"
"This paper shows that the liquid water content (LWC) and the median volume diameter (MVD) can be derived from the images of water droplets using a shadowgraph imaging system with incoherent LED illumination. Icing on structures, such as a wind turbine, is the result of a combination of LWC and MVD, and other parameters, such as temperature, humidity, and wind speed. Today, LWC and MVD are not commonly measured for wind turbines. Systems for measuring these properties are often expensive or impractical in terms of location or remote reading. The aim of this paper is to gain knowledge about how to design a single instrument based on imaging that has the ability to measure these properties with enough precision and accuracy to detect icing conditions for wind turbines. A method to calculate both the LWC and the MVD from the same images is described in this paper. The size of one droplet is determined by measuring the shadow created by the droplet in background illumination. The concentration is calculated by counting the measured droplets and estimating the volumes in which these droplets can be observed. In this paper, the observation volume is shown to be dependent on the particle size and the signal-to-noise ratio for each measured particle. An expected coefficient of variation of the LWC depending on the droplet size is shown to be 2.4% for droplets 10 μm in diameter and 1.6% for 25-μm droplets. This is based on an error estimation of the laboratory measurements calibrated using a micrometer dot scale.","['Atmospheric measurements', 'Particle measurements', 'Lenses', 'Image edge detection', 'Wind turbines', 'Size measurement', 'Instruments']","['Clouds', 'Image processing', 'Machine vision', 'Meteorology', 'Optical microscopy', 'Wind power generation']"
"The received energy has been becoming an efficient and attractive measure for acoustic source localization due to its cost saving in both energy and computation capability. We investigated the acoustic source localization problem based on received energy measurements in sensor networks. Focusing on the non-logarithmic energy attenuation model, we developed and compared a suite of semidefinite programming (SDP)-based source localization methods due to computational efficiency and numerical reliability. First, we proposed a general SDP-based estimator by jointly estimating the source location and the source radiation power. It yields an efficient estimate for both the scenario where the source is located inside the convex hull formed by sensors and the scenario where the source is located outside the convex hull. Next, a min-max approximation is given to cope with the applicable application of the existing energy-based source localization algorithms relying on the Gaussian energy noise assumption. Furthermore, a novel norm approximation method is proposed according to norm equivalence, which can provide a comparable performance with lower computational complexity. Simulations show that our proposed methods exhibit a superior performance than the existing energy-based source localization estimators.","['Sensors', 'Maximum likelihood estimation', 'Acoustics', 'Attenuation', 'Position measurement', 'Optimization']","['Source localization', 'semidefinite programming', 'sensor networks', 'min-max approximation', 'norm approximation']"
"Linear and quadratic Time-Frequency (T-F) transforms are proposed for the signal processing of the Brillouin optical time-domain reflectometry, to improve the system performance, in terms of the transition responsivity and frequency resolution. Various T-F transforms, including linear T-F transform (short time Fourier transform) and quadratic transforms (Choi-Williams, Zhao-Atlas-Marks, smoothed pseudo Wigner-Ville, S-method, and adaptive spectrogram) are applied to the experimental backscattered time-domain spontaneous Brillouin signals. Multiple T-F approaches can be jointly applied to efficiently improve the time and frequency resolutions simultaneously. Results show that the SWPV transform provides the best transition responsivity and frequency resolution simultaneously among the six T-F transforms because of its smoothing operations on frequency and time axis for reducing the cross-term interference.","['Time-frequency analysis', 'Scattering', 'Transforms', 'Signal resolution', 'Temperature measurement', 'Optical fibers', 'Optical fiber sensors']","['Fiber-optics sensors', 'backscattering', 'Brillouin', 'time-frequency analysis']"
"The non-invasive intracranial pressure (NIICP) method based on a skull deformation has been proven to be a significant tool for an assessment of the intracranial pressure (ICP) and compliance. Herein, we present the development and characterization of a novel wireless sensor that uses this method as its working principle and was designed to be easy to use, to have a high resolution, and to achieve a good accessibility. Initially, a brief review of the physiology fundamentals of the ICP and the historic evolution of the NIICP method are mentioned. The sensor architecture and the rationale for the chosen components are then presented, aiming to ensure nanometer displacement measurements, the conversion of analog resolution to digital at a high speed, the fewest amount of distortion, wireless communication, and signal calibration. The NIICP signal has a typical amplitude of5 μm, and thus a resolution of at least 1% of this amplitude is required for an NIICP waveform analysis. We also demonstrate a 40-nm resolution of the sensor using a nanometric displacement test system that can also respond dynamically for NIICP signals from 50 to 180 bpm without any significant distortion (maximum deviation of P2/P1 ratio of 2.6%). The future applications for this device are broad and can enhance a clinical assessment of the intracranial dynamics.","['Blood', 'Sensors', 'Biomedical monitoring', 'Monitoring', 'Intracranial pressure sensors', 'Wireless communication', 'Cranium']","['Non-invasive intracranial pressure', 'intracranial pressure', 'intracranial compliance', 'skull deformation', 'medical device', 'wireless sensor', 'wearable sensor', 'nanometer resolution', 'displacement sensor']"
"A set of Wi-Fi RSSI (Received Signal Strength Indicator) measurements is one of basic sensory observation available for indoor localization. One major drawback of the RSSI based localization is maintenance of the RSSI fingerprint database, which should be periodically updated against measurement pattern changes caused by relocation, removal and malfunction of Wi-Fi APs (access points). To address this problem, a new change detection method is proposed in this paper. First, by machine learning techniques, the RSSI database is reconstructed to a probabilistic feature database by the implementations of PCA (Principal Component Analysis) and GP (Gaussian Process). Then, KL (Kullback-Leibler) divergence is used as a metric to measure the similarity of the existing database and a newly arrived test sets. The proposed method is evaluated by a real experiment at a multi-storey building. For experimental study, different cases that provoke changes of RSSI patterns are considered, and the positioning accuracy is examined by the k-NN (Nearest Neighbor) method. From the experimental results, it is found that the bigger the RSSI pattern changes, the large the KL divergences become. Also, when a modified change detection algorithm as the benchmark, which does not implement the PCA feature extraction, is compared, the proposed algorithm yields accurate and fast computing performances. In addition, the required number of survey points is empirically found associated with the threshold value to trigger the detection alarm.","['Databases', 'Principal component analysis', 'Wireless fidelity', 'Feature extraction', 'Probabilistic logic', 'Detection algorithms', 'Eigenvalues and eigenfunctions']","['Wi-Fi RSSI fingerprint', 'indoor localization', 'feature extraction', 'machine learning', 'Gaussian process', 'KL divergence']"
"Distributed pressure sensing arrays fabricated from fiber Bragg gratings have been demonstrated for real-time monitoring of the dynamic subsurface pressures beneath water waves in a wave tank. Two sensing arrays were used to monitor horizontal and vertical pressures in the tank as periodic wave trains passed overhead. The horizontal and vertical arrays contained 90 and 35 sensing elements, respectively, spaced at 1-cm intervals allowing highly accurate spatial resolution to be achieved in both orientations. The wave tank paddle was programmed to generate wave-trains varying from ~5 to 30-cm peak-to-trough and the pressures measured using the fiber optic array were validated using commercial piezo-electric pressure sensors and video image analysis. The length and sensor separation of the fiber optic sensing array can be varied to suit the location under test, and the fiber optic elements make the devices inherently resistant to corrosion and electromagnetic interference.","['Sensor arrays', 'Optical fibers', 'Transducers', 'Optical fiber sensors', 'Optical arrays']","['Wave tank instrumentation', 'fiber optic sensing', 'wave pressure', 'surf zone']"
"Dense and distributed tactile sensors are critical for robots to achieve human-like manipulation skills. Soft robotic sensors are a potential technological solution to obtain the required high dimensional sensory information unobtrusively. However, the design of this new class of sensors is still based on human intuition or derived from traditional flex sensors. This work is a first step towards automated design of soft sensor morphologies based on optimization of information theory metrics and machine learning. Elementary simulation models are used to develop the optimized sensor morphologies that are more accurate and robust with the same number of sensors. Same configurations are replicated experimentally to validate the feasibility of such an approach for practical applications. Furthermore, we present a novel technique for drift compensation in soft strain sensors that allows us to obtain accurate contact localization. This work is an effort towards transferring the paradigm of morphological computation from soft actuator designing to soft sensor designing for high performance, resilient tactile sensory networks.","['Morphology', 'Strain', 'Capacitive sensors', 'Robot sensing systems', 'Entropy', 'Robustness']","['Soft robotics', 'tactile sensors', 'information entropy', 'optimization methods', 'machine learning']"
"Integration of organic light emitting diodes (OLEDs) and organic photodetectors (OPDs) on flexible plastic substrates promises compact and low-cost optical detection units for multiplex sensors. These units may be laminated to a microfluidic system for sensing applications in a liquid. Here, a 6 × 6 element matrix of alternating blue OLEDs and OPDs is demonstrated on a single flexible plastic substrate. The devices are fabricated by masked thermal evaporation on a 200 μm thick polyethylene terephthalate (PET) foil. The individual device size is 1 mm × 1 mm. Both OLEDs and OPDs are demonstrated to work. The spectral characteristics are shown to be suitable for fluorescence measurements. Signals from fluorescence-labeled spots above the OPDs under OLED excitation are investigated. Successful operation of the OLED-OPD matrix for reflection measurement is demonstrated.","['Organic light emitting diodes', 'Fluorescence', 'Substrates', 'Microfluidics', 'Sensor phenomena and characterization', 'Indium tin oxide']","['OLED', 'OPD', 'flexible', 'sensor', 'fluorescence']"
"Recently, surface electromyography (sEMG) emerged as a novel biometric authentication method. Since EMG system parameters, such as the feature extraction methods and the number of channels, have been known to affect system performances, it is important to investigate these effects on the performance of the sEMG-based biometric system to determine optimal system parameters. In this study, three robust feature extraction methods, Time-domain (TD) feature, Frequency Division Technique (FDT), and Autoregressive (AR) feature, and their combinations were investigated while the number of channels varying from one to eight. For these system parameters, the performance of sixteen static wrist and hand gestures was systematically investigated in two authentication modes: verification and identification. The results from 24 participants showed that the TD features significantly (p<0.05) and consistently outperformed FDT and AR features for all channel numbers. The results also showed that the performance of a four-channel setup was not significantly different from those with higher number of channels. The average equal error rate (EER) for a four-channel sEMG verification system was 4% for TD features, 5.3% for FDT features, and 10% for AR features. For an identification system, the average Rank-1 error (R1E) for a four-channel configuration was 3% for TD features, 12.4% for FDT features, and 36.3% for AR features. The electrode position on the flexor carpi ulnaris (FCU) muscle had a critical contribution to the authentication performance. Thus, the combination of the TD feature set and a four-channel sEMG system with one of the electrodes positioned on the FCU are recommended for optimal authentication performance.","['Biometrics (access control)', 'Authentication', 'Feature extraction', 'Electrodes', 'Thumb', 'Gesture recognition', 'Time-domain analysis']","['Biometrics', 'gesture recognition', 'surface electromyogram (sEMG)', 'feature extraction', 'electrode configuration', 'user verification', 'user identification']"
We present in vitro sensing of glucose using a newly developed efficient optical fiber glucose sensor based on a compound parabolic concentrator (CPC)-tipped polymer optical fiber (POF). A batch of nine CPC-tipped POF sensors with a 35-mm fiber length is shown to have an enhanced fluorescence pickup efficiency with an average increment factor of 1.7 as compared with standard POF sensors with a plane cut fiber tip. in vitro measurements for two glucose concentrations (40 and 400 mg/dL) confirm that the CPC-tipped sensors can efficiently detect both glucose concentrations.,"['Optical fiber sensors', 'Sugar', 'Optical fibers', 'Fluorescence', 'Sensor phenomena and characterization', 'Chemical sensors']","['Compound parabolic concentrator', 'fiber optics sensors', 'fluorescence', 'polymers']"
"Solid-state potassium ion selective electrode (K + ISE) has been the most studied chemical sensors due to its practical importance in biomedical applications. One of the major obstacles that prevented widespread use of solid-state K + ISE has been output potential drift problem. In this paper, we developed an electrochemical sensing unit in which working, counter, and reference electrodes are integrated in a single plane as all-solid-state form. In order to mitigate the output potential drift, a polyaniline intermediate layer and salt-saturated polyvinylebutyral top coating are introduced in the working and reference electrodes, respectively. Using cyclic voltammetry (CV), uniform layers of polyaniline are deposited on carbon electrode, as confirmed by scanning electron microscope observation. Potentiometry and electrochemical impedance spectroscopy measurement on our K + ISE show high sensitivity (60.5 mV/decade), low concentration for the limit of detection (10 -5.8 M), and large range of linear detection (10 -5 -1 M), and superior selectivity of K + ISE against NH4 + , Na + , Mg 2+ , Ca 2+ , and Fe 3+ . With its high potential to be miniaturized, we foresee that our solid-state K + ISE will motivate the future applications in microdevices for clinical analysis, agricultural, and environmental applications.","['Electrodes', 'Ions', 'Electric potential', 'Sensors', 'Potassium', 'Biomembranes', 'Polymers']","['Solid-state ion selective electrode', 'potassium sensor', 'potentiometry', 'signal stability']"
"RGB color sensors are examined as an economical replacement for non-lighting based occupancy sensors such as PIR sensors and ultrasonic sensors. RGB color sensors are inexpensive and able to accurately detect and track occupants in an illuminated space by monitoring color shifts from background color distribution caused by occupants. By examining the temporal dependence of color shifts caused by small occupant movements, it is possible to avoid false occupancy/vacancy detections associated with other sensor types. Additionally, RGB color sensors provide information about the spectral reflectance of the illuminated space and this information can be used to compensate for spectral absorption properties of surfaces that alter the perceived emitted spectral power distribution (SPD) from the lighting. These changes may impact occupant perception and circadian performance.","['Color', 'Sensor systems', 'Sensor phenomena and characterization', 'Acoustics', 'Lighting', 'Monitoring']","['Color sensors', 'occupant centric controls', 'occupancy detection', 'smart sensor systems']"
"The terahertz (THz) technology has found applications ranging from astronomical science and earth observation to compact radars, non-destructive testing, chemical analysis, explosive detection, moisture content determination, coating thickness control, film uniformity determination, structural integrity testing wireless covert communications, medical applications (including skin cancer detection), imaging, and concealed weapons detection. Beyond 5G Wi-Fi and Internet of Things (IoT) are the expected killer applications of the THz technology. Plasmonic TeraFETs such as Si CMOS with feature sizes down to 3 nm could enable a dramatic expansion of all these applications. At the FET channel sizes below 100 nm, the physics of the electron transport changes from the collision dominated to the ballistic or quasi-ballistic transport. In the ballistic regime, the electron inertia and the waves of the electron density (plasma waves) determine the high frequency response that extends into the THz range of frequencies. The rectification and instabilities of the plasma waves support a new generation of THz and sub-THz plasmonic devices. The plasmonic electronics technology has a potential become a dominant THz electronics sensing technology when the plasmonic THz sources join the compact, efficient, and fast plasmonic TeraFET THz detectors already demonstrated and being commercialized.","['Plasmons', 'Field effect transistors', 'Sensor phenomena and characterization', 'Testing', 'Silicon', 'Physics']","['Field effect transistors', 'TeraFETs', 'plasmonics', 'terahertz technology', 'ballistic transport', 'ballistic mobility']"
"A growing number of people all over the world are running. Gathering in-field data with wearable sensors is attractive for runners, clinicians and coaches to improve running performance, avoid injury or return to running after an injury. However, it is yet to be proven that commercially available wearable sensors provide valid data. The objective of this study was to assess the validity of five wearable sensors (Moov Now™, MilestonePod, RunScribe™, TgForce and Zoi) to measure ground reaction force related metrics, step rate, foot strike pattern, and vertical displacement of the center of mass during running. Concurrent/criterion validity was assessed against a laboratory-based system using Pearson's correlation coefficients and ANOVAs. Step rate measurement provided by all wearable sensors was valid (all r > 0.96 and p <; 0.001). Only Zoi provided valid vertical displacement of the center of mass (r = 0.81, p <; 0.001); only TgForce provided meaningful estimates of instantaneous vertical loading rate (r = 0.76, p <; 0.001); only MilestonePod could discriminate between a rear-, mid- and fore-foot strike pattern during running (p <; 0.001). None of the wearable sensors was valid for estimating peak braking force. In conclusion, only a few metrics provided by these commercially available wearable sensors are valid. Potential buyers should therefore be aware of such limitations when monitoring running gait variables.","['Wearable sensors', 'Force', 'Foot', 'Injuries', 'Loading', 'Measurement']","['Accelerometer', 'Bland-Altman', 'inertial measurement unit', 'impact force', 'kinematic']"
"A micro-cantilever fabricated using a combination of picosecond-laser machining and focused ion beam milling directly onto the end of a standard telecommunications optical fiber is demonstrated as a liquid pH sensor. Conventional pH meters typically require relatively large reaction volumes up to ~50 mL, which is not always convenient. The micro-scale nature of this sensor offers the potential for pH measurement on a smaller sample volume down to micro-liter. The fiber end-tip cantilever is coated with a pH sensitive layer, and the pH-induced deflection is monitored interfermetically. A detectable pH range from 4.0 to 10.0 is demonstrated for the cantilevers coated with 16-mercapto-hexadecanoic-1-acid as the functional layer, and a detectable pH range from 4.0 to 9.0 is demonstrated for the cantilevers coated with Al2O3 as the functional layer.","['Optical fiber sensors', 'Optical fibers', 'Cavity resonators', 'Temperature measurement', 'Temperature sensors']","['focused ion beam', 'optical fiber cantilever', 'pH sensing', 'ps-laser machining']"
"The highly infectious and serious nature of coronavirus disease 2019 (COVID-19) has highlighted the need for hospital space disinfection technology and the prevention of human exposure to pathogenic environments. This research developed novel chlorine dioxide (ClO 2 ) sterilization technology to reduce bacteria and viruses in the air and on surfaces. A smart sterilization robot system was also developed to spray disinfectants in operating theaters or patients’ rooms, designed according to the results of controlled experiments and the requirements for hospital disinfection. The system was built incorporated a semi-automatic remote-controlled module and an automatic intelligent disinfection function; that is, it could operate independently according to specific epidemic prevention strategies, which were implemented using a combination of Internet of Things (IoT) applications and a gesture recognition function. The elimination of Escherichia coli ( E. coli ) bacteria on sample plates was 99.8 % effective. This paper reviews the evolution of various disinfection technologies and describes a disinfection robot system in detail.","['Robots', 'Robot sensing systems', 'Microorganisms', 'Sensors', 'COVID-19', 'Chemicals', 'Hospitals']","['Mobile robots', 'medical robotics', 'intelligent systems', 'robot sensing systems']"
"Rapid advancements in information processing and embedded systems require high selective and fast sensors. Conventional gas sensors are not suitable for the detection of isomers of organic compounds due to cross-sensitivity and the response time being limited by slow chemical kinetics. Amperometric gas sensors using conducting polymers modified with metal catalysts are a suitable and robust system due to many tunable properties. In this paper, conducting polymer polyaniline was electrochemically decorated with clusters containing precisely defined number of gold atoms to function as an electro-catalyst. The modified polymer composite showed fast reaction rate for the electro-oxidation of alcohols in both liquid and gas phases. The number of gold atoms affected the catalytic activity. Cyclic voltammograms were measured and results showed discriminable patterns between n-propanol and iso-propanol even at different gas concentrations. Thus, it was demonstrated that gas sensor arrays can be realized by decorating different number of gold atoms on polyaniline electrodes, to yield defined and different selectivity.","['Gold', 'Gas detectors', 'Atomic layer deposition', 'Electrodes', 'Oxidation', 'Chemical sensors']","['Amperometric gas sensor', 'atomic gold', 'conducting polymer', 'electro-catalyst', 'polyaniline', 'propanol']"
"In this study, we present a quantitative evaluation of the accuracy of simultaneous array-radar-based measurements of the displacements caused at two parts of the human body by arterial pulse wave propagation. To establish the feasibility of accurate radar-based noncontact measurement of this pulse wave propagation, we perform experiments with four participants using a 79-GHz millimeter-wave ultra-wideband multiple-input multiple-output array radar system and a pair of laser displacement sensors. We evaluate the accuracy of the pulse wave propagation measurements by comparing the displacement waveforms that are measured using the radar system with the corresponding waveforms that are measured using the laser sensors. In addition, to evaluate the estimates of the pulse wave propagation channels, we compare the impulse response functions that are calculated from the displacement waveforms obtained from both the radar data and the laser data. The displacement waveforms and the impulse responses both demonstrated the good agreement between the results of the radar and laser measurements. The normalized correlation coefficient between the impulse responses obtained from the radar and laser data on average was as high as 0.97 for the four participants. The results presented here strongly support the feasibility of accurate radar-based noncontact measurement of arterial pulse wave propagation.","['Laser radar', 'Atmospheric measurements', 'Pulse measurements', 'Radar measurements', 'Propagation', 'Measurement by laser beam', 'Radar']","['Array radar', 'arterial pulse wave', 'impulse response', 'laser displacement sensor']"
"This paper reports on the first low power (10.4 mW) and ultrasensitive linear Hall-effect integrated circuits (LHEICs) using GaAs-InGaAs-AlGaAs 2D electron gas technology. These LHEICs have a state-of-the-art sensitivity of 533 μV/μT and are capable of detecting magnetic fields as low as 177 nT (in a 10-Hz bandwidth), at frequencies from 500 Hz to 200 kHz. This provides at least an order of magnitude improvement in sensitivity and a factor of four improvements in detectability of small fields, compared with commercial Si linear Hall ICs.","['Sensors', 'Sensitivity', 'PHEMTs', 'Integrated circuit modeling', 'Hall effect', 'Logic gates']","['Linear Hall Effect Integrated Circuit', 'LHEIC', '2DEG', 'GaAs-InGaAs-AlGaAs', 'pHEMT']"
"We report an evaluation of an epitaxially grown uncooled InAs photodiode for the use in radiation thermometry. Radiation thermometry measurements was taken using the photodiode covered blackbody temperatures of 50 °C-300 °C. By determining the photocurrent and signal-to-noise ratio, the temperature error of the measurements was deduced. It was found that an uncooled InAs photodiode, with the peak and cutoff wavelengths of 3.35 and 3.55 μm, respectively, measured a temperature of 50 °C, with an error of 0.17 °C. Many plastics have C -H molecular bond absorptions at 3.43 μm and hence radiate thermally at this wavelength. Our results suggest that InAs photodiodes are well suited to measure the temperature of plastics above 50 °C. When tested with a narrow bandpass filter at 3.43 μm and blackbody temperatures from 50 °C-300 °C, the InAs photodiode was also found to produce a higher output photocurrent, compared with a commercial PbSe detectors.","['Temperature measurement', 'Photodiodes', 'Detectors', 'Wavelength measurement', 'Photoconductivity', 'Optical filters', 'Photonics']","['radiation thermometry', 'temperature measurement', 'InAs photodiodes']"
"We propose a low-cost and low-power consumption device for seismic monitoring consisting of three single-axis accelerometers connected to a data logger with acquisition, synchronization, and transmission functionalities. The device was designed to be densely and prolifically deployed in high seismic risk areas, thus strengthening the Italian seismic network and providing a more accurate estimation of shaking maps. Moreover, the availability of such low-cost and high-performance units can allow the widespread diffusion of smart systems for seismic and structural monitoring, finalized to collapses prevention in critical structures, such as schools and hospitals, as well as constitute the founding nucleus of early warning systems based on Internet of Things architectures. The realized station was submitted to a testing phase, placing it contiguously to a high-performance seismic station located in central Italy and responsible for national seismic monitoring. The test station, installed from September 2016 to March 2017, was able to record the significant and numerous earthquakes that devastated central Italy during this period. The simultaneous acquisition of these seismic events by the sensors of the national seismic network, including that co-sited with the device under test, has furnished sufficient data for the device validation and performance quantification. A comparative analysis was performed through waveforms correlations study, strong motion parameters estimation, and spectral analysis. The proposed device demonstrated performances very close to those of more sophisticated and expensive systems. Therefore, it can effectively replace them or be added in engineering and civil protection applications and, finally, be used in earthquake early warning systems.","['Sensors', 'Earthquakes', 'Monitoring', 'Accelerometers', 'Surface waves', 'Performance evaluation', 'Micromechanical devices']","['Early warning systems', 'earthquake', 'Internet of Things', 'MEMS', 'seismic sensors', 'structural monitoring']"
"In this paper, we present a novel end-to-end learning-based LiDAR sensor relocalization framework, termed PointLoc, which infers 6-DoF poses directly using only a single point cloud as input. Compared to visual sensor-based relocalization, LiDAR sensors can provide rich and robust geometric information about a scene. However, point clouds of LiDAR sensors are unordered and unstructured making it difficult to apply traditional deep learning regression models for this task. We address this issue by proposing a novel PointNet-style architecture with self-attention to efficiently estimate 6-DoF poses from 360° LiDAR sensor frames. Extensive experiments on recently released challenging Oxford Radar RobotCar dataset and real-world robot experiments demonstrate that the proposed method can achieve accurate relocalization performance.","['Laser radar', 'Sensors', 'Location awareness', 'Visualization', 'Feature extraction', 'Robot sensing systems', 'Sensor phenomena and characterization']","['LiDAR sensor relocalization', 'LiDAR point cloud', 'sensor applications']"
"Contactless or non-invasive technology for the monitoring of anomalies in an inconspicuous and distant environment has immense significance in health-related applications, in particular COVID-19 symptoms detection, diagnosis, and monitoring. Contactless methods are crucial specifically during the COVID-19 epidemic as they require the least amount of involvement from infected individuals as well as healthcare personnel. According to recent medical research studies regarding coronavirus, individuals infected with novel COVID-19-Delta variant undergo elevated respiratory rates due to extensive infection in the lungs. This appalling situation demands constant real-time monitoring of respiratory patterns, which can help in avoiding any pernicious circumstances. In this paper, an Ultra-Wideband RADAR sensor enquote XeThru X4M200 is exploited to capture vital respiratory patterns. In the low and high frequency band, X4M200 operates within the 6.0-8.5 GHz and 7.25-10.20 GHz band, respectively. The experimentation is conducted on six distinct individuals to replicate a realistic scenario of irregular respiratory rates. The data is obtained in the form of spectrograms by carrying out normal (eupnea) and abnormal (tachypnea) respiratory. The collected spectrogram data is trained, validated, and tested using a cutting-edge deep learning technique called Residual Neural Network or ResNet. The trained ResNet model’s performance is assessed using the confusion matrix, precision, recall, F1-score, and classification accuracy. The unordinary skip connection process of the deep ResNet algorithm significantly reduces the underfitting and overfitting problem, resulting in a classification accuracy rate of up to 90%.","['Sensors', 'COVID-19', 'Ultra wideband radar', 'Monitoring', 'Lung', 'Spectrogram', 'Support vector machines']","['COVID-19', 'UWB RADAR sensor', 'contactless healthcare', 'respiratory monitoring', 'deep learning', 'ResNet']"
"There is a growing interest in measuring human activities via worn inertial sensors, and situating two accelerometers on a body segment allows accessing rotational kinematic information, at a significantly lower energy cost when compared with gyroscopes. However, the placement of sensors has not been widely considered in the literature. In practice, dual-accelerometer systems should be built as compact as possible to ensure long-term wearability. In this paper, the impact of sensor placement and nature of human activity on signal quality is quantified by individual and differential signal-to-noise ratios (SNRs). To do so, noise-free signals are described by a 2-D kinematic model of a body segment as a function of kinematic variables and sensor location on the segment. Measurements are modelled as kinematic signals disturbed by zero mean additive Gaussian noise. Depending on the accuracy needed, one can choose a minimal SNR to achieve, with such dual-accelerometer arrangement. We estimate SNR and minimal sensor separations for three data sets, two from the public domain and one collected for this paper. The data sets give arm motion profiles for reaching, inertial data collected during locomotion on a treadmill and during activities of daily life. With a dual-accelerometer arrangement, we show that it is possible to achieve a good differential SNR for the analysis of various human activities if the separation between the two sensors and their placement is well chosen.","['Accelerometers', 'Signal to noise ratio', 'Sensors', 'Acceleration', 'Kinematics', 'Angular velocity', 'Noise measurement']","['Accelerometers', 'angular velocity', 'wearable sensors', 'signal to noise ratio', 'biomedical monitoring']"
"We present the design and application of a 64 × 64 pixel SPAD array to portable colorimetric sensing, and fluorescence and x-ray imaging. The device was fabricated on an unmodified 180 nm CMOS process and is based on a square p+/n active junction SPAD geometry suitable for detecting green fluorescence emission. The stand-alone SPAD shows a photodetection probability greater than 60% at 5 V excess bias, with a dark count rate of less than 4 cps/μm2 and sub-ns timing jitter performance. It has a global shutter with an in-pixel 8-bit counter; four 5-bit decoders and two 64-to-1 multiplexer blocks allow the data to be read-out. The array of sensors was able to detect fluorescence from a fluorescein isothiocyanate (FITC) solution down to a concentration of 900 pM with an SNR of 9.8 dB. A colorimetric assay was performed on top of the sensor array with a limit of quantification of 3.1 μm. X-rays images, using energies ranging from 10 kVp to 100 kVp, of a lead grating mask were acquired without using a scintillation crystal.","['Electric fields', 'Junctions', 'Fluorescence', 'Sensor arrays', 'Photonics']","['Single photon avalanche diode', 'SPAD', 'CMOS', 'low-light (vision)', 'photo detector', 'image sensor', 'x-rays']"
"Various jamming techniques have been developed to prevent interferometric synthetic aperture radar from effective detection and observation. In this paper, a thorough analysis of the jamming effects on correlation and interferometric phase is provided. To derive the jamming result, a general signal model for the interference is first presented and the corresponding imaging results are produced through the range-Doppler algorithm. Then, the impacts of the interference on correlation are analyzed. The non-center located jammer decreases the correlation seriously due to the low correlation of the interference. However, the center located jammer clearly increases the correlation when the input jamming-to-signal ratio is large enough. Finally, the jammed interferometric phases for different jammer positions are discussed. It shows that the non-center located jammer results in large phase errors, while for the center located jammer, the interferometric phase approaches a constant. The effects of interference are demonstrated by simulated data based on the TerraSAR system.","['Jamming', 'Interference', 'Correlation', 'Antennas', 'Imaging', 'Synthetic aperture radar', 'Azimuth']","['SAR Interfereometry', 'Interference', 'Correlation', 'jamming to signal ratio']"
"The present investigation deals with the fabrication of CO 2 gas sensor based on La 2 O 3 /SnO 2 metal-oxide material. In this paper, the sensitive material was prepared by La 2 O 3 /SnO 2 nanopowder and the addition of 1 wt.% and 3 wt.% platinum (Pt) using high-speed ball milling method. The sensitive film prepared by sensitive powder was printed on alumina (Al 2 O 3 ) substrate by screen printing method. This film was characterized by X-Ray powder diffraction spectroscopy, and Field-emission scanning electron microscopy. As a result, the prepared 3 wt.% Pt/La 2 O 3 /SnO 2 thick film sensitive paste exhibits a high sensitivity to increasing the CO 2 gas concentration at 225 °C in air atmosphere.","['Powders', 'Gas detectors', 'Temperature sensors', 'Sensitivity', 'Calcination', 'Metals']","['La2O3/SnO2 thick film', 'screen printing', 'CO2 sensor', 'gas sensor', 'Pt']"
"Silicon-based digital electronics have evolved over decades through an aggressive scaling process following Moore's law with increasingly complex device structures. Simultaneously, large-area electronics have continued to rely on the same field-effect transistor structure with minimal evolution. This limitation has resulted in less than ideal circuit designs, with increased complexity to account for shortcomings in material properties and process control. At present, this situation is holding back the development of novel systems required for printed and flexible electronic applications beyond the Internet of Things. In this work we demonstrate the opportunity offered by the source-gated transistor's unique properties for low-cost, highly functional large-area applications in two extremely compact circuit blocks. Polysilicon common-source amplifiers show 49 dB gain, the highest reported for a two-transistor unipolar circuit. Current mirrors fabricated in polysilicon and InGaZnO have, in addition to excellent current copying performance, the ability to control the temperature dependence (degrees of positive, neutral or negative) of output current solely by choice of relative transistor geometry, giving further flexibility to the design engineer. Application examples are proposed, including local amplification of sensor output for improved signal integrity, as well as temperature-regulated delay stages and timing circuits for homeostatic operation in future wearables. Numerous applications will benefit from these highly competitive compact circuit designs with robust performance, improved energy efficiency and tolerance to geometrical variations: sensor front-ends, temperature sensors, pixel drivers, bias analog blocks and high-gain amplifiers.","['Thin film transistors', 'Temperature sensors', 'Schottky barriers', 'Analog circuits']","['Analog electronics', 'contact barriers', 'Schottky barrier', 'source-gated transistors', 'thin-film transistors']"
"Raman spectroscopy has proved to have potential in deep surface analytical applications. We present here, to the best of our knowledge, the first time depth analysis of semi-transparent media by a depth-resolving Raman spectrometer based on an adjustable time-correlated CMOS SPAD (single-photon avalanche diode) line sensor that can measure the depth of target samples embedded in a centimeter-scale semi-transparent medium simultaneously with a normal Raman depth profiling operation and suppress the fluorescence background by means of adjustable picosecond time gating. The variability of the depth derivation was measured to be ± 0.43 cm at depths ranging from 2 to 9 cm. In addition, the advantages of the adjustable picosecond time gating in terms of depth derivation and fluorescence background suppression performance were shown by comparing gate widths ranging from 100 ps to 13 ns. We believe that the technology concerned could pave the way for a new kind of compact, practical depth-resolving Raman spectrometer for deep subsurface analytical applications.","['Logic gates', 'Photonics', 'Raman scattering', 'Fluorescence', 'Detectors']","['Depth-resolving Raman spectrometer', 'depth analysis', 'time-correlated single photon counting (TCSPC)', 'CMOS single-photon avalanche diode (SPAD)']"
"This paper presents a complete optimization of a piezoelectric vibration energy harvesting system, including a piezoelectric transducer, a power conditioning circuit with full semiconductor device models, a battery and passive components. To the authors awareness, this is the first time and all of these elements have been integrated into one optimization. The optimization is done within a framework, which models the combined mechanical and electrical elements of a complete piezoelectric vibration energy harvesting system. To realize the optimization, an optimal electrical damping is achieved using a single-supply pre-biasing circuit with a buck converter to charge the battery. The model is implemented in MATLAB and verified in SPICE. The results of the full system model are used to find the mechanical and electrical system parameters required to maximize the power output. The model, therefore, yields the upper bound of the output power and the system effectiveness of complete piezoelectric energy harvesting systems and, hence, provides both a benchmark for assessing the effectiveness of existing harvesters and a framework to design the optimized harvesters. It is also shown that the increased acceleration does not always result in increased power generation as a larger damping force is required, forcing a geometry change of the harvester to avoid exceeding the piezoelectric breakdown voltage. Similarly, increasing available volume may not result in the increased power generation because of the difficulty of resonating the beam at certain frequencies whilst utilizing the entire volume. A maximum system effectiveness of 48% is shown to be achievable at 100 Hz for a 3.38-cm 3 generator.","['Energy harvesting', 'Integrated circuit modeling', 'Vibrations', 'Optimization', 'Transducers', 'MOSFET', 'Sensors']","['Energy harvesting', 'piezoelectric transducers', 'optimization', 'vibration-to-electric energy conversion']"
"Value of Information (VoI) is a concept to assess the usefulness of information for a specific goal, and has in the last decade experienced a growing interest also for Wireless Sensor Network (WSN) applications and the Internet of Things (IoT). By making the value of information explicit in the form of VoI, WSN and IoT applications should be able to better assess which information to spend their constrained resources on. However, the definition of VoI is highly application-dependent, which has led to a fragmented understanding of VoI, and there is a lack of a comprehensive overview. In this structured review, we first categorize application use cases and examine what VoI is used for, and explore the different approaches to defining VoI. We then provide a well-structured and comprehensive discussion of the specific approaches used in the literature to determine VoI, together with examples of use cases. We categorize the different approaches to calculating VoI, describe their properties systematically and distinguish between observed VoI and expected VoI. We also discuss adaptive VoI approaches and point towards future directions within the field.","['Wireless sensor networks', 'Sensors', 'Monitoring', 'Internet of Things', 'Sensor phenomena and characterization', 'Measurement', 'Systematics']","['Data collection', 'data reduction', 'Internet of Things', 'machine learning', 'scheduling algorithms', 'sensor data', 'sensor networks', 'wireless sensor networks', 'value of information']"
"This work presents a soft, flexible, and low-cost capacitive pressure-sensitive insole developed using resource-efficient single-step 3D printing method. Developed using elastomeric materials, the soft and robust sensory insole can bend and twist in extreme angles. The insole is designed to have four sensing zones to capture the pressure information from the entire contact area. The sensors tested under different condition of applied pressure showed reliable response up to 300kPa without saturation. The sensors exhibit a sensitivity of 2.4MPa−1for range of 0 − 60kPa and 0.526MPa−1 for 60kPa and above with average sensitivity of 1.314 MPa−1 in the entire range. The insole was also tested under varying bending and temperature conditions. Considering the excellent response over a wide pressure range, the presented insole could be used for gait analysis or with anthropomorphic robots for critical information about the terrain morphology. To show the functionality of presented insole, we have also developed an app to display the sensory information obtained via custom-made electronics circuit.","['Sensors', 'Robot sensing systems', 'Three-dimensional displays', 'Capacitive sensors', 'Three-dimensional printing', 'Printers', 'Fabrication']","['3D printing', 'additive manufacturing', 'pressure sensors', 'tactile sensors', 'robotics', 'wearable electronics']"
"In this work, we focus on passive polarimetric ISAR for ship target imaging using DVB-S signals of opportunity. A first goal of the research is to investigate if, within the challenging passive environment, different scattering mechanisms, belonging to distinct parts of the imaged target, can be separated in the polarimetric domain. Furthermore, a second goal is at verifying if polarimetric diversity could enable the formation of ISAR products with enhanced quality with respect to the single channel case, particularly in terms of better reconstruction of the target shape. To this purpose, a dedicated trial has been conducted along the river Rhine in Germany by means of an experimental DVB-S based system developed at Fraunhofer FHR and considering a ferry as cooperative target. To avoid inaccuracies due to data-driven motion compensation procedures and to fairly interpret the polarimetric results, we processed the data by means of a known-motion back-projection algorithm obtaining ISAR images at each polarimetric channel. Then, different approaches in the polarimetric domain have been introduced. The first one is based on the well-known Pauli Decomposition. The others can be divided in two main groups: (i) techniques aimed at separating the different backscattering mechanisms, and (ii) image domain techniques to fuse the polarimetric information in a single ISAR image with enhanced quality. The different considered techniques have been applied to several data sets with distinct bistatic geometries. The obtained results clearly demonstrate the potentialities of polarimetric diversity that could be fruitfully exploited for classification purposes.","['Passive radar', 'Surveillance', 'Digital video broadcasting', 'Scattering', 'Satellite broadcasting', 'Sensors', 'Transponders']","['Passive ISAR', 'polarimetric data', 'satellite signals of opportunity', 'DVB-S-based passive radar']"
"The flexural ultrasonic transducer is a unimorph device which typically comprises a piezoelectric ceramic bonded to a metallic membrane. It is widely applied in industrial applications for metrology and proximity sensing. However, the electromechanical and dynamic characteristics of this class of transducer have only recently been reported, and the influence of different excitation levels on dynamic nonlinearity remains unclear. Dynamic nonlinearity in high-power piezoelectric ultrasonic transducers is familiar, where the performance or dynamic stability of the transducer can significantly reduce under high amplitudes of excitation. Nonlinearity can manifest as measurable phenomena such as resonance frequency drift, influenced by thermomechanical phenomena or structural constraints. There is relatively little reported science of the dynamic nonlinearity in the vibration response of flexural ultrasonic transducers. This study examines the vibration responses of four flexural ultrasonic transducers, showing the existence of dynamic nonlinearity for increases in excitation voltage. An analytical solution of the governing equations of motion for the flexural ultrasonic transducer is presented which complements the experimental investigation, and suggests a close relationship between material properties and nonlinearity. This research demonstrates a detailed dynamic characterization of the flexural ultrasonic transducer, showing the potential for the optimization of dynamic performance in industrial measurement applications.","['Ultrasonic transducers', 'Vibrations', 'Transducers', 'Resonant frequency', 'Ultrasonic variables measurement', 'Dynamics', 'Acoustics']","['Flexural ultrasonic transducer', 'dynamic nonlinearity', 'air-coupled ultrasound', 'analytical representation']"
"Various types of odor sensor systems have been developed for the discrimination and quantification of odorants. Although the use of biological components may improve sensitivity, a few studies have reported on the use of an array of biological elements for odor discrimination. In this paper, odorants were discriminated using an odor biosensor system with cells expressing different types of olfactory receptors. A cell's response to an odorant was acquired as a fluorescent image followed by an image processing technique based on circle Hough transform. The principal component analysis and linear discriminant analysis demonstrated the ability to discriminate between two odorants at different concentrations. The result suggests that the combination of a biosensor and image processing technique has the potential to discriminate between different odorants even if the cells are placed at random.","['Olfactory', 'Fluorescence', 'Sensor systems', 'Image sensors', 'Biosensors', 'Receptor (biochemistry)']","['Circle Hough transform', 'image processing', 'odor biosensor system', 'odor discrimination']"
"In this paper, we develop a method of detecting 1,3,5-trinitroperhydro-1,3,5-triazine [research department explosive (RDX)]. RDX is one of the main components of plastic explosives. Initially, we prepare an anti-RDX monoclonal antibody that specifically binds to RDX. The antibody is prepared from hybridoma cells that are prepared by the iliac lymph node method. The dissociation constant of the antibody against RDX is estimated to be 9.6 nM from the results of indirect competitive enzyme-linked immunosorbent assay. We fabricate a sensor chip on which RDX analogues are immobilized through a self-assembled monolayer with oligo(ethylene glycol) chains. High reliability and low false recognition rate are particularly required for sensors used for detecting explosives. Therefore, the nonspecific adsorption of the fabricated sensor chip is evaluated to confirm whether the sensor chip inhibits this nonspecific adsorption. The response characteristics of the prepared antibody against RDX are examined by indirect competitive assay. Under an incubation time of 0 min in the indirect competitive assay, a detection limit of 40 ppt is realized at a sample injection duration of 6 min.","['Immune system', 'Explosives', 'Rats', 'Strain', 'Sensor phenomena and characterization', 'Lymph nodes']","['Immunosensor', 'monoclonal antibody', 'RDX', 'surface plasmon resonance', 'self-assembled monolayer']"
"This paper discusses the use of force sensing resistor (FSR) technology integrated into sampling wands used for homeland security applications. FSR-integrated wands can be used for the optimization of wipe sampling of surfaces to facilitate enhanced trace contraband collection. Collection efficiencies during wipe sampling are known to be dependent on the applied force, or pressure, used during sampling. Light-emitting diodes designed to switch from red to green at a defined force threshold of 7 N provide feedback to the operator during sampling. The goal of maintaining forces at or above the threshold was successfully demonstrated by testing with a volunteer population of 22. An additional benefit is the reduction in the variability of the force applied by each operator during sampling. Another focus area is the development of prototype sampling wands that fit in the palm of the hand and other wand modifications that increase the reliability of wipe sampling by registering the placement of the collected sample properly on the wipe. This paper also outlines how an array-based FSR can be used to visualize contact area and pressure during wipe sampling.","['Force', 'Light emitting diodes', 'Explosives', 'Loading', 'Force measurement', 'Materials', 'Calibration']","['Explosives', 'narcotics', 'particle collection', 'trace detection', 'wipe sampling']"
"We present the first work that investigates the potential of improving the performance of transportation mode recognition through fusing multimodal data from wearable sensors: motion, sound and vision. We first train three independent deep neural network (DNN) classifiers, which work with the three types of sensors, respectively. We then propose two schemes that fuse the classification results from the three mono-modal classifiers. The first scheme makes an ensemble decision with fixed rules including Sum, Product, Majority Voting, and Borda Count. The second scheme is an adaptive fuser built as another classifier (including Naive Bayes, Decision Tree, Random Forest and Neural Network) that learns enhanced predictions by combining the outputs from the three mono-modal classifiers. We verify the advantage of the proposed method with the state-of-the-art Sussex-Huawei Locomotion and Transportation (SHL) dataset recognizing the eight transportation activities: Still, Walk, Run, Bike, Bus, Car, Train and Subway. We achieve F1 scores of 79.4%, 82.1% and 72.8% with the mono-modal motion, sound and vision classifiers, respectively. The F1 score is remarkably improved to 94.5% and 95.5% by the two data fusion schemes, respectively. The recognition performance can be further improved with a post-processing scheme that exploits the temporal continuity of transportation. When assessing generalization of the model to unseen data, we show that while performance is reduced - as expected - for each individual classifier, the benefits of fusion are retained with performance improved by 15 percentage points. Besides the actual performance increase, this work, most importantly, opens up the possibility for dynamically fusing modalities to achieve distinct power-performance trade-off at run time.","['Sensors', 'Automobiles', 'Public transportation', 'Cameras', 'Machine learning', 'Global Positioning System']","['Human activity recognition', 'transportation mode recognition', 'data fusion', 'machine learning', 'mobile sensing', 'wearable computing']"
"Displacement sensors are found in a variety of applications including gravitational wave detectors, precision metrology, tissue imaging, gravimeters, microscopy, and environmental monitoring. Most of these applications benefit from the use of displacement sensors that offer both high precision and stability. This is particularly the case for gravimetry where measurements are often taken over multi-day timescales. In this paper we describe a custom-built microcontroller-based displacement sensor that has been utilized in a micro-electromechanicalsystem gravimeter. The system runs off battery power and is low-cost, portable, and lightweight. Using an optical shadow sensor technique, and by designing a digital lock-in amplier based around a dsPIC33 microcontroller, we demonstrate a displacement sensitivity of 10 nm/Hz down to 300 s, and an rms sensitivity of 1 nm over timescales of one day. The system also provides real time monitoring/control of temperature, using an AD7195 ratiometric bridge to provide mK control of three separate PT100 sensors. Furthermore, a tilt sensor conditioning circuit is incorporated to drive a pair of electrolytic tilt sensors, resulting in the ability to monitor 2 axis tilt at the level of 1 microradian over approximately 1 day. The sensor system described is thus multifunctional and capable of being incorporated into precision accelerometers/gravimeters, or indeed other applications where long term displacement/temperature monitoring is necessary.","['Temperature sensors', 'Temperature measurement', 'Micromechanical devices', 'Photodiodes', 'Gravity measurement', 'Monitoring']","['Shadow sensor', 'displacement sensor', 'low noise', 'gravimeter', 'gravimetry', 'low noise electronics', 'lock-in amplifier', 'digital lock-in amplifier', 'digital filters']"
We demonstrate the feasibility of long-period gratings (LPGs) written in microstructured polymer optical fibers (mPOFs) for detecting and measuring the strain rate and magnitude of engineering structures. We validate and compare the results of our experimental tests to a commercial fiber Bragg grating sensor. The encouraging results open the way to the use of LPG mPOF sensors in structural health monitoring applications.,"['Strain', 'Loading', 'Optical sensors', 'Optical surface waves', 'Fiber gratings', 'Monitoring']","['Microstructured optical fibers', 'plastic optical fibers (POFs)', 'strain sensing', 'structural health monitoring']"
"We present a novel seismocardiography (SCG)-based approach for real-time cardio-respiratory activity measurement called the Autocorrelated Differential Algorithm (ADA). Measurements were performed on ten male subjects in the supine position for three 7-minute-long sets each, corresponding to 14,619 heartbeats. The ADA utilized temporal variations, windowing, and autocorrelation to produce physiological measurements corresponding to heart rate (HR), and left ventricular ejection time, and estimations of respiration rate, volume, and phase. The versatility of the ADA was investigated in two contexts: physical exertion and heart rate variability. The accuracy of HR measurements at a sampling frequency of 200 Hz resulted in a correlation coefficient (r2) of 0.9808 when compared with a manual annotation of all datasets. Its reproducibility was tested on externally obtained SCG and electrocardiography datasets, which produced anr2of 0.8224. The accuracy and computational time were also characterized by different sampling frequencies to quantify performance. The recommended sampling frequency is 200 Hz corresponding to a computation time of 0.05 s per instantaneous measurement using a standard desktop computer. The ADA delivered real-time SCG measurements with a refresh rate that was dependent on the computational time per measurement, which could be decreased by lowering the sampling frequency. The presented algorithm offers a novel tool toward real-time physiological monitoring in clinical and everyday scenarios.","['Correlation', 'Heart rate variability', 'Frequency measurement', 'Pollution measurement', 'Seismic measurements', 'Signal processing algorithms']","['Seismocardiography', 'autocorrelation', 'heart rate', 'respiration rate', 'cardiac time intervals', 'real-time', 'digital signal processing']"
"Based on a high-throughput trace-explosives detector, two security equipment were developed to check passengers and baggage before boarding an aircraft. The traceexplosives detector consists of an automated particles sampler and a compact mass spectrometer. In the automated particles sampler, particles adhering to a surface of the detection targets are removed by air jets. A cyclone preconcentrator is then used to collect the particles removed from the detection targets. The collected particles are vaporized by a vaporizer, and the vaporized molecules are analyzed by the mass spectrometer. For the passenger screening, the trace-explosives detector was installed into a boarding gate. When a passenger passes an electronic ticket (e-ticket) or IC card over the e-ticket reader of the boarding gate, compressed air jets are emitted from a nozzle toward the e-ticket. The throughput of the passenger screening is ~1200 persons/hour. For the baggage screening, on the other hand, the trace-explosives detector was combined with a conventional X-ray baggage screener. When a bag is put on the conveyor belt of the automated particle sampler, compressed air jets are emitted from nozzles. The trace analysis is finished, while an X-ray image of the bag is being obtained by the X-ray baggage screener. In the both security equipment, the tested explosives' particulate simulants of 2, 4, 6-trinitrotoluene, 1, 3, 5-trinitro-1, 3, 5-triazacyclohexane, and triacetone triperoxide adhered to the detection targets are successfully detected. Accordingly, the newly developed security equipment will be useful tools for improving airport security in the near future.","['Detectors', 'Explosives', 'Logic gates', 'Ion sources', 'Security', 'Ionization', 'Electrodes']","['homeland security', 'explosives detection', 'mass spectrometry']"
"Data collection is one of the most important applications in wireless sensor networks where sensed data are gathered from sensor nodes to the base station. Reporting redundant data leads to the wastage of time and energy, the sensors therefore report only meaningful information to the base station, which are independent and distinct with the lasted ones. This assumption leads to unpredictable changes of data traffic over different sampling intervals in data collection process. In this paper, we first formulate the tight constraints of the problem and then propose a delay-efficient traffic adaptive (DETA) scheme for collecting data from sensor nodes with minimum energy consumption. The DETA scheme minimizes data collection delay by constructing delay-efficient, collision-free schedule, and by using a special mechanism to enable every node to self-adapt with the changes of data traffic. We also conducted simulations to evaluate the performance of the proposed scheme, and the results shown that the proposed scheme significantly decreases data collection delay and energy consumption compared with the existing schemes.","['Data collection', 'Sensors', 'Schedules', 'Base stations', 'Wireless sensor networks', 'Delays', 'Heuristic algorithms']","['Wireless sensor networks', 'data collection', 'dynamic traffic', 'scheduling']"
"This paper proposes an eye control system employing eye gaze tracking techniques that might be helpful for those limb disabled people with healthy eyes. Eye gaze tracking technique is attracting more and more research interest in recent years. With an aim to overcome the shortcomings of existing methods (e.g., high hardware complexity and low detection accuracy), we use the pupil-corneal reflection technique to develop an ameliorated Hough transform algorithm, which is the core unit of the proposed system. We also design a typing function as well as an efficiently blink detection function. With these functions, users can input numbers into the computer with their eyes only through a specifically designed head mount eye control device. Experimental results demonstrate that the proposed ameliorated Hough transform algorithm provides a satisfactory typing accuracy of 87%, much higher than its counterpart in the literature.",[],[]
"The investigation of quickly-evolving flow patterns in high-pressure and high-temperature flow rigs is crucial due to inherent hazards. There is a dire need for a high-speed, non-intrusive imaging technique to identify characteristic flow phenomena to alleviate these hazards. Electrical Impedance Tomography (EIT) enables reconstruction of the admittivity distribution of the flowing medium(s), facilitating the characterisation of its/their flow. The requirement for images at high frame-rates led to simultaneous voltage excitations over electrodes on the periphery of the flow associated to Frequency Division Multiplexing (FDM), doped ONe Excitation for Simultaneous High-speed Operation Tomography (ONE-SHOT) method. In previous studies, we have demonstrated the possibility of the full implementation of simultaneous excitations and measurements strategies for EIT, which maximise the number of measurements for 16 electrodes. This article presents the details of the proposed method and the signal generation/acquisition firmware based on a Field Programmable Gate Array (FPGA) data acquisition system focusing on hardware and software integration and the signal processing involved in realising the ONE-SHOT operation of the EIT. It is shown that the simultaneously generated signals are successfully discriminated and used for image reconstruction, at a rate up to 3906 frames per second (fps). The associated signal-to-noise ratio varies in the 55.6 dB-69.1 dB range, depending on the generated frequency in the range of 3.906 kHz-468.7 kHz.","['Tomography', 'Electrodes', 'Field programmable gate arrays', 'Data acquisition', 'Frequency division multiplexing', 'Sensors', 'Image reconstruction']","['Electrical impedance tomography', 'two-phase flow', 'frequency division multiplexing', 'FPGA', 'ONE-SHOT', 'real-time']"
"A novel refractive index (RI) sensor has been demonstrated based on a simple dual-wavelength fiber ring laser by incorporating two fiber Bragg grating (FBG) filters. The grating filters are made of two FBGs with different cladding thicknesses. By inserting the filters into the fiber ring cavity, a stable dual-wavelength fiber laser can realize at room temperature. The cladding-etched FBG (thinned) acted as the RI sensing elements, while the normal one as temperature reference. As the RI increases, the laser wavelength corresponding to the thinned FBG moves toward longer wavelength, while that corresponding to the normal FBG is unchanged. Then, RI changes can be determined by measuring the separation of the two lasing wavelengths. Moreover, compared with the conventional thinned FBG-based sensors, the fiber laser sensor has a narrow bandwidth, which is beneficial to high precision sensing. The RI measurement resolution of proposed sensor is about 1.44 × 10 -5 RIU.","['Optical fiber sensors', 'Fiber gratings', 'Fiber lasers', 'Measurement by laser beam']","['Refractive index sensing', 'fiber Bragg grating', 'fiber ring laser']"
"The humidity sensing properties of sputter-coated indium-tin oxide (ITO) and printed dielectric structures were tested for samples with sheet resistances ranging from 10 to 50 Q/sq. ITO/Polymer composite sensors were fabricated to form a parallel-plate capacitive-based humidity sensor that could detect relative humidity within a tested range of 5%-95%. The sensors were most stable and gave a linear response between 5% and 75% relative humidity. The capacitive sensors were characterized, using a range of techniques, to establish their capability and performance as humidity monitors. Response time of the humidity sensors was measured to be an average of 31.5 s and the recovery time was measured at an average of 31 s in the capacitive mode. Complex impedance spectroscopy was used to determine the mechanism of action for the sensors, which was found to be both the diffusion of water molecules into the dielectric layer and an increase of ionic conductivity within the dielectric layer. Stability of the humidity sensors was tested at three different humidity levels over seven days and sensors were found to be stable or follow a predictable change for this time span.","['Humidity', 'Indium tin oxide', 'Capacitive sensors', 'Films', 'Capacitance', 'Dielectrics']","['Physical and chemical sensors', 'humidity sensors', 'transparent conducting oxides', 'capacitive sensor', 'indium tin oxide']"
"This paper presents an automatic and robust, image feature-based target extraction, and classification method for multistatic passive inverse synthetic aperture radar range/cross-range images. The method can be used as a standalone solution or for augmenting classical signal processing approaches. By extracting textural, directional, and edge information as low-level features, a fused saliency map is calculated for the images and used for target detection. The proposed method uses the contour and the size of the detected targets for classification, is lightweight, fast, and easy to extend. The performance of the approach is compared with machine learning methods and extensively evaluated on real target images.","['Passive radar', 'Radar imaging', 'Feature extraction', 'Receivers', 'Sensors', 'Image resolution']","['Target classification', 'passive radar', 'ISAR', 'ATR']"
"Image segmentation and classification of surfaces and obstacles in automotive radar imagery are the key technologies to provide valuable information for path planning in autonomous driving. As opposed to traditional radar processing, where clutter is considered as an unwanted return and should be effectively removed, autonomous driving requires full scene characterization. Hence, clutter carries necessary information for situational awareness of the autonomous platform and needs to be fully assessed to find the passable areas. In this paper, we proposed a method of automatic segmentation of automotive radar images based on two main steps: unsupervised image pre-segmentation using marker-based watershed transformation, followed by the supervised segmentation and classification of regions containing objects and surfaces based on the use of statistical distribution parameters. Several distributions were considered to characterize returns from specific region types of interest within the scene (denoted as classes) in calibrated radar imagery-the extracted distribution parameters were assessed for their ability to distinguish each class. These parameters were then used as features in a multivariate Gaussian distribution model classifier. Both the performances of the proposed supervised classification algorithm and the automatically segmented results were investigated using F1-score and Jaccard similarity coefficients, respectively.","['Radar imaging', 'Radar', 'Image segmentation', 'Radar cross-sections', 'Sensors', 'Radar antennas', 'Automotive engineering']","['Automotive radar imagery', 'image calibration', 'image segmentation', 'distribution feature extraction', 'Weibull distribution', 'multivariate Gaussian distribution', 'watershed transformation']"
"This work demonstrates that the use of an unconventional optical fiber allows for generating lossy mode resonances (LMRs), without the need for an external high refractive index (RI) thin film. The basic idea consists of using a few centimeters of an optical fiber having a large core and cladding with higher RI, which is spliced between two multimode fibers. Here, the role of such kind of fiber is got by a commercially available double cladding fiber (DCF) having a W-type RI profile. The possibility to induce and tune the phenomenon is demonstrated, where the resonance wavelength of LMR peaks and mode order can be adjusted by varying the thickness of DCF outer cladding, e.g., through chemical etching. This novel sensing scheme becomes a valid alternative to thin-film coated optical fibers, where LMR-based sensors have been developed so far, due to advantages in terms of simplicity, cost, and stability. The response of fabricated LMR devices is characterized toward surrounding medium RI, demonstrating a sensitivity up to about 1700 nm/RIU in the RI range of 1.33–1.39, which makes this fiber sensor suitable for bio-chemical sensing applications. Low cross sensitivity to temperature is also found.","['Optical fiber sensors', 'Claddings', 'Optical fibers', 'Refractive index', 'Sensors', 'Sensor phenomena and characterization', 'Optical fiber polarization']","['Double cladding fiber (DCF)', 'lossy mode resonance (LMR)', 'optical fiber sensor', 'refractive index (RI) sensor']"
"An olfactory display for blending many ingredients at any recipe was developed. It consists of microdispensers based upon solenoid valves and surface acoustic wave atomizer. The accuracy and stability were much improved compared with previous olfactory display using micropumps. Then, its blending capability was tested using sensory test such as triangle test. It was found that the scent blended in the proposed olfactory display was identical to the one blended in advance in the liquid phase. Finally, the durability of the amorphous Teflon coating used for enhancing atomization capability was investigated. The durability of the amorphous Teflon coating was much improved when the concentration of silane-coupling agent was modified.","['Solenoids', 'Valves', 'Liquids', 'Olfactory', 'Surface acoustic wave devices', 'Reservoirs', 'Electron tubes']","['Olfactory display', 'SAW atomizer', 'micro dispenser', 'solenoid valve', 'sensory test', 'Teflon coating']"
"We propose two on-line algorithms for stride parameter estimation for an in-shoe motion-sensor system (IMS) system: one for on-line stride segmentation based on stable foot-flat detection using foot-sole angle and the other for a three-dimensional zero-velocity-update for accurate stride parameterization. We developed a small and lightweight IMS device, which consists of an inertial measurement unit, micro control unit, and peripheral electrical components, integrated with an insole so that it can be placed inside a shoe at the arch of the foot. Stride parameters, i.e., stride length, walking speed, foot height, circumduction, peak foot sole angle in the dorsiflexion and plantarflexion directions, and toe-in/out angle, that characterize a user’s foot motion are estimated. We recruited 30 healthy participants and evaluated the precision of our IMS system by comparing the stride parameters calculated with this system with those acquired from a motion-capture system. The results indicate the precision of the system in terms of the root mean square error for stride length of 0.069 m, walking speed of 0.094m/s, foot height of 1.5 cm, circumduction of 1.0 cm, peak foot-sole angle in the dorsiflexion of 3.3 deg. and in the plantarflexion directions of 5.9 deg., and toe-in/out angle of 2.5 deg. Our IMS system has good precision regarding all parameters and high reliability and promises to contribute to personal health and wellness services and solutions by serving as a powerful and practical tool for objective gait assessment in real-world contexts.","['Foot', 'Sensors', 'Legged locomotion', 'Motion segmentation', 'Angular velocity', 'Three-dimensional displays', 'Sensor systems']","['Gait measurement', 'inertial measurement unit', 'in-shoe motion sensor', 'stride segmentation', 'zero velocity update']"
"Gas-insulated switchgears (GIS) have become essential parts of electrical power substations due to the associated merits of these capital assets. Although such resilient devices can rarely suffer from failure, partial discharge (PD) is responsible for around 85% of their recorded collapses. Ultra-high frequency (UHF) techniques have been widely used in the detection and localization of PD for a long time because of their immunity to noise and high sensitivity. Understanding electromagnetic (EM) wave behavior in GIS systems is significant for improving the utilization of UHF sensors in PD detection and for the optimal allocation of UHF antennas inside GIS systems. Thus, this paper is devoted to building a detailed 3D finite element (FE) model based on UHF detection techniques to understand the propagation behavior of EM waves inside GIS. A disk-type UHF sensor is used for acquiring EM waves inside the GIS. The sensitivity of the sensor has been obtained using a gigahertz transverse-electromagnetic (GTEM) test cell. The proposed model investigates the impact of multiple disconnecting parts including L-structure, relative angle between PD source and sensors, and disconnecting switches on the propagation of electromagnetic waves based on step 1 of the CIGRE recommendations. To validate the modeled GIS, a simple L-structured model is initially built, and a comparative analysis has been conducted between the built model and the experimental and analytical results from the literature.","['Gas insulation', 'Partial discharges', 'Sensitivity', 'Sensor phenomena and characterization', 'Mathematical model', 'Frequency-domain analysis']","['Electromagnetic propagation', 'gas-insulated switchgears', 'partial discharge', 'time of arrival estimation', 'UHF sensors']"
"We optimize the package of the cilium-type MEMS bionic vector hydrophone introduced by Chenyang and Wendong in 2007, which has the disadvantages of a low receiving sensitivity, narrow frequency band, and fluctuating frequency response curve. Initially, a full parametric analysis of frequency response and sensitivity with different material sound transparent cap were made. Then, we propose and realize an umbrella-type packaged structure with high receiving sensitivity and wide frequency band. The theoretical analysis and simulation analysis are conducted by ANSYS software and LMS virtual. Lab acoustic software. Finally, to verify the practicability of the package, the umbrella-type packaged hydrophone calibration was carried out in the National Defense Underwater Acoustics Calibration Laboratory of China. The test results show that the performance of umbrella-type hydrophone has been greatly improved compared with the previous packaged hydrophone: exhibiting a receiving sensitivity of -178 dB (increasing by 20 dB, 0-dB reference 1 V/μPa), the frequency response ranging from 20 Hz to 2 kHz (broaden one times), the fluctuation of frequency response curve within ±2 dB, and a good dipole directivity.","['Sonar equipment', 'Acoustics', 'Sensitivity', 'Resonant frequency', 'Materials', 'Vectors', 'Frequency response']","['MEMS', 'umbrella-type packaged structure', 'receiving sensitivity', 'frequency response']"
"The Indoor positioning based on the ZigBee received signal strength index has attracted more and more researchers’ attention and, due to its low cost, low hardware power consumption and easy implementation. However, because of multipath effects and shadow effects, traditional indoor positioning algorithms cannot obtain good positioning effects. In order to improve the accuracy of ZigBee indoor positioning, this paper proposes an indoor positioning algorithm of annealing algorithm (SA) and genetic algorithm (GA) optimized neural network (SAGA-BP), and the superiority of this algorithm is proved through simulation and experiment. First, establish the position relationship between the received signal strength indicator(RSSI) and the target position, and arrange the node network structure model to collect signals to establish a fingerprint database. Then use the mechanism of the annealing algorithm combined with the genetic algorithm to optimize the initial weight and initial threshold of the neural network algorithm, so that it can quickly jump out of the local optimal solution and achieve high-precision positioning. Experiments have proved the effectiveness of the positioning algorithm. Compared with BP and GA-BP algorithms, SAGA-BP positioning algorithm has an average error of 0.75m for RSSI signals after acquisition and processing, and an average error of BP positioning algorithm is 1.24m. The average error of GA-BP algorithm is 0.98m. Thus, the SAGA-BP algorithm has higher positioning accuracy.","['Licenses', 'Sensor phenomena and characterization', 'Neural networks', 'Temperature sensors', 'Fingerprint recognition', 'Biological cells', 'Annealing']","['ZigBee', 'indoor positioning', 'positioning accuracy', 'RSSI', 'SAGA-BP', 'neural network']"
"Nowadays, xylene is not only one major air pollutant which threats human health even if its concentration is lower than the human olfactory threshold of 470 ppb, but also one of the typical gases exhaled by the lung cancer patients with a criterion of 10-20 ppb. However, in situ detection of the ppb-level xylene for air quality monitoring and breath analysis remains challenging using the easily fabricated and low cost metal oxide semiconductor gas sensors. Herein, a synergetic p + n field effect transistor (FET) amplification circuit is designed to detect the ppb level xylene. By optimizing the load resistor (R L ) and the p- and n-FET coupling effect, a magnification factor (~7.5) is obtained. This amplification circuit decreases the detection limit of TGS2602 sensor to ~10 ppb xylene with apparent response of about 2.3 and voltage change of >0.5 V, promising for air quality monitoring (the highest permissive limit of 42 ppb) and breath disease analysis (threshold of lung cancer 10-20 ppb). The mechanism is that the matched couple of p + n FETs work synchronically when their (R L + R FET )-I curves nearly coincide with each other. All those results show the prospect of ppb level gas detection with MOX sensors using the synergetic p + n FET amplification circuit.","['Field effect transistors', 'Gas detectors', 'Resistance', 'Resistors', 'Semiconductor device measurement', 'Heating systems']","['Environment and health', 'field effect transistor', 'trace concentration xylene', 'amplification effect', 'metal oxide semiconductor sensor']"
"Suspended vibrating membranes play a vital role as structural elements in electromechanical resonators and form the foundation of modern acoustic transducers. The realization of large scale mechanical resonators based on large and thin membranes, however, still faces several challenges. In this paper, a simple and reproducible process has been developed to transfer millimeter-scale circular and square membranes composed of ~2.5 nm of graphene and a thin film of ~370 nm of poly(methyl methacrylate). The uniqueness of the demonstrated fabrication technique is the ability to produce high yield and non-ruptured suspended membranes with exceptional diameter (side length) to thickness aspect ratios of ~10 000. One of the perceived advantages of building a mechanical resonator from a bilayer structure, in which materials have different mechanical and thermal properties, is that such a structure can enable the use of electrothermal transduction to drive the resonator into resonance and tune its resonant frequencies. Due to the large area of the fabricated membranes, resonant frequencies within the audio range have been obtained. The frequency tuning response of circular and square membranes has been found to be influenced significantly by the magnitude of the applied voltage. A downward shift of the resonant frequency and an increase of the amplitude of vibration have been observed in response to the increase of the applied dc and ac voltages. The demonstrated fabrication technique and tuning mechanism could be employed as a platform for potential acoustic and audio applications.","['Graphene', 'Resonant frequency', 'Substrates', 'Tuning', 'Fabrication', 'Cavity resonators']","['Audio resonator', 'bilayer membrane', 'electro-thermal tuning', 'graphene', 'PMMA']"
"The research on ion-measuring devices has seen a rapid expansion in the last years as a consequence of the growing interest in wearable sensors for biofluids analysis and in portable devices for remote or in-line water and food quality monitoring. As a result, an increasing number of researchers is approaching the field of ion-selective sensors. Despite the apparently simple transduction principle, the theory behind their working mechanism is far from trivial and a correct understanding is necessary for an optimal exploitation of the technology. In current literature, imprecise characterization procedures lead to the definition of misleading sensors parameters, which cannot be effectively used for comparisons. In fact, some unique definitions and procedures applies to this category of sensors, which significantly differ from the traditional ones applied in sensing research. This tutorial aims at highlighting the basic thermodynamic theory and the correct experimental practice for the accurate and reproducible characterization of potentiometric ion-sensors. The most important requirements and design considerations on hardware and software interfaces will also be discussed to give a complete overview of the various aspects of current technology to people approaching this promising area of sensing.","['Sensors', 'Ions', 'Electrodes', 'Monitoring', 'Electric potential', 'Thermodynamics', 'Sensor phenomena and characterization', 'Tutorials']","['Ion-sensing', 'ion-selective electrodes', 'solid-contact', 'theory', 'practice', 'guidelines', 'sensor parameters', 'state-of-the-art']"
A sub-millimeter sized optical scanner driven by electro-thermal actuation is presented. The scanner is composed of a single-mode optical fiber (SMF) with a cantilevered section at its distal tip. The fiber cantilever is electrothermally actuated near its base in a single direction and excited at resonance to obtain large deflectionsat the tip of the fiber. Two-dimensional imaging of an object is demonstrated by simultaneously rotating the object while scanning across its diameter. Illumination light from the optical core of the fiber cantilever is projected through a lens onto the object. Reflected light is collected by the same lens and projected onto a photodetector. An image of the object is reconstructed by interpolation of the detected signal. The resolution of the system was measured to be 16μm by imaging a resolution target. The electro-thermal fiber actuator may provide a new technique for scanning in sub-millimeter sized forward-viewing endoscopic catheters.,"['Optical fiber sensors', 'Optical fibers', 'Actuators', 'Etching', 'Resonant frequency']","['% Chemical etching', 'electro-thermal actuator', 'fiber optic scanner', 'imaging', 'mems', 'micro-cantilever', 'resonance vibration']"
"Resonating inductive sensors are increasingly popular for numerous measurement techniques, not least in non-destructive testing, due to the increased sensitivity obtained at frequencies approaching electrical resonance. The highly unstable nature of resonance limits the practical application of such methods while no comprehensive understanding exists of the resonance distorting behavior in relation to typical measurements and environmental factors. In this paper, a study into the frequency spectrum behavior of electrical resonance is carried out exploring the effect of key factors. These factors, known to distort the electrical resonance of inductive sensors, include proximity to (or lift-off from) a material surface and the presence of discontinuities in the material surface. Critical features of resonance are used as metrics to evaluate the behavior of resonance with lift-off and defects. Experimental results are compared with results from a 2-D finite element analysis model that geometrically mimics the inductive sensor used in the experiments, and with results predicted by an equivalent circuit transformer model. The findings conclusively define the physical phenomenon behind measurement techniques such as near electrical resonance signal enhancement and show that lift-off and defect resonance distortions are unique and measurable and can be equated to exclusive variations in the induced variables in the equivalence circuit model. The resulting understanding found from this investigation is critical to the future development and understanding of a complete model of electrical resonance behavior, integral for the design of novel sensors, techniques, and inversion models.","['Probes', 'Impedance', 'Surface impedance', 'Resonant frequency', 'Sensor phenomena and characterization', 'Frequency measurement']","['Resonance', 'inductive sensing', 'eddy-current', 'ECT', 'NDT', 'electromagnetic testing', 'NERSE']"
"With the rise of the internet of things, wireless sensor network (WSN) technology has gained unprecedented development and has attracted increasing attention from researchers. Due to the inherent characteristics of WSN, such as interaction with the environment, WSN localization becomes an essential and attractive topic in academia and industry. In this paper, the range-based localization problem in a mobile WSN application scenario is considered to be time-varying and modeled as a dynamic matrix equation by introducing the time parameter. Two modified zeroing neurodynamics (ZND) models are proposed and investigated to deal with range-based WSN localization problems from angle of arrival (AOA) measurement and time difference of arrival (TDOA) measurement. In addition, the convergence of the proposed models is theoretically analyzed. Furthermore, computer simulations on WSN localization are carried out to prove the effectiveness of the proposed models in terms of accuracy and robustness to the dynamic environment. Additionally, the application to underwater sensor node localization of underwater acoustic network (UAN) testbed is provided to illustrate the feasibilities of the proposed models for solving UAN localization problem.","['Location awareness', 'Wireless sensor networks', 'Sensors', 'Mathematical models', 'Computational modeling', 'Neurodynamics', 'Covariance matrices']","['Angle of arrival', 'time difference of arrival', 'wireless sensor network localization', 'zeroing neurodynamics model']"
"This paper presents a measurement-based analysis of the Received Signal Strength (RSS) of Bluetooth Low Energy (BLE) signals, under Line-of-Sight (LOS) and Non-Line-of-Sight (NLOS) scenarios, performed in tandem at two universities in Tampere, Finland, and Bucharest, Romania. We adopted the same hardware and methodology for measurements in both places, and paid particular attention to the impact of RSS on various environmental factors, such as LOS and NLOS scenarios and interference in 2.4 GHz band. In addition, we considered the receiver orientation and the different frequencies of BLE advertising channels. We show that snapshot RSS measurements typically have high variability, not easily explainable by classical path-loss models. A snapshot recording is defined here as one continuous recording at fixed device locations in a static setup. Our observations also show that aggregated RSS data (i.e., considering several snapshot measurements together) is more informative from a statistical point of view and more in agreement with current theoretical path-loss models than snapshot measurements. However, in BLE applications such as contact tracing and proximity detection, the receivers typically have access only to snapshot measurements (e.g., taken over a short duration of 10–20 minutes or less), so the accuracy of contact-tracing and proximity detection can be highly affected by RSS instabilities. In addition to presenting the measurement-based BLE RSS analysis in a comprehensive and well-documented format, our paper also emphasizes open challenges when BLE RSS is used for contact tracing, ranging, and positioning applications.","['Advertising', 'Wireless fidelity', 'Fading channels', 'Current measurement', 'Semiconductor device measurement', 'Interference', 'Smart phones']","['Indoor navigation', 'indoor radio communication', 'received signal strength indicator (RSSI)', 'Bluetooth', 'fluctuations']"
"Electrochemical biosensors are widely investigated as they represent attractive analytical tools for detection of a broad range of bio-molecules, thanks to their simplicity, high sensitivity and short response time. Especially, biosensors employing an electrolyte-gated field-effect transistors (EG-FETs) as electrochemical transduction element have gained increasing interest, due to the signal amplification and the intrinsic low voltage range of operation. In this work we report the fabrication of flexible EG-FETs using spray-deposited semiconducting carbon nanotubes (CNTs), with a specific focus on the optimization of the CNT channel to optimize the performance of the resulting CNT-based EG-FET (EG-CNTFET). The transfer and the output characteristic of different devices with varying spraying parameters were tested, finding out that only devices with source-drain resistance of about ≤10kΩshowed proper EG-CNTFET operation: for these devices we recorded a typical p-type behavior with an on–off ratio of 214 A/A up to 469 A/A (depending on number of the spray-deposited CNT layers). The fabricated EG-CNTFETs were functionalized with anti-spermidine antibodies to detect polyamine spermidine - a well-known chemical indicator of food quality. To ensure controlled immobilization and at the same time to preserve the electrical properties of the nanotubes, the spray-deposited films were modified with a bifunctional molecule, which attaches to the CNT via non-covalentπ−πinteractions and leaves a free NHS-ester group for amide coupling of the antibodies. The fabricated EG-CNTFET-based immunosensors showed a linear detection range for spermidine from 10−3 to 102 nM, with the sensitivities ranging from −1.03 to−2.45μA/decade.","['Biosensors', 'Logic gates', 'Substrates', 'Spraying', 'Electrodes', 'Dispersion', 'Carbon nanotubes']","['CNTs', 'electrolyte-gated', 'flexible electronics', 'immunosensors', 'polyamines', 'transistors']"
"Flexural ultrasonic transducers are robust and low cost sensors that are typically used in industry for distance ranging, proximity sensing and flow measurement. The operating frequencies of currently available commercial flexural ultrasonic transducers are usually below 50 kHz. Higher operating frequencies would be particularly beneficial for measurement accuracy and detection sensitivity. In this paper, design principles of High Frequency Flexural Ultrasonic Transducers (HiFFUTs), guided by the classical plate theory and finite element analysis, are reported. The results show that the diameter of the piezoelectric disc element attached to the flexing plate of the HiFFUT has a significant influence on the transducer's resonant frequency, and that an optimal diameter for a HiFFUT transmitter alone is different from that for a pitch-catch ultrasonic system consisting of both a HiFFUT transmitter and a receiver. By adopting an optimal piezoelectric diameter, the HiFFUT pitch-catch system can produce an ultrasonic signal amplitude greater than that of a non-optimised system by an order of magnitude. The performance of a prototype HiFFUT is characterised through electrical impedance analysis, laser Doppler vibrometry, and pressure-field microphone measurement, before the performance of two new HiFFUTs in a pitch-catch configuration is compared with that of commercial transducers. The prototype HiFFUT can operate efficiently at a frequency of 102.1 kHz as either a transmitter or a receiver, with comparable output amplitude, wider bandwidth, and higher directivity than commercially available transducers of similar construction.","['Resonant frequency', 'Acoustics', 'Ultrasonic transducers', 'Transducers', 'Frequency measurement', 'Ultrasonic variables measurement', 'Vibrations']","['Air-coupled ultrasonic transducer', 'finite element analysis', 'flexural ultrasonic transducer', 'ultrasonic measurement']"
"Recent years have seen a huge increase in the study of drones. There is a lot of published articles regarding drone, focusing on control optimization, fault detection, safety mechanisms, etc. In fault detection, most studies focused on the effects of faulty propellers and rotors, and there is very limited academic research on drone arms. In this paper, a fault detection based on the vibration of the multirotor arms using artificial intelligence (AI) is proposed. There are some cases in which, due to accident, the arm of the multirotor crack or loosen. This is normally unnoticeable without disassembly, and if not taken care of, it would have likely resulted in a sudden loss of flight stability, which will lead to a crash. Different types of AI methods are incorporated in this study, namely, fuzzy logic, neuro-fuzzy, and neural network (NN). Their results are compared to determine the best method in predicting the safety of the multirotor. Fuzzy logic and neuro-fuzzy methods provided acceptable decision-making, but the performance of the neuro-fuzzy approach depend on the dataset used because overfit model might give incorrect decision-making. This also applies to the NN technique. Because the vibration data are collected in the laboratory environment without consideration of wind effect, this framework is more suitable for early prediction before flying the multirotor in the outdoor environment.","['Vibrations', 'Sensors', 'Fault detection', 'Propellers', 'Drones', 'Artificial intelligence', 'Real-time systems']","['Artificial intelligence', 'fault detection', 'drones', 'vibration analysis', 'Internet of Things (IoT)']"
"In this paper, the experimental results of a complete battery-less wirelessly-powered 2.45-GHz temperature sensor system with a microcontroller (μC) and an ISM-band transmitter on FR-4 is presented. As the measured results show, the energy harvesting system can provide 2.4 V to turn on the μC , temperature sensor and transmitter with only -15 dBm (32μW) radio frequency input power and up to 0.2°C temperature resolution. The system is able to transmit 32-bit sensor data back to the base station with an extra 16 bits for cyclic redundancy check check at regular intervals. The battery-less μC can be wirelessly powered when located up to a distance of 2 m from a power transmitter supplying less than 50 mW of microwave power using a patch array.","['Temperature sensors', 'Capacitors', 'Batteries', 'Transmitters', 'Integrated circuits', 'Radio frequency', 'Base stations']","['Energy harvesting', 'ISM band', 'microcontroller', 'wireless temperature sensor']"
"We report a theoretical framework to explain the characteristics of Fabry-Pérot (FP) resonances excited in a thin film-based grating consisting of a thin gold layer and a rectangular dielectric grating in the sub-wavelength and near-wavelength grating regimes. The zeroth-order diffraction inside the grating layer forms an FP resonant cavity with effective refractive index arising from an averaging effect between the refractive indices of the grating material and the filling material between the grating grooves. A simplified model based on Fresnel equations and phase matching condition is proposed to predict the FP resonant mode for the grating structure, this is compared with rigorous coupled-wave analysis to determine its range of validity. We also compare the performance of the proposed structure with other thin film-based interferometers for refractive index sensing applications, in terms of, sensitivity, full width at half maximum, figure of merit and dynamic range. The proposed structure has a full width at half maximum around 10 times to 60 times narrower than conventional surface plasmon resonance and conventional FP resonators. Thus, the figure of merit is higher than Kretschmann based surface plasmon resonance and FP structures by a factor of 20 and 2 respectively with a wider dynamic range. The total energy stored in the grating resonant cavity is 5 and 20-fold greater than the surface plasmon resonance configuration and the conventional FP structures. Since the resonator discussed here is an open structure, it is far better suited for liquid sensing compared to a closed FP structure.","['Gratings', 'Sensors', 'Resonators', 'Refractive index', 'Mathematical model', 'Perturbation methods', 'Optical sensors']","['Optical sensors', 'biosensors', 'Fabry-Pérot', 'optical resonators', 'subwavelength grating']"
"Star trackers must be calibrated prior to flight so that they can make accurate measurements of star positions within the instrument field of view. This calibration is usually performed in atmosphere and after the sensor is launched; it is not uncommon to observe a small shift in some of the calibration parameters. In this paper, we explore several autonomous strategies for on-orbit recalibration of star trackers. We present an improved version of a popular camera model, develop optimizations to identify optimal parameter values, and validate performance using the data collected from on-orbit sensors. When compared with human-mediated batch processing, autonomous methods have comparable reliability, performance, and commissioning time. The sensor datasets used in this paper come from six Sinclair Interplanetary ST-16 star trackers launched between November 2013 and July 2014. Both batch and autonomous approaches to on-orbit calibration yield improvements in measurement availability as well as a 20%-80% reduction in residual geometric error compared to ground calibrations.","['Cameras', 'Calibration', 'Telemetry', 'Numerical models', 'Earth', 'Detectors']","['Star trackers', 'calibration', 'parameter estimation', 'extended Kalman filter', 'nonlinear least squares']"
"The nonstationarity of interferences and array errors may bring fatal degradation to the interference cancellation performance of an adaptive beamformer. Covariance matrix tapering (CMT) can produce wide troughs in receiving pattern and becomes a promising solution. However, not only the full adaption methods but also the robust sidelobe canceller widen all nulls symmetrically to the original directions of arrival (DOAs) with the same width. In fact, in most cases, the adaptive pattern does not need symmetrical and equivalent-width nulls since it is of little possibility that different interference poses the same nonstationarity. To cover the worst nonstationarity, traditional CMT methods need to produce a widest null and will suffer a waste of degrees of freedom (DOFs). In this paper, a flexible method for null widening is proposed which can produce wide null with different desired width and asymmetry. The asymmetry is developed from the spatial asymmetrical interference cluster, and the unequal null width is produced by disturbing different interference space with different tapering matrix. Computer simulation result corroborate the feasibility and merits of the proposed method.","['Covariance matrices', 'Interference', 'Array signal processing', 'Telecommunications', 'Sensor phenomena and characterization', 'Symmetric matrices', 'Sensor arrays']","['Subspace extraction', 'null widening', 'covariance matrix taper (CMT)', 'adaptive beamforming']"
"With rising hazardous organic vapours in the environment, the detection of volatile organic vapour compounds (VOCs) is becoming important. To this end, this paper presents a conductive droplet-based disposable sensor. Unlike conventional sensors, the droplet system is easily replaceable and is capable of detecting multiple vapours based on surface tension gradient. The chemiresistive sensor presented here is fabricated on 2.5 μm thick ultra-flexible graphene oxide-chitosan (GOC) with Pt electrodes having 60 μm gap. The electrostatic interaction and strong hydrogen bonds between GO and polysaccharide groups in chitosan provide tunable hydrophobicity and stability to the droplet. With a conductive droplet of ~10 μL of aq. NaCl as an active sensing material dispensed between the Pt electrodes, it was observed that the droplet showed 14-21% change in resistance in the presence of VOCs. A read-out circuit was also designed to get the data from the droplet sensor. The response time for the presented sensor (3-4 seconds) is significantly better than its solid-state sensor counterparts. With attractive features such as disposability, affordability and fast response the presented sensor will find applications in industrial environments, laboratories, health centres, and biomedical devices.","['Substrates', 'Electrodes', 'Methanol', 'Chemical sensors', 'Surface tension', 'Sensor phenomena and characterization']","['Disposable sensors', 'printed electronics', 'microfluidics', 'VOC']"
"A multi-channel front-end for electrochemical sensing is presented. It consists of a multiplexed four-channel readout interface supporting amperometric, voltammetric, and potentiometric measurements. The electronic interface is co-designed according to the target biomarker specifications, and exhibits excellent linearity in both current and voltage sensing. The sensing front-end is characterized with lactate, paracetamol, and lithium sensing, yielding sensitivity of 1.2 ± 0.3μA/mM, 69.6 ± 2nA/μM, and 55.6mV/decade, respectively. These performances are comparable with the ones obtained with a bulky commercial Autolab potentiostat. Moreover, the limit of detection achieved are of 37±8 μM, 2.1± 1.22 μM, and 11± 3.5 μM, respectively, for the aforementioned sensors. These values are more than one order of magnitude lower than the relevant detection range. This successful characterization demonstrates the ability of the proposed system to monitor, in a broader sense, metabolites, drugs, and electrolytes. The programmability, versatility and portability of the front-end interface paves the way for a continuous monitoring of different families of biomarkers, suitable for advanced healthcare diagnosis and wearable physiology.","['Sensors', 'Biomedical monitoring', 'Monitoring', 'Electrodes', 'Hardware', 'Computer architecture', 'Drugs']","['Amperometric sensing', 'health care monitoring', 'multi-channel electrochemical sensing', 'potentiometric sensing', 'wearable physiology']"
"This study proposes a data fusion method for multiradar systems to enable measurement of the respiration of multiple people located at arbitrary positions. Using the proposed method, the individual respiration rates of multiple people can be measured, even when echoes from some of these people cannot be received by one of the radar systems because of shadowing. In addition, the proposed method does not require information about the positions and orientations of the radar systems used because the method can estimate the layout of these radar systems by identifying multiple human targets that can be measured from different angles using multiple radar systems. When a single target person can be measured using multiple radar systems simultaneously, the proposed method selects an accurate signal from among the multiple signals based on the spectral characteristics. To verify the effectiveness of the proposed method, we performed experiments based on two scenarios with different layouts that involved seven participants and two radar systems. Through these experiments, the proposed method was demonstrated to be capable of measuring the respiration of all seven people by overcoming the shadowing issue. In the two scenarios, the average errors of the proposed method in estimating the respiration rates were 0.33 and 1.24 respirations per minute (rpm), respectively, thus demonstrating accurate and simultaneous respiratory measurements of multiple people using the multiradar system.","['Radar', 'Radar imaging', 'Radar measurements', 'Shadow mapping', 'Data integration', 'Sensors', 'Position measurement']","['Biomedical engineering', 'data fusion', 'radar measurement', 'radar imaging', 'radar signal processing']"
"Contagious diseases are the principal cause of mortality, particularly respiratory viruses, a real menace for public health and economic development worldwide. Therefore, timely diagnosis and treatments are the only life-saving strategy to overcome any epidemic and particularly the ongoing prevailing pandemic COVID-19 caused by SARS-CoV-2. A rapid identification, point of care, portable, highly sensitive, stable, and inexpensive device is needed which is exceptionally satisfied by sensor technology. Consequently, the researchers have directed their attention to employing sensors targeting multiple analyses of pathogenic detections across the world. Nanostructured materials (nanoparticles, nanowires, nanobundles, etc.), owing to their unique characteristics such as large surface-to-volume ratio and nanoscale interactions, are widely employed to fabricate facile sensors to meet all the immediate emerging challenges and threats. This review is anticipated to foster researchers in developing advanced nanomaterials-based sensors for the increasing number of COVID-19 cases across the globe. The mechanism of respiratory viral detection by nanomaterials-based sensors has been reported. Moreover, the advantages, disadvantages, and their comparison with conventional sensors are summarized. Furthermore, we have highlighted the challenges and future potential of these sensors for achieving efficient and rapid detection.","['Sensors', 'Viruses (medical)', 'Coronaviruses', 'COVID-19', 'Sensor phenomena and characterization', 'Nanomaterials', 'Biosensors']","['Nanomaterials', 'respiratory viral detection', 'SARS-CoV-2', 'types of sensors']"
"The fluorescence background in Raman spectroscopy can be effectively suppressed by using pulsed lasers and time-gated detectors. A recent solution to reduce the high complexity and bulkiness of the time-gated systems is to implement the detector by utilizing time-resolved single-photon avalanche diodes (SPADs) fabricated in complementary-metal-oxide-semiconductor (CMOS) technology. In this study, we investigate the effects of fluorescence-to-Raman ratio, recording time and excitation intensity on the quality of Raman spectra measured by using one of the furthest developed fluorescence-suppressed Raman spectrometers based on a time-resolved CMOS SPAD line sensor. The objectives were to provide information on the significance of the different causes behind the distortion of the measured Raman spectra with various measurement conditions and to provide general information on the possibilities to exploit the high-intensity non-stationary pulsed laser excitation to gain additional improvement on the spectral quality due to laser-induced fluorescence saturation. It was shown that the distortion of the spectra with samples having short fluorescence lifetimes (~2 ns) and high fluorescence-to-Raman ratios, i.e. with challenging samples, is dominated by the timing skew of the sensor instead of the shot noise caused by the detected events. In addition, the actual reason for the observed improvement in the spectral quality as a function of excitation intensity was discovered not to be the conventionally thought increased number of detected photons but rather the laser-induced fluorescence saturation. At best, 26% improvement to the signal-to-noise ratio was observed due to fluorescence saturation.","['Photonics', 'Raman scattering', 'Distortion', 'Signal to noise ratio', 'Single-photon avalanche diodes', 'Timing']","['Raman spectroscopy', 'non-stationary laser-induced fluorescence saturation', 'complementary metal–oxide–semiconductor single-photon avalanche diode (CMOS SPAD)', 'timing inhomogeneities', 'timing skew', 'spectral quality', 'signal-to-noise ratio (SNR)', 'signal-to-distortion ratio (SDR)']"
"Rapid population growth, urbanization and motorization have brought about secondary effects that have gradually damaged the atmosphere, whose importance is vital for both the survival of all living beings and the climate balance. In this sense, air pollution is a problem that affects current society and is much more critical in developing countries. In this context, in the present paper non-parametric statistical inference techniques are used to carry out the analysis of measurements of health concerning fine particulate matter concentration, PM 2.5 , in an urban park of Quito, Ecuador. In short, the data collected during the measurements were stored in random variables and the Kruskal-Wallis test was used to test if these random variables come from populations with identical distributions. Also, the Wilcoxon signed rank test was used to test if the numerical values collected in the samples of the random variables of interest represent a level of contamination that could be dangerous for human beings. The experimental results show that urban parks and, specifically, trees are a natural filter between the pollution generated in the road and the center of the park. Therefore, the role of trees in the face of vehicular pollution will depend on two variables: the amount and compactness of the vegetation, and the emission levels recorded in the border roads.","['Atmospheric measurements', 'Sensors', 'Urban areas', 'Pollution measurement', 'Air pollution', 'Particle measurements']","['PM₂.₅ concentration', 'air pollution', 'non-parametric statistical inference', 'Kruskal-Wallis test', 'Wilcoxon signed-rank test']"
"The current gold standard for gait analysis involves performing the gait experiments in a laboratory environment with a constrained space. However, there is growing interest in using flexible, efficient, and inexpensive wearable sensors as tools to perform gait analysis. This review aimed to identify and summarize the current advances in wearable sensors for various aspects of gait analysis, such as the application of wearable gait analysis systems, sensor systems and their attachment locations, and the algorithms used for the analysis. The PRISMA guideline was adopted to find relevant studies from the period 2011 to 2020 from several scientific databases. A total of 76 articles were selected based on the inclusion and exclusion criteria. A wearable inertial measurement unit (IMU) attached to the lower limb region was found to be the most common approach for gait analysis. Temporal, spatial, and spatiotemporal features were the most common quantitative gait features extracted from the wearable sensors. The proposed frameworks showed varying performances, and an increased number of sensors did not necessarily improve the estimation performance metrics. A few studies have integrated various machine learning techniques for classification problems, correction algorithms, crosschecking functions, and scoring functions. Finally, this review paper discusses the challenges and future direction of the research on quantitative gait analysis.","['Wearable sensors', 'Sensors', 'Legged locomotion', 'Sensor phenomena and characterization', 'Feature extraction', 'Databases', 'Senior citizens']","['Gait analysis', 'wearable sensors', 'inertial measurement units', 'machine learning', 'walking', 'clinical applications']"
"A temperature sensor based on erbium-doped fiber cascaded peanut taper (EDFCPT) structure in fiber ring laser (FRL) cavity is proposed and studied. The cascaded peanut structure was fabricated by erbium-doped fiber. The cladding mode and core mode interference and the strong thermo-optical effect of erbium-doped fiber were used to realize the high sensitivity measurement of temperature. The mode interference and thermal sensitivity effects of EDFCPT in a broadband super-continuous light source and a fiber ring laser cavity were studied and compared experimentally. Besides, Erbium-doped fiber cascaded peanut (EDFCP) structure was designed to compare performance with EDFCPT. The experimental results show that EDFCPT has a higher signal to noise ratio (SNR) (~50dB) and a narrower 3-dB bandwidth (~0.12nm) than the broadband light source. Among them, the temperature sensitivity of EDFCPT is 571pm/°C which is two times higher than the EDFCP (~249nm/°C) structure at range from 5°C-55°C. The proposed temperature sensor has the advantages of high sensitivity, good repeatability, simple fabrication, compact structure, etc. It has a good application prospect in the field of aerospace and life health monitoring.","['Optical fiber sensors', 'Temperature sensors', 'Sensors', 'Sensitivity', 'Erbium-doped fiber lasers', 'Discharges (electric)', 'Temperature measurement']","['Temperature sensor', 'erbium-doped fiber double peanut taper', 'fiber ring laser']"
"Full-wave analysis of a two-port, broadband, nondestructive microwave sensor for characterizing magnetic sheet materials is presented. The sensor, called a clamped dual ridged waveguide (CDRWG) probe, consists of two DRWGs connected to metal flange plates, which sandwich the unknown magnetic material. Prior CDRWG probe work considered only the dominant DRWG mode. This paper significantly extends and effectively completes the prior work by considering all DRWG modes. The theoretical development of the CDRWG probe, including all higher-order DRWG modes, is presented and discussed. A physical discussion of the higher-order modes, in particular, why they are or are not excited, is included. Finally, experimental results of a lossy magnetic material are presented and analyzed to determine what impact higher-order modes have on CDRWG probe accuracy. The probe's errors in determining the permittivity and permeability of the material are estimated, considering uncertainties in measured scattering parameters and specimen thickness.","['Probes', 'Magnetosphere', 'Apertures', 'Hollow waveguides', 'Magnetic sensors', 'Scattering parameters']","['Green’s function', 'Integral equations', 'Microwave measurements', 'Microwave sensors', 'Moment methods', 'Nondestructive testing', 'Parallel plate waveguides', 'Permeability measurement', 'Waveguides', 'Waveguides', 'open-ended']"
"Global navigation satellite system (GNSS) spoofing causes the victim receiver to deduce false positioning and timing data; this notably threatens navigational safety. Thus, anti-spoofing techniques that improve the reliability of GNSS systems, for which interference detection is critical, are essential. Based on the distortion of tracking loop correlation function symmetry of the target receiver caused by gradual adjustment of induced spoofing signals, we proposed a new induced spoofing detection method that uses the weighted second-order central moment (WSCM) difference in the time-domain transient response of multiple correlators of the left and right peaks to obtain the test statistic, theoretically proving that the test statistic follows Gaussian distribution. The Neyman-Pearson hypothesis test method is used to determine the optimal test threshold and determine whether the receiver is being spoofed. The proposed WSCM-based method for spoofing detection was compared with three conventional methods in Scenarios 4 and 7 of the Texas Spoofing Test Battery database, showing that the detection probability of the proposed method is at least 24.15% higher at a false alarm rate of 10% and is more advantageous at lower false alarm rates and the alert time is shortened by at least 30 seconds, enabling at least a 20% faster detection efficiency. The proposed method overcomes the problem of existing methods, which are associated with difficulties in capturing the subtle time-varying effects of the relative carrier phase between the spoofing and authentic signals; thus, it provides excellent detection accuracy and effectiveness, showing broad potential applicability in GNSS spoofing detection.","['Correlation', 'Correlators', 'Receivers', 'Global navigation satellite system', 'Measurement', 'Distortion', 'Codes']","['GNSS spoofing detection', 'induced GNSS spoofing', 'SQM', 'TEXBAT', 'weighted second-order central moment']"
"Fluid balance is important for a healthy human being. In this paper, a method to measure hydration status was developed and tested towards non-invasive measurement from human skin. Measurement of hydration status was performed by a microwave sensor utilizing a complementary split ring resonator (CSRR). The sensor was modeled, manufactured and then characterized by measuring tailor made skin phantoms based on the realistic electrical properties of skin with different degrees of hydration status. Qualitative longer term (>24 h) evaluation of the sensor was also performed by measuring polyester tissue that was drying over the time. Hydration status, represented by dehydrated, normal and hydrated skin phantoms, based on polyurethane with carbon and ceramic additives, was measured successfully by monitoring the changes in resonance frequency around 5.52 GHz. All results were compared to the dielectric reference measurements done by a commercial laboratory instrument.","['Skin', 'Dielectrics', 'Sensor phenomena and characterization', 'Dielectric measurement', 'Microwave measurement', 'Ions']","['Microwave sensors', 'biomedical monitoring', 'dielectric measurement', 'skin phantoms', 'health monitoring']"
"Dissolved organic matter (DOM) plays an important role in biological, physical and chemical processes in water ecosystems. Different characteristics of DOM, such as origin or formation, can be determined by different fluorescence indices. A portable instrument for field measurement of the fluorescence index (FIX) and the biological index (BIX) on water samples has been developed and characterized. The developed sensor system was tested under different scenarios and showed a sufficient performance where the typical measurement results differed less than ±10% from results achieved by a stationary laboratory fluorescence spectrometer.","['Fluorescence', 'Optical filters', 'Sensor systems', 'Indexes', 'Optical sensors', 'Band-pass filters']","['Environmental monitoring', 'dissolved organic matter (DOM)', 'fluorescence index (FIX)', 'biological index (BIX)', 'portable fluorescence sensor']"
"Lithium represents the main drug for the treatment of Bipolar Disorder (BD) as it acts as a mood stabilizer. However, because of its narrow therapeutic window, psychiatric patients are obliged to frequent tests in hospital to control their lithium blood concentration during therapy. In this paper, we fabricate for the first time a complete non-invasive system for the Therapeutic Drug Monitoring (TDM) of lithium in sweat in people suffering from BD. The wearable electrochemical sensing platform includes a paper fluidics and a stable Reference Electrode (RE). Lithium detection is based on the use of a potentiometric Ion-Selective Electrode (ISE) with a nanostructured solid contact in order to increase the potential stability of the sensor. A chemical chlorination combined with the use of a PVC membrane doped with an Ionic Liquid (IL) is used to minimize possible potential drifts of the RE. The Nernstian response (56.8±3.9 mV /decade) of the system in the range of clinical interest is then proved in aqueous solution and in sweat. The integration of the sensing platform with a paper fluidics transporting fresh sweat sample to the sensing area is also considered. The already tested sweat is collected in a reservoir made of slow adsorbing paper. The complete wearable system is then successfully tested in a simulated setup by using a mannequin. The measured potential is well compared to the predicted ones in the therapeutic window of lithium drug, proving the good sensing capability in artificial sweat.","['Sensors', 'Lithium', 'Electrodes', 'Drugs', 'Biomedical monitoring', 'Monitoring', 'Blood']","['Electrochemistry', 'flexible sensors', 'ion sensors', 'potentiometry', 'sweat', 'wearable']"
"Capacitive sensors have many applications in tactile sensing, human-machine interfaces, on-body sensors, and patient monitoring. Particularly in biomedical applications, it would be beneficial if the sensor is disposable and readily degradable for efficient recycling. In this study, we report a biodegradable capacitive tactile pressure sensor based on sustainable and bio resourced materials. Silver-nanowire-coated rubber tree leaf skeletons are used as transparent and flexible electrodes while a biodegradable clear tape is used as the dielectric layer. The fabricated sensor is sensitive and can respond to low pressures (7.9 mN when pressed with a probe with a surface area of 79 mm 2 / 0.1 kPa) ranging to relatively high pressures (37 kPa), with a sensitivity up to≈4.5×10−3kPa −1 . Owing to all bio resourced constituents, the sensor is biodegradable and does not create electronic waste.","['Electrodes', 'Skeleton', 'Sensors', 'Nanowires', 'Rubber', 'Capacitive sensors', 'Fabrication']","['Bioinspiration', 'capacitive sensors', 'leaf skeleton', 'Ag nanowires']"
"A novel non-contact instantaneous torque sensor is proposed in this paper. The mechanical structure of the torque sensor mainly consists of two eccentric sleeves rotating about an elastic shaft. The measurement of torque is transformed into the measurement of the phase difference between the eccentric sleeves. Eddy current sensors are used to measure distance changes between their probes and the eccentric sleeves. The phase is modulated by the distance changes when any torque applied to the elastic shaft the demodulation principle of the phase relies on solving simple trigonometric functions without any complex signal processing methods. Therefore, the acquisition of torque can be performed instantaneously without any accumulation of time or integer-period sampling. The proposed sensor has a simple structure with no electrical components within the rotational parts. Additionally, the proposed sensor facilitates the measurement of static torque, dynamic torque, and even reciprocating torque over a wide range of angular speeds. The sensor was calibrated by a torsion-testing setup and experimental results indicate that the sensitivity of the sensor is23.05N⋅m/∘, the sum of squares due to error is 0.09449, and the root-mean-squared error is 0.1375. The non-linearity is 0.914%. The proposed sensor accuracy is 0.06%.","['Sensors', 'Torque', 'Torque measurement', 'Shafts', 'Current measurement', 'Phase measurement', 'Eddy currents']","['Torque sensor', 'eccentric modulation', 'phase demodulation', 'high-resolution']"
"Remote condition monitoring systems for rural infrastructure lack “intelligent” analysis and advanced insights offered by recent Internet of Things devices. This is because the extreme and inaccessible operating locations necessitate the conservative use of limited resources, such as battery life and data transmission. Present implementations are often limited to usage data loggers, which are informative of general usage but post-processed advanced insights lag real-time system changes. A lightweight novelty filter is implemented onboard rural handpumps to identify subsets of data as potential infrastructure failure. The “intelligent” summaries of these data subsets are sent to a cloud-based system, where more advanced machine learning approaches are applied to increase the fidelity of potential failure predictions. The proposed method was tested on three independent data sets and found that the on-pump novelty filter could predict failure with up to 61.6% in situ . Incorporating more advanced machine learning methods on the cloud-based platform increased the classifiers’ positive predictive value by at least an additional 10%–73%. This novel method has proven that it is possible for rural operating, resource-constrained devices to use lightweight, onboard machine learning approaches to perform anomaly detection in the embedded system. Distributed inference between the embedded system at the rural node and powerful cloud-based machine learning algorithms offers robust information without the need for expensive hardware or sensors embedded in situ —making the possibility of a large-scale (and perhaps even continent-wide) monitoring system feasible.","['Machine learning', 'Monitoring', 'Condition monitoring', 'Cloud computing', 'Temperature sensors', 'Batteries']","['Distributed inference', 'embedded systems', 'remote condition monitoring', 'machine learning', 'rural development']"
"We propose a novel corridor or hallway gait monitoring system based on radar signal processing, unsupervised learning, and a subject detection, association and tracking method. This paper proposes an algorithm that could be paired with any type of MIMO FMCW radar to capture human gait in a highly cluttered environment without needing radar antenna alteration. We validate algorithm functionality by capturing spatiotemporal gait values (e.g., speed, step points, step time, step length, and step count) of people walking in a long hallway. We show that our proposed algorithm yields an average absolute error for speed estimation between 0.0040 m/s to 0.0435 m/s. These preliminary results demonstrate the promising potential of our algorithm to accurately monitor gait in hallways, which increases opportunities for its applications in institutional and home environments.","['Radar', 'Clutter', 'Signal processing algorithms', 'Legged locomotion', 'Sensors', 'Radar antennas', 'Monitoring']","['Contactless monitoring', 'doppler', 'frequency-modulated continuous-wave', 'multipath', 'multiple-input and multiple-output radar', 'gait analysis']"
"The Magnetic Polarizability Tensor (MPT) is a representative property of an electrically conducting or magnetic object that includes information about the object’s characteristics such as, shape, size and material. The MPT is especially relevant to metal detection (MD) and can be used to improve MD performance by helping to distinguish between objects. This paper describes an instrument intended to measure the MPT of objects such as anti-personnel landmines and metallic clutter, up to 130 mm in diameter. The instrument uses a novel multi-coil geometry to generate a uniform electromagnetic field over the volume containing the test object to accurately determine the MPT. Performance tests of the system shows peak variance in the MPT is approximately 15 mm 3 . Typical experimental repeatability is better than one percent for tests involving copper disks. Additionally, simulated data as well as previously published simulated and experimental data are used as a validation method of the experimental results. Good agreement between these and the measured MPTs of example targets are seen, proving the system’s capability of characterizing metallic objects.","['Coils', 'Landmine detection', 'Metals', 'Magnetic field measurement', 'Transmission line matrix methods', 'Electromagnetic fields', 'Soil measurements']","['Electromagnetic Induction Spectroscopy', 'Magnetic Polarizability Tensor', 'metal detection', 'metal classification']"
"The unmanned aerial vehicle (UAV) assisted communication can significantly improve the coverage and spectrum efficiency of the Internet of Things (IoT). One main feature of IoT is short packet communication (SPC), in which the data transmission uses finite block-length codewords. The transmission rate and the packet error rate will affect the effective throughput of the IoT. In this paper, we investigate the impact of SPC on the performance of UAV-enabled spectrum sharing network. Specifically, the optimization of energy efficiency (EE) in the UAV communication system is considered since the battery of the UAV is usually limited. We design the packet error rate, the sensing duration, the normalized sensing threshold and the UAV’s transmit power to maximize the EE under the constraint that the primary user is sufficiently protected. Since the system parameters are intertwined with each other, we employ a successive optimization algorithm to solve the optimization problem by dividing it into three subproblems. Then, an efficient iterative algorithm is proposed to obtain the optimal parameters. Simulation results reveal that the proposed algorithm can converge to global optimal value and has better EE performance than other benchmark schemes. And there is a fundamental tradeoff between the throughput and the EE.","['Unmanned aerial vehicles', 'Sensors', 'Optimization', 'Error analysis', 'Wireless sensor networks', 'Throughput', 'Internet of Things']","['UAV', 'cognitive Internet of Things', 'short packet communication', 'sensing', 'energy efficiency']"
"Simulation of time-of-flight (ToF) sensors has mainly been used to evaluate depth data processing algorithms, and existing approaches, therefore, focus on the generation of realistic depth data. Thus, current approaches are of limited usefulness for studying alternatives in sensor chip design, since this application area has different requirements. We propose a new physically based simulation model with a focus on realistic and practical sensor parameterization. The model is suitable for implementation on massively parallel processors such as graphics processing units, to allow fast simulation of many sensor frames across a wide range of parameter sets for meaningful evaluation. We use our implementation to evaluate two alternative approaches in continuous-wave ToF sensor design.","['Computational modeling', 'Optical sensors', 'Light sources', 'Lighting', 'Optical imaging', 'Integrated circuit modeling']",['Sensors/sensor phenomena and characterization']
"In acoustic underwater communication and sonar applications, obstacles inside and outside the line-of-sight (LOS) affect signal propagation. Both reflection and diffraction occur in underwater communication and measurement systems due to these obstacles. To the best of our knowledge, the influence of diffraction and reflection is neither described nor modeled for finite pulses yet. We propose and develop a multipath propagation model for spectral diffraction components and phase information of the received signal based on knife-edge diffraction together with reflections, transmission effects, and backscatter. This paper designs a short range underwater ultrasonic experimental system composed of an ultrasonic transceiver with wideband pulses and advanced spectral signal processing. We evaluate our proposed model with measurements made in a water tank with an obstacle moved between the transmitter and receiver. When the model includes all major propagation components and effects, it achieves an accuracy for localization of 97% of the results in the range of twice the obstacle diameter in our test setup.","['Diffraction', 'Acoustics', 'Sensors', 'Ultrasonic variables measurement', 'Mathematical model', 'Receivers', 'Sediments']","['Multipath', 'processing of acoustic wave sensor data', 'propagation model', 'spectral diffraction model', 'ultrasonic sensors', 'underwater', 'wideband pulse']"
"Smart structures of the future will require a cost-effective, easily deployable solution for structural health monitoring. High loads on structures cause stresses that may lead to expansion of gaps, which are of utmost importance when it comes to overall structural health, as they absorb excess stress. Existing methods for direct displacement measurement of expansion joints are not ideal, as they operate under line-of-sight assumptions, are sensitive to moisture, or employ moving parts. In addition, the majority of existing sensors for structural health monitoring are uniaxial, and hence are fundamentally unable to measure 3-D displacement. Importantly, none of the existing wireless sensors for structural health monitoring can be embedded in concrete. We propose a system that uses low-frequency magnetic fields to conduct 3-D displacement measurement directly from within concrete, with a median displacement error of 0.5 mm in all directions, with a maximum separation distance of 50 mm between the transmitter and the receiver. The sensors can be attached to the concrete surface after the building is erected, or can be included in the concrete mix at manufacture, to monitor displacement between gaps in expansion joints, perform crack detection in concrete ties for railroads and in pavements, as well as aid position measurement for the assembly of premanufactured concrete blocks. Embedment in concrete allows operation throughout the lifetime of a structure, providing early warning of impending disaster and helping to inform repair operations.","['Concrete', 'Magnetic sensors', 'Displacement measurement', 'Monitoring', 'Frequency measurement', 'Position measurement']","['Displacement', 'magnetic fields', 'structural health monitoring', 'crack detection', 'damage detection', 'position measurement', 'concrete']"
"A high-sensitivity fiber-optic gas pressure sensor based on a Fabry-Perot interferometer filled with epoxy resin adhesive is proposed. The factors of affecting the pressure sensitivity are theoretically analyzed and verified experimentally. In our experiment, the pressure sensitivity of the epoxy-filled sensor with the thinnest diaphragm thickness of6.61 μmcorresponding to the cavity length of65.45 μmis −530.17 pm/kPa from 200 to 300 kPa. The sensitivity of the proposed pressure sensor with the shortest cavity length of90.43 μmcorresponding to the film thickness of10.65 μmis −378.18 pm/kPa from 200 to 300 kPa. To further improve the sensitivity, the proposed pressure sensor is optimized, which consists of two cascaded Fabry-Perot interferometers separated by a long section of single-mode fiber to form Vernier effect. Such a pressure sensor based on the epoxy-filled Fabry-Perot interferometer and the Vernier effect obtains a high sensitivity of 2.526 nm/kPa in the local measurement range of 233 to 259 kPa and a low detection limit of 0.00892 kPa. With the advantages of high sensitivity, compact structure and simple fabrication, the proposed pressure sensor has potential applications in the biomedical field and ocean exploration.","['Sensitivity', 'Sensors', 'Pressure sensors', 'Optical fiber sensors', 'Sea surface', 'Interference', 'Epoxy resins']","['Fabry-Perot interferometer', 'epoxy resin adhesive', 'fiber-optic gas pressure sensor', 'Vernier effect']"
"This paper presents a mixed-signal closed-loop control system for Lorentz force resonant MEMS magnetometers. The control system contributes to 1) the automatic phase control of the loop, that allows start-up and keeps self-sustained oscillation at the MEMS resonance frequency, and 2) output offset reduction due to electrostatic driving by selectively disabling it. The proposed solution proof-of-concept has been tested with a Lorentz force-based MEMS magnetometer. The readout electronic circuitry has been implemented on a printed circuit board with off-the-shelf components. Digital control has been implemented in an FPGA coded with VHDL. When biased with 1 V and a driving current of 300 \mu ~A_{\textrm {rms}}, the device shows 9.75 pA/ \mu \text{T}sensitivity and total sensor white noise of 550 nT/ \sqrt {\textrm {Hz}}. Offset when electrostatic driving is disabled is 793 \mu \text{T}, which means a 40.1% reduction compared when electrostatic driving is enabled. Moreover, removing electrostatic driving does not worsen bias instability, which is lower than 125 nT in both driving cases.","['Magnetometers', 'Micromechanical devices', 'Wires', 'Electrostatics', 'Magnetic sensors', 'Lorentz covariance']","['MEMS', 'magnetic sensor', 'magnetometer', 'Lorentz force', 'offset suppression', 'digital control']"
"In clinical, the center of pressure (CoP) is commonly used for accessing the stability of a person’s postural control, which is highly associated with various neurological diseases and movement disorders such as Alzheimer’s disease, Parkinson’s disease, chronic ankle instability. Such a disease usually has a long development or rehabilitation process which requires long-term CoP monitoring. The current CoP evaluation process does not meet the requirement, as it is often complicated and expensive through either the lab-based equipment or the clinical evaluation procedure. Different wearable sensor-based systems with less cost and restrictions have emerged, but their way of CoP calculation requires deliberate calibration of the positions of their sensors, which are not feasible in daily CoP monitoring. In this study, we developed a long-term CoP monitoring system in a smart-shoe form. First, a thin and flexible smart insole with optimal sensor locations was designed to be compact and energy sufficient for a whole-day usage. Then, a user-friendly app on the smartphone with a cloud-based data managing system was developed for applications in both clinical and home environments. Additionally, a simplified CoP estimation model was created without the need for calibration. Lastly, a machine learning-based human activity recognition method was incorporated to make the CoP detection process more automatic. Through a thorough validation test with the clinical level lab equipment, our system can generate the CoP measurements with high accuracy.","['Sensors', 'Foot', 'Monitoring', 'Hardware', 'Force', 'Diseases', 'Bluetooth']","['Activity recognition', 'center of pressure (CoP)', 'plantar pressure', 'smart insole', 'static postural control']"
"Food portion size estimation (FPSE) is critical in dietary assessment and energy intake estimation. Traditional methods such as visual estimation are now replaced by faster, more accurate sensor-based methods. This article presents a comprehensive review of the use of sensor methodologies for portion size estimation. The review was conducted using the PRISMA guidelines and full texts of 67 scientific articles were reviewed. The contributions of this article are three-fold: i) A taxonomy for sensor-based (SB) FPSE methods was identified, classifying the sensors (as wearable, portable and stationary) and the methodology (as direct and indirect). ii) A novel comprehensive review of the state-of-the-art SB-FPSE methods was conducted and 5 sensor modalities (Acoustic, Strain, Imaging, Weighing, and Motion sensors) were identified. iii) The accuracy of portion size estimation and the applicability to free-living conditions of these SB-FPSE methods were assessed. This article concludes with a discussion of challenges and future trends of SB-FPSE.","['Sensors', 'Estimation', 'Size measurement', 'Wearable sensors', 'Sensor phenomena and characterization', 'Taxonomy', 'Systematics']","['Sensors', 'food intake sensors', 'food portion size estimation', 'food volume', 'dietary assessment']"
"An in situ optical oyster heart rate sensor generates signals requiring frequency estimation with properties different to human ECG and speech signals. We discuss the method of signal generation and highlight a number of these signal properties. An optimal heart rate estimation approach was identified by application of a variety of frequency estimation techniques and comparing results to manually acquired values. Although a machine learning approach achieved the best performance, accurately estimating 96.8% of the heart rates correctly, a median filtered autocorrelation approach achieved 93.7% with significantly less computational requirement. A method for estimating heart rate variation is also presented.","['Heart rate', 'Correlation', 'Optical sensors', 'Frequency estimation', 'Estimation']","['Biomedical signal processing', 'Frequency estimation', 'Machine learning']"
"Convolutional neural network (CNN), one of the branches of deep neural networks, has been widely used in image recognition, natural language processing, and other related fields with great success recently. This paper proposes a novel framework with CNN to classify objects in a point cloud captured by LiDAR on urban streets. The proposed BA-CNN algorithm is composed of five steps: (i) removing ground points, (ii) clustering objects, (iii) transforming to bearing angle images, (iv) ROI selection, and (V) identifying objects by CNN. In the first step, ground points are removed by the multi-threshold-based ground detection to reduce the processing time. Then, a flood-fill-based clustering method is used for object segmentation. Those individual point cloud objects are converted to bearing angle (BA) images. Then, a well-trained CNN is used to classify objects with BA images. The main contribution of this paper is proposing an efficient recognition method that uses the information from point clouds only. In contrast, because most 3D object classifiers use the fusion of point clouds and color images, their models are very complicated and take a colossal amount of memory to store the parameters. Since the ground point detection and object clustering process all points along with the scanline-major order and layer-major order, the proposed algorithm performs better in terms of time consumption and memory consumption. In the experiment, three scenes from KITTI dataset are used for training and testing the proposed BA-CNN classifier, and the proposed BA-CNN achieves high classification accuracy.","['Three-dimensional displays', 'Laser radar', 'Convolutional neural networks', 'Sensors', 'Feature extraction', 'Object recognition', 'Image recognition']","['Bearing angle image', 'instance segmentation', 'object classification', 'convolutional neural network (CNN)', 'light detection and ranging (LiDAR)', 'point cloud segmentation']"
"Effects of a fully natural aging of MEMS accelerometers are evaluated with regard to changes in their performance. Two models of commercial dual-axis accelerometers (two pieces of ADXL 202E and 203 by Analog Devices Inc.) with analog outputs were tested over a period of about 10 and 4 years, respectively. A custom computer controlled test rig was used for performing relevant experimental studies, employing the gravitational acceleration as the reference source. A methodology of determining the proposed indicators of aging phenomena is presented and discussed. Changes of the offset voltage and the scale factor were observed and a way of evaluating the overall error due to combined influence of these two parameters is proposed. It was found out that the changes of the output voltage generated by the tested accelerometers were considerable, resulting in respective maximal errors of about 52 mg (2.6%) (ADXL 202E) or 20 mg (1%) (ADXL 203). Simple ways of reducing the effects of aging are proposed.","['Accelerometers', 'Aging', 'Micromechanical devices', 'Sensors', 'Calibration', 'Acceleration', 'Temperature measurement']","['Aging', 'lifetime stability', 'long-term reliability', 'MEMS accelerometer', 'accuracy', 'tilt']"
"The ability to orientate and navigate is critically important for the survival of all migratory birds and other animals. Progress in understanding the mechanisms underlying these capabilities and, in particular, the importance of sensitivity to the Earth's magnetic field has, thus far, been constrained by the limited number of techniques available for the analysis of often complex behavioral responses. Methods used to track the movements of animals, such as birds, have varied depending on the degree of accuracy required. Most conventional approaches involve the use of a camera for recording and then measuring an animal's head movements in response to a variety of external stimuli, such as changes in magnetic fields. However, video tracking analysis (VTA) will generally provide only a 2D tracking of head angle. Moreover, such a video analysis can only provide information about movements when the head is in view of the camera. In order to overcome these limitations, the novel invention reported here utilizes a lightweight (<;10 g) inertial measurement unit (IMU), positioned on the head of a homing pigeon, which contains a sensor with tri-axial orthogonal accelerometers, gyroscopes, and magnetometers. This highly compact (20.3 × 12.7 × 3 mm) system can be programmed and calibrated to provide measurements of the three rotational angles (roll, pitch, and yaw) simultaneously, eliminating any drift, i.e., the movement of the pigeon's head is determined by detecting and estimating the directions of motion at all angles (even those outside the defined areas of tracking). Using an existing VTA approach as a baseline for comparison, it is demonstrated that the IMU technology can comprehensively track a pigeon's normal head movements with greater precision and in all three axes.","['Birds', 'Head', 'Magnetic heads', 'Tracking', 'Sensors', 'Accelerometers', 'Cameras']","['Animal behavior', 'tracking', 'inertial measurement unit (IMU)', 'monitoring behavior', 'image recognition', 'accelerometer']"
"We develop a field-programmable gate array (FPGA)-based multichannel measurement system for sensing multiplexed fiber Bragg gratings (FBGs), using a high-speed wavelength-swept laser. The wavelength-swept laser, operated with Fourier-domain mode locking (FDML), exhibits a sweep frequency of 50.7 kHz and sweep band of ~60 nm. A breakthrough in multichannel and real-time measurement is achieved by implementing an FPGA with unique parallel-processing circuits. This FPGA enables the implementation of a high-speed centroid-peak-detection circuit for FBG detection, which can be operated with a sampling frequency of 250 MHz. In multichannel measurement systems, the light propagating through each sensor path produces a delay, which reduces the measurement accuracy. This can be solved by implementing a delay-correction technique, which utilizes the time difference between two spectra obtained by a bidirectional scan of the FDML laser. The system is demonstrated to exhibit multichannel sensing and high measurement time resolution of 9.9 μs, without being affected by the delay.","['Delays', 'Wavelength measurement', 'Measurement by laser beam', 'Field programmable gate arrays', 'Real-time systems', 'Optical fiber sensors']","['Field-programmable gate arrays', 'optical fiber sensors', 'sensor systems', 'system integration']"
"In this paper, we propose and demonstrate a new method to reconstruct the Brillouin frequency shift profile in high spatial resolution Brillouin optical frequency domain analysis (BOFDA) sensors. The method aims to compensate the distorting terms affecting BOFDA measurements, which originate from the modulation impressed on the acoustic wave involved in the scattering phenomenon. We show that these terms can be easily removed by applying a high-pass filter to the data acquired in the frequency-domain, at the cost of a degradation of the signal-to-noise ratio. A numerical analysis, as well as experimental tests carried out at 8-mm spatial resolution, validate the proposed technique.","['Scattering', 'Optical fiber sensors', 'Frequency modulation', 'Spatial resolution', 'Acoustic waves', 'Transfer functions']","['Brillouin scattering', 'optical fiber sensors', 'strain measurement', 'temperature measurement']"
"We develop a high-speed interrogation system with a wavelength-swept laser for sensing multiplexed fiber Bragg gratings (FBGs). High-speed and multipoint sensing is achieved by means of a buffered Fourier domain mode-locked (FDML) laser, a recently proposed high-speed wavelength-swept laser. This laser increases the measurement rate several times by tailoring the laser output with a buffer stage. The developed buffered FDML laser successfully achieves a measurement rate of 202.8 kHz using a general FDML laser driven at a sweep rate of 50.7 kHz. In order to detect reflection signals of FBGs at the high measurement rate, the measurement system introduces digital signal processing using a field programmable gate array (FPGA). However, FBG measurements with wavelength-swept lasers are affected by the propagation time (delay) in the optical fiber connecting from the laser to the FBG sensor, which reduces measurement accuracy. To overcome this limitation, the developed system uses a delay correction method with bidirectional sweep of the buffered FDML laser. The interrogation system with the buffered FDML laser achieves a time resolution of 4.9 μs without being affected by the delay.","['Measurement by laser beam', 'Optical buffering', 'Fiber lasers', 'Wavelength measurement', 'Sensors', 'Optical fiber sensors', 'Delays']","['Optical fiber sensors', 'fiber lasers', 'sensor systems', 'real-time systems']"
"This paper is to develop analytical models for the underwater capacitive micromachined ultrasonic transducer (CMUT) to understand its large deflection effect from the water pressure. To accurately model the displacement profile of the CMUT under the water pressure, Von Kármán equations and the perturbation method are employed to calculate the membrane deformation from a uniform pressure. The equations for an annular-ring plate model are first applied to calculate the displacement profile of the uniform CMUT membrane. The lateral force due to the membrane elongation is considered in the proposed model, which is used to calculate the displacement profiles for both conventional and collapse mode CMUTs under different external pressures. When compared with finite-element method results, the proposed model can predict the displacement profiles of the conventional-mode CMUT under water pressure ranging from 0.8 to 4 MPa with an error of <;1%. It can also estimate CMUT membrane that operates in collapse mode with an error in the deflection profile for <;4.7% from 5 to 14 MPa. In addition, it is worth to mention that the proposed model can cover the small deflection scenarios but with relatively larger error under collapse mode.","['Mathematical model', 'Force', 'Electrodes', 'Ultrasonic imaging', 'Substrates', 'Finite element analysis', 'Analytical models']","['CMUT', 'lateral force', 'perturbation method', 'Von Kármán equations']"
"The development of soft electronics is critical to the realization of artificial intelligence that comes into direct contact with humans, such as wearable devices, and robotics. Furthermore, rapid prototyping and inexpensive processes are essential for the development of these applications. We demonstrate here an additive, low-cost method for fabricating polydimethylsiloxane based soft electronics by inkjet printing. Herein, a novel approach using a water-soluble polyvinyl alcohol layer as the substrate, inexpensive, fully digital fabrication of capacitive pressure sensors is enabled by sandwiching mesh-like conductive layers and microstructured dielectric in a straightforward, convenient manner. These sensors exhibit improved sensitivity (4 MPa −1 ) at low pressures (< 1 kPa) in contrast to sensors with a flat elastomer dielectric and can still detect large pressures around 50 kPa, having excellent long-term repeatability over 2000 cycles, without significant hysteresis (≤ 8.5 %). The tactile sensing ability of the fabricated devices was demonstrated in a practical application. Moreover, sensor characteristics are easily adjustable, simply by changing printing parameters or tuning the ink solution. The proposed approach provides scalable solution for fabricating high-sensitivity printed sensors for e-skin and human-machine interfaces.","['Sensors', 'Dielectrics', 'Electrodes', 'Sensor phenomena and characterization', 'Fabrication', 'Capacitive sensors', 'Resistance']","['Polydimethylsiloxane', 'capacitive pressure sensor', 'inkjet printing', 'printed electronics', 'soft electronics', 'polyvinyl alcohol']"
"A microcantilever at the end face of an integrated optical fiber is reported, fabrication is uniquely achieved using a precision dicing saw. The methodology is a single-step rapid process, capable of achieving trenches with high aspect ratio (>10:1). The platform on which fabrication is made is a monolithic, integrated optical fiber. This integrally fuses optical fiber to a planar substrate using flame hydrolysis deposition and high temperature consolidation (>1000 °C). This paper is the first report of a fiber-tip cantilever using the technique and this integrated platform. As an approach to quantify the optical response of such a multicavity arrangement, a method using Mason's rule is presented. This is used to infer the spectral responses of individual cavities formed and through physical actuation, an estimation of the cantilever's spring constant is made.","['Cavity resonators', 'Optical fiber sensors', 'Optical fibers', 'Optical device fabrication', 'Optical interferometry']","['Integrated optics', 'optical fiber devices', 'optical fibers', 'optical interferometry', 'optical sensors']"
"We propose an interrogation-system with automatic recognition and delay correction functions of fiber Bragg gratings (FBGs) by pulse modulation with a wavelength-swept laser. By pulse-modulating the light of a wavelength-swept laser, light with an arbitrary wavelength bandwidth can be extracted as the pulsed light. In the automatic recognition function of FBGs, the pulsed light is sequentially controlled to match the wavelength bandwidth of each FBG. This recognition method enables the selection and detection of a reflection signal from a single FBG. Therefore, reflection signals from multiple FBGs can be recognized individually. When multiple FBGs are installed at long distances, the reflection signal of each FBG is affected by the propagation time (delay). In the interrogation system with the wavelength-swept laser, the delay lowers the measurement accuracy. Therefore, a delay correction function using bidirectional sweeping of the wavelength-swept laser is used. The wavelength-swept laser using Fourier domain mode locking (FDML) is driven at a sweep frequency of 50.7 kHz with a sweep bandwidth of ~60 nm. This study demonstrates that pulse-modulated FDML laser can automatically recognize reflection signals from multiple FBGs installed at arbitrary long distances. When the recognition process is complete, the interrogation system can perform real-time measurement with a time resolution of 9.9 μs, without being affected by delay that occurs when installing at a long distance.","['Reflection', 'Delays', 'Fiber gratings', 'Measurement by laser beam', 'Optical fiber sensors', 'Wavelength measurement']","['Optical fiber sensors', 'fiber lasers', 'sensor systems', 'real-time systems']"
"A new fiber-optic system with an active unbalanced Michelson interferometer (AUMI) was designed for multi-zone perimeter intrusion detection by employing only two single-mode fibers for each sensing zone. A pump laser at the monitor site was used to remotely pump the erbium-doped fibers located, respectively, at the two arms of the AUMI for each sensing zone. Configured into a fiber laser cavity, the AUMI served as an optical switch that would modulate the optical power when one arm of the AUMI was perturbed by intrusion to induce a phase offset variation. In the study, an armored cable with a length of 200 meters in total was divided into four sections each guarding a perimeter zone. The experimental results showed that the AUMI system correctly responded to perturbations induced by footstepping on a fiber cable buried underground, vibrating a netted fence, or knocking a window. A high alarm-upon-intrusion rate and zero false alarm rate could be reached by using the presented detection algorithm.","['Optical fiber cables', 'Optical fiber sensors', 'Optical fiber couplers', 'Optical interferometry', 'Laser excitation']","['Pump laser', 'Michelson interferometer', 'multi-zone perimeter intrusion detection', 'fiber laser cavity']"
"In this study, we developed an odor sensor system using chemosensitive resistors, which outputted multichannel data. Mixtures of gas chromatography stationary materials (GC materials) and carbon black were used as the chemosensitive resistors. The interaction between the chemosensitive resistors and gas species shifted the electrical resistance of the resistors. Sixteen different chemosensitive resistors were fabricated on an odor sensor chip. In addition, a compact measurement instrument was fabricated. Sixteen channel data were obtained from the measurements of gas species using the instrument. The data were analyzed using machine learning algorithms available on Weka software. As a result, the sensor system successfully identified alcoholic beverages. Finally, we demonstrated the classification of restroom odor in a field test. The classification was successful with an accuracy of 97.9%.","['Annealing', 'Resistors', 'Sensor phenomena and characterization', 'Sensor systems', 'Resistance', 'Chemical sensors']","['GC materials', 'carbon black', 'odor sensor', 'artificial olfaction', 'chemical sensor', 'sensor array', 'odor discrimination', 'Weka']"
"A commonly used parameter in Structural Health Monitoring (SHM) to ensure the reliability and structural integrity of a lightweight structure is the mechanical strain. Usually, a strain gauge is the standard solution to measure local strain. The aim of this paper is to measure strain through two-dimensional Electrical Impedance Tomography (2D EIT) in combination with a planar elastoresistive sensor. Thereby, the quantitative value of the conductivity change as a function of strain is reconstructed. The advantage of this method compared to a conventional strain gauge is that the EIT allows simultaneous and spatial strain and damage monitoring by a single sensor. In the presented test setup, the sensor is bent uniformly over the sensor surface and the calculated EIT results are verified by using the Montgomery method. The article presents the main steps for strain measurement using 2D EIT, starting with an appropriate derivation of a Finite Element (FE) model of the sensor as well as the measurement data processing. Furthermore, the selection of the hyperparameter, which controls the EIT reconstruction algorithm, is presented. The selection of a suitable hyperparameter is essential for the reconstruction of actual conductivity values. The presented selection approach is based on the combinational use of the L-Curve (LC) method and the fixed Noise Figure (NF) method.","['Sensors', 'Strain', 'Tomography', 'Strain measurement', 'Conductivity', 'Voltage measurement', 'Monitoring']","['Electrical Impedance Tomography (EIT)', 'hyperparameter', 'Montgomery method', 'strain sensor', 'Structural Health Monitoring (SHM)']"
"Large area graphene-poly (methyl methacrylate) (PMMA) closed cavity resonator has been fabricated. The resonator has been formed by transferring an ultra-large graphene-PMMA membrane over 3.5 mm diameter circular closed cavity with 220 μm depth. The graphene-PMMA membrane includes 6-layer graphene and 450 nm PMMA film. A modified graphene-PMMA dry transfer method has been developed in this work. Using the Kapton tape supporting frame, the graphene-PMMA membrane has been dry transferred onto the substrate with a small membrane's static deformation of around 180 nm. The membrane's static deformation aspect ratio (suspended membrane's diameter over the membrane's deformation) is around 19,500. The graphene-PMMA closed cavity resonator has been actuated mechanically, acoustically and electro-thermally. The dynamic behaviour of the membrane suspended over the closed cavity shows that the (1, 1) mode dominates the graphene-PMMA membrane's resonance with a resonant frequency of around 10 kHz and suggests the device is under good gas encapsulation. Acoustic vibration amplitude sensitivity of graphene-PMMA membrane over the closed cavity is measured to be around 6 μm/Pa. The membrane's dynamic behaviour, simulated under similar mechanical and electro-thermal actuation conditions, has been shown to be consistent with the trend of the device's experimental results. The strain in the suspended graphene-PMMA membrane is estimated to be 0.04 ± 0.01 %.","['Graphene', 'Substrates', 'Cavity resonators', 'Strain', 'Resonant frequency', 'Optical resonators', 'Sensors']","['Graphene', 'graphene dry transfer', 'PMMA', 'Raman', 'resonant frequency', 'acoustic resonator', 'MEMS']"
"The design and test of a robot-compatible, radiation detection instrument providing simultaneous γ-ray imaging and γ-ray spectroscopy is described. The sensing system comprises a cerium bromide inorganic scintillation detector and a cylindrical, lead slot collimator that is configured with a robot-compatible, on-board data acquisition system. The mount for the sensor is a lightweight, bespoke 3-axis gimbal actuated by servos for pan, tilt and rotation of the collimator for imaging capability. This paper discusses the integration of this relatively low-cost radiation detection apparatus with a commercially available, Robot-Operating-System-controlled robotic platform (a Clearpath Robotics™ Jackal). The detection system is compliant with the power and mass payload constraints of the robot. Its performance has been evaluated by means of two practical examples: a) measurements in a laboratory environment to assess the ability of the system to resolve two caesium-137 point sources, and b) deployment at the Jožef Stefan Institute TRIGA Mark II research reactor to assess the ability of the system to characterise the γ-ray emission at 1 kW from a horizontal tangential beam port in the reactor hall and from the reactor sample pool above the core.","['Robot sensing systems', 'Instruments', 'Imaging', 'Detectors', 'Collimators', 'Inductors', 'Lead']","['Robotics and automation', 'environmental monitoring and control', 'detection', 'estimation and classification based on sensor data']"
"Device wearability and operating time are trending topics in recent state-of-art works on surface ElectroMyoGraphic (sEMG) muscle monitoring. No optimal trade-off, able to concurrently address several problems of the acquisition system like robustness, miniaturization, versatility, and power efficiency, has yet been found. In this tutorial we present a solution to most of these issues, embedding in a single device both an sEMG acquisition channel, with our custom event-driven hardware feature extraction technique (named Average Threshold Crossing), and a digital part, which includes a microcontroller unit, for (optionally) sEMG sampling and processing, and a Bluetooth communication, for wireless data transmission. The knowledge acquired by the research group brought to an accurate selection of each single component, resulting in a very efficient prototype, with a comfortable final size (57.8mm \times25.2mm \times22.1mm) and a consistent signal-to-noise ratio of the acquired sEMG (higher than 15 dB). Furthermore, a precise design of the firmware has been performed, handling both signal acquisition and Bluetooth transmission concurrently, thanks to a FreeRTOS custom implementation. In particular, the system adapts to both sEMG and ATC transmission, with an application throughput up to 2 kBs ^{-1}and an average operating time of 80 h (for high resolution sEMG sampling), relaxable to 8Bs ^{-1}throughput and about 230 h operating time (considering a 110 mAh battery), in case of ATC acquisition only. Here we share our experience over the years in designing wearable systems for the sEMG detection, specifying in detail how our event-driven approach could benefit the device development phases. Some previous basic knowledge about biosignal acquisition, electronic circuits and programming would certainly ease the repeatability of this tutorial.","['Muscles', 'Monitoring', 'Feature extraction', 'Biomedical monitoring', 'Wireless sensor networks', 'Wireless communication', 'Force']","['Bio-inspired electronics', 'surface electromyography', 'bluetooth low energy', 'event-based', 'information synthesis', 'power efficiency', 'wearable system']"
"Recent improvements in cameras and image processing techniques have advanced research on object detection and classification. The cameras capable of capturing omnidirectional images are advantageous for detecting surrounding objects, we focus on their usage method. Omnidirectional camera images generally have lower resolution than available cameras, leading to difficulty in identifying objects farther from the camera. To address this problem, we propose a hybrid camera system that first detects indistinct target regions during omnidirectional viewing, then uses a pan-tilt camera to capture a high-resolution image of the target. We also propose an algorithm for reducing the number of required complementary shots to realize efficient shooting.","['Cameras', 'Image resolution', 'Surveillance', 'Object recognition', 'Imaging', 'Object detection', 'Image color analysis']","['Omnidirectional camera', 'hybrid camera system', 'high-resolution image capturing', 'PT camera control', 'indistinct image region']"
"A laser radar receiver channel targeted at pulsed time-of-flight laser radar applications using 5ns…20ns laser pulses at the 1550nm wavelength region has been designed. The receiver includes a transimpedance preamplifier, post-amplifiers and analog output buffer and has a bandwidth of 15MHz and input reduced total equivalent current noise of ~3nArms without the contribution of the APD noise. The receiver supports both the single shot and transient recording operation modes. By utilizing the time domain compensation methods, the single shot timing walk is at the level of +/−100ps within a wide dynamic range of 1: 36 000 of the received echo amplitudes. It is demonstrated that the designed receiver allows the pulsed time-of-flight measurement in the single shot mode to non-cooperative targets up to distances of several hundreds of meters with a laser pulse power of 4 W and receiver aperture of 20mm.","['Receivers', 'Laser radar', 'Avalanche photodiodes', 'Timing', 'Signal to noise ratio', 'Semiconductor lasers', 'Measurement by laser beam']","['Laser radar', 'lidar', 'optical receiver', 'timing discrimination']"
"This paper describes the design, implementation, and evaluation of an environmental logging microsystem (ELM) for operation at elevated pressure and in corrosive environments, at temperatures up to 125°C. The ELM units are intended to be deployed in large quantities, allowed to collect data, and then retrieved, interrogated, and re-charged. Powered by a rechargeable battery embedded within the system, each ELM incorporates pressure and temperature sensors, control electronics, optical communication elements, and power management and battery re-charging circuits. The pressure sensor is a customized capacitive transducer chip on a sapphire substrate; details are provided in a companion paper. The electronic components and battery used in ELM are selected on the basis of functionality and form factor; packaged components are selected for ease of assembly and for added protection against the environment. The pressure sensor, electronics and battery are assembled on a flexible circuit board, folded into a stack with dimensions 7.2 mm × 6.6 mm × 6.5 mm, and encapsulated in a steel tube filled with optically transparent silicone caulk. This encapsulation provides mechanical protection against shock and abrasion, as well as chemical protection against high salinity environments, while allowing the ambient pressure and temperature to be transferred to the sensing elements. Results are reported from high-temperature and high-pressure tests reaching 125°C and 7,250 psi in brine and other corrosive environments in laboratory conditions. Field tests that were conducted in a brine well to a maximum depth of 1,235 m are also described. The recorded data were post-processed to interpret the environmental pressure and temperature.","['Temperature sensors', 'Sensors', 'Pressure sensors', 'Micromechanical devices', 'Temperature measurement', 'Temperature distribution', 'Steel']","['Low power microsystem', 'downhole sensing', 'high pressure', 'upstream', 'Internet of Things']"
"This paper presents a CMOS photonic sensor covering multiple applications from ambient light sensing to time resolved photonic sensing. The sensor is made of an array of gated pinned photodiodes (PPDs) averaged using binning and passive switched-capacitor (SC) charge sharing combined with ultra-low-power amplification and analog-to-digital conversion. The chip is implemented in a 180 nm CMOS image sensor (CIS) process and features high sensitivity, low-noise and low-power performance. Measurement results demonstrateμWhealth monitoring through Photoplethysmography (PPG), 10 ps resolution for time resolved light sensing and mm precision for time-of-flight (ToF) distance ranging obtained with a frame rate of 50 Hz and 20 dB ambient light rejection.","['Sensor phenomena and characterization', 'Photonics', 'Optoelectronic and photonic sensors', 'Semiconductor device measurement', 'Photodiodes', 'Light emitting diodes']","['PPD', 'PPG', 'single-point ToF', 'time resolved', 'ultra-low-power', 'CMOS', 'sensor']"
"The past years have witnessed a boost in fall detection-related research works, disclosing an extensive number of methodologies built upon similar principles but addressing particular use-cases. These use-cases frequently motivate algorithm fine-tuning, making the modelling stage a time and effort consuming process. This work contributes towards understanding the impact of several of the most frequent requirements for wearable-based fall detection solutions in their performance (usage positions, learning model, rate). We introduce a new machine learning pipeline, trained with a proprietary dataset, with a customisable modelling stage which enabled the assessment of performance over each combination of custom parameters. Finally, we benchmark a model deployed by our framework using the UMAFall dataset, achieving state-of-the-art results with an F1-score of 84.6% for the classification of the entire dataset, which included an unseen usage position (ankle), considering a sampling rate of 10 Hz and a Random Forest classifier.","['Sensors', 'Accelerometers', 'Data models', 'Machine learning algorithms', 'Wrist', 'Machine learning']","['Fall detection', 'inertial sensors', 'accelerometer', 'machine learning', 'deep learning']"
"The measurement of nanophotonic sensors currently requires the use of external measuring equipment for their read-out such as an optical spectrum analyzer, spectrophotometer, or detectors. This requirement of external laboratory-based measuring equipment creates a “chip-in-a-lab” dilemma and hinders the use of nanophotonic sensors in practical applications. Making nanophotonic sensors usable in everyday life requires miniaturization of not only the sensor chip itself but also the equipment used for its measurement. In this paper, we have removed the need of external measuring equipment by monolithically integrating 1-D grating structures with a complementary metal-oxide-semiconductor (CMOS) integrated circuit having an array of photodiodes. By doing so, we get a direct electrical read-out of the refractive index changes induced when applying different analytes to grating structures. The gratings are made of CMOS compatible silicon nitride. Employing a nanophotonic sensor made of CMOS compatible material allows fabrication of the integrated sensor chip in a commercial CMOS foundry, enabling mass production for commercialization with low cost. Our results present a significant step toward transforming present laboratory-based nanophotonic sensors into practical portable devices to enable applications away from the analytical laboratory. We anticipate the work will have a major impact on technology for personalized medicine, environmental, and industrial sensing.","['Gratings', 'Sensor systems', 'Semiconductor device measurement', 'Optical sensors', 'Detectors', 'CMOS technology']","['Diffraction gratings', 'nanophotonics', 'optoelectronics', 'photodetectors', 'sensors']"
"The immense progress in physiological signal acquisition and processing in health monitoring allowed a better understanding of patient disease detection and diagnosis. With the increase in data volume and power consumption, effective data compression, signal acquisition, transmission, and processing techniques are essential, especially in telemonitoring healthcare applications. An emerging research area focuses on integrating compressed sensing (CS) with physiological signals to deal with a massive amount of physiological data, transmission bandwidth, and power-saving purposes. A review of CS for physiological signals is presented in this article, including electroencephalography (EEG), electrocardiography (ECG), electromyography (EMG), and electrodermal activity (EDA), focusing on the pros and cons of CS in treating such signals and the suitability of CS for hardware implementation. Furthermore, we emphasize performance matrices, such as compression ratio (CR), signal-to-noise ratio (SNR), Percentage Root-mean-square Difference (PRD), and processing time to evaluate the performance of CS. We also investigate the current practices, challenges, and opportunities of using CS in healthcare applications.","['Biomedical monitoring', 'Compressed sensing', 'Feature extraction', 'Sparse matrices', 'Electroencephalography', 'Electromyography', 'Electrocardiography']","['Compressed measurements', 'compressed sensing (CS)', 'electrocardiography (ECG)', 'electrodermal activity (EDA)', 'electroencephalography (EEG)', 'electromyography (EMG)', 'galvanic skin response (GSR)', 'physiological signals']"
"A recent investigation explored a new measuring concept used in partial discharges (PD) measurements in gas insulated substations (GIS), consisting of a magnetic loop antenna. The sensor’s frequency response was characterized up to some tens of MHz. This paper proposes an improved version of the sensor with an extended bandwidth (BW) one order of magnitude higher: a resonance, attributed to a common mode current in the mounting hole, is identified and eliminated employing ferrite beads in the feeder cables. Moreover, this publication proposes an electric circuit model that fully covers the transverse electromagnetic mode (TEM) frequency range in GIS. The electric model is compared against experimental measurements using a 1 GHz bandwidth testbench, giving accurate results. Two contributions are achieved in this research: an improved magnetic loop antenna with extended bandwidth and an accurate electric circuit model. This publication paves the way for further research on time resolution and signal postprocessing techniques for magnetic loop antennas in GIS.","['Antennas', 'Antenna measurements', 'Partial discharges', 'Substations', 'Sensor phenomena and characterization', 'Magnetic resonance', 'Gas insulation']","['Magnetic loop antenna', 'partial discharges', 'transfer function', 'electric circuit', 'GIS', 'broadband antenna', 'VHF', 'high voltage']"
"This study explores the potential for using FBG strain sensing to enable recognition of the shaft misalignment condition in electric machine drivetrains through observation of machine frame distributed relative strain. The sensing principles, design and installation methods of the proposed technique are detailed in the paper. The scheme was applied on a purpose built wind turbine generator representative laboratory test rig and its performance evaluated in an extensive experimental study involving a range of healthy and misaligned shaft operating conditions. The obtained experimental data demonstrate the reported method's capability to enable recognition of generator shaft misalignment conditions and thus its health monitoring. Finally, it is shown that the thermal variation of the generator frame structure inherent to its operation, combined with the FBG sensor intrinsic thermo-mechanical cross sensitivity, has no detrimental impact on the fidelity and usability of the observed strain measurements.","['Sensors', 'Shafts', 'Monitoring', 'Fiber gratings', 'Strain', 'Generators']","['Electric machines', 'FBG sensors', 'misalignment monitoring', 'strain sensing', 'wind turbine generators']"
"Moisture distribution information is a critical element in drying processes. The drying of products by employing high-power microwave (MW) technology is widely used in the industry. Although microwaves allow volumetric and selective heating resulting in a significant reduction of processing time and energy consumption, there is always a risk of non-uniform moisture distribution in the final product. This paper investigates the capability of a designed electrical capacitance tomography (ECT) sensor to estimate the moisture distribution of polymer foams in a microwave drying process. The moisture distribution is estimated based on the non-intrusive contactless measurements of the electrical capacitances between the electrodes mounted on a frame around the target polymer foam. The obtained moisture information can be employed as feedback to a controller to adjust the power level of each microwave source in the microwave system to reduce or eliminate the non-homogeneity of the moisture distribution inside the polymer foam. In a series of experiments, we first examine the capability of the ECT sensor in estimating the moisture distribution in a stationary foam. We extend the tests to estimating the moisture distribution in a case where the foam is moving on a conveyor belt. Several study variables are taken, including the sample size, the sample location, the moisture percentage, the conveyor belt speed, and the microwave power. These experiments show that the sensor has a satisfactory accuracy in estimating the moisture distribution of the foam, and the ECT measurements can be further used in a closed-loop control system.","['Moisture', 'Electrodes', 'Microwave ovens', 'Electromagnetic heating', 'Microwave imaging', 'Microwave measurement', 'Electric variables measurement']","['Electrical capacitance tomography', 'moisture measurements', 'microwave drying']"
"Single Photon Avalanche Diode sensor arrays operating in direct time of flight mode can perform 3D imaging using pulsed lasers. Operating at high frame rates, SPAD imagers typically generate large volumes of noisy and largely redundant spatio-temporal data. This results in communication bottlenecks and unnecessary data processing. In this work, we propose a neuromorphic processing solution to this problem. By processing the spatio-temporal patterns generated by the SPADs in a local, event-based manner, the proposed 128\times 128pixel sensor-processor system reduces the size of output data from the sensor by orders of magnitude while increasing the utility of the output data in the context of challenging recognition tasks. To test the proposed system, the first large scale complex SPAD imaging dataset is captured using an existing 32\times 32pixel sensor. The generated dataset consists of 24000 recordings and involves high-speed view-invariant recognition of airplanes with background clutter. The frame-based SPAD imaging dataset is converted via several alternative methods into event-based data streams and processed using the proposed 125\times 125receptive field neuromorphic processor as well as a range of feature extractor networks and pooling methods. The output of the proposed event generation methods are then processed by an event-based feature extraction and classification system implemented in FPGA hardware. The event-based processing methods are compared to processing the original frame-based dataset via frame-based but otherwise identical architectures. The results show the event-based methods are superior to the frame-based approach both in terms of classification accuracy and output data-rate.","['Sensors', 'Photonics', 'Airplanes', 'Image sensors', 'Atmospheric modeling', 'Neuromorphics']","['Single photon avalanche diode', 'event-based vision', 'event-based processors', 'feature extraction']"
"Floor Sensors (FS) are used to capture information from the force induced on the contact surface by feet during gait. On the other hand, the Ambulatory Inertial Sensors (AIS) are used to capture the velocity, acceleration and orientation of the body during different activities. In this paper, fusion of the stated modalities is performed to overcome the challenge of gait classification from wearable sensors on the lower portion of human body not in contact with ground as in FS. Deep learning models are utilized for the automatic feature extraction of the ground reaction force obtained from a set of 116 FS and body movements from AIS attached at 3 different locations of lower body, which is novel. Spatio-temporal information of disproportionate inputs obtained from the two modalities is balanced and fused within deep learning network layers whilst reserving the categorical content for each gait activity. Our approach of fusion compensates the degradation in spatio-temporal accuracies in individual modalities and makes the overall classification outcomes more accurate. Further assessment of multi-modality based results show significant improvements in f-scores using different deep learning models i.e., LSTM (99.90%), 2D-CNN (88.73%), 1D-CNN (94.97%) and ANN (89.33%) respectively.","['Sensors', 'Artificial intelligence', 'Feature extraction', 'Sensor phenomena and characterization', 'Sensor fusion', 'Acceleration', 'Foot']","['Floor sensors', 'ambulatory inertial sensors', 'inertial measurement unit', 'deep learning', 'artificial neural networks', 'convolutional neural networks', 'long short-term memory']"
"Objective: Utilization of inertial measurement units (IMU) data for ground reaction force (GRF) prediction has been widely studied and documented when these sensors attach to the body segments. However, it was inconvenient and required people's cooperation. A novel approach of the current study was setting IMU sensors mounted underneath the walking surface to measure footstep induced structural vibration. We aimed to conduct the force plate to validate the prediction accuracy of this approach. Methods: Fifteen hundred steps were recorded from five individuals. Twenty-four measured features from four IMU sensors were treated as inputs to the long short-term memory model for multidimensional GRF predictions. The GRF data from the force plate were considered as the ground truth for comparisons. The accuracy performance was determined by the normalized root mean square error (NRMSE) method. Results: The averaged NRMSE was 6.05%, 3.93%, and 4.37% for Fx, Fy, and Fz, respectively. Conclusion: The accuracy was comparable with IMU sensors attached to the body, particularly in the vertical direction. The current study demonstrated the feasibility of this approach and successfully predicted ground reaction force with high accuracy. Significance: The validation of IMU sensors mounted underneath the walking surface for GRF prediction provides an alternative method for biometrics in gait.","['Legged locomotion', 'Force', 'Sensor phenomena and characterization', 'Predictive models', 'Accelerometers', 'Gyroscopes']","['Inertial measurement unit', 'ground reaction force', 'long short-term memory', 'gait']"
"Collaborative navigation is the most promising technique for infrastructure-free indoor navigation for a group of pedestrians, such as rescue personnel. Infrastructure-free navigation means using a system that is able to localize itself independent of any equipment pre-installed to the building using various sensors monitoring the motion of the user. The most feasible navigation sensors are inertial sensors and a camera providing motion information when a computer vision method called visual odometry is used. Collaborative indoor navigation sets challenges to the use of computer vision; navigation environment is often poor of tracked features, other pedestrians in front of the camera interfere with motion detection, and the size and cost constraints prevent the use of best quality cameras resulting in measurement errors. We have developed an improved computer vision based collaborative navigation method addressing these challenges using a depth (RGB-D) camera, a deep learning based detector to avoid using features found from other pedestrians and for controlling the inconsistency of object depth detection, which would degrade the accuracy of the visual odometry solution if not controlled. Our analysis show that our method improves the visual odometry solution using a low-cost RGB-D camera. Finally, we show the result for computing the solution using visual odometry and inertial sensor fusion for the individual and UWB ranging for collaborative navigation.","['Cameras', 'Navigation', 'Feature extraction', 'Collaboration', 'Sensors', 'Computer vision', 'Visual odometry']","['Collaborative navigation', 'computer vision', 'depth cameras', 'indoor navigation', 'Kalman filtering', 'object detection']"
"Bacterial extracellular vesicles (EVs) are nano- scale lipid-enclosed packages that are released by bacteria cells and shuttle various biomolecules between bacteria or host cells. They are implicated in playing several important roles, from infectious disease progression to maintaining proper gut health, however the tools available to characterise and classify them are limited and impractical for many applications. Surface-enhanced Raman Spectroscopy (SERS) provides a promising means of rapidly fingerprinting bacterial EVs in a label-free manner by taking advantage of plasmonic resonances that occur on nanopatterned surfaces, effectively amplifying the inelastic scattering of incident light. In this study, we demonstrate that by applying machine learning algorithms to bacterial EV SERS spectra, EVs from cultures of the same bacterial species ( Escherichia coli ) can be classified by strain, culture conditions, and purification method. While these EVs are highly purified and homogeneous compared to complex samples, the ability to classify them from a single species demonstrates the incredible power of SERS when combined with machine learning, and the importance of considering these parameters in future applications. We anticipate that these findings will play a crucial role in developing the laboratory and clinical utility of bacterial EVs, such as the label-free, noninvasive, and rapid diagnosis of infections without the need to culture samples from blood, urine, or other fluids.","['Microorganisms', 'Strain', 'Machine learning', 'Raman scattering', 'Proteins', 'Plasmons', 'Extracellular']","['Plasmonic', 'SERS', 'biosensor', 'extracellular vesicles', 'Raman spectroscopy', 'nonlinear optics', 'Escherichia coli', 'outer membrane vesicles', 'exosomes']"
"A novel proximal spectral-reflectance-based plant discrimination sensor for use in selective herbicide spraying systems is developed and its dynamic outdoor performance is experimentally assessed for two plants. For plant illumination, the sensor uses a new stabilized three-wavelength laser diode module that sequentially emits identically polarized laser light beams through a common aperture, along one optical path. Each laser beam enters a multi-spot beam generator, which produces 15 parallel, collimated laser beams spaced over a 230-mm span. The intensity of the reflected light from each beam is detected by a high-speed line scan image sensor. Plant discrimination is based on calculating two different normalised difference vegetation indices, and experimental results show that by improving the stability of the laser diodes, a plant discrimination rate greater than 90% can be achieved with a travelling speed of 7.5 km/h for canola and wild radish, which is a dominant weed in the canola crop field.","['Laser beams', 'Measurement by laser beam', 'Semiconductor lasers', 'Optical sensors', 'Optical reflection', 'Optical imaging']","['Optical spectroscopy', 'laser measurement applications', 'precision agriculture']"
"The glycated hemoglobin (HbA1c) is regarded as an essential biomarker for diabetes management. Having an elevated HbA1c level significantly increases the risk of developing diabetes-related health complications. Accurate prediction of HbA1c can greatly improve the way diabetic patients are treated and can potentially avoid related consequences. This study devises a framework to predict HbA1c levels 2-3 months in advance by using blood glucose data collected through continuous glucose monitoring (CGM) sensors and leveraging advanced feature extraction and machine learning techniques. The CGM data may often contain missing values due to sensor issues or not wearing the sensor for some period. Thus, in the paper, a novel missing data estimation method has been proposed for a single data point, multiple data points, and entire day CGM data imputation. The CGM data have been rigorously investigated, and pertinent features were created along with a multi-stage multi-class (MSMC) classification model to predict futuristic HbA1c levels. To evaluate the developed framework, a total of 150 patients' data were sourced from Sidra Medicine, Doha, Qatar, for analysis. The proposed three-staged and five-staged MSMC models predicted HbA1c levels 2-3 months in advance and obtained overall classification accuracies of 88.65% and 83.41%, respectively.","['Sensors', 'Diabetes', 'Estimation', 'Feature extraction', 'Glucose', 'Blood', 'Predictive models']","['CGM sensor', 'diabetes management', 'feature extraction', 'HbA1c prediction', 'missing data estimation']"
"In this work, a new custom design of an anomaly detection and classification system is proposed. It is composed of a convolutional Auto-Encoder (AE) hardware design to perform anomaly detection which cooperates with a mixed HW/SW Convolutional Neural Network (CNN) to perform the classification of detected anomalies. The AE features a partial binarization, so that the weights are binarized while the activations, associated to some selected layers, are non-binarized. This has been necessary to meet the severe area and energy constraints that allow it to be integrated on the same die as the MEMS sensors for which it serves as a neural accelerator. The CNN shares the feature extraction module with the AE, whereas a SW classifier is triggered by the AE when a fault is detected, working asynchronously to it. The AE has been mapped on a Xilinx Artix-7 FPGA, featuring an Output Data Rate (ODR) of 365 kHz and achieving a power dissipation of333μW/MHz. Logic synthesis has targeted TSMC CMOS 65 nm, 90 nm, and 130 nm standard cells. Best results achieved highlight a power consumption of138μW/MHz with an area occupation of 0.49 mm 2 when real-time operations are set. These results enable the integration of the complete neural accelerator in the CMOS circuitry that typically sits with the inertial MEMS on the same silicon die. Comparisons with the related works suggest that the proposed system is capable of state-of-the-art performances and accuracy.","['Sensors', 'Monitoring', 'Sensor phenomena and characterization', 'Feature extraction', 'Computational modeling', 'Micromechanical devices', 'Field programmable gate arrays']","['Anomaly detection', 'FPGA', 'artificial intelligence', 'autoencoder', 'classification', 'in-sensor computing', 'ultra-low-power']"
"Real-time monitoring of multiple (bio)chemical agents, molecules and gases is in a high demand, particularly for the future internet-of-things (IoTs) and point-of-care tests (POCT), that are connected via the 5G ecosystem. Here, we propose a lightweight, multi-agent (bio)-chemical wireless sensor based on graphene field-effect transistor (GFET) circuits, taking advantage of GFET's dual functionalities, i.e., frequency modulation and (bio-)chemical sensing. The GFET-based radio-frequency (RF) modulators circuits can convert the continuous wave (CW) monotonic signal to multiple harmonics, with conversion efficiencies sensitively depending on densities of (bio-)chemical agents. Specifically, we exploit a machine learning (ML)-based readout method to extract the concentration levels of the (bio-)chemical dopants from the harmonic spectrum. Further, we show that by increasing the order of GFET circuits and thus the number of detectable harmonics, the neural network performance and the overall readout accuracy can be enhanced. The proposed GFET-based wireless sensor could be ultracompact, ultralow-profile, portable and flexible, thus potentially benefiting a wide range of applications in IoTs, POCTs, and Industry 4.0.","['Sensors', 'Harmonic analysis', 'Chemicals', 'Sensor phenomena and characterization', 'Graphene', 'Sensor systems', 'Wireless sensor networks']","['Graphene field effect transistor', 'radio frequency modulator', 'multi-agent wireless harmonic sensor', 'machine learning', 'neural network', 'internet of things']"
"We describe a simple method for detecting formaldehyde using low resolution non-dispersive UV absorption spectroscopy. A two channel sensor was developed, making use of a strong absorption peak at 339 nm and a neighboring region of negligible absorption at 336 nm as a reference. Using a modulated UV LED as a light source and narrow laser-line filters to select the desired spectral bands, a simple detection system was constructed specifically targeted at formaldehyde. By paying particular attention to sources of noise, a minimum detectable absorbance of 5×10 -5 absorbance units (AU) was demonstrated with a 20 s averaging period (as ΔI/I 0 ). The system was tested with formaldehyde finding a limit of detection of 4.3 ppm for a 195 mm gas cell. As a consequence of the low gas flow rates used in our test system, a time period of over 8 min was used in further tests, which increased the minimum detectable absorbance to 2 × 10 -4 AU, 17 ppm of formaldehyde. The increase was the result of thermal drift caused by unwanted temperature variation of the UV LED and the filters, resulting in a zero uncertainty estimated at -560 ppm °C -1 and 100 ppm °C -1 respectively.","['Absorption', 'Optical filters', 'Sensors', 'Spectroscopy', 'Band-pass filters', 'Light emitting diodes', 'Gases']","['Gas detectors', 'light emitting diodes', 'optical sensors', 'pollution measurement', 'spectroscopy', 'ultraviolet sources']"
"This paper describes a new method for optimizing both Rogowski coil's temperature coefficient and its high frequency response. The proposed scheme is based on finding a value for coil termination resistance and for its temperature coefficient, which result in negligible change of coil output voltage versus temperature while still allowing for any damping ratio desired. The method is tested in a proof-of-concept setup with an openable, rigid Rogowski coil. The result shows that a well-damped LC-resonance of the coil is possible without having to make a compromise with temperature coefficient and vice versa. The proof-of-concept measurement is done for a broad temperature range around room temperature and works for any range where the coil`s temperature expansion is sufficiently linear. At the same time, the frequency response is preserved, giving an extension of at least an order of magnitude with respect to previous work.","['Coils', 'Temperature measurement', 'Current measurement', 'Resonant frequency', 'Resistance', 'Frequency measurement', 'Electrical resistance measurement']","['Current measurement', 'inductive transducers', 'power systems', 'RLC circuits', 'smart grids', 'temperature dependence']"
"Fiber Bragg gratings (FBGs) are known for their uses in applications ranging from civil engineering to medicine. A bare FBG is small and light; hence, it can be easily embedded into hosting materials. However, conventional fabrication methods are generally time-consuming with reproducibility issues. A more recent strategy has been proposed to develop novel FBG-based systems by encapsulating the grating within 3-D-printed structures. This process, known as 3-D printing, is characterized by several advantages like rapid prototyping, printing precision, and high customization. The possibility of quickly personalizing the 3-D-printed sensors by customizing the infill settings makes this technique very appealing for medical purposes, especially for developing smart systems. However, the influence of printing settings on the sensor response has not been yet systematically addressed. This work aimed at combining FBG with the most popular 3-D printing technique (the fused deposition modeling [FDM]) to develop four 3-D-printed sensors with different printing profiles. We chose two patterns (triangle and gyroid) and two infill densities (30% and 60%) to investigate their influence on the sensors’ response to strain, temperature, and relative humidity (RH), and on the hysteresis behavior. Then, we preliminary assess the sensor performance in a potential application scenario for FBG-based 3-D printing technology: the cardiorespiratory monitoring. The promising results confirm that our analysis can be considered the first effort to improve the knowledge about the influence of printing profiles on sensor performance and, consequently, pave the way to develop highly performant 3-D-printed sensors customized for specific applications.","['Sensors', 'Fiber gratings', 'Sensor phenomena and characterization', 'Three-dimensional printing', 'Three-dimensional displays', 'Optical fiber sensors', 'Fabrication']","['3-D printing technology', 'fiber Bragg grating (FBG) sensors', 'fused model deposition', 'infill properties', 'medical applications', 'metrological characterization', 'wearable systems']"
"Mobile crowdsensing networks have emerged to show elegant data collection capability in loosely cooperative network. However, in the sense of coverage quality, marginal works have considered the efficient (less participants) and effective (more coverage) designs for mobile crowdsensing network. We investigate the optimal coverage problem in distributed crowdsensing networks. In that, the sensing quality and the information delivery are jointly considered. Different from the conventional coverage problem, ours only select a subset of mobile users, so as to maximize the crowdsensing coverage with limited budget. We formulate our concerns as an optimal crowdsensing coverage problem, and prove its NP-completeness. In tackling this difficulty, we also prove the submodular property in our problem. Leveraging the favorable property in submodular optimization, we present the greedy algorithm with approxima√ tion ratio O( √k), where k is the number of selected users. Such that the information delivery and sensing coverage ratio could be guaranteed. Finally, we make extensive evaluations for the proposed scheme, with trace-driven tests. Evaluation results show that the proposed scheme could outperform the random selection by 2× with a random walk model, and over 3× with real trace data, in terms of crowdsensing coverage. Besides, the proposed scheme achieves near optimal solution comparing with the bruteforce search results.","['Sensors', 'Mobile communication', 'Mobile computing', 'Algorithm design and analysis', 'Electronic mail', 'Monitoring', 'Approximation algorithms']","['Mobile crowdsensing', 'coverage', 'information delivery', 'submodularity']"
"This paper describes a complete solution of a new dropped wireless sensor network (called “abandoned” since it is never picked up) dedicated to intelligence operation. The sensor network, named SEXTANT for Smart sEnsor X for Tactical situation AssessmeNT and presented in this paper is the achievement of several years of research and development in the fields of data fusion and smart sensors at the French Aerospace Lab. The main contribution of this paper is the presentation of experimental results obtained with our Joint Tracking and Classification (JTC) algorithm. The originality of this algorithm relies on the use contextual information in the target tracking processing and data fusion for target classification processing.","['Sensors', 'Target tracking', 'Wireless sensor networks', 'Sensor phenomena and characterization', 'Classification algorithms', 'Signal processing algorithms', 'Intelligent sensors']","['Wireless sensor network', 'mbed protocol', 'data fusion', 'multiple target tracking', 'classification']"
"We present a gastric gas sensor based on conjoined dual optical fibers functionalized with sensitive optical dyes for sensing gases in both fluidic and gaseous environments. The sensor aims to sense various concentrations of carbon dioxide (CO 2 ) and ammonia (NH 3 ), which are two significant biomarkers of H. pylori infection in the stomach. It is known that CO 2 and NH 3 are released during the hydrolysis of urea by H. pylori, a bacterium that may cause stomach cancer with relatively high probability. CO 2 and NH 3 sensitive optical dyes, cresol red ion pair and zinc tetraphenylporphyrin, are embedded in silica beads and then functionalized onto the thin PDMS-coated fiber tip. Each type of dye provides a unique spectral emission response when excited with light ranging from 450 to 700 nm. Two SMA connector legs of the as-functionalized sensor are connected to an external light source for illumination and a ultraviolet-visible-near infrared (UV-Vis-NIR) spectrometer for signal collection/readout. To perform the measurements, one fiber illuminates while the other fiber collects the back-scattered light and feeds it to the UV-Vis-NIR spectrometer to measure the change in light spectrum as a function of CO 2 or NH 3 concentration. This method is easy and flexible and achieves ppm level sensitivity to targeted gas analytes. The proposed sensor can be integrated into a customized tethered capsule for adjunctive diagnosis of H. pylori infection to improve the accuracy of visual endoscopic inspection.","['Optical sensors', 'Gas detectors', 'Stomach', 'Optical device fabrication', 'Biomedical optical imaging', 'Optical imaging']","['gastric gas', 'fiber optics', 'optical dyes']"
"Electroencephalogram (EEG) plays a significant role in the analysis of cerebral activity, although the recorded electrical brain signals are always contaminated with artifacts. This represents the major issue limiting the use of the EEG in daily life applications, as the artifact removal process still remains a challenging task. Among the available methodologies, artifact subspace reconstruction (ASR) is a promising tool that can effectively remove transient or large-amplitude artifacts. However, the effectiveness of ASR and the optimal choice of its parameters have been validated only for high-density EEG acquisitions. In this regard, this study proposes an enhanced procedure for the optimal individuation of ASR parameters, in order to successfully remove artifacts in low-density EEG acquisitions (down to four channels). The proposed method starts from the analysis of real EEG data, to generate a large semisimulated dataset with similar characteristics. Through a fine-tuning procedure on this semisimulated data, the proposed method identifies the optimal parameters to be used for artifact removal on real data. The results show that the algorithm achieves an efficient removal of artifacts preserving brain signal information, also in low-density EEG signals, thus favoring the adoption of the EEG also for more portable and/or daily-life applications.","['Electroencephalography', 'Measurement', 'Standards', 'Pollution measurement', 'Sensor phenomena and characterization', 'Tuning', 'Signal to noise ratio']","['Artifact removal', 'artifact subspace reconstruction (ASR)', 'brain computer interface (BCI)', 'electroencephalography', 'low-density system', 'measurement system']"
"We present a new Hall sensor design for the accurate and robust measurement of linear displacement. Implemented in CMOS, the sensor is based on a novel gradient measurement concept combining Hall elements with integrated magnetic concentrators. In typical applications with practical Ferrite magnets, the peak output voltage of the Hall transducers is only around 1.7 mV at the maximum operating temperature of 160°C, and thus requires high-performance low-offset readout electronics. Over its 15-mm linear displacement range, the sensor’s total error is 1% including manufacturing tolerances, trimming accuracy, temperature, aging effects, and practical magnet constraints. In addition, the sensor is immune to magnetic stray fields up to 5 mT, complying with the most stringent automotive norm.","['Magnetic sensors', 'Magnetic field measurement', 'Magnetic domains', 'Mechanical sensors', 'Magnetostriction', 'Magnetic fields']","['Automotive electronics', 'Hall effect', 'magnetic sensors']"
"Objectives: Population ageing and the subsequent increase of joint disorders prevalence requires the development of non-invasive and early diagnostic methods to enable timely medical assistance and promote healthy aging. Over the last decades, acoustic emission (AE) monitoring, a technique widely used in non-destructive testing, has also been introduced in orthopedics as a diagnostic tool. This review aims to synthesize the literature on the use of AE monitoring for the assessment of hip and knee joints or implants, highlighting the practical aspects and implementation considerations. Methods: this review was conducted as per the PRISMA statement for scoping reviews. All types of studies, with no limits on date of publication, were considered. Articles were assessed and study design parameters and technical characteristics were extracted from relevant studies. Results: conducted search identified 1379 articles and 64 were kept for charting. Seven additional articles were added at a later stage. Reviewed works were grouped into studies on joint condition assessment, implant assessment, and hardware or software development. Native knees and hip implants were most commonly assessed. The most researched conditions were osteoarthritis, implant loosening or squeaking in vivo and structural damage of implants in vitro. Conclusion: in recent years, AE monitoring showed potential of becoming a useful diagnostic tool for lower limb pathologies. However, further research is needed to refine the existing methods and assess their feasibility in early diagnostics. Significance: The current state of research on AE monitoring for hip and knee joint assessment is described and future research directions are identified.","['Monitoring', 'Implants', 'Sensors', 'Hip', 'Bones', 'Sensor phenomena and characterization', 'Internet']","['Acoustic emission', 'joints', 'implant', 'medical diagnosis', 'orthopedics']"
"In this article, a commercial low cost solution for increasing DVB-T based passive radar (also referred to as passive coherent location) robustness with respect to channel allocation and range resolution, is designed. IDEPAR demonstrator is updated to include a new recording system in charge of DVB-T channels reallocation. The use of commercial hardware introduces frequency mismatching between acquired channels that compromises system operation. To face channel frequency alignment, fulfilling the requirements imposed by the high Doppler resolution typical of passive radars, a novel compensation algorithm is proposed, based on the minimization of signal dispersion in the Cross-Ambiguity Function domain. The well-known cyclic prefix van de Beek method is used as reference. Results show an increase in target signal to interference ratio and detection performances, as well as a reduction of clutter dispersion when the novel proposed algorithm is applied. The low cost solution is also compared to a high performance one capable of acquiring a wide bandwidth of sparse DVB-T channels, and performing channel reallocation by digital signal processing. Results show that both systems reached similar performances in terms of range resolution and SNR improvement.","['OFDM', 'Wideband', 'Passive radar', 'Digital video broadcasting', 'Radio spectrum management']","['Passive radar', 'passive coherent location', 'multistatic radar', 'DVB-T', 'COTS components', 'channel allocation', 'frequency alignment', 'wideband']"
"The ubiquity foreseen by the Internet-of-Things (IoT) and powered by the 5G advancements has motivated research on wireless solutions for critical applications, in particular, Industrial IoT (IIoT). Nevertheless, there is little or no research on a unified design methodology for IIoT that tackles the conflicting wireless system performances of Power, Latency, and Reliability (PLR). Obtaining such a framework is vital for empowering further development and fair comparison in future IIoT designs. Thereby, this paper presents a novel design methodology to tackle PLR trade-off in IIoT Wireless Systems (IIoT-WS). This new methodology uses a meet-in-the-middle system approach to design an entire PLR RF system, and a custom Multiple-Criteria Decision Analysis (MCDA) to help decide the best design variables that are both resource-efficient and PLR-balanced for a given application. Finally, to quantify the methodology for critical wireless systems, a lab-demonstrator for automotive application is designed and its performance compared to other wireless systems standards.","['Wireless communication', 'Wireless sensor networks', 'Design methodology', 'Reliability', 'Receivers', 'Bandwidth']","['Industrial IoT', 'WSN', 'URLCC', 'IIoT-WS', 'MCDA', 'system design', 'Pareto front', 'multi-objective optimization', 'reliable communications', 'low power', 'low latency']"
"Reducing energy consumption of wireless sensor nodes extends battery life and/or enables the use of energy harvesting and thus makes feasible many applications that might otherwise be impossible, too costly or require constant maintenance. However, theoretical approaches proposed to date that minimize Wireless Sensor Network energy needs generally lead to less than expected savings in practice. We examine the experiences of tuning the energy profile for two near-production wireless sensor systems and demonstrate the need for: microbenchmark-based energy consumption profiling; examining start-up costs; and monitoring the nodes during long-term deployments. The tuning exercise resulted in reductions in energy consumption of: 93% for a multihop Telos-based system (average power 0.029 mW); 94.7% for a single hop Ti-8051-based system during startup; and 39% for a Ti-8051 system post start-up. This paper shows that reducing the energy consumption of a node requires a whole system view, not just measurement of a typical sensing cycle. We give both generic lessons and specific application examples that provide guidance for practical WSN design and deployment.","['Wireless sensor networks', 'Wireless communication', 'Monitoring', 'Engines', 'Temperature sensors', 'Current measurement']","['Wireless sensor networks', 'low-power electronics', 'Internet of Things', 'data compression']"
"To assist endodontic therapy, in this paper, a force sensor is designed and fabricated for measuring the axial force and the bending moment simultaneously. By detecting the bending moment, we can know the severity of bending. Axial force detection is also desired, as excessive axial forces may result in instrument buckling or even root canal perforation. This paper installs three sensing cells in each of two planes to detect the bending direction. Each sensing cell consists of a pressure-sensitive electric conductive rubber and an electrode. The size of the force sensor is 1.4-mm wide and 6-mm high for matching the size of endodontic instruments and making it feasible in the endodontic treatment. Experimental results on accuracy, repeatability, and nonlinearity are presented to validate the proposed sensor.",[],[]
"There is an increasing desire to conduct autonomous inspection of nuclear sites using robots. However, the presence of gamma radiation in nuclear sites induces degradation in vision sensors. In this paper, the effects of gamma radiation on a robot vision sensor (CMOS camera) used for radiological inspection is examined. The analyses have been carried out for two types of images at different dose rates: a) dark images b) illuminated images. In this work, dark images and chessboard images under illumination are analysed using various evaluation metrics to evaluate the effect of gamma radiation on CMOS Integrated Circuit (IC) and electronic circuitry of the sensor. Experimental results manifest significant changes in electrical properties like the generation of radiation-induced photo signal in sensing circuitry and radiation-induced noise affecting the visual odometry of the robot. System-level degradation for gamma dose rates upto 3 Gy/min intensifies, making data from the imaging sensor unreliable for the visual odometry. However, images captured for gamma dose rate upto 3 Gy/min can be used for surveillance purpose.","['Robots', 'Sensors', 'Cameras', 'Robot vision systems', 'CMOS image sensors', 'Inspection', 'Degradation']","['Gamma-induced image degradation', 'CMOS image sensor', 'robotic inspection', 'vision sensor degradation']"
"Metabolites are the ultimate readout of disease phenotype that plays a significant role in the study of human disease. Multiple metabolites sometimes serve as biomarkers for a single metabolic disease. Therefore, simultaneous detection and analysis of those metabolites facilitate early diagnostics of the disease. Conventional approaches to detect and quantify metabolites include mass spectrometry and nuclear magnetic resonance that require bulky and expensive equipment. Here, we present a disposable sensing platform that is based on complementary metal-oxide-semiconductor process. It contains two sensors: an ion sensitive field-effect transistor and photodiode that can work independently for detection of pH and color change produced during the metabolite-enzyme reaction. Serum glucose and cholesterol have been detected and quantified simultaneously with the new platform, which shows good sensitivity within the physiological range. Low cost and easy manipulation make our device a prime candidate for personal metabolome sensing diagnostics.","['Sensors', 'Diseases', 'Sugar', 'Biochemistry', 'Semiconductor device measurement', 'Ions', 'Chemicals']","['Simultaneous detection', 'CMOS-based sensor system', 'coronary heart disease', 'serum metabolites', 'personal diagnostics']"
"Conventionally, silicon is less often selected as the material of dielectric metasurfaces in the visible band than other lossless materials, including titanium dioxide, silicon nitride, and gallium nitride. The reason is its relatively high extinction coefficient and resulting low transmittance. This study demonstrated that accurately designed nanopillars made of single-crystal silicon could be used satisfactorily on a metasurface, even in the visible band. Four line-focusing metasurface lenses were designed to verify the lens performance effectiveness of silicon nanopillars in the visible band. In addition, a combination of the character projection and the variable-shaped beam modes in the electron beam lithography was operated to evaluate the compatibility between mass productivity and accuracy. We successfully obtained a highly efficient line-focusing metasurface lens composed of single crystalline silicon nanopillars. The parameters of the metasurface lens at a wavelength of 532 nm were as follows: lens thickness, 300 nm; focal length, 3.91 mm; square aperture, 2 mm; numerical aperture, 0.25; measured transmittance, 38.4% to 46.8%; and measured beam spot width,3.68μm(full width at half maximum, FWHM) at the focal point. The results obtained in this study show a promising use of silicon metasurface for optical sensor applications in the visible band.","['Metasurfaces', 'Silicon', 'Lenses', 'Fabrication', 'Apertures', 'Sensor phenomena and characterization', 'Optical sensors']","['Dielectric metasurface', 'metasurface lens', 'metalens', 'single-crystal silicon', 'micro-optical line generator', 'planar optics']"
"Recent research treats radar emitter classification (REC) problems as typical closed-set classification problems, i.e., assuming all radar emitters are cooperative and their pulses can be pre-obtained for training the classifiers. However, such overly ideal assumptions have made it difficult to fit real-world REC problems into such restricted models. In this paper, to achieve online REC in a more realistic way, we convert the online REC problem into dynamically performing subspace clustering on pulse streams. Meanwhile, the pulse streams have evolving and imbalanced properties which are mainly caused by the existence of the non-cooperative emitters. Specifically, a novel data stream clustering (DSC) algorithm, called dynamic improved exemplar-based subspace clustering (DI-ESC), is proposed, which consists of two phases, i.e., initialization and online clustering. First, to achieve subspace clustering on subspace-imbalanced data, a static clustering approach called the improved ESC algorithm (I-ESC) is proposed. Second, based on the subspace clustering results obtained, DI-ESC can process the pulse stream in real-time and can further detect the emitter evolution by the proposed evolution detection strategy. The typically dynamic behavior of emitters such as appearing, disappearing and recurring can be detected and adapted by the DI-ESC. Extinct experiments on real-world emitter data show the sensitivity, effectiveness, and superiority of the proposed I-ESC and DI-ESC algorithms.","['Radar', 'Clustering algorithms', 'Task analysis', 'Heuristic algorithms', 'Training', 'Feature extraction', 'Sensors']","['Radar emitter classification', 'data stream clustering', 'imbalanced data stream', 'subspace clustering']"
"The metal wear debris in mechanical transmission can feedback the characteristics of mechanical equipment failure, and its online monitoring efficacy directly affects the operating performance of mechanical equipment. Specific to the need for online fault detection of the mechanical transmission system, a double-coil reverse excitation method is adopted to provide a magnetic field environment for metal wear debris testing in the lubricating oil circuit of the mechanical transmission system. Based on the magnetic field distribution model of a single-coil solenoids, a coupling magnetic field distribution model of reverse-symmetric double-coil magnetic excitation is established. On the basis of detecting the coils to obtain the signals of magnetic field disturbance when metal water debris passing through the magnetic excitation field, an online monitoring sensor for metal wear debris with a reverse-symmetric double-coil magnetic excitation balance magnetic field is developed. Taking the monitoring of metal wear debris in a large-aperture flow channel as an example, a semi-physical simulation test has been carried out on metal wear debris in different grain diameters and textures, whose experimental results indicate that this sensor can detect100 μmferromagnetic metal and1000 μmnon-ferromagnetic metal in an 8 mm flow channel, which, as a detection sensor for metal wear debris, can also be used for metal wear debris monitoring in the mechanical lubricants of mechanical transmission and engines.","['Sensors', 'Metals', 'Magnetic sensors', 'Magnetic fields', 'Coils', 'Capacitive sensors', 'Monitoring']","['Metal wear debris', 'electromagnetic induction', 'symmetric magnetic excitation', 'electromagnetic detection']"
"With the advantages of comfortable wearing and outdoor usage, the myoelectric gesture recognition techniques have gained much attention in the field of human-machine interaction (HMI). The purpose of this study is to optimize model structure and transfer generalized features to improve the robustness of myoelectric hand motion decoding. We derived the hand motion recognition framework from the muscle synergy theory, which is formulated as a temporal convolutional (TC) model of array sEMG signals, then a hierarchical myoelectric decoding model was proposed to predict simultaneous and continuous hand motion. The model was trained by the methods of unsupervised low-level feature learning and automated data labeling to minimize training supervision. Extensive experiments on the public sEMG database (17 subjects in Biopatrec) show that the TC model can extract muscle synergy features with higher fidelity ( R 2 = 0.85±0.23) than the traditional instantaneous mixture model, the results of online test demonstrate robust myoelectric decoding on multiple simultaneous and continuous hand motions. More importantly, the analysis of weights visualization shows that the low-level feature representation layer of TC model can be migrated across the individuals, which provides a transferrable feature extraction layer for generalized hand motion decoding.","['Muscles', 'Mathematical model', 'Decoding', 'Feature extraction', 'Sensors', 'Electrodes', 'Convolution']","['Muscle synergies', 'sEMG array', 'hand motion prediction', 'generalization', 'myoelectric decoding model']"
"An Electrical Impedance Tomography (EIT) system has been developed for dynamic three-dimensional imaging of changes in conductivity distribution in the human head, using scalp-mounted electrodes. We attribute these images to changes in cerebral perfusion. At 100 frames per second (fps), voltage measurement is achieved with full-scale signal-to-noise ratio of 105 dB and common-mode rejection ratio > 90 dB. A novel nonlinear method is presented for 3-D imaging of the difference in conductivity distribution in the head, relative to a reference time. The method achieves much reduced modelling error. It successfully localizes conductivity inclusions in experimental and simulation tests, where previous methods fail. For > 50 human volunteers, the rheoencephalography (REG) waveform is observed in EIT voltage measurements for every volunteer, with peak-to-peak amplitudes up to approx.50 μVrms. Images are presented of the change in conductivity distribution during the REG/cardiac cycle, at 50 fps, showing maximum local conductivity change of approx. 1% in grey/white matter. A total of 17 tests were performed during short (typically 5s) carotid artery occlusions on 5 volunteers, monitored by Transcranial Doppler ultrasound. From EIT measurements averaged over complete REG/cardiac cycles, 13 occlusion tests showed consistently decreased conductivity of cerebral regions on the occluded side, and increased conductivity on the opposite side. The maximum local conductivity change during occlusion was approx. 20%. The simplicity of the carotid artery intervention provides a striking validation of the scalp-mounted measurement system in imaging cerebral hemodynamics, and the REG images indicate its unique combination of sensitivity and temporal resolution.","['Electrical impedance tomography', 'Conductivity', 'Voltage measurement', 'Electrodes', 'Scalp', 'Imaging', 'Current measurement']","['Cerebral', 'EIT', 'electrical impedance tomography', 'SNR', 'hemodynamic', 'REG', 'rheoencephalogram', 'transient hyperemic response', 'THR']"
"In this work, we investigate the application of tilted fiber Bragg grating (TFBG) sensors during ex vivo laser ablation of porcine hepatic tissues. Initially, TFBG’s ability to measure the surrounding refractive index (RI) for different sucrose concentrations and the possibility to measure the RI of the targeted tissue during laser ablation (LA) is analyzed. After, the temperature sensing modality of TFBG is investigated in detail. We have implemented an algorithm for quasi-distributed spatial temperature profile reconstruction along TFBG. The algorithm models the TFBG core mode spectrum as a chain of Bragg gratings (each Bragg grating is modeled via coupled mode theory), where each grating is sensitive to local temperature changes. After, the Gaussian-shape temperature profile along the TFBG is reconstructed using the iterative optimization technique. Temperature measurements have been compared with highly-dense FBG array measurements and with conventional TFBG point temperature measurements based on the core mode tracking techniques (maximum tracking, X-dB Bandwidth, centroid methods). Overall, the proposed reconstruction algorithm is able to provide a quasi-distributed temperature profile along TFBG, which is not possible to obtain using conventional point temperature measurements based on the TFBG’s core mode tracking. The resulted root-mean-square error in comparison to FBG array reference measurements is 7.8±1.7 °C. In general, the results show that the main reliable sensing modality of TFBG during LA is temperature monitoring, which can be significantly improved by the proposed algorithm.",[],[]
"This paper presents an ultra-low-power, widedynamic-range interface circuit for capacitive and resistive sensors. The circuit is implemented as a switched-capacitor circuit using programmable capacitors to achieve high configurability. The circuit was fabricated using a CMOS 0.18 μm process. Different types of capacitive and resistive sensors were measured using the interface to demonstrate its support for multi-sensor systems with an ultra-low-power budget. Experimental results show that the circuit is able to interface various sensors within the overall capacitance range of 0.6-550 pF and resistance range of 3.7-5100 kΩ, while consuming only 0.39-3.56 μW from a 1.2 V supply. A proximity, gesture, and touch-sensing system is also developed consisting of the designed interface circuit and a sensor element that is able to detect the displacement of an object up to 15 cm from the sensing electrodes consuming only 0.83 μW from a 1.2 V supply.","['Capacitive sensors', 'Capacitance', 'Sensor phenomena and characterization', 'Intelligent sensors', 'Power demand', 'Transistors']","['Ambient intelligence', 'C2V', 'capacitive sensor', 'charge-sensitive amplifier', 'CMOS', 'current source', 'environmental sensor', 'force sensor', 'humidity sensor', 'interface circuit', 'Internet-of-Things (IoT)', 'light sensor', 'proximity sensor', 'resistive sensor', 'switched capacitor circuit', 'ultra-low-power']"
"This article presents a wireless temperature sensor tag able to work in both fully passive mode and in semi-passive mode when assisted by a flexible thermoelectric generator (TEG). The sensor tag consists of an EPC C1G2/ISO 18000-6C ultrahigh-frequency (UHF) radio frequency identification (RFID) integrated circuit (IC) connected to a low-power microcontroller unit (MCU) that samples and collects the temperature from a digital temperature sensor. With a temperature gradient as low as 2.5 °C, the test results show that the TEG provides an output power of 400μW with an output voltage of 40mV. Bymeans of an up-converter in order to boost the TEG output voltage, this harvester supplies the power required to the sensor tag for a 2-conv/s data rate in semi-passive mode. Moreover, when the tag operates in semi-passive mode, a communication range of 22.2 m is measured for a 2-W effective radiated power (ERP) reader. To the best of our knowledge, the proposed TEG-assisted sensor tag shows the longest communication range and the only one that provides stable external power at low-temperature gradients. The measured performance and the chosen architecture allow using the wireless sensor in multiple industrial or biomedical applications.","['Sensors', 'Temperature sensors', 'Radiofrequency identification', 'Integrated circuits', 'Wireless sensor networks', 'Wireless communication', 'Sensor phenomena and characterization']","['Energy harvesting', 'integrated circuit (IC)', 'Internet of Things (IoT)', 'radio frequency identification (RFID)', 'thermoelectric generator (TEG)']"
"Because of long distance of railway lines, it is difficult to find an appropriate method to inspect the rail track condition efficiently and accurately. In this article, a machine vision system based on driving recorder and image signal processing is proposed to evaluate the rail curvature automatically. The proposed machine vision system consists of four modules including the video acquisition module, the image extraction module, the image processing module, and the track condition assessment module. Three classic edge detection methods are adopted and compared for rail edge detection. In line with the videos of driving recorder, coordinate systems for train and rail are defined in the Lagrangian space, and the track curvature is estimated using the proposed chord offset method and double measurement method. For evaluating the track condition, an index describing the concordance between the train and track is defined. In the case study, a set of videos from the driving recorders of trains during their in-service operations are analyzed by the proposed technique, and the obtained results are verified by comparison with those obtained by a track geometry inspection vehicle. It is shown that the proposed technique can evaluate the track curvature accurately. Moreover, the influence of the position of deployed driving recorder, the focal length and anti-shake of camera on the accuracy of evaluation results is discussed. It is testified that the proposed technique provides a simple and reliable way to inspect the track curvature.","['Rails', 'Image edge detection', 'Geometry', 'Inspection', 'Sensors', 'Rail transportation']","['Track curvature', 'machine vision', 'driving recorder', 'edge detection', 'onboard inspection']"
"Objective: Falls are often accompanied by huge social costs, and fall risk assessment is essential to protect the elderly from serious injuries and reduce financial burdens. The standard timed up and go (TUG) balance assessment test focuses on the total walking time and scenarios without environmental changes, which is flawed in providing rich information related to falls and evaluating the gait adaptability in response to environmental changes. Therefore, a fall risk assessment system that relies on a variable environment is actually needed. Methods: We have constructed an environment-adaptive TUG (EATUG) test system with three terrain surfaces (levels/obstacles/stairs). One hundred and three elderly from Shenzhen Luohu Hospital is recruited to participate in the experiment. The wearable inertial sensors attached to the two shanks are used to acquire data, and the gait parameters that may be related to falls are extracted and quantified. Results: Most of the parameters have significant differences between the high-risk group and the low-risk group (e.g., peak power, maximum radius, double support, etc., p <; 0.001). In addition, the average sensitivity and specificity of fall risk prediction are 85.7% and 92.9%, while the average accuracy is 9.52% higher than the standard TUG test. Conclusion: The EATUG test system can provide richer gait characteristics and fall-related information, which is a good improvement on the drawbacks of the standard TUG test. Significance: The proposed test system is expected to replace the standard TUG test and be used for fall screening of high-risk elderly in the community to reduce the occurrence of falls.","['Senior citizens', 'Standards', 'Risk management', 'Legged locomotion', 'Stairs', 'Injuries', 'Gyroscopes']","['Falls risk assessment', 'timed up and go (TUG) test', 'the elderly', 'wearable inertial sensors', 'environment-adaptive TUG (EATUG) test']"
"Developments in sensing technology have shown that UWB radars are becoming increasingly valuable sensing devices that can be used for monitoring of humans in military/police and civilian areas. It is known that the applicability of particular methods of human localization depends on the character of persons' motion. With respect to this finding, the researchers' attention has been aimed at two fundamental directions. While the former is focused on the localization of moving persons (MP), the latter approach is intended to localize static persons (SP). Then, a proper fusion of the methods developed for MP and SP localization allows monitoring of persons moving with an unknown time-variable character of motion (MP-SP). The analyses of the currently known methods of MP and SP localization in terms of their use for MP-SP localization have shown that while MP localization methods are in principle well developed, SP localization methods are not sufficiently adapted for their use in MP-SP monitoring. Motivated by these findings, we would like to introduce a new radar signal processing procedure for SP localization (SPL) that could be an efficient component of algorithms to be applied for MP-SP monitoring. The novel features of the proposed SPL consist especially in a new approach to SP detection and inclusion of SP tracking in SPL. Moreover, SPL is characterized by relatively low computational complexity and is, therefore, suitable for real-time implementation. Experimental results have shown that SPL introduced in this paper provides very good performance for multiple person localization for line-of-sight and through-the-wall scenarios.","['Location awareness', 'Radar', 'Ultra wideband radar', 'Monitoring', 'Clutter', 'Radar tracking', 'Backscatter']","['Breathing', 'detection', 'localization', 'signal processing', 'static person', 'tracking', 'ultra-wideband (UWB) radar']"
"In this paper, we developed a sensitive and simple electrochemical method for the rapid detection of Escherichia coli in water samples. The general principle of the assay utilizes the enzyme β-D-glucuronidase. This enzyme was induced by adding methyl-β-D-glucuronide sodium salt and its activity promoted the cleavage of 8-hydroxyquinoline glucuronide to the electroactive compound 8-hydroxyquinoline. This cleavage product was further oxidized on the working electrode of a potentiostat using cyclic voltammetry. The obtained current output signal in a specific voltage range (400 to 600 mV) indicated enzyme activity and subsequently was an evidence for E. coli cells in the sample. For our experiment, we designed a low-cost potentiostat and show an evaluation of this instrument. First, the β-Dglucuronidase assay was tested with various concentrations of enzyme solutions before living E. coli cells were investigated. Our presented method allowed a clear and a sensitive identification of 1 colony-forming unit of E. coli without any interference from other investigated bacterial strains. Comprising only few working steps (filtration, incubation, and voltammetric analysis), the method allowed incorporation into an automated prototype that delivered results similar to those obtained from samples treated in laboratory.","['Electrodes', 'Biochemistry', 'Microorganisms', 'Sensor systems', 'Substrates', 'Voltage measurement']","['8-hydroxyquinoline glucuronide', 'voltammetry', 'Escherichia coli detection', 'electrochemical oxidation', 'methyl-β-D-glucuronide sodium salt']"
"This article demonstrates the identification of 10 persons with 99% accuracy achieved by combining micro-Doppler signatures of sit-to-stand and stand-to-sit movements. Data from these movements are measured using two radars installed above and behind the person. Images of Doppler spectrograms generated using the measured data are combined and input to a convolutional neural network. Experimental results show the significantly better accuracy of the proposed method compared with conventional methods that do not perform data combination. The accuracy of identifying 10 participants having similar ages and physical features was 96-99%, despite the relatively small training set (number of training samples: 50-90 Doppler radar images per person). These results suggest that combining sit-to-stand and stand-to-sit movements provides sufficient information for accurate person identification and such information can be remotely acquired using Doppler radar measurements.","['Identification of persons', 'Doppler radar', 'Spectrogram', 'Sensor phenomena and characterization', 'Radar imaging']","['Doppler radar', 'identification of persons', 'machine learning', 'motion measurement']"
"This paper presents a second-generation integrated circuit for the Active Books neural stimulation microsystem. It provides multi-channel stimulation with versatile control of stimulation profiles and reduced crosstalk from other stimulation channels. The new design features enhanced safety by monitoring the temperature and humidity inside the micropackage, and the peak electrode voltage at any stimulating electrode. The humidity sensor uses an interdigitated capacitor covered by a passivation layer and a polyimide covering. To boost sensitivity in the operating range of interest, the temperature sensor uses a temperature-insensitive current that is subtracted from a proportional-to-absolute-temperature current. A 3-b analog-to-digital converter is used to record the peak electrode voltage. All sensor data is sent to an implanted central hub using bidirectional connection with error checking. Both the stimulation electronics and sensors are integrated on a 6.2 mm×4 mm silicon die using XFAB’s 0.6-μmCMOS high-voltage process. No post-processing steps are involved. The stimulator uses a five-wire cable to provide the power supply and bidirectional data signals. The chip operates from a 7.5–18 V power supply and can generate stimulation currents of 1 mA, 4 mA or 8 mA with a pulse duration of 2μs–1.07 ms. The humidity sensor output varies linearly with relative humidity (RH) with a normalized sensitivity of 0.04%/%RH over the range of 20–90%RH. The temperature sensor has a nonlinearity of 0.4% over the range of 20–90 °C and a resolution of 0.12 °C. The stimulator is the first of its kind to include integrated temperature and humidity sensors.","['Temperature sensors', 'Humidity', 'Electrodes', 'Temperature measurement', 'Safety', 'Metals']","['Active Books', 'humidity sensor', 'implant safety', 'integrated stimulator', 'temperature sensor', 'voltage sensor']"
"Intracranial hypertension can potentially cause ischemia, herniation, and even death. Mannitol dehydration treatment is a common method for reducing increased intracranial pressure (ICP). However, the efficacy varies among patients and improper use of mannitol may cause severe side effects. This study aims to find ways to individually evaluate and provide guidance for mannitol dehydration treatments based on dynamic brain electrical impedance tomography (EIT). First, clustering methods were applied to analyze the relationship between EIT data and patients' different responses to mannitol. With the clustering results as well as empirical experiences, doctors could establish a rule for classifying the efficacy of mannitol dehydration treatments. Then, to rapidly categorize following EIT datasets, an automatic classifier was developed using the multiclass support vector machines (SVMs). Additionally, to enhance the accuracy of the classifier, a novel priori and principle components (PPCs) based index was introduced as the input data for training the classifier. The experimental results demonstrated that different clustering methods yielded similar results and the optimal number of clusters was three, which corresponded to three types of efficacy of mannitol. For the automatic classifier, its classification accuracy improved to 93.3% with the PPCs index. As such, it can be concluded that brain EIT promises to be an effective tool for individually evaluating the efficacy of the dehydration treatments and make guidance for rational use of mannitol.","['Monitoring', 'Image reconstruction', 'Impedance', 'Electrical impedance tomography', 'Clustering methods', 'Medical services']","['Electrical impedance tomography', 'cerebral edema', 'intracranial pressure', 'mannitol dehydration', 'individual evaluation', 'support vector machines']"
"More and more devices, such as Bluetooth and IEEE 802.15.4 devices forming Wireless Personal Area Networks (WPANs) and IEEE 802.11 devices constituting Wireless Local Area Networks (WLANs), share the 2.4 GHz Industrial, Scientific and Medical (ISM) band in the realm of the Internet of Things (IoT) and Smart Cities. However, the coexistence of these devices could pose a real challenge—co-channel interference that would severely compromise network performances. Although the coexistence issues has been partially discussed elsewhere in some articles, there is no single review that fully summarises and compares recent research outcomes and challenges of IEEE 802.15.4 networks, Bluetooth and WLANs together. In this work, we revisit and provide a comprehensive review on the coexistence and interference mitigation for those three types of networks. We summarize the strengths and weaknesses of the current methodologies, analysis and simulation models in terms of numerous important metrics such as the packet reception ratio, latency, scalability and energy efficiency. We discover that although Bluetooth and IEEE 802.15.4 networks are both WPANs, they show quite different performances in the presence of WLANs. IEEE 802.15.4 networks are adversely impacted by WLANs, whereas WLANs are interfered by Bluetooth. When IEEE 802.15.4 networks and Bluetooth co-locate, they are unlikely to harm each other. Finally, we also discuss the future research trends and challenges especially Deep-Learning and Reinforcement-Learning-based approaches to detecting and mitigating the co-channel interference caused by WPANs and WLANs.","['IEEE 802.15 Standard', 'Wireless personal area networks', 'Bluetooth', 'Interference', 'Wireless LAN', 'Interchannel interference', 'Sensors']","['Internet of Things', 'WPANs', 'WLANs', 'Bluetooth', 'IEEE 802.15.4', 'interference mitigation', 'deep learning', 'reinforcement learning', 'heterogeneous networks']"
"Light detection and ranging (LiDAR) sensors are promising for automated transportation to detect the surrounding environment. However, most LiDAR solutions are complex and bulky. By designing a MEMS-mirror-based LiDAR, we can improve the volume constraints, but MEMS mirrors could limit scanning angles. In this work, we simulated and demonstrated a MEMS LiDAR system to solve the current obstacles. Combining a MEMS mirror and a wide-angle lens into the system, small-volume and large field-of-view (FOV) LiDAR systems can be realized. We use ray tracing optical simulation software to design a pair of aspherical lenses to expand the scanning angle. After the laser beam passes through the wide-angle lens, the FOV can be increased to 104 degrees. The distortion of the wide-angle lens is controlled below 3%, making the scanned image precise to the actual situation. In order to experimentally demonstrate the small-volume MEMS scanning LiDAR, a modular laser rangefinder is used with a MEMS mirror. The entire system of the LiDAR scanner is around 15 cm×5cm×2.5cm. In the natural light environment for wide-angle LiDAR measurement, the maximum error is less than 2%. Finally, an image processing program is written to convert the scanned data into a 3D point cloud image, and the generated image proves the complete function of the proposed LiDAR.","['Lenses', 'Laser radar', 'Micromechanical devices', 'Mirrors', 'Optical distortion', 'Optical sensors', 'Optical receivers']","['LiDAR', 'wide-angle scanning', 'time of flight', 'optical system design', 'portable', 'MEMS mirror', 'point cloud diagram']"
"Torque measurements are used in a number of controls applications, but indirect coupling, size, and the quality of commercially available sensor materials can limit their utility. Here, a compact magnetostrictive torque sensor is made by electrodeposition of Fe 1-x Ga x (0.1<;x<;0.4, aka Galfenol), onto Cu shafts using rotating cylinder electrodes (RCEs). The alloy composition is controlled by tuning the RCE rotation rate between 500 and 2000 RPM, while the electrode potential is varied from 1.15 to 1.20 V. Direct coupling of Fe 1-x Ga x to the shaft with the electrodeposition process enables magnetic anisotropy to be induced via shaft surface texturing, as seen by a 260% increase in susceptibility along a 400 grit texturing direction versus perpendicular to the texture and compared with the isotropic behavior of films deposited on polished shafts. Inverse magnetostriction-based torque sensing is demonstrated by measuring stray fields from Fe 1-x Ga x films as torque loads of 0-16.9 Nm were applied to the shaft. Films electrodeposited on circumferentially textured shafts had torque responses almost 1.5 times greater than films electrodeposited on longitudinally textured shafts and five times greater than films on polished shafts.","['Shafts', 'Torque', 'Films', 'Electrodes', 'Magnetic sensors', 'Electron tubes']","['Sensors', 'magnetic sensors', 'instrumentation and measurement', 'torque measurement', 'force sensors', 'thin film sensors', 'magnetic devices', 'magnetostrictive devices', 'magnetomechanical effects', 'magnetoelasticity', 'magnetostriction']"
"Tight integration of low-cost ultrawideband (UWB) ranging sensors with mass-market Global Navigation Satellite System (GNSS) receivers is gaining attention as a high-accuracy positioning strategy for consumer applications dealing with challenging environments. However, due to independent clocks embedded in commercial-off-the-shelf (COTS) chipsets, the time-scales associated with sensor measurements are misaligned, leading to inconsistent data fusion. Centralized, recursive filtering architectures can compensate for this offset and achieve accurate state estimation. In line with this, a GNSS/UWB tight integration scheme based on an extended Kalman filter (EKF) is developed that performs online time calibration of the sensors’ measurements by recursively modeling the GNSS/UWB time-offset as an additional unknown in the system state-space model. Furthermore, a double-update filtering model is proposed that embeds optimizations for the adaptive weighting of UWB measurements. Simulation results show that the double-update EKF algorithm can achieve a horizontal positioning accuracy gain of 41.60% over a plain EKF integration with uncalibrated time-offset and of 15.43% over the EKF with naive time-offset calibration. Moreover, a real-world experimental assessment demonstrates improved root-mean-square error (RMSE) performance of 57.58% and 31.03%, respectively.","['Calibration', 'Global navigation satellite system', 'Sensors', 'Receivers', 'Satellites', 'Distance measurement', 'Mathematical models']","['Extended Kalman filter (EKF)', 'global navigation satellite system (GNSS)', 'tight integration', 'time calibration', 'ultrawideband (UWB)']"
"Radio frequency (RF)-based localization yields centimeter-accurate positions under mild propagation conditions. However, propagation conditions predominant in indoor environments (e.g. industrial production) are often challenging as signal blockage, diffraction and dense multipath lead to errors in the time of flight (TOF) estimation and hence to a degraded localization accuracy. A major topic in high-precision RF-based localization is the identification of such anomalous signals that negatively affect the localization performance, and to mitigate the errors introduced by them. As such signal and error characteristics depend on the environment, data-driven approaches have shown to be promising. However, there is a trade-off to a bad generalization and a need for an extensive and time-consuming recording of training data associated with it. We propose to use generative deep learning models for out-of-distribution detection based on channel impulse responses (CIRs). We use a Variational Autoencoder (VAE) to predict an anomaly score for the channel of a TOF-based Ultra-wideband (UWB) system. Our experiments show that a VAE trained only on line-of-sight (LOS) training data generalizes well to new environments and detects non-line-of-sight CIRs with an accuracy of 85%. We also show that integrating our anomaly score into a TOF-based extended Kalman filter (EKF) improves tracking performance by over 25%.","['Feature extraction', 'Location awareness', 'Sensors', 'Reliability', 'Training', 'Support vector machines', 'Receivers']","['NLOS identification', 'NLOS mitigation', 'channel quality estimation', 'CIR', 'UWB', 'deep learning', 'VAE']"
"Partial discharge is a well-established metric for condition assessment of high-voltage plant equipment. Traditional techniques for partial discharge detection involve physical connection of sensors to the device under observation, limiting sensors to monitoring of individual apparatus, and therefore, limiting coverage. Wireless measurement provides an attractive low-cost alternative. The measurement of the radiometric signal propagated from a partial discharge source allows for multiple plant items to be observed by a single sensor, without any physical connection to the plant. Moreover, the implementation of a large-scale wireless sensor network for radiometric monitoring facilitates a simple approach to high voltage fault diagnostics. However, accurate measurement typically requires fast data conversion rates to ensure accurate measurement of faults. The use of high-speed conversion requires continuous high-power dissipation, degrading sensor efficiency and increasing cost and complexity. Thus, we propose a radiometric sensor which utilizes a gated, pipelined, sample-and-hold based folding analogue-to-digital converter structure that only samples when a signal is received, reducing the power consumption and increasing the efficiency of the sensor. A proof of concept circuit has been developed using discrete components to evaluate the performance and power consumption of the system.","['Sensors', 'Radiometry', 'Power demand', 'Partial discharges', 'Wireless sensor networks', 'Monitoring', 'Wireless communication']","['Analog-digital conversion', 'partial discharge measurement', 'radiometers', 'UHF measurements', 'wireless sensor networks']"
"Among the different temperature measurement techniques providing micrometer resolution none of them provide fast response and easy access to close distances to the target surface in difficult to access areas. Optical fiber pyrometers provide that access but previous works used large optical fibers with numerical apertures limiting the minimum spot size to be measured. In this study, we propose a novel two colour optical fiber pyrometer based on a low diameter and numerical aperture optical fiber, low-noise photodetectors and high-gain transimpedance amplifiers with a high spatial resolution in the micrometre range and fast response. Using standard optical fibers and related devices provides also a low-cost system. The developed pyrometer presents a high spatial resolution of 16 μm for a target surface at 25 μm with a wide temperature range of 300 to 1200°C it being the highest spatial resolution for this kind of temperature systems. Theoretical analysis and measurements for different pyrometer configurations are reported. This study will help further the microthermography applications in machining processes.","['Spatial resolution', 'Temperature measurement', 'Optical fiber sensors', 'Optical fiber amplifiers', 'Photodetectors', 'Optical surface waves']","['Multimode fiber', 'optical fiber', 'single mode fiber', 'spatial resolution', 'temperature measurement', 'two color pyrometer']"
"A fiber based high sensitive bend sensor is proposed and demonstrated using a uniquely designed partially doped core fiber (PDCF). The fabrication method of PDCF with two core regions, namely an undoped outer region with a diameter of ~ 9.5 μm encompassing a doped, inner core region with a diameter of 4.00 μm is explained. The mechanism of bending effect in proposed PDCF and the experimental setup for amplified spontaneous emission (ASE) based sensor and fiber laser based sensor is illustrated. For ASE sensor, the higher ASE power level loss as the spooling radius is reduced from 20 to 3 cm is measured. The gain peak shift to shorter wavelength with respect to the decrease of the spooling radius from 20 to 3 cm due to higher bending loss at smaller bending radius is observed. The results are in agreement with overlap factor variation of PDCF. As expected from ASE peaks variation, the fiber laser sensor spectral operation is changed from 1539 to 1530 nm range. This phenomenon is due to higher mode field diameter of longer wavelength and result of optical filtering at longer wavelengths. The experimental results showed the output of the ASE is also highly stable, with no observable variation in the power output over a measurement period of 1 h. The PDCF is also temperature insensitive.","['Optical fiber sensors', 'Optical fiber amplifiers', 'Erbium', 'Doping', 'Refractive index']","['Optical sensors', 'partially doped core fiber', 'fiber laser sensor']"
"Electromagnetically induced transparency (EIT) originates from quantum physics, where a narrow transparent peak appears in the opaque band due to the destructive interference between quantum states of atoms and molecules. Similar phenomena can be realized based on strong-coupling resonators with a similar spectrum of transmission peaks and abrupt dispersion variations. These classical systems, ranging from elastic to optical, are named analogs of EIT. The sharp resonant peaks with high-quality factors in the spectrum exhibit powerful potentials in sensors with ultrahigh sensitivity. In order to better understand the development history of EIT-like metamaterials and their specific applications in the field of sensors, this article makes a brief review of the EIT-like phenomenon in metamaterials. First, we conduct the universal mathematical formulation based on the coupling oscillator model. Then, we classify specific metamaterial designs and practical applications of EIT-like devices in acoustic, electromagnetic, and optical waves, respectively. We also summarize the recent technologies of dynamic modulations of EIT-like metamaterials and discuss future research directions.","['Couplings', 'Metamaterials', 'Atom optics', 'Sensor phenomena and characterization', 'Optical sensors', 'Laser transitions', 'Springs']","['Electromagnetically induced transparency (EIT)', 'metamaterials', 'resonators', 'spectrum', 'strong coupling']"
"A novel background subtraction approach using an RGB-D camera and an adaptive blind updating policy is introduced. This method in the initialization creates a model to store background pixels to compare each pixel of the new frame with the model in the same location to identify background pixels. The background-model update presented in this paper uses regular and blind updates which also has different criteria from existing methods. In particular, blind update frequently changes based on the background changes and the speed of moving object. This will allow the scene model to adapt to the changes in the background, detecting the stationary moving object and reducing the ghost phenomenon. In addition, the proposed bootstrapping segmentation and shadow detection are added to the system to improve the accuracy of the algorithm in shadow and depth camouflage scenarios. The proposed method is compared with the original method and the other state of the art algorithms. The experimental results show significant improvement in those videos that stationary object appears. In addition, the benchmark results also indicate strong and stable results compared to the other state of the art algorithms.","['Adaptation models', 'Image color analysis', 'Sensors', 'Object detection', 'Cameras', 'Subtraction techniques', 'Heuristic algorithms']","['GPS-denied environments', 'dynamic environments', 'object detection', 'nonparametric background subtraction', 'background-model update', 'segmentation']"
"This work presents multiplexable fiber optical sensors to perform multiple-point pH measurements. Using a femtosecond laser direct writing approach, array of intrinsic Fabry-Peìrot interferometer (IFPI) sensors were inscribed in standard single-mode fibers. A sol-gel dip coating process was used to deposit Palladium-doped Titanium Dioxide (Pd-TiO 2 ) sensory film on IFPI sensors to perform pH measurement, while an uncoated sensor was used to measure temperature of aquatic solutions to remove influence of temperature fluctuation on pH measurements. Pd nanoparticles in sensory films acts as catalyst to convert hydrogen ion in aquatic solutions Pd hydride, which produce strains to IFPI sensors. White light interferometry demodulation algorithm was applied to resolve strain exerted to IFPI sensors induced by pH value changes in aquatic solution. The sensor exhibits reversible and reproducible response in aquatic solution with pH value from 1.0 to 7.0 at the room temperature. The response time of the pH sensors was approximately 7 s for all measurements performed. The sensor technology demonstrated in this paper has a potential to perform multi-point pH and temperature measurements using a single fiber.","['Sensors', 'Optical fiber sensors', 'Optical fibers', 'Sensor arrays', 'Temperature sensors', 'Sensor phenomena and characterization', 'Optical interferometry']","['Fabry-Perot', 'interferometers', 'optical fiber sensors', 'pH measurement']"
"We propose and demonstrate a fiber optic multi-point bending measurement system that uses Bragg gratings inscribed along a multi-core fiber (MCF) and a silicon avalanche photodiode (Si-APD) that produces two-photon absorption (TPA) photocurrent for 1.55-μmlight. Because the TPA photocurrent from the Si-APD is proportional to the mean square of the optical intensity, the intensity correlation between 1.55-μmprobe and reference beams is measured from it without using complex electrical circuits. The proposed system has different lengths of optical fibers between each core of MCF and input/output ends of a single mode fiber optic system, which makes different the correlation signal between the reference beam and the probe beam reflected from different Bragg gratings in MCF. That enables the reflection spectra of all the Bragg gratings in MCF to be separately measured even when they are overlapped in the wavelength domain. As a result, the bending radius and its direction are measured without using multiple detectors or optical switching devices. We conducted proof-of-concept experiments by using a 7-core MCF with almost the same Bragg gratings inscribed in each core at three points along the fiber. We also demonstrated an example of application to shape measurement.","['Optical fiber sensors', 'Optical fibers', 'Bending', 'Probes', 'Optical variables measurement', 'Optical reflection', 'Wavelength measurement']","['Bending measurement', 'fiber Bragg grating', 'multicore fiber', 'optical fiber sensor', 'two-photon absorption process']"
"Two correlated sources are transmitted over a block Rayleigh fading multiple access channel (MAC), where information recovery from only one of the sources is aimed at, and the other is used as a helper which helps recover the source sequences at the destination. In this article, the successful transmission is defined as in the case the transmission rates satisfy the intersection of both the MAC region and the Slepian-Wolf (SW) region with a helper, referred to as h-SW region, even though it is a sufficient condition. Then, the outage is defined as the average probability that the two regions do not have an intersection. The explicit expressions of the outage probability of the system are derived, in the form of multiple integrals with respect to the probability density function of instantaneous signal-to-noise ratios of each link. To have an in-depth insight regarding the contributions of some special cases to the successful transmission probability, the occurrence probabilities of these cases are also evaluated. The results indicate that outage probabilities decrease as the correlation between the source and helper information increases. The impacts of geometric gains of each link and the correlation of each link's fading variation are also considered. The most significant finding of this article is that h-SW over a MAC improves the throughput efficiency compared to the orthogonal transmission both in independent and correlated block Rayleigh fading channels without sacrificing the outage probability.","['Probability', 'Power system reliability', 'Sensors', 'Rayleigh channels', 'Signal to noise ratio', 'Correlation']","['Helper', 'multiple access channel', 'outage probability', 'Slepian-Wolf theorem']"
"An optical sensor based on a self-standing porous silicon (PS) membrane is presented. The sensor was created by electrochemically etching a heavily doped p-type silicon wafer with an organic electrolyte that contained dimethylformamide. After fabrication, a high-current density close to electropolishing was applied in order to allow the detachment from the substrate using a lift-off method. The PS membrane was integrated in a microfluidic cell for sensing purposes, and reflectance spectra were continuously obtained while the target substance was flowed. A comparison of the bulk sensitivity is achieved when flowing through and over the pores is reported. During the experiments, a maximum sensitivity of 770 nm/RIU measured at 1700 nm was achieved. Experimental sensitivity values are in good agreement with the theoretical calculations performed when flowing through the PS membrane, it means that the highest possible sensitivity of that sensor was achieved. In contrast, a drop in the sensitivity of around 25% was observed when flowing over the PS membrane.","['Sensors', 'Sensitivity', 'Silicon', 'Etching', 'Substrates', 'Microfluidics', 'Refractive index']","['Dimethylformamide', 'flow-through', 'lift-off', 'porous membrane', 'porous silicon', 'self-standing', 'sensing']"
"Automation is a key technology for a sustainable and secure meat sector in the future, both in terms of productivity and work environment. New robotic technologies, such as the so-called “meat factory cell,” (MFC) aim to contribute to this goal, but they require new “smart” tools that provide sensor feedback, which enable robots to perform complex tasks. This article presents one such tool: the smart knife, which gives real-time feedback on its contact status with meat, as well as cutting depth. The tool and the system are described, and its operation evidenced via electromagnetic (EM) simulation using the Ansys High-Frequency Structure Simulator. Furthermore, the performance of the knife is validated using pork loin meat: in the worst case, knife is shown to have an error of 1.78% for contact detection, and a mean error of 7.66 mm (±1.45 mm) for depth detection. This article also presents brief discussion regarding eventual use of the knife as part of the MFC control system, in addition to future work to be performed.","['Blades', 'Sensors', 'Robot sensing systems', 'Robots', 'Steel', 'Solid modeling', 'Task analysis']","['Microwave sensors', 'robot sensing systems', 'sensor systems and applications', 'smart knife']"
"The design and testing of optical sensors for a cello compatible with magnetic resonance imaging (MRI) are presented. The interface is used in neuroimaging experiments, allowing for the first time, the acquisition of cello performance gestures captured inside the MRI scanner. Left-hand fingering and right-hand bowing gestures were captured with optical sensors embedded in the fingerboard, bridge, and bow. Finger-string interaction was sensed through diffuse reflection using plastic optical fibres potted in a 3D-printed fingerboard. Acoustic string vibration was sensed through the transmissive amplitude modulation of an emitting-receiving fibre pair mounted in a specially designed bridge. Bow position and force was transduced with a combination of optical sensors, including optical flow, macrobending loss under compressive strain, and with a three-axis Faraday rotator. The sensors provided sufficient resolution and dynamic range to measure cello gestures inside the MRI scanner, and no interference from the scanner was noted in the acquired signals. Further MRI compatibility testing confirmed that the interface and sensors had no meaningful effect on field homogeneity and no image artifacts were noted. The interface is currently being used in neuroscience experiments.","['Optical sensors', 'Magnetic resonance imaging', 'Optical imaging', 'Optical device fabrication', 'Bridges', 'Optical fibers']","['Fibre optic sensors', 'MRI-compatible interface', 'musical performance gestures']"
"Nanoelectromechanical (NEMS) resonators are promising uncooled thermal infrared (IR) detectors to overcome existing sensitivity limits. Here, we investigated NEMS trampoline resonators made of silicon nitride (SiN) as thermal IR detectors. Trampolines have an enhanced responsivity of more than two orders of magnitude compared to state-of-the-art SiN drums. The characterized NEMS trampoline IR detectors yield a sensitivity in terms of noise equivalent power (NEP) of 7 pW/Hz−−−√and a thermal response time as low as 4 ms. The detector area features an impedance-matched metal thin-film absorber with a spectrally flat absorption of 50% over the entire mid-IR spectral range from 1 to 25μm.","['Detectors', 'Resonators', 'Sensors', 'Gold', 'Temperature measurement', 'Temperature sensors', 'Silicon nitride']","['Low-pressure chemical vapor deposition (LPCVD) silicon nitride (SiN)', 'nanoelectromechanical (NEMS)', 'thermal infrared (IR) detector', 'trampoline resonator']"
"Cardiovascular disease (CVD) is a widespread disease and the leading cause of death worldwide. Home care is essential for patients with CVD, and it involves the daily monitoring of important CVD-related vital signs using methods including electrocardiography (ECG), heart rate monitoring, pulse oximetry (SpO2), and continuous blood pressure measurement. However, a wearable device that can monitor these parameters simultaneously remains unavailable; herein, we propose a lightweight, highly integrated sensor that can do so. In this sensor, an analog front end (AFE) integrated chip (IC) is implemented in the sensor to detect one-lead ECG and two-wavelength photoplethysmography (PPG) signals. The highly integrated IC minimizes both the size and power requirement of the sensor. Moreover, its comprehensive functions include adjustable gain, current compensation, lead off, and fast recovery—all of which are crucial for wearable applications in large populations. Accordingly, the sensor can apply the adaptive adjust method to automatically adjust the IC parameters to suit a range of applications and users. In addition, the heart rate is calculated from the R-R interval from the ECG signals, whereas the SpO2 is calibrated with a univariate quadratic equation with less than 1% mean error. Our sensor can also calculate the systolic blood pressure (SBP) and diastolic blood pressure (DBP) by using a support vector machine based calibration-less model with the features of infrared PPG and ECG signals. The model is trained on our pre-collected wearable dataset and has a mean error ± standard deviation of −2.10 ± 7.07 mmHg for SBP and 0.04 ± 7.34 mmHg for DBP in 16 volunteers. In conclusion, this paper reports on a multimodal, vital sign monitoring sensor—with small size, low power, and dynamic compatibility—suitable for patients with CVD under home care.","['Electrocardiography', 'Light emitting diodes', 'Sensors', 'Biomedical monitoring', 'Integrated circuits', 'Monitoring', 'Decoding']","['Vital sign', 'multimodal', 'analog front end', 'SpO2', 'blood pressure']"
"We analyze the response of surface plasmon (SP) sensors using a transmission line model. We illustrate this analysis with particular reference to a layered structure in which plasmon hybridization occurs. By applying the appropriate resonant condition to the system, we derive a circuit model which predicts the responsivity of different modes. This gives new physical insight into the sensing process. We discuss how the change in the sample region may be modeled as a change in the reactance in the equivalent circuit and from this, it follows that a single parameter can determine the change in resonance position with reactance. This approach is used to predict the response of a generic sensor to binding of an analyte and the bulk change of refractive index. This parameter arises naturally from the circuit representation in a way not readily accessible with the transfer matrix approach. The parameters can be expressed in terms of theQof a resonant circuit and confirms the intuition that a highQis associated with poor responsivity, however, we demonstrate that there is another circuit parameter, the resistance at resonance, that can mitigate this effect, providing a route for optimization of the sensor properties.","['Sensors', 'Power transmission lines', 'Gold', 'Integrated circuit modeling', 'Impedance', 'RLC circuits', 'Refractive index']","['Surface plasmons', 'multilayer design', 'biological sensing and sensors', 'transmission line model']"
"We present a compact, versatile Hall readout system with digital output, fully integrated in 180nm technology. The core of the system is an instrumentation amplifier architecture that provides inherent anti-aliasing filtering, where the anti-aliasing characteristic is locked into a shape that maximally prevents aliasing to low frequencies. The efficiency for blocking out-of-band white noise is comparable to that of a second-order filter, eliminating the need for an explicit anti-aliasing filter before the ADC. Chopping/spinning is applied for up-modulating offset and 1/f noise to just beyond the signal band. A mostly-digital ripple reduction loop (RRL) is added for mitigating offset-related dynamic range limitations. In this, a bilinear integrator is introduced for eliminating the impact of the RRL on the system's DC gain. Moreover, the resolution of the DAC generating the analog offset compensation is reduced significantly, and the effect thereof is eliminated by digital noise cancellation logic. The one-step amplification and the simple, low-resolution DAC for offset compensation both aid in keeping the area footprint low: the analog circuits (including DAC and ADC) only occupy 0.21mm 2 . Notable performance characteristics are an input-referred noise floor of 55nT/ √{Hz} within a 410kHz bandwidth, a current consumption of only 5.1mA, and a 47dB dynamic range. The amplifier architecture can be easily applied as an analog preconditioning circuit in other sensor readout situations as well.","['Sensors', 'Bandwidth', 'Sensor phenomena and characterization', 'Instruments', 'Sensor systems', 'Magnetic domains', 'Temperature sensors']","['Instrumentation amplifiers', 'sensor readout', 'hall sensor', 'in-the-loop sampling amplifier (ILSA)']"
"Low-complexity and privacy-respecting human sensing is a challenging task in smart environments as it requires the orchestration of multiple sensors, low-impact machine learning (ML) methods, and resource-constrained Internet of Things (IoT) devices. Client/server-based architectures are typically employed to support sensor fusion. However, these architectures need data to be moved to/from the cloud or data centers, which is contrary to the fundamental requirement of the IoT applications to limit costs, complexity, memory footprint, processing, and communication resources. In this article, we propose the design and implementation of an integrated edge device targeting human sensing for indoor smart spaces applications envisioned in Industry 5.0 applications. The proposed device implements the cumulative sum (CUSUM) method for data distillation from multiple sensors and adopts a low-complexity random forest algorithm (RFA) to sense and classify body movements: in particular, the device integrates both infrared (IR) and ultrasonic (US) sensors. This article discusses the benefits of the combined use of CUSUM and RFA methods against classical ML approaches in terms of accuracy, complexity, computing time, and storage. The proposed architecture and processing steps are validated experimentally by targeting the fall detection problem in a smart space environment. RFA reduces the complexity by at least three times compared to classical ML tools based on the analysis of space and time features (convolutional neural networks and long short-term memory): processing time is in the order of 0.1 s while accuracy is about 94%.","['Sensors', 'Sensor fusion', 'Feature extraction', 'Intelligent sensors', 'Sensor phenomena and characterization', 'Complexity theory', 'Thermal sensors']","['Edge processing', 'fall detection', 'Internet of Things (IoT)', 'machine learning (ML)', 'random forest', 'sensor fusion']"
"This study focuses on the analysis and early detection of freeze-damage in tangerines using a specific double-needle sensor and Electrochemical Impedance Spectroscopy (EIS). Freeze damage may appear in citrus fruits both in the field and in postharvest processes resulting in quality loss and a difficult commercialization of the fruit. EIS has been used to test a set of homogeneous tangerine samples both fresh and later frozen to analyze electrochemical and biological differences. A double-needle electrode associated to a specifically designed electronic device and software has been designed and used to send an AC electric sinusoidal signal 1 V in amplitude and frequency range [100Hz to 1MHz] to the analyzed samples and then receive the electrochemical impedance response. EIS measurements lead to distinct values of both impedance module and phase of fresh and frozen samples over a wide frequency range. Statistical treatment of the received data set by Principal Components Analysis (PCA) and Partial Least Squares Discriminant Analysis (PLS-DA) shows a clear classification of the samples depending on the experienced freeze phenomenon, with high sensitivity (1.00), specificity (≥ 0.95) and confidence level (95%). Later Artificial Neural Networks (ANN) analysis based on 20-3-1 architecture has allowed to create a mathematical prediction model able to correctly classify 100% of the analyzed samples (CCR =100% for training, validation and test phases, and overall classification), being fast, easy, robust and reliable, and an interesting alternative method to the traditional laboratory analyses.","['Sensors', 'Impedance', 'Software', 'Sensor phenomena and characterization', 'Voltage measurement', 'Frequency measurement', 'Electrodes']","['Double-needle sensor', 'electrode', 'electrochemical impedance spectroscopy (EIS)', 'tangerine', 'freeze damage', 'detection', 'artificial neural networks (ANN)']"
"Single cell isolation is a crucial process step in many biological and pharmaceutical applications, as the number of technologies and assays for single cells is constantly rising. In this study, we propose a simple yet effective method for isolating single cells from a homogeneous solution. Therefore, we equip a high- throughput nano-dispenser system with a novel fluorescence-based in-flight cell detection sensor, which scans the dispensed droplets for the presence of a fluorescent cell on the fly. Based on initial studies on the dispensing physics, four different illumination and detection configurations of the sensor are presented and investigated. Finally, the system's performance in terms of detection rate, efficiency and process time is determined using cell-sized polystyrene microbeads as a reference standard. The results are very promising, as a 96-well plate is filled with single beads in under 60 seconds with a reproducible efficiency of more than 95%. Based on its generic design, the cell detection sensor is adaptable to virtually any low-volume dispenser, which is a major innovation over existing technologies.","['Liquids', 'Containers', 'Limiting', 'Throughput', 'Sensor systems', 'Physics']","['Single cell dispensing', 'fluorescent cell detection', 'sensor implementation']"
"With the advent of high-precision gyro laboratory prototypes and the widespread use of gyros in high-dynamic fields, the requirements for navigation calculations will definitely increase. The traditional cone error compensation algorithm is very effective in pure cone motion environment with small half cone angle. However not only the triple cross product term of the noncommutativity error is ignored but also the cross-product terms of the angular increment are simplified in this algorithm for convenience. Thus, the attitude error is extremely huge when the half cone angle is large or in high dynamic environments. Aiming at the above problems, this paper proposes an improved high-precision attitude compensation algorithm based on Taylor series expansion. In this improved algorithm, the neglected and simplified items indicated above are both considered carefully. On this basis, the compensation coefficients are given after deducing in detail. Our algorithm does not need to know the coefficient model in advance, so it is more practical and has advantages in deriving high-order algorithms or sculling compensation algorithms. In order to verify the performance of the algorithm, pure coning and high dynamic environment simulations are performed. And the results show that the improved attitude compensation algorithm can obtain higher precision, which proving its feasibility and effectiveness.","['Heuristic algorithms', 'Approximation algorithms', 'Mathematical model', 'Navigation', 'Dynamics', 'Quaternions', 'Error compensation']","['Noncommutativity error', 'attitude compensation', 'equivalent rotation vector', 'triple cross product']"
"Bruxism is a masticatory muscle activity that can involve involuntary clenching or grinding of teeth. As bruxing occurs subconsciously, patients are often unaware of it until injury or damage has occurred. Here, a novel method of intraoral detection of bruxism is proposed. A pressure sensor was used to detect the deformation of the masseter muscle as it approaches the upper molars during bruxism. Bruxism episodes detected using the masseter pressure sensor were compared with simultaneous recording of masseter muscle electromyogram (EMG) activity in eight subjects (5 male and 3 female, age 39.8+/−13.1). Each subject carried out a series of bruxism and non-bruxism simulated activities while wearing the masseter pressure device and recording masseter surface EMG. A linear discriminant analysis model was developed to classify recorded pressure data as bruxism or non-bruxism events and the model was tested using cross-validation. The accuracy of the sensor and machine-learning model was compared to that of the established method of detecting bruxism using masseter EMG. The masseter pressure and masseter EMG methods for detecting bruxism returned similar results with accuracy values of 82.2% and 82.8% respectively. Detecting bruxism through masseter pressure changes using a novel intra-oral device in combination with a machine-learning algorithm yields similar accuracy to established methods using masseter EMG. The masseter pressure approach has minimal impact on bite, it is less obtrusive than surface EMG, as it is discretely enclosed in the user’s mouth, and offers potential as a platform for long-term home monitoring of bruxism.","['Electromyography', 'Pressure sensors', 'Muscles', 'Teeth', 'Sensor phenomena and characterization', 'Skin', 'Machine learning']","['Bruxism', 'electromyography', 'masseter muscle', 'micro-electro-mechanical-system', 'pressure sensor', 'linear discriminant analysis']"
"The phenomenology of synaesthesia provides numerous cognitive benefits, which could be used towards augmenting interactive experiences with more refined multisensorial capabilities leading to more engaging and enriched experiences, better designs, and more transparent human-machine interfaces. In this study, we report a novel framework for the transformation of odours into the visual domain by applying the ideology from synaesthesia, to a low cost, portable, augmented reality/virtual reality system. The benefits of generating an artificial form of synesthesia are outlined and implemented using a custom made electronic nose to gather information about odour sources which is then sent to a mobile computing engine for characterisation, classification, and visualisation. The odours are visualised in the form of coloured 2D abstract shapes in real-time. Our results show that our affordable system has the potential to increase human odour discrimination comparable to that of natural syneasthesia highlighting the prospects for augmenting human-machine interfaces with an artificial form of this phenomenon.","['Olfactory', 'Sensor phenomena and characterization', 'Sensor arrays', 'Visualization', 'Gas detectors', 'Electronic noses', 'Engines']","['Augmented reality', 'e-nose', 'electronic nose', 'olfaction', 'syneasthesia', 'artificial syneasthesia', 'human-machine interface']"
"Monitoring of complex industrial processes can be achieved by obtaining process data by utilising various sensing modalities. The recent emergence of deep learning provides a new routine for processing multi-sensor information. However, the learning ability of shallow neural networks is insufficient, and the data amount required by deep networks is often too large for industrial scenarios. This paper provides a novel deep transfer learning method as a possible solution that offers an advantage of better learning ability of the deep network without the requirement for a large amount of training data. This paper presents how Transformer with self-attention trained from natural language can be transferred to the sensor fusion task. Our proposed method is tested on 3 datasets: condition monitoring of a hydraulic system, bearing, and gearbox dataset. The results show that the Transformer trained from natural language can effectively reduce the required data amount for using deep learning in industrial sensor fusion with high prediction accuracy. The difficult and uncertain artificial feature engineering which requires a large workload can also be eliminated, as the deep networks are able to extract features automatically. In addition, the self-attention mechanism of Transformer aids in the identification of critical sensors, hence the interpretability of deep learning in industrial sensor fusion can be improved.","['Sensors', 'Sensor fusion', 'Feature extraction', 'Deep learning', 'Task analysis', 'Transfer learning', 'Transformers']","['Transfer learning', 'deep learning', 'natural language processing', 'sensor fusion', 'sensor data processing', 'smart manufacturing']"
"Air pollution is known to be harmful to human health and the environment. Official air quality monitoring stations have been established across many smart cities around the world. Unfortunately, these monitoring stations are sparsely located and consequently do not provide high-resolution spatio-temporal air quality information. This article demonstrates how a dense sensor network deployment offers significant advantages in providing better and more detailed air quality information. We use data from a dense sensor network consisting of 126 low-cost sensors (LCSs) deployed in a highly populated district in Nanjing, China. Using data obtained from 13 existing reference stations installed in the same district, we propose three LCS validation methods to evaluate the performance of LCSs in the network. The methods assess the reliability, accuracy of tests, and failure and anomaly detection performance. We also demonstrate how the reliable data generated from the sensor network provides deep insights into air pollution information at a higher spatio-temporal resolution. We further discuss potential improvements and applications derived from the dense deployment of LCSs in cities.","['Sensors', 'Air pollution', 'Pollution measurement', 'Monitoring', 'Urban areas', 'Reliability', 'Earth']","['Air quality', 'anomaly detection', 'low-cost sensors (LCSs)', 'reference stations', 'sensor network', 'sensor validation']"
"Head-mounted displays (HMDs) have made a virtual reality (VR) accessible to a widespread consumer market, introducing a revolution in many applications. Among the limitations of current HMD technology, the need for generating high-resolution images and streaming them at adequate frame rates is one of the most critical. Super-resolution (SR) convolutional neural networks (CNNs) can be exploited to alleviate timing and bandwidth bottlenecks of video streaming by reconstructing high-resolution images locally (i.e., near the display). However, such techniques involve a significant amount of computations that makes their deployment within area-/power-constrained wearable devices often unfeasible. This research work originated from the consideration that the human eye can capture details with high acuity only within a certain region, called the fovea. Therefore, we designed a custom hardware architecture able to reconstruct high-resolution images by treating foveal region (FR) and peripheral region (PR) through accurate and inaccurate operations, respectively. Hardware experiments demonstrate the effectiveness of our proposal: a customized fast SR CNN (FSRCNN) accelerator realized as described here and implemented on a 28-nm process technology is able to process up to 214 ultrahigh definition frames/s, while consuming just 0.51 pJ/pixel without compromising the perceptual visual quality, thus achieving a 55% energy reduction and a×14times higher throughput rate, with respect to state-of-the-art competitors.","['Resists', 'Image reconstruction', 'Hardware', 'Superresolution', 'Rendering (computer graphics)', 'Wearable computers', 'Visualization']","['Hardware architecture', 'low power', 'super-resolution (SR)', 'wearable devices']"
"Additive Manufacturing (AM), or 3D printing, is a powerful technology that is revolutionizing the way designers create products across many industries. AM technology is severely limited by the lack of effective methods for in situ characterization of multi-material properties and composition during printing. The ability to detect the composition of multi-material printed inks in real time is an emerging need for a wide range of manufacturing applications. In this study, dielectric properties of embedded metal microparticles in a dielectric matrix are measured and characterized as a function of particle size, shape, volume percentage and frequency. Unexpectedly, there was no observation of any percolation threshold. The impedance was found to decrease with the percentage of metal filler as expected. While particle shape seemed to have significant effect on the impedance, there was little to no correlation between particle size and impedance. The resulting data can be used to generate a calibration curve correlating metal loading with impedance or capacitance. We discuss how this data can be used for in situ sensing of local ink composition, which does not currently exist, to facilitate greater control over the resulting properties and functionality of printed materials.","['Ink', 'Metals', 'Powders', 'Sensors', 'Capacitors', 'Frequency measurement', 'Three-dimensional printing']","['3D printing', 'additive manufacturing', 'in situ monitoring', 'material sensor', 'metal-polymer composites']"
"Many conventional Doppler analysis techniques for radar sensors suffer from velocity ambiguity when confronted with targets that are moving at speeds higher than the Nyquist velocity. This paper proposes a signal processing method for estimating accurately the Doppler velocity of targets from such sub-Nyquist radar data. The hypothesis tested in the paper is the possibility of circumventing the velocity ambiguity by exploiting the high range resolution of ultra-wideband radar sensors, more specifically incorporating the texture method applied to the signal envelope intensity profile. We employ a millimeter-wave radar sensor operating at 60 GHz to measure a rotating cylinder and a walking human to investigate the applicability of the proposed approach. The experimental results indicate that the proposed method can estimate the Doppler velocity accurately without aliasing from sub-Nyquist data, demonstrating that the use of the texture method is promising for resolving the ambiguity caused by sub-Nyquist sampling. The advantage of the proposed method is that the ambiguity can be resolved simply with the signal processing technique, and the approach does not require any custom hardware, such as nonuniform samplers. An important future study based on this paper is the application of the proposed approach to develop a low-cost radar system with a low sampling rate that can measure fast-moving targets.","['Doppler effect', 'Doppler radar', 'Sensors', 'Signal resolution', 'Velocity measurement', 'Radar measurements']","['Ultra-wideband radar', 'sub-Nyquist', 'velocity ambiguity', 'texture method', 'doppler analysis']"
"This paper presents a new fiber Bragg grating (FBG)-based technique to precisely detect the position of temperature or strain along tens of kilometers sensing fiber. The amplitude of backscattered signal is due to the internal Bragg grating, which increases the reflectivity of the backscattered signal. Therefore, a reflected signal from FBG is used to map the strain or temperature event. Moreover, the experimental results demonstrate the feasibility of FBG-based positioning method. Also, the positioning error is ~10 cm with spatial resolutions of 2, 5, 10, 20, 50, and 100 cm. In addition, the positioning error is independent of ambient temperature.","['Strain', 'Temperature sensors', 'Optical fibers', 'Optical fiber sensors', 'Bragg gratings', 'Temperature measurement']","['Brillouin scattering', 'Fiber Bragg Grating (FBG)', 'time-domain positioning', 'optical fiber distributed sensor']"
"We present the design and characterization of a fully-integrated array of 16 \times 16Single-Photon Avalanche Diodes (SPADs) with fast-gating capabilities and 16 on-chip 6 ps time-to-digital converters, which has been embedded in a compact imaging module. Such sensor has been developed for Non-Line-Of-Sight imaging applications, which require: i) a narrow instrument response function, for a centimeteraccurate single-shot precision; ii) fast-gated SPADs, for time-filtering of directly reflected photons; iii) high photon detection probability, for acquiring faint signals undergoing multiple scattering events. Thanks to a novel multiple differential SPAD-SPAD sensing approach, SPAD detectors can be swiftly activated in less than 500 ps and the full-width at half maximum of the instrument response function is always less than 75 ps (60 ps on average). Temporal responses are consistently uniform throughout the gate window, showing just few picoseconds of time dispersion when 30 ns gate pulses are applied, while the differential non-linearity is as low as 250 fs. With a photon detection probability peak of 70% at 490 nm, a fill-factor of 9.6% and up to 1.6 \cdot 10^{8}photon time-tagging measurements per second, such sensor fulfills the demand for fully-integrated imaging solutions optimized for non-line-of-sight imaging applications, enabling to cut exposure times while also optimizing size, weight, power and cost, thus paving the way for further scaled architectures.","['Single-photon avalanche diodes', 'Sensors', 'Photonics', 'Imaging', 'Sensor phenomena and characterization', 'Nonlinear optics', 'Logic gates']","['3-D imaging', 'fast-gating', 'image sensor', 'non-line-of-sight imaging', 'single-photon avalanche diodes', 'SPAD array', 'time-correlated single-photon counting', 'time-of-flight', 'time-to-digital converters']"
"Our purpose was to demonstrate the possibility of providing foot-healthcare application by using an in-shoe motion sensor (IMS) through validating the feasibility of applying an IMS for measuring the first metatarsophalangeal angle (FMTPA), which is the most important parameter regarding the common foot problem hallux valgus. Methods: The IMS signals can represent foot motions when the mid-foot and hindfoot were modelled as a rigid body. FMTPAs can be estimated from the foot-motion signals measured using an IMS embedded beneath the foot arch near the calcaneus side using a machine-learning method. The foot-motion signals were collected from 50 participants with different FMTPAs. The true FMTPAs were assessed from digital photography. Correlation-based feature-selection processes (significance level {p} < 0.05 ) were used to search for the predictors from the foot-motion signals. Leave-one-subject-out cross-validation, root mean squared error, and intra-class coefficients were used for FMTPA-estimation model evaluation. Results: Eleven FMTPA-impacted gait-phase clusters, which were used to construct effective foot-motion predictors, were observed in all gait-cycle periods except terminal swing. The range of the foot motion in the sagittal and coronal planes significantly correlated with the FMTPA ( {p} < 0.05 ). Linear regression could be the best method for constructing an FMTPA estimation model with a root mean squared error and intra-class correlation coefficient of 4.2 degrees and 0.789, respectively. Conclusion: The results indicate the reliability of our FMTPA estimation model constructed from foot-motion signals and the possibility to providing foot-healthcare applications by using an IMS.","['Foot', 'Legged locomotion', 'Footwear', 'Monitoring', 'Particle measurements', 'Atmospheric measurements', 'Intelligent sensors']","['Inertial sensor', 'foot-motion', 'foot health', 'gait analysis', 'hallux valgus']"
"This study presents a method for predicting location classes of a room such as a kitchen, and restroom, where a user is located by discovering location-specific sensor data motifs in sensor data observed by user’s sensor devices, such as smartwatch, without requiring labeled training data collected in a target environment. For example, we can observe similar waveforms corresponding to kitchen knife chopping actions using body-worn accelerometers in kitchens and can also observe similar sound features by active sound probing in bathrooms because of their water-resistant walls. This indicates that such location-specific sensor data motifs can be inherent information for location class prediction in almost every environment. This study proposes a novel method that automatically detects location-specific motifs from time series sensor data by calculating a score that represents the “location specificity” of each motif in a time series. Previous studies on location class prediction assume that location-specific sensor data are always observed in a room or use handcrafted rules and templates to detect location-specific sensor data resulting in difficulties in applying them to several realistic environments. In contrast, our method, named IndoLabel, can automatically discover short sensor data motifs, specific to a location class, and can automatically build an environment-independent location classifier without requiring handcrafted rules and templates. The proposed method was evaluated in real house environments using leave-one-environment-out cross-validation and achieved a state-of-the-art performance although labeled training data in the target environment was unavailable.","['Sensors', 'Sensor phenomena and characterization', 'Feature extraction', 'Training', 'Wireless fidelity', 'Time series analysis', 'Semantics']","['Indoor positioning', 'location class prediction', 'frequent pattern mining']"
"Nowadays, the flexible localization solution for various devices for workplace safety is one of the most demanding research questions. Notably, it is expected to provide an acceptable level of precision in different types of environments empowered by wearable technology and Internet-of-Things (IoT) devices. Existing leading localization technologies are adapted for certain conditions, for example, Wi-Fi, Bluetooth low energy (BLE), and ultra-wideband (UWB) are used for indoor areas and various global navigation satellite system (GNSS)-based ones for outdoors. This work focuses on investigating the long-range wide-area network (LoRaWAN) (868-MHz band) as a potential candidate to bridge this gap, being one of the most reliable and recognized communication technologies for the Industrial IoT (IIoT). In the past, the research community had a lot of critics with respect to the applicability of LoRaWAN for localization, while the vision is facing tremendous change over the past two years. The purpose of this work is to assess the feasibility of LoRaWAN as a localization solution for work safety applications in the industrial scenario from different angles. The work is based on two measurement campaigns conducted at the Brno University of Technology (BUT), Brno, Czech Republic, and the University Politechnica of Bucharest (UPB), Bucharest, Romania. The campaigns cover both indoor and outdoor scenarios and provide the practical limitations of the positioning in standalone and {k}-nearest neighbors ( {k}-NN) powered localization systems. According to the results, LoRaWAN-based localization with relatively dense gateways (GWs) deployment allows for achieving a meter-level accuracy, which may be suitable for the localization of workers.","['Location awareness', 'Fingerprint recognition', 'Wearable computers', 'Wireless fidelity', 'Sensor phenomena and characterization', 'Industrial Internet of Things', 'Indoor environment']","['Error analysis', 'Industrial Internet of Things (IIoT)', 'Industry 40', 'Internet of Things (IoT)', 'localization', 'long-range wide-area network (LoRaWAN)', 'measurement error', 'position measurement', 'wearables']"
"Monitoring the gases released during breathing or via the skin has gained significance towards diagnosing diseases. In this study, a sensor chip capable of detecting nonanal gas, which is known to be a marker of lung cancer, was developed. The gas detection agent used was vanillin, which underwent aldol condensation with nonanal in the presence of a basic catalyst, resulting in the formation of an unsaturated aldehyde. Porous glass was used as the reaction field to carry the detection agent. Alkali-resistant porous glass was chosen because conventional porous glasses show low durabilities under basic conditions, as they primarily consist of SiO 2 . Nonanal can be detected through changes in the absorption spectrum of the sensor. An accumulate-type sensor was used in this study, which exhibited a linear relationship between the degree of absorption changes at 470 nm and nonanal concentration in the 60 ppb-1.3 ppm range. Therefore, this biological marker gas sensor is effective for the early diagnosis of diseases. The alkali-resistant porous glass sensor chip exhibited a higher degree of absorption change than the conventional porous glass sensor chip.","['Glass', 'Sensors', 'Gas detectors', 'Sensitivity', 'Absorption', 'Sensor phenomena and characterization', 'Chemicals']","['Aldol condensation', 'alkali-resistant glass', 'gas sensor', 'nonanal', 'porous glass', 'vanillin']"
"The 3D time-of-flight (ToF) cameras have recently received a lot of attention due to their wide range of applications. Despite remarkable advancements in ToF imaging, state-of-the-art ToF cameras are still afflicted by the power hungriness of their illumination sources. To tackle this problem, we exploited existing lighting infrastructure, which ensures the ubiquitous presence of modulated light sources in indoor spaces that serve as opportunity illuminators. We explored the bistatic geometry for passive imaging using the pulse-based ToF approach. Our work is inspired by the recently introduced visible light communication (VLC) or light-fidelity (Li-Fi) infrastructure. VLC allows the infrastructure to provide indoor simultaneous illumination, communication, and sensing (SICS). To this end, we designed a bistatic geometry for the purpose of attaining passive 3D imaging. Such capabilities are achieved by exploiting the pulse shape of the autocorrelation function of real optical signals generated by VLC/Li-Fi modules (e.g., OpenVLC and LiFiMAX). We demonstrated passive imaging by means of matched filtering. In this work, we also studied different sampling strategies in the time-shift domain, including uniform, random, and sparse rulers, which is another step forward toward preserving high depth accuracy with a minimal number of measurements. The proposed methodology achieved successful depth reconstruction with negligible root-mean-square error (RMSE) for the low signal-to-noise ratio (SNR) of the measurements. Parametric models, such as Gaussian and sum of sines, are used to characterize the cross correlation functions and allow for robust parametric depth retrieval from a few measurements. Moreover, we attained a 20-mm worst case error for a target at 25 cm. The experiment proved that the bistatic passive depth reconstruction is feasible.","['Sensors', 'Imaging', 'Cameras', 'Lighting', 'Three-dimensional displays', 'Light emitting diodes', 'Visible light communication']","['Bistatic', 'depth imaging', 'light-fidelity (Li-Fi)', 'OpenVLC', 'passive sensing', 'sampling', 'time-of-flight (ToF)', 'visible light communication (VLC)']"
"This paper presents a multi-channel acoustic transducer that works within the audible frequency range (250-5500 Hz) and mimics the operation of the cochlea by filtering incoming sound. The transducer is composed of eight thin film piezoelectric cantilever beams with different resonance frequencies. The transducer is well suited to be implanted in middle ear cavity with an active volume of 5 mm×5mm×0.62mm and mass of 4.8 mg. Resonance frequencies and piezoelectric outputs of the beams are modeled with Finite Element Method (FEM). Vibration experiments showed that the transducer is capable of generating up to 139.36 mVppunder 0.1 g excitation. Test results are consistent with the FEM model on frequency (97%) and output voltage (89%) values. Device was further tested with acoustic excitation on an artificial tympanic membrane and flexible substrate. Under acoustic excitation, 50.7 mVppoutput voltage generated under 100 dB Sound Pressure Level (SPL). Output voltages observed in acoustical and mechanical characterizations are the highest values reported to the best of our knowledge. Finally, to assess the feasibility of the transducer in daily sound levels, it was excited with a speech sample and output signal was recovered. Time-domain waveforms of the recorded and recovered signals showed close patterns.","['Transducers', 'Ear', 'Auditory system', 'Sensors', 'Cochlear implants', 'Resonant frequency', 'Micromechanical devices']","['Acoustic transducer', 'fully implantable cochlear implant', 'MEMS', 'thin film piezoelectric']"
"We introduce a multi-sensor array consisting of multiple microphones and a stereo-vision system that enables to track three-dimensional (3D) flight paths of bats. Field applicability is a major design factor which led to a portable battery powered system. Eight microphones form the acoustic array that allows the detection and localization of ultrasonic echolocation calls. Two near infrared sensitive cameras provide a visual identification of the three dimensional flight path. We minimized costs of this multi-sensor arrays by developing the system from ground up, including a novel acoustic signal recording and processing hardware. Our acoustic data acquisition system is the first to sample eight microphones simultaneously at 1MS/s/CH with 16 Bit resolution without the necessity of computers or laptops. All channels are processed in real time in the frequency and time domain to evaluate the occurrence of echolocation calls. Hereby, long term deployments of the system are achieved since only relevant signals are recorded. Parallel to the acoustic acquisition, the system records images of two near-infrared (IR) sensitive cameras at 12Hz frame rate each. Combined with IR illumination 3D flight paths can be extracted in post-processing. In addition, the system is able to log several environmental parameters, such as temperature and humidity. Recorded datasets can be associated with a global position and time-stamp through a GPS module and transformed to GPS coordinates with a three-axis accelerometer to determine the angle. The overall system (microphones, cameras, and sensors) was designed to be affordable, user-friendly, battery powered and without the necessity of an additional laptop.","['Acoustics', 'Cameras', 'Microphone arrays', 'Hardware', 'Sensors', 'Data acquisition']","['Microphone', 'array', 'stereo camera', 'infrared', 'automation', 'ultrasound', 'source localization', 'bat']"
"A new tunable electro-optical and thermal-optical modulator based on a liquid crystal (LC) filled side hole fiber (SHF) in fiber ring laser (FRL) system has been proposed and experimentally demonstrated. The LC is penetrated into the air hole of SHF, and the refractive index (RI) of LC will be altered when an external electrical field or temperature difference is applied due to thermal-optical effects and electro-optic effects. SHF is used as both a filter and a sensing head in the laser cavity. Besides, the resonance wavelength is more sensitive to the temperature variations due to high thermal-optic coefficient when incident light interacts with the internal infiltrated LC. Meanwhile, the sensor has good performance with a high extinction ratio (>42 dB) and a narrow 3-dB bandwidth (less than 0.08 nm). The consequent thermal and electronic sensitivity are measured to be −4.23 nm/°C and 0.604nm/V, respectively. Moreover, the excellent and reliable performance expected to monitor the temperature and electric field as the wavelength-tunable electro-optical devices within a small temperature change range is needed.","['Optical fiber sensors', 'Optical fibers', 'Temperature sensors', 'Modulation', 'Electric fields', 'Refractive index', 'Optical variables control']","['Electro-optical modulator', 'thermal optical modulator', 'fiber ring laser sensor']"
"In this paper, we propose a new Quartz Crystal Microbalance (QCM) sensor with temperature control, which can assess outgassing properties during spacecraft development. It will be referred to as the “Twin-QCM sensor”. There are two types. The Twin-Cryogenic QCM (Twin-CQCM) sensor is warmed by a built-in heater with an operating temperature of -190 to +125°C, and the Twin-ThermoelectricQCM (Twin-TQCM) sensor is warmed and cooled by a built-in Peltier module that can control the temperature within -80 to +125°C. Using a temperature compensation technique, the temperature-dependent drift of the frequency was found to be less than ±10 ppm over all operating temperatures. An RTD temperature sensor was mounted on the quartz crystal to improve the accuracy of temperature measurement. To confirm the accuracy, an additional temperature sensor installed at the center of the crystal. The sensor output temperature value was compared to that of the additional sensor, with the difference between both temperature sensors being +0.4 to +2.6°C in the temperature range from -130 to +100°C. Through the measurement of the sensor's dynamic range, it was found that the deposited contaminant film in a vacuum increasingly changed such physical properties as viscoelasticity as the temperature increases.","['Temperature sensors', 'Sensors', 'Temperature measurement', 'Electrodes', 'Sensor phenomena and characterization', 'Quartz crystals', 'Oscillators']","['Quartz crystal microbalance (QCM)', 'temperature compensation', 'spacecraft', 'outgas', 'contamination', 'atomic oxygen']"
"We have devised a method that employs moving electrode electrochemical impedance spectroscopy to monitor the sedimentation of particles in conductive suspensions. In contrast to standard electrochemical cells with a fixed geometry, our cell has a flexible design with a movable counter electrode that allows precise adjustment of the electrode distance. Measuring the electrical impedance at various electrode spacings and utilizing the linear dependence of this function on the electrode displacement enables probing of a small section of the sample. This has considerable advantages when heterogenous liquids (e.g., suspensions) are to be analyzed. We applied our moving electrode approach to various test cases and obtained the following results: (i) We demonstrated by experiment that the bulk conductivity can be measured correctly even if particle sediments cover the electrode surface. (ii) We studied monodisperse suspensions of various compositions and investigated the effect of particle concentrations and size on conductivity. (iii) We monitored the particle sedimentation process and, by combining experimental and theoretical results, identified a correlation between the growing mass of the sedimentation layer and the impedance measured. The intended application of our approach is to monitor crystallization processes in ionic liquids for use in zeolite synthesis.","['Electrodes', 'Conductivity', 'Atmospheric measurements', 'Particle measurements', 'Impedance', 'Resistance', 'Sensors']","['Conductivity', 'electrochemical impedance spectroscopy', 'sedimentation', 'suspension', 'zeolite synthesis']"
"Intersatellite laser interferometers feature quadrant photoreceivers to produce electrical signals from the interfered optical beams. In the particular case of Laser Interferometer Space Antenna, the expected optical ac beat note has an amplitude of the order of nanowatts. This requires photoreceivers with an input current noise density of a few pA·Hz-1/2 in each channel up to 25 MHz. In addition, the significant number of photoreceivers in a single spacecraft imposes tight constraints on the power consumption per device. We present the experimental characterization of a quadrant photoreceiver based on discrete heterojunction bipolar transistors and an off-the-shelf 0.5-mm-diameter InGaAs quadrant photodiode, showing an input current noise density of 1.9 pA·Hz -1/2 at 25 MHz, a 3-dB bandwidth of 37 MHz, and a total power consumption of 178 mW.","['Interferometers', 'Heterojunction bipolar transistors', 'Power demand', 'Bandwidth', 'Integrated circuits', 'Laser beams']","['Photoreceiver', 'transimpedance amplifier', 'heterodyne laser interferometry', 'intersatellite metrology', 'gravitational waves', 'geodesy']"
"A novel Doppler shift based technique for measurement of free-swimming fish speed in marine farms using acoustic telemetry tags was developed and evaluated in this study. The proposed method can potentially augment current telemetry systems with a new biologically relevant measurement without significantly changing the size and energy constrained tag-side of the telemetry systems. For speeds in the range of 20cm s -1 -110cm s -1 an overall relative rms error of less than 10% in measured speed based on the proposed Doppler method was achieved in the tests conducted at a fully stocked commercial fish cage, with an rms error of 7.85cm s -1 (std. dev. 7.5cm s -1 ). The study thus demonstrates the feasibility of measuring the swimming speeds of individual free-ranging fish using this method.","['Acoustics', 'Sports', 'Receivers', 'Sensors', 'Telemetry', 'Acoustic measurements', 'Transmitters']","['Acoustic signal processing', 'acoustic telemetry', 'Doppler measurement', 'fast Fourier transform', 'marine aquaculture', 'sensor phenomena & characterization']"
"Automatic speech recognition (ASR) based on surface electromyography (sEMG) sensors is an important technology converting electrical signals into computer-readable textual messages, which can overcome the limitation of acoustic sensors that are easily contaminated by environmental noises. However, current placements of sEMG sensors mainly depend on the experimenter's experience, which could miss important information about the major muscular activities and lead to the decline of classification performance. In this study, 120 closely-spaced sEMG sensors were utilized to collect high-density sEMG signals for recognizing ten digits in English and Chinese. The linear discriminant analysis classifier was used to classify the speaking tasks, and the sequential forward selection algorithm was utilized for analyzing the optimal position of the sensors. The results showed that the HD sEMG energy maps could help visualize the dynamic muscle activities during the speaking process, and significantly different muscular contraction patterns were observed for different speaking tasks. The classification accuracies when using the facial sensors were significantly lower than those on the neck, although with the same number of sensors. Moreover, the classification rates could be higher than 90% with only 15 optimally selected sensors that were mainly distributed on the neck instead of the face. This study suggests that the neck muscles could be the main contributor, and more sEMG sensors should be placed on the neck to improve the ASR performance. The findings of this study could provide valuable clues for the development of a practical sEMG-based speech recognition system, especially for patients with speaking disorders.","['Sensors', 'Muscles', 'Neck', 'Sensor systems', 'Speech recognition', 'Face recognition', 'Sensor phenomena and characterization']","['Automatic speech recognition', 'high-density surface electromyography', 'sensors placement', 'sequential forward selection algorithm']"
"The advancement in smart materials allows researchers to seek smart textiles for wearable health monitoring. Here, a washable and flexible textile-based dry electroencephalography (EEG) electrode that can detect brain activities has been developed. The EEG electrodes were constructed from an electrically conductive cotton fabric with67.23 Ω/sq produced through printing PEDOT:PSS/PDMS conductive polymer composite on cotton fabric via screen printing. The mechanical properties like flexural rigidity and tensile strength of the conductive fabric were compared against the bare base material and a PEDOT:PSS-printed fabric. The result from an SEM revealed a uniform printing of the PEDOT:PSS/PDMS on the fabric. The signal-to-noise ratio of the textrode was higher than the Ag/AgCl dry electrode i.e. 17.32 (+3.1%) which open the door for long-term EEG monitoring. Moreover, the electrode can give clear and reliable EEG signals up to 15 washing cycles, 60 bending cycles, 5 multiple uses, and 8 hours of continued use.","['Electrodes', 'Electroencephalography', 'Fabrics', 'Printing', 'Cotton', 'Monitoring', 'Skin']","['Brain activity', 'dry EEG electrode', 'e-textile', 'PEDOT:PSS/PDMS', 'textrode']"
"Magnetic tracking systems have been widely investigated in biomedical engineering due to the transparency of the human body to static magnetic fields. We recently proposed a novel human-machine interface for prosthetic application, namely the myokinetic interface. This controls multi-articulated prostheses by tracking magnets implanted in the residual muscles of individuals with amputation. Previous studies in this area focused solely on the choice and tuning of the localization algorithm. Here, we addressed the role of the intrinsic properties of the sensors, by analysing their effects on the tracking accuracy and on the computation time of the localization algorithm, through experimentally-verified computer simulations. We observed that the tracking accuracy is primarily affected by the localization rate, which is directly related to the sampling frequency of the sensors, and less significantly affected by the sensor resolution. The computation time, instead, proved positively correlated to the number of MMs, and negatively correlated with the localization rate. Our results may contribute to the development of novel human-machine interfaces for prosthetic limbs and could be extended to a broad range of applications involving magnetic tracking.","['Sensors', 'Location awareness', 'Magnetic sensors', 'Magnetostatics', 'Target tracking', 'Sensor phenomena and characterization', 'Muscles']","['Human-machine interface', 'magnetic field', 'magnetic tracking', 'myokinetic interface', 'sensor selection', 'upper limb prosthetics']"
"Goal: Gait monitoring is useful for diagnosing movement disorders or assessing surgical outcomes. This paper presents an algorithm for estimating pelvis, thigh, shank, and foot kinematics during walking using only two or three wearable inertial sensors. Methods: The algorithm makes novel use of a Lie-group-based extended Kalman filter. The algorithm iterates through the prediction (kinematic equation), measurement (pelvis position pseudo-measurements, zero-velocity update, and flat-floor assumption), and constraint update (hinged knee and ankle joints, constant leg lengths). Results: The inertial motion capture algorithm was extensively evaluated on two datasets showing its performance against two standard benchmark approaches in optical motion capture (i.e., plug-in gait (commonly used in gait analysis) and a kinematic fit (commonly used in animation, robotics, and musculoskeleton simulation)), giving insight into the similarity and differences between the said approaches used in different application areas. The overall mean body segment position (relative to mid-pelvis origin) and orientation error magnitude of our algorithm ( n=14 participants) for free walking was 5.93 ± 1.33 cm and 13.43 ±1.89 ° when using three IMUs placed on the feet and pelvis, and 6.35 ± 1.20 cm and 12.71 ±1.60 ° when using only two IMUs placed on the feet. Conclusion: The algorithm was able to track the joint angles in the sagittal plane for straight walking well, but requires improvement for unscripted movements (e.g., turning around, side steps), especially for dynamic movements or when considering clinical applications. Significance: This work has brought us closer to comprehensive remote gait monitoring using IMUs on the shoes. The low computational cost also suggests that it can be used in real-time with gait assistive devices.","['Kinematics', 'Sensors', 'Pelvis', 'Foot', 'Thigh', 'Legged locomotion', 'Tracking']","['Biomedical monitoring', 'gait analysis', 'IMUs', 'Kalman filters', 'lie group theory', 'motion analysis', 'pose estimation', 'wearable sensors']"
"A fiber laser cavity was employed for the first time in fiber-optic multi-zone perimeter intrusion detection system. A length of four-core armored fiber cable was configured into multiple sections with each defensing a perimeter zone. For each defensed perimeter zone, only three single-mode fibers of the four-core fiber cable were employed for intrusion detection. Two of them were used for constructing a Michelson interferometer, which sensed the disturbance excited by intruders, whereas, the other one carried light between the sensing site and the receiver site. The most distinctive part of the proposed system lies in the use of a fiber laser cavity with the Michelson interferometer serving as an end mirror for intrusion detection at each perimeter zone. Experimental results showed that upon intrusion, a 100% alarm rate could be reached by properly setting the thresholds of three intrusion parameters for alarm and that the properly set thresholds could reliably trigger alarms upon intrusion under various weather conditions.","['Optical fiber cables', 'Optical fiber sensors', 'Optical interferometry', 'Optical fiber couplers', 'Laser excitation']","['Fiber-optic intrusion detection', 'multiple perimeter zones', 'Michelson interferometer', 'fiber laser cavity']"
"Fish detection in shallow water conditions or close to the bottom poses specific challenges, which often can hardly be addressed using typical acoustical or optical means. This significantly limits the possibilities of fish population monitoring and the use of fish detection for optimizing fishing techniques in such environments. Here, we investigate the potential of electrical impedance spectroscopy (EIS) as a remote-sensing technique for fish detection, to address this issue. The approach utilizes a set of electrodes to generate a low-intensity alternating electric current through water and to measure the impedance. We conducted a number of experiments in which a fish was swimming in a glass tank. We measured changes in impedance at different frequencies, using different electrode arrangements, while simultaneously tracking the fish position with a pair of cameras. We also created numerical models of the experimental setup using the finite-element method. The results of simulations were compared to the experimental data to test the accuracy of predictions and to determine if such a model could be a useful tool for designing detection setups for specific environmental conditions. We measured the robustness of the system to the presence of air bubbles as a potential source of disturbances. Our results show a clear correlation between the observed drops in both real and imaginary parts of the measured impedance and the position of the fish. Model simulations showed qualitatively and quantitatively similar results, indicating that such models could be suitable for practical applications.","['Fish', 'Impedance', 'Electrodes', 'Impedance measurement', 'Frequency measurement', 'Cameras', 'Current measurement']","['Electrical impedance', 'fish detection', 'remote sensing', 'spectroscopy']"
"Force sensing is a key enabler for getting haptic feedback, useful in a variety of applications, especially in the fields of robotics, automation, and health. Indeed, equipping machines, vehicles, robots, and even humans with force sensors provides controlled processes and production, safe and enhanced external interaction, and capability to perform efficient manipulation and precise movements. Aiming to develop an alternative solution to electrical force sensors, in this work a fiber Bragg grating (FBG) is embedded inside a patch made of silicone rubber. Such embedding strategy allows to make the FBG sensitive to the force variations, obtain a flexible patch having a moldable shape, and protect the most fragile areas of the optical fiber. Moreover, due to its high flexibility and stretchability, the sensing patch can be easily employed as portable and wearable device. Besides reporting fabrication process and results of the performed force tests, this work provides a systematic study of the FBG embedding in a silicone matrix. Indeed, for this purpose, three sensing patches having different thicknesses are developed and tested in temperature, strain, and force, finding that the patch thickness influences the sensing performances of the device. The resulting force sensitivity varies in the range from 9.2 to 19.0 pm/N, based on the sensor thickness. Temperature sensitivity, instead, is comparable with respect to bare FBGs, while strain sensitivity is enormously reduced, obtaining a patch able to insulate the FBG from the strain variations.","['Fiber gratings', 'Sensors', 'Robot sensing systems', 'Optical fiber sensors', 'Temperature sensors', 'Force', 'Sensitivity']","['Fiber Bragg grating (FBG) embedding', 'FBGs', 'fiber optic sensors', 'force sensing', 'haptic feedback', 'silicone']"
"The availability of inertial navigation sensors in smartphones has facilitated the development of pedestrian dead reckoning (PDR) models on a large scale. These models often consist of a step detection algorithm combined with a heading estimation routine. Common approaches to step detection include searching for peaks/valleys in the acceleration signal, principle frequency estimation, and machine learning techniques. Since the sensors embedded in smart devices are prone to noise, the position error grows unbounded if unchecked and requires periodical corrections based on external measurements. In this work, we propose a novel step detection algorithm based on sine-wave approximation of the acceleration signal. This method detects step fractions as well as full steps, which allows for continuous and real-time updates. The step detection algorithm is combined with a heading estimation routine described in our previous work to obtain a stand-alone PDR model. To mitigate error accumulation, we fuse the proposed model with position and heading measurements provided by a commercial indoor positioning system based on ultrasound. We evaluate the performance of the PDR and fused model in an open office environment, by walking along a trajectory while carrying a smartphone in hand or in the pocket. The results demonstrate the feasibility of the sine-wave approximation approach to step detection, as well as the expected benefits of fusing PDR with the ultrasonic system.","['Acoustics', 'Accelerometers', 'Sensors', 'Detection algorithms', 'Position measurement', 'Legged locomotion', 'Estimation']","['Indoor positioning', 'pedestrian dead reckoning', 'step detection', 'ultrasonic positioning system']"
"Formaldehyde is a carcinogenic indoor air pollutant emitted from common wood-based materials. Low-cost sensing of formaldehyde is difficult due to inaccuracies in measuring low concentrations and susceptibility of sensors to changing indoor environmental conditions. Currently gas sensors are calibrated by manufacturers using simplistic models which fail to capture their complex behaviour. We evaluated different low-cost gas sensors to ascertain a suitable component to create a mobile sensing node and built a calibration algorithm to correct it. We compared the performance of 2 electrochemical sensors and 3 metal oxide sensors in a controlled chamber against a photo-acoustic reference device. In the chamber the formaldehyde concentrations, temperature and humidity were varied to assess the sensors in diverse environments. Pre-calibration, the electrochemical sensors (mean absolute error (MAE) = 70.8 ppb) outperformed the best performing metal oxide sensor (MAE = 335 ppb). A two-stage calibration model was built, using linear regression followed by random forest, where the residual of the first stage acted as a input for the second. Post-calibration, the metal oxide sensors (MAE = 154 ppb) improved compared to their electrochemical counterparts (MAE = 78.8 ppb). Nevertheless, the uncalibrated electrochemical sensor showed overall superior performance hence was selected for the mobile sensing node.","['Sensors', 'Gas detectors', 'Temperature sensors', 'Calibration', 'Metals', 'Sensor phenomena and characterization', 'Pollution measurement']","['Air quality monitoring', 'formaldehyde detection', 'metal oxide semiconductor', 'electrochemical sensors', 'machine learning', 'gas sensors']"
"This paper reports a novel construction of a micromachined MEMS magnetometer characterized by static magnetic fields of CERN's reference dipole with a custom made capacitive read-out. The magnetic flux density is characterized via the vibration modes of the MEMS structure, which are sensed capacitively. The device consists of a single-crystal silicon clamped-free plate (cantilever) carrying a thin conductor. The cantilever and the thin film metal electrodes are separated by a small gap, forming a vibrating plate capacitor. Movements of the cantilever are read out conveniently by electronic circuits. A static magnetic field generates a force density acting on the conductor that alternates according to the frequency of the current. When the electrical current is known, the deflection amplitude of the cantilever is a measure of the component of the magnetic flux density that points perpendicular to the current. The highest vibration amplitudes are expected, in the vicinity of resonance frequencies of the micromachined structure. At ambient pressure, the prototype sensor has a measured resonance frequency of 3.8 kHz for the fundamental mode and 20 kHz for the first antisymmetric mode. In experiments, the magnetic flux of the dipole has been characterized between 0.1 and 1 T, with a relative uncertainty of 3 × 10 -4 .","['Magnetic sensors', 'Magnetic fields', 'Sensor phenomena and characterization', 'Microelectromechanical systems', 'Resonant frequency']","['MEMS', 'micro( $\\mu$ )-wire', 'resonant magnetic field sensor', 'capacitive read-out']"
"This work quantifies water contamination in jet fuel (Jet A-1), using silica-based Bragg gratings. The optical sensor geometry exposes the evanescent optical field of a guided mode to enable refractometery. Quantitative analysis is made in addition to the observation of spectral features consistent with the emulsification of water droplets and Stokes' settling. The measurements are observed for cooling and heating cycles between ranges of 22 °C and -60 °C. The maximum spectral sensitivity for water contamination was 2.4 pm/ppm-v, with a resolution of <;5 ppm-v.","['Fuels', 'Bragg gratings', 'Gratings', 'Optical sensors', 'Water pollution', 'Refractive index', 'Optical fibers']","['Bragg gratings', 'fuels', 'integrated optics', 'optical devices', 'optical waveguides']"
"In practical engineering applications, the multivariate signal contains more fault feature information than the single-channel signal. How to realize synchronous extraction of fault features from the multivariate signal is of great significance in fault diagnosis of rotary machinery. Dynamic mode decomposition (DMD) has attracted much attention due to its excellent dynamic feature extraction ability. However, DMD lacks mode aliasing property when dealing with the multivariate signal, which may lead to the loss of critical fault feature information. Cater to this problem, this article proposed a multivariate DMD (MDMD) algorithm that is the multivariate extension of DMD. First, snapshot tensors are defined to convert multivariate signals to tensor format. Then, the MDMD algorithm is proposed by introducing tensor operations into the original DMD algorithm, where a tensor low tubal rank component extraction framework is constructed to enable simultaneous extraction of bearing fault features from the multivariate signal, to enhance the robustness and effectiveness of the algorithm. Finally, both numerical simulations and experiments verify that the proposed MDMD has higher fault diagnosis accuracy than other multivariate signal-processing methods.","['Feature extraction', 'Tensors', 'Heuristic algorithms', 'Fault diagnosis', 'Signal processing algorithms', 'Matrix decomposition', 'Sensors']","['Bearing fault diagnosis', 'multivariate dynamic mode decomposition (MDMD)', 'multivariate signal processing', 'transient feature extraction']"
"Understanding the land surface temperature (LST) trends is crucial for policymakers and stakeholders to develop adaptation and mitigation strategies suitable for a sustainable environment coping in the face of climate change. This article presents a systematic review of the studies related to delineating spaceborne sensor-based LST trends, including information on the instruments and constellations of satellites (missions) that provide thermal infrared (TIR) and passive microwave (PMW) observations. About 99% of the studies used TIR, where 76% were Moderate Resolution Imaging Spectroradiometer (MODIS, onboard Terra/Aqua) observations. Opportunities, challenges, and research gaps for using the TIR and PMW observations were also explored, with instruments onboard either polar-orbiting or geostationary satellites. We identified that the calibrated dataset (e.g., processed, harmonized, and standardized) is extremely limited for each constellation, with multiple satellites and instruments, to make it fully useful for the entire mission period. A few problematic methodological concepts were identified, including using a few images in a longer time series. Using only a few images, acquired on different calendar months in different years, would not provide the true annual trends over the study period because they can be influenced by seasonal variations. To estimate the warming or cooling daytime, nighttime, or diurnal LST trends, the use of MODIS observations could be useful, even though it does not acquire images during the maximum or minimum temperature in a daily cycle. This article indicated further investigations into those research gaps and recommended directions to overcome most of these limitations.","['Land surface', 'Climate change', 'Temperature measurement', 'Microwave generation', 'Thermal energy', 'Infrared sensors']","['Land surface temperature (LST)', 'Moderate Resolution Imaging Spectroradiometer (MODIS)', 'passive microwave (PMW)', 'thermal infrared (TIR)', 'trend']"
"This paper describes a novel approach for optical strain sensing based on a hetero-core fiber optic macro-bending sensor. A gauge substrate for the proposed sensor was designed to transduce the tensile and compression strain by changing to a gentle bending curvature on the fiber; thus, the hetero-core fiber detects the strain on the substrate as the transmitted light lost. Because optical fibers perform as a flexible structure in a buckling scheme owing to their slenderness, the mechanical properties of the proposed sensor can be widely tuned by the shape and mechanical properties of the gauge substrate. Experiments demonstrated that the proposed strain gauge with a spring-structured substrate made of SUS304 showed a strain applied within ±3100 μ with a sensitivity of up to -9.14 × 10 -4 dB/ με, and responded to temperature changes from 0 °C to 60 °C with -1.10 × 10-2 dB/°C owing to the thermal expansion of the substrate. Furthermore, both the strain and temperature sensitivities are tunable by changing the geometrical parameters of the gauge substrate.","['Strain', 'Substrates', 'Strain measurement', 'Sensitivity', 'Optical fiber sensors']","['Fiber optic sensor', 'strain gauge', 'structural health monitoring', 'hetero-core']"
"This paper describes the development of an array of coplanar capacitive sensors applied to marine icing. Current atmospheric icing monitoring systems consider single phase conditions in their operation. Marine icing conditions present a unique environment where the liquid water phase effects cannot be neglected and require a novel approach. We have conducted an initial proof of concept and propose a new icing monitoring system which can distinguish between the individual phases. A numerical model confirmed our initial hypothesis of the system’s ability to discriminate the multiphase domains based on the array of geometrically dissimilar capacitive sensors. In addition, we also developed a novel experimental technique based on a comparative study under constant conditions to eliminate the need for an independent ice accretion monitoring system normally required in sensor development. The new approach promises a better characteristic in marine icing monitoring systems or in similar applications where multiphase dielectric is present.","['Ice', 'Capacitive sensors', 'Electrodes', 'Sensor phenomena and characterization', 'Sensor arrays', 'Monitoring']","['Capacitive sensors', 'multiphase dielectric material', 'coplanar electrodes', 'finite element analysis', 'capacitance-to-frequency conversion', 'linearly independent characteristics', 'least squares equations', 'offshore industry', 'wind power generation', 'marine icing']"
"An influence model of collimating lens aberrations in an autocollimation system based on the ray-tracing method is established. With the ray-tracing method, this model can accurately compute the direction and position of each light ray passing through the collimating lens in an autocollimation system, thereby obtaining the light-spot position of each light ray on the sensor and determining the systematic error caused by aberrations. Eventually, this model serves to compensate for the measurement error of autocollimators and improve the accuracy. Simulations and experiments demonstrate that the influence quantity of angular measurement value due to aberrations reaches 0.74 arcsec within the range of ±500 arcsec. Also, in a large-aberration situation, the measurement accuracy is promoted to 3.33 arcsec, approximately five times higher than that without compensation.","['Lenses', 'Sensors', 'Ray tracing', 'Sensor phenomena and characterization', 'Mathematical models', 'Computational modeling', 'Optics']","['Angular measurement accuracy', 'autocollimation system', 'collimating lens aberrations', 'influence model', 'ray-tracing method']"
"Depth estimation using monocular camera sensors is an important technique in computer vision. Supervised monocular depth estimation requires a lot of data acquired from depth sensors. However, acquiring depth data is an expensive task. We sometimes cannot acquire data due to the limitations of the sensor. View synthesis-based depth estimation research is a self-supervised learning method that does not require depth data supervision. Previous studies mainly use the convolutional neural network (CNN)-based networks in encoders. The CNN is suitable for extracting local features through convolution operation. Recent vision transformers (ViTs) are suitable for global feature extraction based on multiself-attention modules. In this article, we propose a hybrid network combining the CNN and ViT networks in self-supervised learning-based monocular depth estimation. We design an encoder–decoder structure that uses CNNs in the earlier stage of extracting local features and a ViT in the later stages of extracting global features. We evaluate the proposed network through various experiments based on the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) and Cityscapes datasets. The results showed higher performance than previous studies and reduced parameters and computations. Codes and trained models are available at https://github.com/fogfog2/manydepthformer .","['Estimation', 'Transformers', 'Feature extraction', 'Computational modeling', 'Cameras', 'Image reconstruction', 'Costs']","['Depth estimation', 'monocular sensor estimation', 'self-attention', 'self-supervised', 'transformer']"
"Piezoelectric transducers are used in many applications due to electromechanical coupling, which allows mechanical and electrical quantities to be related. This principle is used in the development of structural damage detection techniques such as electromechanical impedance (EMI) commonly used in structural health monitoring (SHM), where structural health is monitored by measuring and analyzing the electrical impedance of a transducer. As a piezoelectric transducer is a primarily capacitive device, its reactance becomes small at high frequencies, reducing sensitivity to structural damage and increasing the excitation current required from the measuring system. Therefore, this paper proposes the use of a negative capacitance (NC) interface with the transducer to improve sensitivity to structural damage and reduce the excitation current. The improvement obtained with the NC interface is theoretically analyzed using an electromechanical model and experimentally validated with tests on an aluminum structure. The results conclusively indicate a significant improvement in the sensitivity to structural damage and a reduction in excitation current, which are critical for detecting incipient damage and developing onboard monitoring systems where the output current drive is usually low.","['Capacitance', 'Impedance', 'Piezoelectric transducers', 'Monitoring', 'Sensitivity']","['Piezoelectric transducers', 'SHM', 'impedance', 'negative capacitance', 'damage detection', 'sensitivity']"
"This paper presents the manufacturing process and electromechanical characterization of highly stretchable textile strain sensors. These sensors can be used in self-sensing soft robots or motion tracking. To improve linearity and compliance of silver-plated yarns, the braiding technology is used to fabricate both resistive and capacitive strain sensors. Braiding is a highly productive and automated process and the resulting sensor properties can be improved. Moreover, the braids can be further processed to complex textiles. In the fabrication process different base materials are used and the braiding density as well as the composition of the conductive threads in the braid are varied. With the produced samples electro-mechanical characterization experiments are carried out to analyze the sensors’ properties, e.g. gauge factor, linearity, hysteresis and compliance. The experimental results are correlated with the material and process parameters. Both, braiding density and base material, have an enormous influence on the sensor characteristics. The most promising sensor type was integrated into a soft actuator. This demonstrates the feasibility of such textile sensors to provide self-sensing capabilities and proprioception to soft robots.","['Sensors', 'Yarn', 'Capacitive sensors', 'Electrodes', 'Sensor phenomena and characterization', 'Strain', 'Textiles']","['Robot sensing systems', 'stretchable electronics', 'wearable sensors']"
"There have been significant advances in 3D object detection using LiDAR and camera fusion for autonomous driving. However, it is surprisingly difficult to effectively design fusion location and fusion strategies for point cloud-based 3D object detection networks. In this paper, we propose a novel two-stage sequential fusion (TSF) method. In the first stage of fusion, TSF generates the enhanced point cloud by combining the raw point cloud and semantic information of image instance segmentation. In the second stage, the proposals generated by LiDAR baseline is used to complete the No-Maximum Suppression (NMS) together with the 2D object detection results. Numerous experiments on the KITTI validation set show that our method outperforms state-of-the-art multimodal fusion-based methods on the three classes in 3D performance (Easy, Moderate, Hard): cars (89.94%, 82.76%, 76.04), pedestrians (70.74%, 63.47%, 56.56%), and cyclists (84.72%, 64.22%, 56.78%). In ablation, we analyze the augmented effect of fusion module on the LiDAR baseline detection capability, and study the best trade-off between running time and accuracy.","['Three-dimensional displays', 'Point cloud compression', 'Object detection', 'Feature extraction', 'Laser radar', 'Sensors', 'Image segmentation']","['3D object detection', 'multimodal sensor fusion', 'sequential fusion', 'two-stage structure']"
"As today’s agriculture industry is facing numerous challenges, including climate changes, encroachment of the urban environment, and lack of qualified farmers, there is a need for new practices to ensure sustainable agriculture and food supply. Consequently, there is an emphasis on upgrading farming practices by shifting toward smart farming (SF)—utilizing advanced information and communication technologies to improve the quantity and quality of the crop with minimal labor interference. SF has gained lots of interest in recent years utilizing a variety of technological innovations in the field, which imposes a challenge on farmers and technology integrators to identify suitable technologies and best practices for a particular application. This article provides a survey of the most recent SF scientific literature to identify common practices toward technology integration, challenges, and solutions. The survey was conducted on 588 papers published on the IEEE database following Cochrane methods to ensure appropriate analysis and interpretation of results. The papers’ contributions were analyzed to identify necessary technologies that constitute SF, and consequently, research themes were identified. The identified themes are sensors, communication, big data, actuators and machines, and data analysis. Besides presenting an in-depth analysis of each identified theme, this article discusses integrating more than one technology in systems to achieve independency. The most common SF systems are remote monitoring, autonomous, and intelligent decision-making systems.","['Sensors', 'Sensor phenomena and characterization', 'Temperature sensors', 'Wireless sensor networks', 'Smart agriculture', 'Climate change', 'Capacitive sensors']","['Actuators', 'automation', 'data analysis', 'deep learning (DL)', 'Internet of Things (IoT)', 'irrigation systems', 'lowpower wide area network (LPWAN)', 'machine learning (ML)', 'microcontrollers', 'remote monitoring', 'robotics', 'smart farming (SF)', 'wireless sensor networks (WSNs)']"
"During the COVID-19 pandemic, there has been an increasing rollout of non-contact fever screening solutions to assist in curbing the spread of disease. This study begins by describing how screening for disease has historically been performed. It proposes four measurement characteristics of an ideal screening solution: non-contact, effective, rapid and low-cost measurements. Next, it reviews the existing literature on fever-screening using non-contact infrared thermometer (NCIT) devices as well as infrared thermography (IRT) devices, as these are two technologies which have experienced increasing use. For this review, 185 research papers were identified, 21 research studies were included after inclusion and exclusion criteria were applied. A total of 35 experiments were identified for analysis and their results tabulated. Of these studies, 66% are IRT and 34% are NCIT, with a median sample size of 430 subjects. 26 experiments involve febrile participants, with a median febrile percentage of 11.22 % of population. The reported sensitivity of febrile detection using NCIT varies from 3.7% to 97% and when using IRT it varies from 15% to 100%. Both indoor and outdoor studies are investigated, as well as those conducted in acute and non-acute settings. The results of this review show a clear lack of consensus on the effectiveness of these systems. Overall, these results indicate that sensitivity and specificity are reduced when using IRT and NCIT technologies compared to other thermometers used in medical practice. Their use should be carefully assessed based on the risks present in each particular measurement scenario.","['Pandemics', 'Airports', 'Temperature measurement', 'COVID-19', 'Australia', 'Sea measurements', 'Medical services']","['Clinical decision-making', 'fever screening', 'non-contact infrared thermometer', 'thermal camera', 'thermography', 'thermometry']"
"The last years have seen a growing body of literature on data-driven pedestrian inertial navigation. However, despite this, it is still unclear how to efficiently combine classical models and other a priori information with existing machine learning frameworks. In this paper, we first categorize existing approaches to data-driven pedestrian inertial navigation, including approaches where a machine learning algorithm is embedded into an overarching classical framework and purely data-driven frameworks. We then propose an estimation framework where navigation estimates obtained by classical means are fed to a machine learning algorithm which is trained to correct and improve the estimates. Further, we describe three symmetries that can be used to constrain the proposed estimation framework and thereby improve its performance. These are 1) the rotational symmetry of pedestrian dynamics, 2) the rotational symmetry of the sensors, and 3) the temporal symmetry of pedestrian dynamics. To demonstrate the usefulness of the proposed framework, we use data from foot-mounted inertial sensors utilizing zero-velocity updates under mixed walking and running. Machine learning corrections are implemented using both neural networks and Gaussian processes.","['Inertial navigation', 'Mathematical models', 'Estimation', 'Machine learning', 'Kinematics', 'Machine learning algorithms', 'Measurement uncertainty']","['Inertial navigation', 'Gaussian processes', 'neural networks', 'pedestrian navigation']"
"Clamp-on ultrasonic transit time difference flow meters provide opportunities for metering where it is impractical or undesirable to cut into an existing pipeline to install an alternative flow meter. Up until now, it has been difficult to perform this type of measurement on thin-walled metal pipes, due to the difficulty of interpreting the guided wave modes in the combined pipe wall and internal fluid system, but a new method has been reported recently that utilises these guided wave modes for flow measurement. Through computational modelling, and construction and testing of different transducers, the design considerations for clamp-on transducers are highlighted and their impact on guided wave flow rate measurement is evaluated. The design features considered include a curved contact face to provide focusing of the ultrasound within the pipe and a scattering surface to reduce internal reflections. It is found that additional unwanted ultrasonic modes can be minimised by ultrasonic transducer wedge design features such as profiling the curvature of the transducer to conform to the pipe wall or creating a scattering edge to minimise internal wedge reflections. It is also observed that minimising these unwanted modes does not offer any advantage for the transit time difference measurement used in calculating flow.","['Transducers', 'Acoustics', 'Fluids', 'Ultrasonic variables measurement', 'Ultrasonic imaging', 'Fluid flow measurement', 'Flowmeters']","['Electromechanical sensors', 'fluid flow measurement', 'guided wave', 'Huygens’ principle', 'ultrasonic transducers']"
"The demand for safety-boosting systems is always increasing, especially to limit the rapid spread of COVID-19. Real-time social distance preservation is an essential application toward containing the pandemic outbreak. Few systems have been proposed which require infrastructure setup and high-end phones. Therefore, they have limited ubiquitous adoption. Cellular technology enjoys widespread availability and their support by commodity cellphones, which suggest leveraging it for social distance tracking. However, users sharing the same environment may be connected to different telecom providers of different network configurations. Traditional cellular-based localization systems usually build a separate model for each provider, leading to a drop in social distance performance. In this article, we propose CellTrace , a deep learning-based social distance preserving system. Specifically, CellTrace finds a cross-provider representation using a deep learning version of canonical correlation analysis. Different providers’ data are highly correlated in this representation and used to train a localization model for estimating the social distances. In addition, CellTrace incorporates different modules that improve the deep model’s generalization against overtraining and noise. We have implemented and evaluated CellTrace in two different environments with a side-by-side comparison with the state-of-the-art cellular localization and contact tracing techniques. The results show that CellTrace can accurately localize users and estimate the contact occurrence, regardless of the connected providers, with a submeter median error and 97% accuracy, respectively. In addition, we show that CellTrace has robust performance in various challenging scenarios.","['Location awareness', 'Diseases', 'Social factors', 'Human factors', 'Correlation', 'Wireless fidelity', 'Sensors']","['Cellular localization', 'contact tracing', 'indoor localization', 'social distance tracking']"
"Pedestrian positioning with wearable devices is a significant application of attitude tracking. It tracks the attitude (with heading angle being the most important part) of the device in real time and provides positioning services for users based on the information of step length provided by Pedestrian Dead-Reckon (PDR), which is a cheap and efficient positioning method at present. However, amid a train of positioning methods, the joint estimate of tracking is given by a train of methods based on the direction of gravity and the earths magnetic field direction. Considering the measurement of gravity that the gravity accelerometer is exposed to heavy noise due to the complex movement of human body during walking with uniform swing arm posture and forward speed, this paper proposed a novel estimate method based on the Kalman filter with multi-state constraints and the usage of low-cost sensors, which fulfills the estimation with the sequential observation of magnetic field. Compared with other related work, this method proposed in this paper eliminates the dependence on gravity direction, avoiding the influence of heavy noise caused by additional linear acceleration in motion state, and reduces the influence of insufficient observation when using magnetic field observation alone. The performance of the proposed method is evaluated by real-world experimentation results.","['Magnetometers', 'Gravity', 'Sensors', 'Gyroscopes', 'Accelerometers', 'Estimation', 'Mathematical model']","['Orientation estimation', 'sequential geomagnetic observation', 'multi-state constrain', 'wearable device positioning']"
"To realize the quantitative analysis and research of traditional Chinese medicine (TCM) acupuncture manipulations, miniaturized multidimensional force sensors are necessary to measure the lifting–thrusting force and twisting torque in the process of acupuncture, which records the manipulations of excellent physicians for relevant physicians to learn and improve their manipulations. A miniature multiaxis force/torque ( {F}/{T} ) sensor for acupuncture is developed in this article. The elastomer of the design adopts hollow thin-wall cross beam structure with inverted {T} floating beam, which retains the space for the strain gauge to be pasted, reduces the coupling between dimensions, and improves the sensitivity and accuracy. This design has the advantages of miniaturization, lightweight, high precision, and convenient needle substitution. The elastomer is processed with polyetheretherketone (PEEK) material to reduce the mass. First, the feasibility of the hollow thin-wall cross beam structure with inverted {T} floating beam is verified by ANSYS finite element simulation. Then, the hardware system based on STM32 is designed to realize signal acquisition, amplification, and transmission. Next, the static calibration and decoupling of the sensor are accomplished with the calibration device. The calibration results indicate that the accuracy of the sensor is high (the coupling error is less than 2%). Finally, the acupuncture force measurement system is constructed to complete the acupuncture manipulation experiment. The experimental results demonstrate that our system can accurately obtain the needle force information and can distinguish different acupuncture manipulations, which verifies the feasibility of the sensor while realizing the quantitative characterization of acupuncture manipulations.","['Acupuncture', 'Sensors', 'Force', 'Needles', 'Force measurement', 'Strain', 'Sensor phenomena and characterization']","['Acupuncture manipulation', 'decoupling', 'miniature', 'multiaxis force/torque (F/T) sensor', 'quantitative characterization']"
"This paper explores the application of molecular communication via diffusion-based nano-sensor networks (MCSNs) for data gathering applications in in-body medical systems. For MCSNs, the large and varying propagation delay in the channel presents a fundamental challenge for channel access, leading to packet collisions. Although packet collisions are well-studied for traditional wireless sensor networks, to date, there are no such studies conducted for MCSNs. Another fundamental challenge is the limited capabilities of the nano-sensors, rendering the existing solutions inefficient. Therefore, a novel light-weight time-division multiple access (TDMA)-based data gathering multiple access control (MAC) protocol is proposed. Light-weight here implies that long information, such as timestamps, is not exchanged. TDMA-based here implies that each nano-sensor is designated an exclusive time-slot. The lack of a channel model for spherical transmitters impairs theoretical analysis of the probability of packet collisions. To overcome this, the propagation delay is approximated as a Normal distribution. The model is validated using the widely popular particle-based simulation, which is also used to determine the packet duration considering ON-OFF shift keying. Building upon these analyses, a system-level simulator is developed to evaluate the proposed MAC protocol. For the first time, the packet collision probability is characterized under varied distances and diffusion coefficients. A key finding is the correlation between the channel utilization and the time-slot occupancy ratio, which can be used as a tool to optimize the performance of an MCSN. Finally, comparisons with conventional TDMA reveals that the proposed protocol can offer better performance.","['Propagation delay', 'Time division multiple access', 'Protocols', 'Media Access Protocol', 'Sensors', 'Molecular communication (telecommunication)', 'Sensor phenomena and characterization']","['Data gathering', 'molecular communications', 'multiple access control', 'packet collisions', 'nano-sensor networks', 'time division multiple access']"
"Cognitive impairment diseases are becoming more and more prevalent mainly due to population aging and the increase in life expectancy. Sensory and monitoring systems may allow people with mild cognitive impairments (MCIs) or at early stages of dementia to live at home for longer with more independence and security. This work presents a wireless sensor network (WSN) based on wearables that obtains indoor and outdoor location and step information, reporting them over a long-range WAN (LoRaWAN) network. Each wireless wearable sensor (WWS) uses a global navigation satellite system (GNSS) module for outdoor positioning, a proposed indoor room-level localization system based on infrared sensors, an accelerometer for a step detector algorithm, and a long-range (LoRa) radio link to send the measured information with low-power consumption achieving a large coverage range. These sensory data are recorded in a database and presented to the medical services and caregivers through a user web application. This can be used to detect anomalous changes in daily patients’ routines, as well as to know the user’s position in cases where the patient may be disoriented. In addition, alerts are launched in caregivers’ smartphones to report about any risky situation, such as the patient leaving an allowed area or staying in one place for too long. Therefore, the proposed sensory system may support and extend the ability of people with MCI or at early stages of dementia to live independently, it helps detect behavioral changes and it keeps caregivers’ peace of mind.","['Wearable computers', 'Monitoring', 'Wireless sensor networks', 'Sensors', 'Global navigation satellite system', 'Dementia', 'Real-time systems']","['Global navigation satellite system (GNSS)', 'indoor positioning', 'infrared sensors', 'long-range WAN (LoRaWAN)', 'mild cognitive impairments (MCIs)', 'step detector', 'wireless sensor network (WSN)']"
"Event cameras provide high temporal resolution and high dynamic range and have application prospects in areas such as visual navigation, robotics, and image reconstruction. Event cameras output an asynchronous stream of address events that can be highly polluted by noise due to lighting and motion conditions. In order to better analyze and process noise with different spatiotemporal correlation, we introduce a method using an event address map (EAM) representation and classify event noise into background noise, near-edge noise, and recessive noise. In addition, since near-edge noise and recessive noise have a closer spatiotemporal correlation with real events, they are difficult to suppress with current denoising methods. Accordingly, we propose an adaptive EAM denoising method by adjusting the time window to form the EAM by measuring the number of events after pre-denoising and performing median filtering and opening. The method can better adapt to scenes of different motions to process near-edge noise, and can generate events to fill recessive noise. The output EAM can be directly used for subsequent process. Alternatively, to provide high temporal resolution, we restore the stream of address events from the output EAM. To suitably evaluate event denoising performance on real-world data, we introduce the average local event variance and amount of recessive noise, and the proposed method achieves improvements of 24.94% and 72.79%, respectively, compared with conventional denoising.","['Cameras', 'Noise reduction', 'Background noise', 'Correlation', 'Spatiotemporal phenomena', 'Sensor phenomena and characterization', 'Brightness']","['Event camera', 'dynamic vision sensor', 'signal denoising', 'performance evaluation']"
"We investigated cognitively demanding tasks on patterns of human gait in healthy adults with a deep learning methodology that learns from raw gait data. Age-related differences were analyzed in dual-tasks in a cohort of 69 cognitively healthy adults organized in stratified groups by age. A novel spatio-temporal deep learning methodology was introduced to effectively classify dual-tasks from spatio-temporal raw gait data, obtained from a unique tomography floor sensor. The approach outperformed traditional machine learning approaches. The most favorable classification F-score obtained was of 97.3% in dual-tasks in a young age group experiment for a total of 12 users. The deep machine learning methodology outperformed classical machine learning methodologies by 63.5% in the most favorable case. Finally, a 2D manifold representation was obtained from trained deep learning models' data, to visualize and identify clusters from features learned by the deep learning models. This study demonstrates a novel approach to dual-task research by proposing a data-driven learning methodology with stratified age-groups.","['Task analysis', 'Machine learning', 'Legged locomotion', 'Sensor systems', 'Data models', 'Databases']","['Deep learning', 'dual task', 'floor sensor system', 'age-related differences']"
"During the last decades, innovative aircraft health management systems have been receiving increasing interest from original equipment manufacturers (OEMs) and aircraft operators. Their implementation could lead to substantial benefits: drastic cuts in turnaround time, operation costs, and life cycle costs (LCCs) as well as sharp increases in system availability, safety, and reliability. An interconnectivity step-up is, hence, needed to guarantee a seamless data transfer. In this article, an integrated open-source solution for reliable data transmission and near real-time graphical visualization is proposed. After a comprehensive calibration and verification campaign performed on a test stand, the overall system has been successfully validated on structural data measured using a network of fiber Bragg gratings (FBGs) mounted on a radio-controlled model aircraft. The result is an effective and robust system able to monitor near real-time critical parameters and health status of structures. With this system, the temperature and displacements of the structure can be displayed on a heat map arranged on a 3-D model and visualized through a computer application on the ground. The proposed methodology can be applied to heterogeneous scenarios, ranging from maintenance planning activities to performance checks, providing an all-in-one solution for flight data management as well as other applications in the structural monitoring domain.","['Sensors', 'Aircraft', 'Fiber gratings', 'Atmospheric modeling', 'Real-time systems', 'Sensor phenomena and characterization', 'Data visualization']","['Computer graphics', 'data visualization', 'databases', 'graphical user interfaces', 'Middleware', 'optical fiber sensors', 'remotely piloted aircraft', 'wireless communication']"
"Currently, Radio Frequency Identification (RFID) is one of the key technologies to realize the Internet of Things (IoT), which is widely used in our daily life. However, there are some security threats such as impersonation attack, replay attack in existed RFID systems. In addition, most physical-layer authentication methods for RFID systems are mainly to authenticate tags without authenticating the user. In this paper, we present the design and implementation of Au-Hota, a system that can authenticate the tag and the user simultaneously, and can resist replaying attack and impersonation attack that cannot be solved by most physical-layer methods. Our idea is to assign a unique identifier to the user based on the inductive coupling between two adjacent tags. When the user draws the identity identifier on the tag, different identity identifiers correspond to unique phase features so that simultaneous authentication of the tag and the user can be achieved under the premise of ensuring security. The system is fully compatible with Ultra High Frequency (UHF) RFID systems without any modification. We implement a prototype of the system and conduct extensive experiments to evaluate performance. Our results demonstrate that the system has good authentication performance.","['Authentication', 'Radiofrequency identification', 'Couplings', 'Security', 'Fingers', 'Resists', 'Law']","['Impersonation attack', 'replay attack', 'RFID', 'user authentication']"
Measurement of total ionizing dose in a radiation field is efficiently carried out by ionisation chambers. The paper details the design of a mixed-signal ASIC for the front-end electronics of ionisation chambers. A single chip solution for ultra-low current measurement is designed by combining the current processing analog section realized using low leakage thick gate transistors and the data handling digital section implemented using fast thin gate transistors. The design succeeds in limiting the cross coupling between the two circuit domains using deep n-wells and guard rings. The ASIC fabricated in 130 nm technology attains a wide dynamic range of −7 fA to−20μAwith maximum error in measurement less than ±4 %. The ASIC occupies an area of 3.52 mm 2 and has a total power consumption of 17.4 mW. The femtoampere range input leakage current of the ASIC contributed primarily by the ESD diodes was found to be varying exponentially with temperature. Dose rate measurements from5 μSv/h to 7.4 Sv/h is demonstrated by interfacing the ASIC to an ionisation chamber.,"['Current measurement', 'Ionization chambers', 'Atmospheric measurements', 'Transistors', 'Registers', 'Detectors', 'Pressure measurement']","['ASIC', 'ionisation chamber', 'femtoampere current measurement', 'mixed signal design', 'radiation monitoring', 'leakage current']"
"Additive manufacturing (AM) offers new manufacturing solutions for the integration of smart functionalities in engineering structures. In this paper, an analytical model is presented for an embedded load sensing element based on a liquid-filled capillary. During the additive manufacturing process, the capillary is integrated in the region where the strain is to be determined. The embedded capillary deforms as the structure deforms under an applied load, as such altering the pressure inside the capillary. The monitoring of the capillary pressure allows monitoring the loads and thus usage of the component. This paper presents a model describing the behavior of the sensing element under uniform tensile stress. The sensitivity of the load sensing element per unit longitudinal strain depends on the bulk modulus of the liquid inside the capillary and the Poisson coefficient of the surrounding material. The current work further compares the analytical model against static tension-compression tests of powder bed fused stainless steel (AISI 316L) test specimen with an integrated capillary filled with a liquid (water). Similarly, the validation of the model is then checked against a dynamic four-point bending test on a Ti-6Al-4V specimen produced by powder bed fusion.","['Sensors', 'Strain', 'Monitoring', 'Three-dimensional printing', 'Analytical models', 'Metals', 'Sensitivity']","['Additive manufacturing', 'embedded load sensing', 'usage monitoring', 'structural health monitoring', 'effective structural health monitoring']"
"We present a highly reproducible method of fabricating a tapered intrinsic Fabry–Perot interferometer (IFPI) device with 5–6μmdiameter at the taper waist. A femtosecond laser was applied to inscribe an IFPI with 3-cm cavity length in a single-mode fiber. A CO2 laser-heated tapering process enabled by digitally controlled mirrors and a precision motorized fiber feed system was used to create a stable heating zone with the desired temperature profile for tapering the fiber IFPI cavity. The well-engineered tapering process produced tapered IFPI devices with insertion loss less than 0.3 dB at 1550 nm. A strong evanescent field exposed by the taper Section was explored for refractive index (RI) sensing. Using swept optical frequency domination reflectometry (OFDR), the tapered IFPI fiber sensor achieved a minimal RI sensing resolution of2×10−5. This article demonstrates an integrated laser fabrication technique to produce tapered fiber optic devices for sensing applications.","['Optical fiber sensors', 'Optical fibers', 'Fiber lasers', 'Sensors', 'Optical fiber dispersion', 'Heating systems', 'Laser beams']","['Evanescent wave', 'intrinsic Fabry–Perot interferometer (IFPI)', 'optical fiber sensors', 'refractive index (RI) sensors', 'tapered fibers']"
"Bespoke force sensors made with active polymer composites are inexpensive, thin and flexible, hence popular in wearable electronics, however their wider application is limited due to the lack of literature studying their voltage response related errors. We present the voltage response characterization of bespoke force sensors made with an active polymer composite, silver coated fabric, stainless steel thread, and silver epoxy. Characterization of the effects of static and dynamic loading was completed with a mechanical testing machine. Static tests consisted of loading and unloading at 0.01, 0.1, 0.5 and 1 N/s, and drift tests for 120 minutes up to 10 N every 1 N. Dynamic tests consisted of a sinusoidal load of 5 N ±1 N applied at 0.05, 0.1, and 0.5 Hz for 60 min. The force-voltage relationships were modeled using an exponential function. Maximum mean drift error was observed when applying different static loads for 120 minutes each. Drift error is minimal at 5 s (<;1%) and at 60 min (<;5%) with loads under 1 N. Maximum hysteresis of 18% was observed at the 1 N/s loading rate. The maximum drift error after 1 h of dynamic loading was observed at 0.5 Hz and is minimal (-0.00004%). The cost of fabricating these sensors is very low compared with commercially available options. These sensors can be fabricated in any shape and size with the added advantage of being able to set the location of the electronic connections as desired.","['Hysteresis', 'Loading', 'Force sensors', 'Force', 'Sensor phenomena and characterization', 'Fabrics']","['Conductive materials testing', 'force sensors', 'instrumentation', 'transducers', 'wearable sensors']"
"The current trend of shifting computing from the cloud to the edge of the Internet of Things is influencing deep learning applications. Moving intelligence closer to the point of need entails advantages in terms of performance, power consumption, security, and privacy. The problem arises with data sources that generate a massive amount of information, making data processing challenging for edge devices. This is the case of point clouds generated by LIDAR sensors. Implementations at the edge become even more challenging when heavy processing algorithms such as deep neural networks are selected. However, deep neural networks are the state-of-the-art solution to carry out object classification tasks as they provide the best results in terms of accuracy when working with high data volumes. This work demonstrates that the processing of point cloud-based sensors using deep neural networks at the edge is becoming feasible with the emergence of new devices with high computing capacity combined with reduced power consumption. In this regard, a characterization of first-in-class deep learning classification algorithms working with point cloud data as inputs and running over different state-of-the-art edge processing architectures is provided. A broad range of devices, including CPUs, GPU-based, SoC FPGA-based, and deep learning neural accelerators, have been evaluated in terms of inference time, classification accuracy, and power consumption. As a result, it demonstrates that neural accelerators with integrated host CPUs represent the best trade-off between power consumption and performance, making them a perfect solution for IoT applications at the edge level.","['Point cloud compression', 'Sensors', 'Three-dimensional displays', 'Computer architecture', 'Laser radar', 'Image edge detection', 'Deep learning']","['Deep neural networks', 'edge computing', 'LIDAR', 'neural accelerators', 'object classification', 'point cloud']"
"To develop daily monitoring systems that can identify elderly people with cognitive functioning and health literacy impairments, this study examined elderly participants during the sit-to-stand (STS) movement by using a Doppler radar to gather kinematic information for assessing any associations between those data and factors related to both cognitive functioning and health literacy. More specifically, we used Doppler radar systems to measure kinematic parameters related to the STS movement among 170 community-dwelling elderly participants aged 65 years and older, who were classified into good or bad health literacy groups and cognitive-healthy or cognitive- impairment groups based on the results of conventional paper-based tests designed to assess health literacy and cognitive functioning, respectively. The Doppler radar measured kinematic parameters for not only velocity, acceleration, and jerk (the time derivative of acceleration), but also higher-order derivative parameters: snap (the time derivative of jerk) and crackle (the time derivative of snap). Intergroup differences between the measured kinematic parameters were then compared using Welch's t-test. The results revealed that health literacy impairments were associated with velocity, acceleration, and jerk, while cognitive impairments were associated with the higher-order derivative parameters, snap and crackle. These findings show that the Doppler radar is an effective tool to distinguish impairments between health literacy and cognitive functioning based on the extraction of higher-order derivative parameters during the STS movement. The results should lead to the development of unconstrained monitoring systems that can detect early signs of impairments in both cognitive functioning and health literacy among the elderly.","['Doppler radar', 'Kinematics', 'Sensors', 'Velocity measurement', 'Senior citizens', 'Particle measurements', 'Monitoring']","['Doppler radar', 'motion analysis', 'statistical analysis']"
"Surface-plasmons of metals have been utilized to enhance the Raman spectra of various adsorbed moieties for over decades. While amplification of the spectral intensity takes place on most of the metals, due to their superb properties, Au, Ag and Cu surfaces represent the benchmark in surface-enhanced Raman spectroscopy. In this paper, we show that Cu-Pd bimetal and CuPt alloy nanotubes derived from Cu nanowires by simple galvanic exchange reactions are suitable for the efficient enhancement of Raman spectra when dispersed on Si surfaces. Amplification factors of120×on Cu nanowires,150×on Cu-Pd bimetal nanotubes and250×on CuPt alloy nanotubes in reference to the substrate are measured for rhodamine 6G and methyl violet model compounds. We also show that the nanotubes dispersed on Au surfaces can contribute to a further intensity enhancement of the substrate and detect analytes adsorbed from 10 −6 M analyte concentrations. Our results obtained using bimetallic and alloy nanomaterials shed light on a new strategy to synthetize and apply new types of metal nanostructures and compositions for surface-enhanced Raman spectroscopy in the future.","['Nanowires', 'Nanotubes', 'Gold', 'Silicon', 'Surface treatment', 'Crystals']","['SERS', 'Cu nanowires', 'galvanic exchange reaction', 'CuPt alloy', 'Cu-Pd bimetal']"
"A composite relative position sensor is studied in this paper. The sensor outputs square signal corresponding to the cogging cycle of long stator track, which is used as the spatial sampling pulse for high-speed maglev track measurement system, and also provides relative position, speed and direction information. The sensor uses only one group of detection coils to identify the cog and two photoelectric switches to identify the direction, thus effectively reducing the mass and size of the sensor. To make sure the sensor works normally when it vibrates vertically and passes through the track joint, the processing methods are given respectively. The factors that affect the dynamic response characteristics are analyzed and compared, it is considered that the discharge time constant of the demodulation has a great influence, and based on this, the detection speed limit of the sensor is calculated as 253km/h. The dynamic test is carried out by using the equivalent inductance method, and the results show that the sensor can respond normally and output the results correctly within the speed of 200km/h. In addition, it is demonstrated that the sensor can detect accurately within a detection distance of 22mm and a lateral deviation of ± 22mm through platform test.","['Sensors', 'Coils', 'Forging', 'Position measurement', 'Sensor phenomena and characterization', 'Sensor systems', 'Radar tracking']","['Maglev train', 'track measurement', 'relative position sensor', 'dynamic response', 'equivalent inductance method']"
"Porous materials have demonstrated to be ideal candidates for the creation of optical sensors with very high sensitivities. This is due both to the possibility of infiltrating the target substances into them and to their notable surface-to-volume ratio that provides a larger biosensing area. Among porous structures, polymeric nanofibers (NFs) layers fabricated by electrospinning have emerged as a very promising alternative for the creation of low-cost and easy-to-produce high performance optical sensors, for example, based on Fabry-Pérot (FP) interferometers. However, the sensing performance of these polymeric NFs sensors is limited by the low refractive index contrast between the NFs porous structure and the target medium when performing in-liquid sensing experiments, which determines a very low amplitude of the FP interference fringes appearing in the spectrum. This problem has been solved with the deposition of a thin metal layer (~ 3 nm) over the NFs sensing layer. We have successfully used these metal-coated FP NFs sensors to perform several real-time and in-flow refractive index sensing experiments. From these sensing experiments, we have also determined that the sponge-like structure of the NFs layer suffers an expansion/compression process that is dependent of the viscosity of the analyzed sample, what thus gives the possibility to perform a simultaneous dual sensing of refractive index and viscosity of a fluid.","['Optical sensors', 'Refractive index', 'Sensitivity', 'Reflectivity', 'Metals', 'Optical refraction']","['Fabry-Perot', 'nanofibers layer', 'Nanophotonics', 'polymers', 'porous structure', 'real-time sensor', 'refractive index', 'viscosity']"
"Inertial measurement units (IMUs) are used in biomechanical and clinical applications for quantifying joint kinematics. This study aimed to assist researchers new to IMUs and wanting to develop an inexpensive IMU system to estimate the relative angle between IMUs, while understanding the different algorithms for estimating angular kinematics. Thus, there were three subgoals: 1) to present a low-cost and convenient IMU system utilizing two 6-axis IMUs for computing the relative angle between the IMUs; 2) to examine seven methods for estimating the angular kinematics of an IMU; and 3) to provide an open-source code and working principles of these methods. The raw gyroscopic and accelerometer data were preprocessed. The seven methods included gyroscopic integration (GI), accelerometer inclination (AC), basic complementary filter (BCF), Kalman filter (KF), digital motion processor (DMP, a proprietary algorithm), Madgwick filter (MW), and Mahony filter (MH). An apparatus was designed to test nine conditions that computed angles for rotation about three axes (roll, pitch, yaw) and three movement speeds (50°/s, 150°/s, 300°/s). Each trial lasted 25 min. The root-mean-squared error (RMSE) between the gold-standard value measured from the apparatus’ encoder and the value calculated from each of the seven methods was determined. For roll and pitch, all methods accurately quantified angles (RMSE < 6°) at all speeds. For yaw, all methods except AC and DMP displayed RMSE < 6° at all speeds. AC could not be used for yaw angle computation, and DMP displayed RMSE >6°. Researchers can utilize appropriate methods based on their study’s application.","['Magnetometers', 'Testing', 'Sensors', 'Accelerometers', 'Synchronous motors', 'Quaternions', 'Codes']","['Accelerometer', 'complementary filter', 'gyroscope', 'Kalman filter (KF)', 'Madgwick filter (MW)', 'Mahony filter (MH)']"
"Simultaneously occurring random events are often reported by multiple nodes. However, the accuracy of the event detection at every node is dependent on the node’s relative position from the event, and hence, not reliable. Moreover, the factors influencing the event inference are so many, that the accuracy of such an event detection is compromised. Targeting the problem of accurate event inference in the detection of priority events, such as forest fire, a fuzzy rule-based method is proposed. Four parameters are identified for which fuzzyfied values are obtained by a membership function for every variable. A set of 256 rules are used to generate different permutations of the fire index with respect to the identified variables. Extensive analysis of the results proves the efficacy of the proposed scheme with a significantly reduced error rate of 2.01% for humidity and an error rate of 1.94% for temperature.","['Sensors', 'Forestry', 'Temperature sensors', 'Event detection', 'Wind speed', 'Humidity', 'Temperature measurement']","['Data gathering', 'energy efficiency', 'event classification', 'inference', 'intensity discrimination', 'IoT', 'knowledge extraction']"
"This paper provides a tutorial on the most recent advances of event-driven metering (EDM) while indicating potential extensions to improve its performance. We have revisited the effects on signal reconstruction of (i) a fine-tuned procedure for defining power variation events, (ii) consecutive-measurements filtering that refers to the same event, (iii) spike filtering, and (iv) timeout parameter. We have illustrated via extensive numerical results that EDM can provide high-fidelity signal reconstruction while decreasing the overall number of acquired measurements to be transmitted. Its main advantage is to only store samples that are informative based on predetermined events, avoiding redundancy and decreasing the traffic offered to the underlying communication network. This tutorial highlights the key advantages of EDM and points out promising research directions.","['Time measurement', 'Power measurement', 'Data acquisition', 'Energy measurement', 'Sensors', 'Signal reconstruction', 'Tutorials']","['Event-driven data acquisition', 'smart grids', 'advanced electricity metering']"
"In recent years, extensive investigations have been geared toward finding unobtrusive solutions for monitoring cardiorespiratory activity as an alternative to traditional clinical methods. Among others, the ones based on fiber Bragg grating (FBG) sensors reveal remarkable promise for monitoring respiratory rate (RR) and heart rate (HR). The present study investigates the performance of a mattress based on a 13-FBG array for HR continuous estimation. First, a metrological characterization was performed to assess system characteristics under frequencies simulating typical HR values [i.e., 60, 90, and 120 beats per minute (bpm)]. Then, the proposed device was tested on eight healthy volunteers (both males and females) in the presence of different breathing stages (i.e., quiet breathing and tachypnea) while mimicking common sleeping postures (i.e., supine, left side, and prone). The assessment of HR measurements under different breathing regimes and postures has rarely been addressed in FBG-based technologies. The achieved results suggest that the proposed mattress has promising capability in reliably estimating HR values. These results together with the ones obtained in terms of RR monitoring in a recent study reveal the high potential for monitoring cardiorespiratory activity.","['Fiber gratings', 'Sensors', 'Monitoring', 'Heart rate', 'Loading', 'Instruments', 'Hysteresis']","['Cardiorespiratory monitoring', 'fiber Bragg grating (FBG) sensors', 'heart rate (HR) monitoring', 'smart mattress', 'unobtrusive monitoring']"
"Identifying and monitoring overpronation and oversupination in the long term during activities of daily living are essential for people’s ambulatory health. Using an in-shoe motion sensor (IMS) with power-saving functions is a potential solution. In this study, we challenged the development of an estimation model of foot function using the foot center of pressure excursion index (CPEI) as an index via linear multivariate regression, which is sufficiently light for this type of IMS. Data collected from 65 and 17 participants were involved in model construction and validation, respectively. We validated ten scenarios simulating daily living activities, including walking on different surfaces, using different shoes, with or without carrying a bag, and indoors and outdoors. We applied statistical parametric mapping (SPM) to determine significant predictors and performed our original feature selection algorithm, leave-one-subject-out least absolute shrinkage and selection operator (LASSO), to compress the volume of the predictors. We successfully discovered significant sex-specific predictors for foot function estimation from foot motion and constructed large effect-sized sex-specific foot function estimation models that achieved high-precision CPEI estimation. In the validation, the model successfully estimated a maximum of 99.0% and 100.0% males’ and females’ data under the same experimental conditions with the training data and 92.8%–100.0% and 85.8%–100.0% data in different scenarios. The constructed models are effective and possible to provide applications for long-term foot function monitoring by using an IMS.","['Foot', 'Sensors', 'Footwear', 'Biological system modeling', 'Data models', 'Motion detection', 'Indexes']","['Foot motion', 'gait analysis', 'in-shoe motion sensor (IMS)', 'overpronation']"
"Today’s wearable Internet of Things (IoT) devices, which have numerous practical applications, suffer from the limited lifetime of batteries due to the high power consumption of conventional inertial activity sensors. Recently, kinetic energy harvesters have been employed as a source of energy as well as context information to replace conventional activity sensors. However, the harvested power from human movements using miniaturized kinetic transducers may not be sufficient to enable a perpetual and self-powered activity recognition system. In this article, we propose a novel mechanism of fused signal-based human activity recognition (FusedAR), which employs miniaturized wearable solar and kinetic energy harvesters simultaneously as an energy source as well as an activity sensor. As human activities engender distinct movement patterns and interact and interfere with the ambient light differently, the kinetic and solar energy harvesting (SEH) signals incorporate unique information about the underlying activities while generating sufficient power. After detailed experiments, we find that the FusedAR, which employs both solar and kinetic energy signals, achieves superior activity recognition performance by up to 10%, particularly in outdoor and night-time contexts, and can recognize not only activities but also contexts, compared with the individual energy harvesting signals. Furthermore, our analysis demonstrates that FusedAR, in addition to significant energy generation, consumes up to 22% less power than the highly optimized conventional three-axis accelerometer-based mechanisms, achieving energy-positive human activity recognition (HAR) leading toward perpetual, uninterrupted, and autonomous operation of wearable IoT devices.","['Sensors', 'Biomedical monitoring', 'Transducers', 'Internet of Things', 'Wearable computers', 'Kinetic energy', 'Energy harvesting']","['Context detection', 'energy harvesting', 'energy-positive sensors', 'fused signals', 'human activity recognition (HAR)', 'Internet of Things (IoT)', 'kinetic', 'solar', 'wearables']"
"We investigate through simulations the gold–C60–gold molecular junction as a novel single-molecule amperometric gas sensor. We find it promising for NO and NO2 detection in air and at room temperature, with current variations of the order of the microampere, and presenting the potential capability of achieving the single-molecule sensitivity along with selectivity in the presence of common atmospheric gases. Furthermore, and for the first time, we investigate the current modulation mechanism due to target–sensor intermolecular interactions, providing theoretical insights into the functioning and exclusive properties of this novel device. In particular, we show and motivate the peculiar voltage-dependent response of the sensor that we relate to the distinctive mechanism of transport modulation occurring in the presence of a specific target. Finally, we discuss sensing reliability in air and the effects of probable fabrication process variability on sensing performance. Our results motivate future works on molecular dot-based chemical sensors in terms of the sensor–target exclusive interactions and detection principles, oriented to device-level engineering to find optimal operating conditions.","['Gold', 'Sensor phenomena and characterization', 'Couplings', 'Fullerenes', 'Chemical sensors', 'Temperature sensors', 'Junctions']","['Amperometric detection', 'C₄H₁₀', 'C₆₀', 'CH₄', 'CO', 'CO₂', 'dot', 'electronics', 'fullerene', 'gas', 'gold', 'junction', 'molecular', 'nanogap', 'NH₃', 'nitric oxide (NO)', 'NO₂', 'quantum', 'sensor', 'single-molecule']"
"Magnetic polarizability tensors (MPTs) have become popular for characterising conducting permeable objects and assisting with the identification of hidden objects in metal detection for applications in security screening, humanitarian demining and scrap sorting. A rigorous mathematical justification of the complex symmetric rank 2 MPT object characterisation has been established based on the leading order term in an asymptotic expansion of the perturbed field for small objects. However, the accuracy of an MPT object characterisation is limited by the tensor’s small number of independent coefficients. By considering higher order terms in the asymptotic expansion, generalised magnetic polarizabilty tensors (GMPTs) have been introduced and the purpose of this work is to show that GMPT coefficients can, for the first time, be measured in practice. GMPTs offer the possibility to better discriminate between objects and, hence, the potential for better classification and identification, overcoming the limitations of a rank 2 MPT object characterisation. In a metal detector, the low-frequency background fields generated by a set of coils is almost always non-uniform and using GMPTs allow us to make a virtue of this. In this work we include both measurements and simulations to demonstrate the advantages that using GMPTs offer over using an MPT characterisation alone.","['Tensors', 'Metals', 'Sensor phenomena and characterization', 'Coils', 'Frequency measurement', 'Time measurement', 'Indexes']","['Electromagnetic induction spectroscopy', 'magnetic polarizability tensor', 'metal detection', 'metal classification']"
"Voice activity detection (VAD) aims for detecting the presence of speech in a given input signal, and is often the first step in voice -based applications such as speech communication systems. In the context of personal devices, own voice detection (OVD) is a sub-task of VAD, since it targets speech detection of the person wearing the device, while ignoring other speakers in the presence of interference signals. This article first summarizes recent single and multi-microphone, multi-sensor, and hearing aids related VAD techniques. Then, a wearable in-ear device equipped with multiple microphones and an accelerometer is investigated for the OVD task using a neural network with input embedding and long short-term memory (LSTM) layers. The device picks up the user’s speech signal through air as well as vibrations through the body. However, besides external sounds the device is sensitive to user’s own non-speech vocal noises (e.g. coughing, yawning, etc.) and movement noise caused by physical activities. A signal mixing model is proposed to produce databases of noisy observations used for training and testing the frame-by-frame OVD method. The best model’s performance is further studied in the presence of different recorded interference. An ablation study reports the model’s performance on sub-sets of sensors. The results show that the OVD approach is robust towards both user motion and user generated vocal non-speech sounds in the presence of loud external interference. The approach is suitable for real-time operation and achieves 90–96 % OVD accuracy in challenging use scenarios with a short 10 ms processing frame length.","['Speech processing', 'Databases', 'Feature extraction', 'Training', 'Hidden Markov models', 'Task analysis', 'Sensors']","['Acoustic measurements', 'accelerometer measurements', 'human voice processing', 'artificial neural networks']"
"Exploring life conditions on the near-Earth planets and satellites before carrying out human missions is an important task for space agencies. For that purpose, scientific space missions usually include instruments to measure climatological variables. Within this space instrumentation and measurement context, dust sensors (DSs) aim to measure dust particles in suspension and provide valuable information for persons and equipment life conditions, while they must deal with low signal-to-noise ratios (SNRs). For example, the Exomars mission is focused to characterize the weather on Mars surface and include up to four DSs based on different technologies: infrared (IR), laser, interferometry, impact sensors, and electric field activity sensors. Due to the tight budget in terms of area, weight, power consumption, and data budget in aerospace instruments, as well as ionizing radiation and extreme temperatures, current solutions present low scalability and configurability. In this article, a novel system proposal that extracts valuable information from noisy signals obtained from IR sensors aimed to measure airborne dust is presented. The solution provides competitive capabilities in terms of power consumption, data budget, SNR, and reconfigurability. It has been implemented in a rad-hard Microsemi field programmable gate array (FPGA) (RT54SX32S). Therefore, robustness and scalability are guaranteed. The results reported a maximum power consumption of up to 141 mA (@12 Vdc), a sensitivity of 19.5 mV (input signal), and a data budget of 32 B/s. This research possesses great potential to further instruments not only in planetary exploration but also at Earth applications.","['Sensors', 'Instruments', 'Extraterrestrial measurements', 'Mars', 'Signal to noise ratio', 'Noise measurement', 'Detectors']","['Computing on-the-edge', 'digital lock-in (LI) amplifier', 'dust particles', 'field programmable gate array (FPGA)', 'infrared (IR) sensors', 'Mars exploration']"
"In this paper we present a state estimation scheme for Unmanned Aircrafts (UAs) utilizing dynamics based models and multi-sensor data fusion. Employing the UA dynamics in estimation can substantially enhance the estimator performance, but obtaining accurate dynamics parameters for each UA is computationally costly and complex. To eliminate these issues, we propose two decoupled Extended Kalman Filters (EKFs), namely the Rotational Decoupled Extended Kalman Filter (RDEKF) and the Translational Decoupled Extended Kalman Filter (TDEKF). The dynamics parameters in these filters are identified in real-time using the Deep Neural Network and the Modified Relay Feedback Test (DNN-MRFT) approach. This approach doesn’t demand prior knowledge of the UA physical parameters, requiring only an Inertial Measurement Unit (IMU) and a positioning system for model classification. Our estimation scheme provides position, velocity and attitude estimates, in addition to smooth lag-free inertial acceleration estimates. We show experimentally the advantages of our approach on trajectory tracking problems that uses low rate position sensors. We also demonstrate how utilizing the estimated acceleration in feedback control can reduce the tracking error of an optimally tuned system by 43%. Moreover, the proposed estimator produces smooth estimates that leads to a reduction of controller action by 6.6%, when compared to kinematic based estimators. We compare the achieved results against other methods that require full prior knowledge of the UA parameters or the noise models, and show advantages in performance and real-time capability.","['Real-time systems', 'Kalman filters', 'Rotors', 'Estimation', 'Accelerometers', 'Relays', 'Kinematics']","['Acceleration feedback', 'state estimation', 'DNN-MRFT', 'dynamic model', 'Kalman filter', 'unmanned aircraft (UA)']"
"Wearable sensing has enabled physiological monitoring in everyday life. Clinically, cardiovascular and respiratory monitoring commonly requires separate devices. However, for feasibility in wearable systems, they are integrated into a single device. Since seismocardiography (SCG) records the motion of the chest wall, it has the potential to monitor both vital signs. The primary application of SCG is cardiovascular monitoring. As a result, its ability to detect respiration has been less defined. This study characterizes how SCG is affected by respiration and consequently, how respiratory metrics can be derived from these effects. The sternal motion of 40 subjects was recorded using an inertial measurement unit with a spirometer for reference. Three scenarios were examined with the subject at rest, holding their breath at peak inhalation, and holding their breath when exhaled. Three main aspects of respiratory modulation were explored: amplitude modulation, frequency modulation, and baseline wandering. The SCG amplitude was observed to be dependent on respiratory volume whereas baseline and frequency modulation were dependent on inhalation phase. All three effects were employed separately in the calculation of respiratory rate. Estimations from baseline wandering, amplitude modulation, and heart rate modulation produced r-squared values of 0.71, 0.44, and 0.66, respectively. The accuracy of tidal volume measurement was limited by a high inter-subject variability. Estimation of the respiratory waveform produced an average r-squared of 0.76 using multivariate linear regression. This demonstrates the potential of SCG as a tool for respiratory monitoring within an integrated, non-invasive cardiorespiratory monitoring system from a single point of contact.","['Monitoring', 'Biomedical monitoring', 'Electrocardiography', 'Recording', 'Vibrations', 'Amplitude modulation', 'Accelerometers']","['Seismocardiography', 'gyrocardiography', 'respiration', 'accelerometers', 'wearable devices', 'cardiovascular monitoring']"
"In this article, we present secure cooperative localization for connected automated vehicles (CAVs) based on consensus estimation through leveraging shared but possibly attacked sensory information from multiple adjacent vehicles. First, the communication topology between the CAVs, node kinematic model, and node measurement model for each vehicle are introduced. Then, a consensus Kalman information filter (CKIF) is applied to fuse the shared information from connected vehicles. Since the sensory information might be attacked, an attack detection algorithm based on the generalized likelihood ratio test (GLRT) is adopted. A delay-prediction framework is proposed to maintain the accuracy and real-time performance of the detection algorithm. Next, a rule-based attack isolation method is used to defend the attack. Finally, the proposed secure cooperative localization algorithm is validated in extensive numerical simulation experiments. The results confirm that leveraging information from multiple vehicles in a cooperative manner leads to better accuracy and resilience for vehicle localization under attacks.","['Location awareness', 'Sensors', 'Global navigation satellite system', 'Radar', 'Estimation', 'Topology', 'Kalman filters']","['Attack detection and defense', 'connected automated vehicles (CAVs)', 'consensus estimation', 'secure cooperative localization']"
"Management of crowd information in public transportation (PT) systems is crucial, both to foster sustainable mobility, by increasing the user’s comfort and satisfaction during normal operation, as well as to cope with emergency situations, such as pandemic crises, as recently experienced with coronavirus disease (COVID-19) limitations. This article presents a taxonomy and review of sensing technologies based on the Internet of Things (IoT) for real-time crowd analysis, which can be adopted in the different segments of the PT system (buses/trams/trains, railway/metro stations, and bus/tram stops). To discuss such technologies in a clear systematic perspective, we introduce a reference architecture for crowd management, which employs modern information and communication technologies (ICTs) in order to: 1) monitor and predict crowding events; 2) implement crowd-aware policies for real-time and adaptive operation control in intelligent transportation systems (ITSs); and 3) inform in real time the users of the crowding status of the PT system, by means of electronic displays installed inside vehicles or at bus/tram stops/stations and/or by mobile transport applications. It is envisioned that the innovative crowd management functionalities enabled by ICT/IoT sensing technologies can be incrementally implemented as an add-on to state-of-the-art ITS platforms, which are already in use by major PT companies operating in urban areas. Moreover, it is argued that, in this new framework, additional services can be delivered to the passengers, such as online ticketing, vehicle access control and reservation in severely crowded situations, and evolved crowd-aware route planning.","['Sensors', 'Real-time systems', 'COVID-19', 'Monitoring', 'Pandemics', 'Sensor systems', 'Sensor phenomena and characterization']","['Adaptive systems', 'communication systems', 'coronavirus disease (COVID-19)', 'crowd management', 'intelligent transportation system (ITS)', 'Internet of Things (IoT)', 'public transportation (PT) systems', 'sensing technologies', 'smart cities', 'sustainable mobility']"
"This article presents the design strategy and experimental validation of a battery-free power supply for wireless sensor nodes (WSN) on an industrial study case. The power supply is based on the principle of vibration energy harvesting (VEH). The general architecture of the linear generator with electronic is presented. It is composed of commercially available components as a MIDE PPA 1014 piezoelectric cantilever beam and a LTC3588 circuit to extract and shape the electrical energy. The energy source comes from mechanical vibrations measured on the industrial environment in operation. A tunable mechanism of the resonance frequency is added in order to have a wider range of use than the natural range of a linear harvester. To adapt the VEH according with the source, it resonant frequency range can be tuned with a dedicated tip-mass. Then a fine adjustment within a range of about 20 Hz is set using both a moving clamping device and a temporarily wired electronic device working as a maximum power point finder (MPPF). To achieve a long lifetime, the storage is done using balanced supercapacitors. Two operational demonstrators are shown. The test benches as well as numerous experimental tests are presented. Shaped according to the industrial environment (49.0 Hz @ 2.7 m/s 2 ), the VEH is capable of delivering continuously 100mA@3.3V with 200 mA peaks. When the power harvested ( \approx 2.9 mW) is upper than the sensor average power, it offers the capability to store 17.5 J at 18.5 V. As a result, from this work, a WSN can successfully operate over a significantly long period of time despite fluctuations in the vibration source.","['Wireless sensor networks', 'Vibrations', 'Sensors', 'Resonant frequency', 'Supercapacitors', 'Sensor phenomena and characterization', 'Generators']","['Piezoelectric generator', 'vibration energy harvesting', 'energy management', 'tuning frequency', 'supercapacitor']"
"This work considers the feasibility of using a novel optical fiber-based sensor, employing a terbium-doped gadolinium oxysulfide (Gd2O2S:Tb) inorganic scintillator, as a real-time in vivo dosimetry solution for applications in low-dose-rate (LDR) prostate brachytherapy (BT). This study specifically considers the influence of scintillator geometry (hemisphere tip versus cylindrical cavity), polymethyl methacrylate (PMMA) fiber core diameter (0.5 versus 1.0 mm), and sensor housing material (stainless steel versus plastic) on the measured scintillation signal. Characterization measurements were performed using a silicon photon-multiplier (SiPM) detector and a commercial water phantom system, integrated with custom 3-D printed components to allow for precise positioning of the LDR BT radiation source with respect to the optical fiber sensor (OFS). Significant differences in the rate of fall-off in the scintillation signal, with distance from the source, were observed between the different scintillator geometries considered. The hemisphere tip geometry was shown to be the most accurate, tracking with the expected fall-off in dose-rate, within measurement uncertainty. Reducing the fiber core diameter from 1.0 to 0.5 mm resulted in a sixfold reduction in the detected scintillation signal. A further 57% reduction was observed when housing the 0.5-mm fiber within a stainless steel LDR BT needle applicator. Initial results demonstrate the feasibility of employing an OFS, for applications in LDR BT, given the excellent agreement of measurements with theoretical expectations. Furthermore, a calibration process has been described for converting the detected scintillation signal into absorbed dose/dose rate, using our water phantom-based experimental setup.","['Sensors', 'Optical fiber sensors', 'Phantoms', 'Scintillators', 'Sensor phenomena and characterization', 'Three-dimensional displays', 'Optical fibers']","['Brachytherapy (BT)', 'in vivo dosimetry', 'optical fiber', 'radiation dosimetry', 'silicon photomultiplier']"
"Hemorrhage is one risk of percutaneous intervention in the brain that can be life-threatening. Steerable needles can avoid blood vessels thanks to their ability to follow curvilinear paths, although knowledge of vessel pose is required. To achieve this, we present the deployment of laser Doppler flowmetry (LDF) sensors as an in-situ vessel detection method for steerable needles. Since the perfusion value from an LDF system does not provide positional information directly, we propose the use of a machine learning technique based on a Long Short-term Memory (LSTM) network to perform vessel reconstruction online. Firstly, the LSTM is used to predict the diameter and position of an approaching vessel based on successive measurements of a single LDF probe. Secondly, a “no-go” area is predicted based on the measurement from four LDF probes embedded within a steerable needle, which accounts for the full vessel pose. The network was trained using simulation data and tested on experimental data, with 75% diameter prediction accuracy and 0.27 mm positional Root Mean Square (RMS) Error for the single probe network, and 77% vessel volume overlap for the 4-probe setup.","['Needles', 'Probes', 'Sensors', 'Doppler effect', 'Computational modeling', 'Imaging', 'Position measurement']","['Laser Doppler flowmetry', 'vessel reconstruction', 'percutaneous intervention', 'steerable needle', 'machine learning', 'long short-term memory (LSTM)']"
"Body Area Networks (BAN) are wireless networks designed for deployment on or within the human body. These networks are primarily intended for application within themedical domain due to their capabilities for enablingwireless monitoring of physiological signals, and remote administration of medical devices. Due to their intended use case, securing these devices is paramount. In recent years, several key generationand agreement schemes that rely upon physiological signals of the wearer are developed. However, we have found that the application of Electrocardiogram(ECG) signals in this context may not be appropriate due to a potential vulnerability, wherein previously recorded ECG signals could be used against current and future key agreement attempts to compromise their security. This is a violation of temporal variance which is one of a few properties that make ECG signals suitable for use in key agreement schemes. By extracting the QRS complex from prior recordings and distributing them apart from one another we can construct synthetic signals that have a high level of coherence, and thus allow for the key to be intercepted. Based on the conducted experiments we have found that the proposed attack method yields a 0.7 coherence level regardless of how far away the adversary is from the target. Thismakes the success of such an attack extremely likely and is therefore a real threat to the security of these schemes.","['Electrocardiography', 'Sensors', 'Physiology', 'Security', 'Feature extraction', 'Receivers', 'Medical services']","['Body area networks', 'body sensor networks', 'authentication', 'key generation', 'synthetic signals']"
"With the aging of population, the number of total hip replacement (THR) surgeries increases year by year. To improve the success rate of THR operation, the measurement of acetabular prosthesis angles should be precise during the surgery. We design an inertial measurement unit based real-time measurement system, which measures the initial attitude of the pelvis coordinate, and traces the real-time attitude of the pelvis coordinate and prosthesis axis. By collecting real-time data of diff- erent pelvic and prosthesis attitudes in quaternion representation through accelerometers and gyroscopes, the relative position between pelvis and prosthesis is calculated in real time. An optimized algorithm is proposed to obtain prosthesis implant angles accurately from the data. The experimental results show that the measured implant angles agree with that calculated by radiographic value, the root mean square errors (RMSE) are less than 3.52 degrees, which is within the safe range of THR requirement. Furthermore, no matter how the implant angles change, the measured values agree with reference values, the RMSE is less than 3.27 degrees. The experimental results show that our system can trace the real-time dislocation of the prosthesis during the THR surgeries.","['Prosthetics', 'Real-time systems', 'Pelvis', 'Implants', 'Magnetic field measurement', 'Sensor phenomena and characterization', 'Licenses']","['Total hip replacement (THR)', 'inertial measurement unit (IMU)', 'real-time measurement', 'acetabular prosthesis implant angles']"
"The growth of Artificial Intelligence (AI) and the Internet of Things (IoT) sensors has given rise to a synergistic paradigm known as AIoT, wherein AI functions as the decision-maker and sensors collect information. However, a substantial proportion of AIoT rely on cloud-based AI, which process wirelessly transmitted raw data, increasing power consumption and reducing battery life at sensor nodes. Edge-AI has emerged as a promising alternative, implementing AI directly on sensor nodes, eliminating the need of raw data transmission. Despite its potential, there is a scarcity of hardware architectures optimized for resource-constrained platforms, such as field programmable gate arrays (FPGAs), particularly for low-frequency sensors. This work presents a shared-scale integer-only recurrent neural network (RNN) implemented on a Lattice ICE40UP5K FPGA using a resource-minimized time and layer-multiplexed (TLM) hardware architecture. This architecture adopts real-time processing, setting clock frequency to complete a single RNN timestep preceding the next sensor sample, reducing power consumption significantly. Measurements on this FPGA implementing our proposed architecture applied to a pretrained RNN on cow behavior show a power consumption of 360 \mu \text{W}at a clock frequency of 146 kHz and negligible accuracy loss at 8-bit bitwidth. This finding suggests that our methods lead to the most accurate implementation of animal behavior estimation with a power consumption below 500 \mu \text{W}on an FPGA. The implementation in Systemverilog and Python code is publicly available, enabling adaptation of the RNN for various tasks involving low-frequency sensors on resource-constrained FPGAs, thereby contributing to the further advancement and democratization of Edge-AI solutions.","['Sensors', 'Field programmable gate arrays', 'Artificial intelligence', 'Internet of Things', 'Quantization (signal)', 'Power demand', 'Clocks']","['Artificial intelligence (AI)', 'edge-AI', 'field programmable gate array (FPGA)', 'Internet of Things (IoT)', 'machine learning', 'precision livestock farming (PLF)', 'quantization', 'recurrent neural network (RNN)']"
"Nitrate and nitrite are reaction products that result, for example, from the application of nitrogenous fertilizers to agricultural land. That part of the nitrogen not absorbed by the plants can pass into the groundwater in the form of nitrate or nitrite. This can also lead to contamination of drinking water via the groundwater, with all its negative side effects. In the present article an automatic system is presented, which performs independently the measurement of nitrate and nitrite at a water body. For the measurement of these parameters commercially available test strips are used, which are evaluated automatically. The determination of the nitrite/nitrate value is based on the Griess reaction which leads to a color change of the test pads depending on the concentration of the analyte. The sample is applied to the test pads with an automated dispenser system and the color change is measured with an RGB sensor. The nitrate/nitrite concentration present can be determined from the R/G ratio and a nomogram. The measuring accuracy of the system meets the requirements of the European Drinking Water Directive for the determination of the nitrate and nitrite value. The limit of detection (LOD) was determined for the self-designed sensor with LOD = 2.003 mg/l for nitrate over a measurement range of 0–50 mg/l nitrate and a LOD of 0.611 mg/l nitrite over a measurement range of 0–10 mg/l nitrite and 0.013 mg/l for a measurement range of 0–1 mg/l nitrite.","['Strips', 'Sensors', 'Sensor systems', 'Pollution measurement', 'Optical sensors', 'Soil', 'Water pollution']","['Nitrate', 'nitrite', 'optical sensor', 'test strips', 'water quality']"
"A new fiber-optic perimeter intrusion detection system employing only one single-mode fiber as a disturbance sensor for each perimeter zone is presented. A disturbance signal generated through interference between lights from two resonators, i.e., a linear laser cavity and an extended resonator, was used for determining intrusion for each defended zone. A single-mode fiber was used not only for sensing the intrusion-induced disturbance but also for transmission of pump and signal lights. Although many papers have reported multi-perimeter-zone intrusion detection systems, the study presents, for the first time, a system with a single-mode fiber used all along the perimeter that is in a line pattern and is sectioned into multiple defended zones. An outdoor test for a four perimeter-zone intrusion detection system was carried out with a short section of fiber attached on a netted fence in each defended zone. Experimental results demonstrated that there was no cross interference between any two defended-zone light circuits, and that a high alarm-upon-intrusion rate and a zero false alarm rate could be reached by using the presented detection algorithm.","['Optical fiber sensors', 'Optical fiber cables', 'Optical fiber couplers', 'Wavelength division multiplexing', 'Intrusion detection', 'Erbium-doped fiber lasers', 'Optical interferometry']","['Perimeter intrusion detection', 'multiple defended zones', 'linear laser cavity', 'extended resonator', 'pump laser', 'erbium-doped fiber']"
"This paper presents the results from the characterization of pneumatic touch sensors (sensing bulbs) designed to be integrated into myoelectric prostheses and body-powered prostheses. The sensing bulbs, made of silicone, were characterized individually (single sensing bulb) and as a set of five sensors integrated into a silicone glove. We looked into the sensing bulb response when applying pressure at different angles, and also studied characteristics such as repeatability, hysteresis, and frequency response. The results showed that the sensing bulbs have the advantage of responding consistently to pressure coming from different angles. Additionally, the output (pneumatic pressure) is dependent on the size of interacting object applied to the sensing bulb. This means that the sensing bulb will give higher sensation when picking up sharper objects than blunt objects. Furthermore, the sensing bulb has good repeatability, linearity with an error of 2.95± 0.40%, and maximum hysteresis error of 2.39± 0.17% on the sensing bulb. This well exceeds the required sensitivity range of a touch sensor. In summary, the sensing bulb shows potential for use in prosthetic hands.","['Force', 'Tactile sensors', 'Capacitive sensors', 'Prosthetic hand']","['Pneumatic', 'touch sensor', 'non-invasive', 'sensory feedback', 'sensing glove']"
"This study proposes a novel calibration-free cuffless blood pressure monitor (BPM). Unlike the cuffless BPM that requires timing correction with the traditional BPM, the calibration-free cuffless BPM can directly estimate the arterial blood pressure (BP) based on the hemodynamics mathematical model. Our proposed device integrates a pair of piezoelectric ceramics as pressure sensors to sense the pressure wave in the radial artery. It calculates the local pulse wave velocity (PWV) with our waveform detection algorithm of our proposed one. A photoplethysmogram (PPG) probe is set between the pair of pressure wave sensors. This PPG probe is used to monitor the radial artery’s PPG intensity ratio (PIR). We design PPG signal processing algorithms to quantify the PIR. We recruited 129 participants for the BP monitoring experiment. Compared with the reference sphygmomanometer, the error mean ± standard deviations (STDs) systolic BP (SBP) was 2.1 ± 3.4, and the correlation r-value was 0.97. The diastolic BP (DBP) was 0.8 ± 4.2, the correlation r-value was 0.90, and p < 0.05 is taken as statistically significant. A new type of wearable continuous calibration-free BPM can replace the situation that requires the use of traditional ambulatory BPM and reduce patient discomfort. Our proposed BP measurement passed all ANSI/AAMI/ISO 81060-2:20181_5.2.4.1.2 data analysis criterion 1 and 2 standard requirements.","['Sensors', 'Biomedical monitoring', 'Sensor phenomena and characterization', 'Heuristic algorithms', 'Arteries', 'Finite impulse response filters', 'Monitoring']","['Blood pressure monitor (BPM)', 'photoplethysmogram (PPG) intensity ratio', 'photoplethysmogram', 'pulse wave velocity (PWV)', 'systolic blood pressure (SBP)']"
"The flexural ultrasonic transducer has traditionally been limited to proximity measurement applications, such as car-parking systems and industrial metrology. Principally, their classical form is unsuitable for environments above atmospheric 1 bar pressure, due to an internal air cavity which creates a pressure imbalance across the transducer’s vibrating membrane. This imbalance leads to physical deformation and degradation of the transducer’s structure, restricting the membrane’s capacity to vibrate at resonance to transmit and receive ultrasound. There is a requirement for ultrasonic sensors which can withstand environments of elevated pressure, for example in ultrasonic gas metering. Recent research demonstrated the dynamic performance of flexural ultrasonic transducers with vented structures, allowing the pressure to balance across the transducer membrane. However, a hermetically sealed transducer is a more practical and robust solution, where the internal components of the transducer, such as the piezoelectric ceramic disc, will be protected from harmful environmental fluids. In this research, the design and fabrication of a new form of flexural ultrasonic transducer for environments of elevated pressure is demonstrated, where the internal air cavity is filled with an incompressible fluid in the form of a non-volatile oil. Dynamic performance is measured through acoustic microphone measurements, electrical impedance analysis, and pulse-echo ultrasound measurement. Together with finite element analysis, stable ultrasound measurement is achieved above 200 bar in air, opening the possibility for reliable ultrasound measurement in hostile environments of elevated pressure.","['Ultrasonic transducers', 'Transducers', 'Ultrasonic variables measurement', 'Acoustics', 'Oils', 'Fabrication', 'Bars']","['Elevated pressure', 'flexural ultrasonic transducer', 'oil filled transducer']"
"The Internet of Nano-Things (IoNT) is an emerging paradigm in which devices sized to the nanoscale (nanonodes) and transmitting in the terahertz (THz) band can become decisive actors in future medical applications. Flow-guided nanonetworks are well-known THz networks aimed at deploying the IoNT inside the human body, among other issues. In these networks, nanonodes flowing through the bloodstream monitor-sensitive biological/physical parameters and dispatch these data via electromagnetic (EM) waves to a nanorouter implanted in human tissue, which operates as a gateway to external Internet connectivity devices. Under these premises, two shortcomings arise. First, the use of the THz band greatly limits the nanonode’s communication range. Second, the nanonodes lack resources for processing, memory, and batteries. To minimize the impact of these concerns in EM nanocommunications, a novel dynamic multihop routing scheme is proposed to model in-body, flow-guided nanonetwork architecture. To this end, a reinforcement learning-based framework is conceived, combining the features of EM nanocommunications and hemodynamics or fluid dynamics applied to the bloodstream. A generic Markov decision process (MDP) approach is derived to maximize the throughput metric, analytically modeling: 1) the movement of the nanonodes in the bloodstream as laminar flow; 2) energy consumption (including energy-harvesting issues); and 3) prioritized events. A thoroughly THz flow-guided nanonetwork case of study is also defined. Under the umbrella of this case, diverse testbeds are planned to create a procedure of evaluation, validation, and discussion. Results reveal that multihop scenarios obtain better performance than direct nanonode-nanorouter communication, specifically, the two-hop scenario, which, for instance, quadrupled the throughput in a hand vein without sharply penalizing other aspects such as energy consumption.","['Nanobioscience', 'Routing', 'Nanoscale devices', 'Analytical models', 'Nanocommunication (telecommunication)', 'Internet', 'Throughput']","['Internet of Nano-Things (IoNT)', 'Markov decision process (MDP)', 'nanocommunications', 'terahertz (THz) flow-guided nanosensor networks']"
"Early detection of esca disease in grapevines is fundamental to calibrating proper pesticide use in sustainable vineyard management systems. This article is a proof of concepts showing that the Raman spectroscopy (RS) combined with a chemometric analysis can be used as a nondestructive sensor to implement precision agriculture for early detection of the disease through a new biochemical marker. Symptomatic and symptomless branches from an esca-developing vine and from healthy esca-free vines from a vineyard were investigated by RS and chemometrics, using the final outcome of the specific disease syndrome as the final benchmark. As a result, the model is always able to correctly classify all the test samples, showing auspicious results to perform precision agriculture-based future therapeutic approaches.","['Diseases', 'Pipelines', 'Stress', 'Standards', 'Sensor phenomena and characterization', 'Raman scattering', 'Fluorescence']","['Chemometric', 'esca disease', 'precision agriculture', 'Raman spectroscopy (RS)']"
"Measuring diameter change in flexible tubular structures embedded in opaque material is challenging. In this article, we present a soft braided coil embedded in an elastomer tube as a method to continuously measure such a change in diameter. By measuring the inductance change in the braided coil, we estimate the instantaneous diameter with a simple inductance model. In applying this method, we demonstrate that diameter waves in a vascular phantom, a model of a radial artery embedded in a viscoelastic wrist structure, can be recorded continuously. Four sensors were made, and their ability to measure physiologically relevant simulated pulse waves was assessed. Several pressure pulse profiles were generated using a precision digital pump. Inductance of the coil was measured simultaneously as the change in diameter was recorded using an optical laser/mirror deflection measurement. One sensor was then embedded in a vascular phantom model of the human wrist. The diameter of the simulated radial artery was recorded via ultrasound and estimated from coil inductance measurements. The diameter estimates from the inductance model corresponded well with the comparator in both experimental setups. We demonstrate that our method is a viable alternative to ultrasound in recording diameter waves in artery models. This opens opportunities in empirical investigations of physiologically interesting fluid-structure interaction. This method can provide new ability to measure diameter changes in tubular systems where access is obstructed.","['Electron tubes', 'Sensors', 'Inductance', 'Temperature sensors', 'Strain', 'Mirrors', 'Arteries']","['In vitro experimentation', 'inductance', 'soft electronics', 'measurement methods']"
"Continued development of communication technologies has led to widespread Internet-of-Things (IoT) integration into various domains, including health, manufacturing, automotive, and precision agriculture. This has further led to the increased sharing of data among such domains to foster innovation. Most of these IoT deployments, however, are based on heterogeneous, pervasive sensors, which can lead to quality issues in the recorded data. This can lead to sharing of inaccurate or inconsistent data. There is a significant need to assess the quality of the collected data, should it be shared with multiple application domains, as inconsistencies in the data could have financial or health ramifications. This article builds on the recent research on trust metrics and presents a framework to integrate such metrics into the IoT data cycle for real-time data quality assessment. Critically, this article adopts a mechanism to facilitate end-user parameterization of a trust metric tailoring its use in the framework. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd-sourced or other unreliable data collection techniques such as that in IoT. The article further discusses how the trust-based framework eliminates the requirement for a gold standard and provides visibility into data quality assessment throughout the big data model. To qualify the use of trust as a measure of quality, an experiment is conducted using data collected from an IoT deployment of sensors to measure air quality in which low-cost sensors were colocated with a gold standard reference sensor. The calculated trust metric is compared with two well-understood metrics for data quality, root mean square error (RMSE), and mean absolute error (MAE). A strong correlation between the trust metric and the comparison metrics shows that trust may be used as an indicative quality metric for data quality. The metric incorporates the additional benefit of its ability for use in low context scenarios, as opposed to RMSE and MAE, which require a reference for comparison.","['Data integrity', 'Measurement', 'Internet of Things', 'Big Data', 'Data models', 'Sensor phenomena and characterization', 'Quality assessment']","['Big data model', 'data quality', 'Internet of Things (IoT)', 'machine learning', 'trust']"
"This work investigates the estimation of rotational speeds based on neural networks and high sampled gearbox vibration data. A total of 18 different network architectures consisting of fully connected network layers, long short-term memory cells, and different activation functions are compared. In addition, the impact of the sampling rate of the vibration data, the signal length, and the input data type is investigated. Vibration data in the time domain, frequency domain, and a combination of both are examined as input data. Finally, the number of neurons is varied to obtain the best possible rotational speed estimation. In order to generate an appropriate dataset for the training of the neural networks, an experimental setup is presented, which is used to record the vibration data of the gearboxes. It becomes apparent that especially vibration data in the frequency domain are suitable for the estimation of the rotational speed. Furthermore, it could be shown that the low sampling rates result in an inaccurate speed estimation. Likewise, signal lengths that are too short result in inaccurate estimates. Overall, the speed can be estimated with an accuracy of 7.88 rpm at an average speed of the test dataset of 202 rpm.","['Velocity control', 'Vibrations', 'Estimation', 'Logic gates', 'Biological neural networks', 'Sensors', 'Gears']","['Gearbox vibration', 'neural network', 'rotational speed estimation', 'signal processing', 'vibration analysis']"
"Doppler-based radar systems have been seen as a promising tool to assess vital signs, since they are capable to monitor the respiratory and cardiac signal remotely, by measuring the chest-wall displacement. However, due to the spectral overlap of these signals, their proper separation is a challenging task. In this paper, we demonstrate the effectiveness of using Discrete Wavelet Transform in the cardiac signal extraction, by comparing this method with other approaches widely used in literature, namely a standalone Band-Pass Filtering, the Ensembled Empirical Mode Decomposition, the Continuous Wavelet Transform and the Wavelet Packet Decomposition. The comparison metrics were defined taking into consideration the heart rate computation accuracy, and also the peak detection consistency to further evaluate the Heart Rate Variability. The efficiency of those methods is also tested considering real application scenarios, characterized by non-controlled monitoring environment conditions and the ability to equally assess the vital signs of different subjects, regardless their physical stature.","['Discrete wavelet transforms', 'Continuous wavelet transforms', 'Heart rate variability', 'Wavelet packets', 'Signal resolution', 'Radar', 'Wavelet analysis']","['Doppler radar', 'continuous wave', 'vital signs', 'bio-radar', 'cardiac signal', 'discrete wavelet transform', 'wavelet packet decomposition', 'ensemble empirical mode decomposition', 'continuous wavelet transform']"
"Current force sensors used to capture fingertip interaction forces lack compliance to the fingertip tissue resulting in the loss of touch sensation of the user. 3D printing offers the possibility to create personalized soft sensing structures. This work evaluates a 3D printed soft sensor that measures normal and shear interactions forces based on the deformations of the thumb and index fingertips of 7 subjects using an instrumented object. Due to the use of (carbon doped) thermoplastic materials, the signals provided by these sensing structures suffer from nonlinearities. Therefore, two compensation models, based on a neural network and recurrent neural network analogous to an electrical model are used to compensate for the nonlinear effects. The performance of the sensors was analysed using the normalized cross-correlation and the root-mean-square error. The output of the force sensors are highly correlated with the applied shear and normal force components. When paired with compensation models the correlation and error of the sensor output can be further improved. These results indicate that the proposed flexible fingertip interaction force sensors have a high potential for future applications.","['Sensors', 'Strain measurement', 'Three-dimensional displays', 'Task analysis', 'Force sensors', 'Force', 'Instruments']","['3D printing', 'conductive', 'fingertip', 'flexible', 'force', 'interaction', 'neural network', 'piezo-resistive', 'recurrent', 'shear force', 'soft']"
"Metal-assisted chemical etched (MACE; also known as MacEtch or MCCE) nanostructures are utilized widely in the solar cell industry due to their excellent optical properties combined with a simple and cost-efficient fabrication process. The photodetection community, on the other hand, has not shown much interest toward MACE due to its drawbacks, including insufficient surface passivation, increased junction recombination, and possible metal contamination, which are especially detrimental to p-n photodiodes. Here, we aim to change this by demonstrating how to fabricate high-performance MACE p-n photodiodes with above 90% external quantum efficiency (EQE) without external bias voltage at 200–1000 nm and dark current less than 3 nA/cm2 at −5 V using industrially applicable methods. The key is to utilize an induced junction created by an atomic layer deposited (ALD) highly charged Al2O3 thin film that simultaneously provides efficient field-effect passivation and full conformality over the MACE nanostructures. Achieving close to ideal performance demonstrates the vast potential of MACE nanostructures in the fabrication of high-performance low-cost p-n photodiodes.","['Photodiodes', 'Silicon', 'Nanostructures', 'Radiative recombination', 'Optical surface waves', 'Dark current', 'Surface waves']","['MACE', 'photodetector', 'responsivity', 'Si']"
"Walking at home can provide valuable information about locomotor efficiency, anticipation of daily hazards and general well-being. Here, we present a multidisciplinary method to reconstruct locomotor trajectories while walking at home with a capacitive proximity sensing device — the SensFloor — which was installed in a real occupied apartment in the city center of Montpellier in France. Our recognition method is based on the spatio-temporal statistical probability of body location corresponding to sensors’ activation. The results led to the localization of the inhabitant in the two-dimensional floor space, and their tracking over a 24-hour period. More precisely, our technique enabled us to distinguish human-related behavior from the location of static objects. It also allowed us to successfully identify locomotor trajectories in a highly confined space, including those from two simultaneously walking individuals in different rooms. It allowed us to obtain valuable information on spatial behavior (trajectory, stationarity) but also on temporal behavior (occupancy time, walking duration). As this technique compensates for the already established low accuracy of capacitive sensors, our method offers innovative possibilities to study locomotor metrics at home using relatively inexpensive sensing technology.","['Sensors', 'Legged locomotion', 'Floors', 'Intelligent sensors', 'Capacitive sensors', 'Trajectory', 'Sensor phenomena and characterization']","['Intelligent floor', 'locomotor trajectories', 'modeling', 'SensFloor', 'walking at home']"
"In this work, we present a novel, sensitive, easy-to-fabricate, flexible amperometric sensor constituted by screen-printed silver (Ag) electrodes functionalized with a copper (Cu) film electrodeposited on top of a spray coated network of single-walled carbon nanotubes (SWCNTs). The Cu/SWCNTs/Ag electrode showed excellent catalytic activity towards the electro-reduction of nitrate ions at neutral pHwith a significant increasein cathodicpeak currents incomparison with the electrode without SWCNTs (Cu/Ag). The developed Cu/SWCNTs/Ag sensor showed a wide linear detection range from0.5 μMto 6.0 mM (0.31 mg/l to 372.02 mg/l) with good sensitivity (18.39μA/mM) and a calculated limit of detection (LOD) of 0.166 nM(10.29μg/l). It also showed a good selectivity (maximum standard deviation (SD) was 3.25μA) towards different interfering ions (Fe2+, Na+, Cu2+, SO2−4, CH3COO−, Cl−, NO−2 and HCO−3), as well as good reproducibility,mechanical durability, time and temperature stability. In real sample analysis (tap and river water), the sensor exhibited good agreement with the compared outcome of high-performance liquid chromatography (HPLC) measurements, proving to be a promising analytical tool for the detection of nitrate in water.","['Sensors', 'Electrodes', 'Temperature sensors', 'Surface treatment', 'X-ray scattering', 'Sensor phenomena and characterization', 'Ions']","['SWCNTs', 'electrochemical', 'nitrate sensor', 'flexible substrate', 'copper electrodeposition', 'screen printing']"
"This paper presents the Flying Lidar (Flydar) aerial robot for simultaneous localization and mapping (SLAM). The Flydar integrates a single laser and capitalizes on the rotating dynamics of a nature-inspired aerial robot for omnidirectional scanning. This paper presents a 2.5D SLAM approach using the Flydar and does not assume that the Flydar’s hovering plane, and thus the laser scan, is parallel to the ground plane, which is a key assumption in our previous work. A quaternion-based filter performs active correction of the Flydar’s laser scan due to its transient attitude. A dual-accelerometer approach was incorporated to estimate the Flydar’s scan rate during high angular rates flight beyond the gyroscope dynamic range. The proposed attitude filter output was experimentally evaluated statically on a bench-top setup and dynamically in flight, with an rms inclination estimate error of up to 1.1° and 0.38° respectively. The attitude-corrected lidar scan was used to estimate the robot pose for 2.5D SLAM. The 2.5D SLAM was experimentally validated on the Flydar and demonstrated to be superior to pure 2D SLAM in loop closure, position estimate drift, and rms error. Significantly, the 2.5D SLAM using the Flydar reports a low rms Euclidean error of up to 0.083 m, which is a 32.0% error reduction compared with our previous work, which uses 2D SLAM.","['Sensors', 'Simultaneous localization and mapping', 'Robots', 'Laser radar', 'Robot kinematics', 'Estimation', 'Accelerometers']","['Aerial robotics', 'lidar', 'robotics and automation', 'sensor applications']"
"The bovine serum albumin (BSA) protein is employed in genosensors as nonspecific binding blocker. In this work, the influence of BSA on the electrochemical response of a DNA-based biosensor is explored by electrochemical impedance spectroscopy (EIS) and double pulse voltammetry technique. Each buffer component of the hybridization buffer is studied on bare gold electrodes and on single-stranded DNA (ssDNA)-functionalized electrodes. The BSA is found to have the higher influence on the sensor sensitivity. The superficial modification induced by BSA greatly affects the electrochemical signal due to steric hindrance, which decreases the device sensitivity. Thus, we propose a novel treatment method of the BSA by using the proteinase K. The proteinase K reduces the steric hindrance, improving the charge transfer at the electrode/solution interface and enhancing the genosensor sensitivity. The proposed method is based on a rapid BSA treatment. The BSA solution containing the proteinase K is kept for 1 h at 37 ° C to obtain BSA fragmentation and then heated up to 95 ° C for 2 min to inactivate the enzyme and denature the fragments. The method was tested successfully on a genosensor for the detection of the Campylobacter Jejuni. Double pulse current measurements clearly discriminate the presence of complementary DNA (cDNA) fragments with any other noncomplementary DNA (NcDNA) fragments.","['Electrodes', 'Proteins', 'DNA', 'Surface treatment', 'Sensor phenomena and characterization', 'Sensitivity', 'Probes']","['Bovine serum albumin (BSA)', 'differential pulse voltammetry (DPV)', 'electrochemical genosensors', 'electrochemical impedance spectroscopy (EIS)', 'proteinase K']"
"The behavioral monitoring of farmed animals such as cattle is a fundamental element of precision farming in which it enables unobtrusive ongoing health monitoring. This application presents two ubiquitous challenges typical of sensing applications of the Internet of Things: limited dataset size and dataset imbalance. Recently, data augmentation has emerged as a way of addressing their negative influences on the training process without overburdening the data acquisition phase. However, there remains no consensus regarding which methods should be applied to time series and in what combination. Here, we present the first comprehensive analysis that synergistically combines multiple approaches. These approaches are benchmarked on a dataset of triaxial accelerometer time series, which were acquired from six freely roaming cows through a collar-mounted sensor and labeled by experienced human observers according to five behaviors. Our results indicate that integrating data augmentation with the training process can substantially improve the time-series classification performance while retaining a fixed convolutional neural network architecture. The improvement is maximized when the dataset is balanced by applying a suitable sampling scheme and the negative influence of data duplication is reduced via generating synthetic time series with Fourier surrogates. With the proposed approach, the overall accuracy is improved from 90% to 96%, and the classification accuracy of an under-represented behavior, namely, grazing, is elevated from 45% to 91%. This work provides a direction toward a general methodology, motivating research on other datasets and applications.","['Behavioral sciences', 'Time series analysis', 'Sensors', 'Training', 'Monitoring', 'Cows', 'Sensor phenomena and characterization']","['Accelerometer', 'animal behavior', 'data augmentation', 'Fourier surrogates', 'imbalanced dataset', 'sensor data processing', 'time series']"
"Surface acoustic wave (SAW) devices are generally fabricated on rigid substrates that support the propagation of waves efficiently. Although very challenging, the realisation of SAW devices on bendable and flexible substrates can lead to new generation SAW devices for wearable technologies. In this paper, we report flexible acoustic wave devices based on ZnO thin films coated on various substrates consisting of thin layers of metal (e.g., Ni/Cu/Ni) and/or polymer (e.g., polyethylene terephthalate, PET). We comparatively characterise the fabricated SAW devices and demonstrate their sensing applications for temperature and ultraviolet (UV) light. We also investigate their acoustofluidic capabilities on different substrates. Our results show that the SAW devices fabricated on a polymer layer (e.g. ZnO/PET, ZnO/Ni/Cu/Ni/PET) show enhanced temperature responsivity, and the devices with larger wavelengths are more sensitive to UV exposure. For actuation purposes, the devices fabricated on ZnO/Ni/Cu/Ni layer have the best performance for acoustofluidics, whereas insignificant acoustofluidic effects are observed with the devices fabricated on ZnO/PET layers. We propose that the addition of a metallic layer of Ni/Cu/Ni between ZnO and polymer layers facilitates the actuation capability for the acoustofluidic applications while keeping temperature and UV sensing capabilities, thus enhancing the integration of sensing and acoustofluidic functions.","['Sensors', 'Substrates', 'Surface acoustic wave devices', 'Zinc oxide', 'Surface acoustic waves', 'Temperature sensors', 'II-VI semiconductor materials']","['Acoustic wave', 'thin film', 'flexible', 'bendable', 'sensing', 'acoustofluidics', 'multilayers']"
"Millimeter-wave (mm-wave) frequency-modu- lated continuous wave (FMCW) radars are increasingly being deployed for scenario perception in various applications. It is expected that the mutual interference between such radars will soon become a significant problem. Therefore, to maintain the reliability of the radar measurements, there must be procedures in place to mitigate this interference. This article proposes a novel interference mitigation technique that utilizes the pulse compression principle for interference compression and mitigation. The interference in the received time-domain signal is compressed using an estimated matched filter. Afterward, the compressed interference is discarded, and the signal is repaired in the pulse-compressed domain using an autoregressive (AR) model. Since the interference spans fewer samples after compression, the signal can be restored more accurately in the compressed domain. Real outdoor measurements show that the interference is effectively suppressed down to the noise floor using the proposed scheme. A signal to interference and noise ratio (SINR) gain of approximately 14 dB was achieved in the experimental data, supporting this study. Moreover, the results indicate that this method is also applicable to situations where multiple interference sources are present.","['Interference', 'Radar', 'Chirp', 'Sensors', 'Time-domain analysis', 'Time-frequency analysis', 'Radar detection']","['Automotive radar', 'interference cancellation', 'millimeter wave (mm-wave) radar', 'pulse compression', 'radar signal processing', 'signal reconstruction']"
"Interpersonal touch interactions are an essential social behavior for physical and mental health. We previously developed a smart bracelet-type device, called EnhancedTouch, which quantitatively measures interpersonal hand-to-hand touch interactions by actively transmitting and receiving a modulated electrical current through the human hands. However, it remains unclear how the current will flow between devices. Such ambiguity is due to the fact that a general instrument such as an oscilloscope needs to be connected to the ground of the device, which enhances a coupled ground reference and results in a stable performance compared to a device operating under a more realistic situation, i.e., a wireless wearable device. This paper presents the design guidelines of such devices. We introduce the details of design and the implementation of a bracelet device, which is a revised version of EnhancedTouch. A pair of devices is used to evaluate the signal transmission by means of the signal-to-noise ratio (SNR) with a ground-free wireless measurement system. An experiment with four conditions were conducted to study the following effects: 1) the distance between devices and the contact area of the hands, 2) the distance between users, 3) the height from the floor to the devices, and 4) the configuration of electrodes on the signal transmission. The results indicate that either a shorter distance between devices or a larger contact area of the hands results in a relatively high SNR. In addition, the results also suggest that the configuration of electrodes significantly affects the SNR.","['Couplings', 'Electrodes', 'Sensors', 'Current measurement', 'Body area networks', 'Wireless sensor networks', 'Wearable sensors', 'Haptic interfaces']","['Body area network', 'EnhancedTouch', 'galvanic coupling', 'interpersonal touch', 'sensor electronic circuits', 'wearable sensors']"
"Time-of-Flight cameras are active sensors that are able to capture both the light intensity reflected by each observed point in the scene and the distance between these points and the camera. Enhancing intensity images with a depth modality enables capturing surfaces in 3D and boosts the applicability of these sensors. Nevertheless, high-level information still needs to be extracted from the data stream in order to accomplish high-level tasks, like recognition or classification. Ideally, the semantic gap between sensor output and high-level requirements should be as small as possible, in order to reduce both computational cost and failure probability. An additional depth modality helps in this regard, but there are further cues that can be seen by a ToF sensor that have remained underexploited so far. In this paper we take the first steps towards a trimodal ToF camera, which adds a valuable material modality to the classical intensity and depth modalities. To this end, ToF raw data is used to obtain Fourier samples of the material impulse response function (MIRF) to modulated illumination. The MIRF depends on the surface- and sub-surface-level scattering mechanisms of the material and, thus, can be used to identify materials of different nature. Consequently, distinctive feature vectors can be obtained from the Fourier measurements. Additionally, including the incidence angle in the feature vectors allows capturing the MIRF dependency on this parameter. Experimental validation confirms the feasibility of this approach. We also constructed a live demonstrator of our trimodal ToF camera.","['Cameras', 'Sensors', 'Scattering', 'Surface roughness', 'Rough surfaces', 'Harmonic analysis', 'Lighting']","['Time-of-flight', 'material sensing', 'Fourier sampling', 'superpixel segmentation', 'MIRF']"
"The rising popularity of additive manufacturing processes leads to an increased interest in possibilities and methods for related process monitoring. Such methods ensure improved process quality and increase the understanding of the manufacturing process, which in turn is the basis for stable component quality, e.g., required in the aerospace industry or in the medical sector. For laser powder bed fusion, a handful of process monitoring tools already exist, such as optical tomography, thermography, pyrometry, imaging, or laser power monitoring. Although these tools provide helpful information about the process, more information is required for an accurate in-depth understanding. In this article, advanced approaches in eddy current testing (ET) are combined, such as single wire excitation, magnetoresistive (MR) sensor arrays, and heterodyning to build up a system that can be used for online process monitoring of laser powder bed fusion. In addition to detailed information about the developed ET system and underlying signal processing, the first results of magnetoresistance-based online ET during the laser powder fusion process are presented. While producing a step-shaped cuboid, each layer is tested during recoating. Test results show that not only the contours of the topmost layer are detected but also the contours of previous layers covered by powder. At an excitation frequency of 1 MHz, a penetration depth of approx.400 μmis obtained. To highlight the possibilities of ET for online process monitoring of laser powder bed fusion, results are compared with postexposure images of the integrated layer control system (LCS).","['Wires', 'Probes', 'Process monitoring', 'Sensors', 'Powders', 'Switches', 'Data acquisition']","['Additive manufacturing', 'eddy current testing (ET)', 'giant magnetoresistance (GMR)', 'Haynes282', 'heterodyning', 'laser powder bed fusion (LPBF)', 'nondestructive testing', 'process monitoring']"
"Pressure sensors with capability to detect small physical movements and mechanical deformations have been widely used in wearable and medical applications. However, devices that are commercially available currently require complex designs and fabrication and present only a limited force-range sensitivity. To simplify the design, a thermoplastic polyurethane (TPU)/carbon nanotubes (CNTs) composite film has been developed using a melt extrusion technique followed by compression molding. Pressure sensors were made from these films, whose piezoresistive response has been analyzed as a function of the concentrations of CNTs, around the percolation threshold. The changes in the voltage of the device with applied pressure were continuously measured using a voltage divider system coupled with an electromechanical test machine that dynamically loaded the sensors under compression. The voltage divider system was tuned to obtain the best sensitivity and signal/noise (S /N) ratio for each device tested. The results showed that sensors containing a target of 2.5 wt% CNTs had market leading sensitivity and repeatability during long-term stability testing and showed high durability during underwater testing, indicating that such devices can be used as a promising robust pressure-sensitive sensor in wearable devices.","['Sensors', 'Sensor phenomena and characterization', 'Sensitivity', 'Force', 'Wearable sensors', 'Scanning electron microscopy', 'Pressure sensors']","['Carbon nanotubes (CNTs)', 'electromechanical behavior', 'piezoresistive', 'sensor', 'thermoplastic polyurethane (TPU)']"
"In this work, we propose a novel pipeline method for laser line extraction from images with a polarization image sensor. The proposed method is specifically developed for strong laser beam reflections from metal surfaces. For the preprocessing stage, we propose a demosaicing algorithm for color polarizer filter array (CPFA) sensors. This can be implemented by using either one-quarter or full resolution of the sensor. In addition, we propose two methods for optimizing the information available in a 12-channel color polarization image. The first method is based on the minimum linearly polarized irradiance, and the second method is based on the linear polarization intensity. These preprocessing and optimization methods are combined with laser line extraction methods. The laser line extraction is done with either the polarized finite impulse response (FIR) center of gravity (COG), where the laser line coordinates are computed from the filtered laser intensity distribution, or with the polarized FIR-Peak, where the laser line coordinates are calculated from the first derivative of the filtered laser signal. The performance of the proposed algorithms is studied experimentally using a laser line scanner assembly, made of a polarization camera, and a laser line projector operating in the blue wavelength range.","['Image sensors', 'Sensors', 'Lasers', 'Image color analysis', 'Cameras', 'Reflection', 'Polarization']","['Aluminum scanning', 'laser line extraction', 'polarization imaging', 'polarization sensor', 'reflective metal scanning', 'structured light sensor']"
"Graphene-based field-effect transistors (gFETs) are gaining popularity for the realization of biological sensors as their graphene active area provides a convenient basis for attaching organic substances, such as appropriately engineered receptors. The presence of a particular biological agent then translates in the modification of the electrical characteristics of the gFET. We thus developed a compact, portable system that is able to accurately measure those electrical characteristics with high accuracy, by automatically compensating its own offsets and errors. The acquisition device we here present is able to measure drain currents with a nominal accuracy of 0.1% and with an rms noise as low as 22 pA, up to a maximum of 125 \mu \text{A}(22 bits effective resolution), and gFET channel resistances with a nominal accuracy of 0.01% \pm 0.1 \Omegaand with an rms noise as low as 2.13 \mu \text{V}in the range from 100 \Omegato 1 \text{M}\Omega. Due to its performance, small dimensions, and long battery life, it can be used both for scientific research, where portability and ease of use are key features when operating in potentially hazardous environments due to the presence of biological agents, and as a fully automated detector when coupled with the appropriate sensor, as it can perform thousands of measures on a single battery charge and be completely remotely controlled over a Bluetooth low energy (BLE) connection.","['Sensors', 'Graphene', 'Batteries', 'Connectors', 'Voltage measurement', 'Current measurement', 'Wireless communication']","['Bluetooth sensor', 'graphene sensor', 'portable measurement system', 'readout circuit', 'source measure unit (SMU)', 'wireless sensor']"
"The article describes a reference and training set free incrementally trained deep learning algorithm for camera-based respiration monitoring systems. The algorithm uses a model based discriminator to find salient areas having respiration like periodic motion. It stores the first principle component of the found waveforms into two slowly growing set along with negative, uncorrelated motion patterns. Using these samples, it trains a deep neural network classifier incrementally to recognize respiration from sudden and motion intensive situations. The classifier had no forgetting mechanism and it is able to adapt quickly the changing respiration patterns and conditions. The algorithm has been validated in a total of 24 hours diverse recording captured in the neonatal intensive care unit (NICU) of the I st Dept. of Pediatrics and, II. Dept. of Obstetrics and Gynecology, Semmelweis University, Budapest, Hungary and in the COHFACE publicly available dataset of adult subjects. The clinical data set evaluation resulted in mean absolute error (MAE) 6.9 and root mean squared error (RMSE) of 9.8 breaths per minute, respectively, the MAE was below 5 breaths per minute for over 50% of the time. The algorithm was assessed in the COHFACE dataset of adult subjects as well with respiration estimation MAE and RMSE values of 0.95 and 1.7 breaths per minute.","['Monitoring', 'Neural networks', 'Histograms', 'Pediatrics', 'Sensors', 'Estimation', 'Training']","['Respiration monitoring', 'NICU', 'incubator', 'real-time', 'deep learning', 'incremental learning']"
"In the dairy industry, the spreading of phages of Lactococcus lactis (LL) prevents the proper lactic fermentation, causing waste of contaminated products and economic losses. This work presents a cheap biosensing method for rapidly detecting the LL phages. The detection is based on live LL bacteria covering the sensor electrodes, whose electrical response is measured by electrochemical impedance spectroscopy (EIS). Solutions contaminated by phages induce bacteria lysis, clearly reducing the bacteria coverage over the electrodes and leading to evident parametrical shifts in the charge transfer resistance and in the impedance phase at 400 Hz. Experimental tests with laboratory contaminated samples confirm the better detection capability of screen-printed gold sensors compared to the interdigitated gold electrodes. Two measurement protocols, called spill-out and drop-in methods, are evaluated to optimize the sensor detection capability and time. The EIS results are compared with optical absorbance measurements at 600 nm, in order to validate the proposed detection method with107-PFU/mL phages and with a detection time of about 5 h. Finally, the proposed method is tested successfully with milk-based solutions. Evident shifts between phage-contaminated and non-contaminated sensors are measured in the charge transfer resistance of more than one order of magnitude with impedance phase differences of 43°.","['Electrodes', 'Bacteriophages', 'Microorganisms', 'Sensors', 'Pollution measurement', 'Biosensors', 'Charge transfer']","['Electrochemical biosensor', 'electrochemical impedance spectroscopy (EIS)', 'impedance phase', 'Lactococcus lactis (LL)', 'phage', 'resistance to charge transfer']"
"In this study, we successfully demonstrated a700 μmthick shear-force sensor with60×60high resolution array by introducing a combination of an originally developed pressure-sensing elastomer and a newly proposed taxel (tactile pixel) structure with four point-symmetric electrodes. When shear forces are applied to the sensor substrate in several directions by lifting a weight, four sets of resistances between the electrodes are read through low-temperature polycrystalline-silicon (LTPS) thin-film transistors (TFTs) in each taxel and clear outputs corresponding to that direction can be confirmed. Furthermore, as a result of evaluating the object recognition performance of the tactile sensor by using jigs with engraved small dimples, it was confirmed that the recognized minimum size of the shape was 1.5 mm, which suggests that the spatial resolution of this sensor is superior to the human hand perception ability. Because this sensor is ultra-thin, not bulky, and potentially applicable to flexible substrates, it is quite promising as a sensor that can be mounted on the fingertip of a robot hand with multiple functions.","['Sensors', 'Thin film transistors', 'Robot sensing systems', 'Electrodes', 'Substrates', 'Sensor phenomena and characterization', 'Tactile sensors']","['Tactile sensor', 'pressure sensor', 'shear-force sensor', 'thin-film transistors', 'pressure sensing elastomer']"
"Single-coil magnetic induction tomography (MIT) has thus far relied upon traditional bridge techniques to measure inductive losses, falling well under ~1 ohm. These measurements have been plagued by both noise and especially drift. This work considers methods based upon the Texas Instruments LDC-1101 chip, which measures LC circuit admittance while in resonance, from which we compute loss. Inductive loss is measured in a 4.0 cm diameter circular loop coil and compared with a quantitative theoretical result while the coil is axially positioned above a 2% aqueous potassium chloride solution contained in a 14 cm diameter petri dish filled to a depth of 2.0 cm. Results accurately capture the tail behavior of inductive loss as a function of coil-target distance. Then, using IR camera technology to track coil position, several `manual' scans are performed over phantoms prepared from sodium chloride-doped agarose components. In particular, this work considers the ability of single coil scans to capture corners of square plugs, gaps between plugs and side-by-side plugs that differ in conductivity. With drift and noise greatly reduced, the role of sample size is tested, showing that going beyond about 350 samples produces little further benefit to image quality. Though coil position is tracked to within ±0.25 mm, the random nature of manual positioning suggests that a more deliberate positioning scheme is needed, e.g. robotically.","['Loss measurement', 'Conductivity', 'Frequency measurement', 'Semiconductor device measurement', 'Eddy currents', 'Phantoms', 'Sensors']","['Electrical conductivity distribution', 'inductive loss', 'magnetic induction tomography', 'scanning single coil MIT', 'Texas Instruments LDC-1101']"
"Photonic crystals (PhCs) are periodically structured dielectric materials that have attracted significant research interest in the last two decades for their ability to slow down the group velocity of a propagating pulse envelope with promising sensing applications. This review article discusses the properties of 2-D PhCs including the slow-light phenomenon, bandgap generation, and application of these properties for gas and liquid sensing. Waveguide generation by introducing defects with light guiding and confinement is discussed. In addition, for 2-D PhC line waveguides, a comprehensive review on the slow-light principle and phenomenon of slow-light enhanced sensing for gases and liquids is discussed. 1-D and 3-D PhCs are also reviewed for bandgap generation and defects in PhCs along with present fabrication challenges and future trends. Our study highlights an increase in the detection capabilities of PhC-based sensors paving way for high-sensitivity detectors with applications in ubiquitous monitoring of gases and liquids.","['Sensors', 'Photonic band gap', 'Scattering', 'Lattices', 'Dielectric constant', 'Three-dimensional displays', 'Slabs']","['Gas sensing', 'liquid sensing', 'photonic bandgap (PBG)', 'photonic crystals (PhCs)', 'slow-light']"
"Driving behavior is an important aspect of maintaining and sustaining safe transport on the roads. It also directly affects fuel consumption, traffic flow, public health, and air pollution along with psychology and personal mental health. For advanced driver assistance systems (ADASs) and autonomous vehicles, predicting driver behavior helps to facilitate interaction between ADAS and the human driver. Consequently, driver behavior prediction has emerged as an important research topic and has been investigated largely during the past few years. Often, the investigations are based on simulators and controlled environments. Driving behavior can be inferred using control actions, visual monitoring, and inertial measurement unit (IMU) data. This study leverages the IMU data recorded using a smartphone placed inside the vehicle. The dataset contains the accelerometer and gyroscope data recorded from the real traffic environment. Extensive experiments are performed regarding the use of a different set of features, the combination of original and derived features, and binary versus multiclass classification problems; a total of six scenarios are considered. Results reveal that “timestamp” is the most important feature and using it with accelerometer and gyroscope features can lead to a 100% accuracy for driver behavior prediction. Without using the “timestamp” feature, the number of wrong predictions for “slow” and “normal” classes is high due to the feature space overlap. Although derived features can help elevate the performance of the models, the models show inferior performance to that of using the “timestamp” feature. Deep learning models tend to show poor performance than machine learning models where random forest and extreme gradient boosting machines show a 100% accuracy for multiclass classification.","['Behavioral sciences', 'Vehicles', 'Hidden Markov models', 'Sensors', 'Accelerometers', 'Gyroscopes', 'Trajectory']","['Autonomous vehicles (AVs)', 'driver behavior', 'feature engineering', 'inertial measurement unit (IMU)', 'machine learning (ML)']"
"The recognition of gait pattern variation is of high importance to various industrial and commercial applications, including security, sport, virtual reality, gaming, robotics, medical rehabilitation, mental illness diagnosis, space exploration, and others. The purpose of this paper is to study the nature of gait variability in more detail, by identifying gait intervals responsible for gait pattern variations in individuals, as well as between individuals, using cognitive demanding tasks. This work uses deep learning methods for sensor fusion of 116 plastic optical fiber (POF) distributed sensors for gait recognition. The floor sensor system captures spatiotemporal samples due to varying ground reaction force (GRF) in multiples of up to 4 uninterrupted steps on a continuous 2×1 m area. We demonstrate classifications of gait signatures, achieving up to 100% F1-score with Convolutional Neural Networks (CNN), in the context of gait recognition of 21 subjects, with imposters and clients. Classifications under cognitive load, induced by 4 different dual tasks, manifested lower F1-scores. Layer-Wise Relevance Propagation (LRP) methods are employed to decompose a trained neural network prediction to relevant standard events in the gait cycle, by generating a “heat map” over the input used for classification. This allows valuable insight into which parts of the gait spatiotemporal signal have the heaviest influence on the gait classification and consequently, which gait events, such as heel strike or toe-off, are mostly affected by cognitive load.","['Sensors', 'Legged locomotion', 'Spatiotemporal phenomena', 'Sensor systems', 'Floors', 'Task analysis', 'Optical fibers']","['Deep convolutional neural networks (CNN)', 'cognitive load', 'ground reaction force (GRF)', 'sensors fusion', 'interpretable neural networks']"
"This paper describes a novel, distributed sensor network with parallel processing capability based on the instruction systolic array (ISA). A new computing paradigm is introduced where spatially distributed sensors are closely coupled to the processing elements, and the whole array forms a parallel computer. This may find applications in wearable devices for sensing the position and other metrics of a human body and rapidly processing that data. A new programming model to implement the distributed computer on fabrics is described. The fabric-based distributed computing concept has been validated using a number of parallel applications, including a real-time shape sensing and reconstruction application. The exemplar wearable system based on the ISA concept has been realized using off-the-shelf microcontrollers and sensors. Results show that the application executes on the prototype ISA implementation in real time, thus confirming the viability of the proposed architecture for fabric-resident computing devices.","['Sensor arrays', 'Process control', 'Sensor systems', 'Wearable computers']","['Instruction systolic array', 'sensor system networks', 'distributed sensor networks', 'sensor array', 'system on fabrics', 'shape reconstruction', 'wearable sensing devices', 'wearable system', 'fabric-resident computing device']"
"We explore the potentiality of high frequency impedance measurements with CMOS nano-electrode (NE) arrays for nano-plastic pollutant particles monitoring in water. This technology offers benefits as nano-scale resolution, high parallelization, scalability, label-free single particle detection, and automatic measurements without operator intervention. Simple models are proposed for size and concentration estimation. The former integrates measurements of adjacent electrodes and shows uncertainty comparable to the nominal one with mean prediction error lower than 45% down to 50 nm radius. The latter accounts for noise in the definition of the sensing volume. We report a worst case concentration error lower than a factor 1.7 under stationary and continuous flow, which demonstrates the potential of this technology for automated measurements.","['Sensors', 'Nanobioscience', 'Pollution measurement', 'Water pollution', 'Particle measurements', 'Electrodes', 'Capacitance']","['CMOS nanoelectrode arrays', 'environmental sensor', 'high-frequency impedance measurements', 'label-free pollutant detection', 'microplastic', 'nanoplastic', 'single particle detection', 'sub-micrometer particle sizing', 'water monitoring']"
"Detecting when a person leaves a room, or a house is essential to create a safe living environment for people suffering from dementia or other mental disorders. The approaches based on wearable devices, e.g. GPS bracelets may detect such events require periodic maintenance to recharge or replace batteries, and therefore may not be suitable for certain types of users. On the other hand, camera-based systems require illumination and raise potential privacy concerns. In this paper, we propose a device-free walking direction detection approach based on RF-sensing, which does not require a person to wear any equipment. The proposed approach monitors the signal strength fluctuations caused by the human body on ambient wireless links and analyses its spatial patterns using a convolutional neural network to identify the walking direction. The approach has been evaluated experimentally to achieve up to 98% classification accuracy depending on the environment.","['Sensors', 'Wireless sensor networks', 'Wireless communication', 'Convolutional neural networks', 'Convolution', 'Feature extraction', 'Transceivers']","['Activity recognition', 'machine learning', 'Internet of Things', 'RF-sensing', '802.15.4']"
"This paper presents a shrinkage-based particle filter method for tracking a mobile user in wireless networks. The proposed method estimates the shadowing noise covariance matrix using the shrinkage technique. The particle filter is designed with the estimated covariance matrix to improve the tracking performance. The shrinkage-based particle filter can be applied in a number of applications for navigation, tracking, and localization when the available sensor measurements are correlated and sparse. The performance of the shrinkage-based particle filter is compared with the posterior Cramer-Rao lower bound, which is also derived in this paper. The advantages of the proposed shrinkage-based particle filter approach are demonstrated via simulation and experimental results.","['Sensors', 'Mobile communication', 'Time measurement', 'Covariance matrices', 'Modeling', 'Global Positioning System', 'Noise measurement']","['Wireless sensor networks', 'tracking problems', 'received signal strength measurements', 'particle filter', 'covariance matrix', 'shrinkage estimation']"
"The classical form of a flexural ultrasonic transducer is a piezoelectric ceramic disc bonded to a circular metallic membrane. This ceramic induces vibration modes of the membrane for the generation and detection of ultrasound. The transducer has been popular for proximity sensing and metrology, particularly for industrial applications at ambient pressures around 1 bar. The classical flexural ultrasonic transducer is not designed for operation at elevated pressures, such as those associated with natural gas transportation or petrochemical processes. It is reliant on a rear seal which forms an internal air cavity, making the transducer susceptible to deformation through pressure imbalance. The application potential of the classical transducer is therefore severely limited. In this study, a venting strategy which balances the pressure between the internal transducer structure and the external environment is studied through experimental methods including electrical impedance analysis and pitch-catch ultrasound measurement. The vented transducer is compared with a commercial equivalent in air towards 90 bar. Venting is shown to be viable for a new generation of low cost and robust industrial ultrasonic transducers, suitable for operation at high environmental pressure levels.","['Transducers', 'Ultrasonic transducers', 'Ultrasonic variables measurement', 'Ultrasonic imaging', 'Pressure measurement', 'Fixtures', 'Bars']","['Flexural ultrasonic transducer', 'elevated Pressure', 'air-coupled ultrasound', 'unimorph']"
"Brain–computer interfaces (BCIs) are an integration of hardware and software communication systems that allow a direct communication path between the human brain and external devices. Among the existing BCI paradigms, steady-state visually evoked potentials (SSVEPs) have gained momentum in the development of noninvasive BCI applications as they are characterized by adequate signal-to-noise ratio (SNR) and information transfer rate (ITR). In recent years, the adoption of augmented reality (AR) head-mounted displays (HMDs) to render the flickering stimuli necessary for SSVEPs elicitation has become an attractive alternative to traditional computer screens (CSs). In fact, the increase in system wearability anticipates the possibility of adopting BCIs in contexts other than research laboratory. This has contributed to a steadily-increasing interest in BCIs, as also confirmed by the recent literature dedicated to the topic. In this evolving scenario, this review intends to provide a comprehensive picture of the current state-of-the-art in relation to the latest advancement of wearable BCIs based on SSVEPs classification and AR technology. The goal is to provide the reader with a systematic comparison of different technological solutions realized over the last years, thus making future research in this direction more efficient.","['Electroencephalography', 'Brain-computer interfaces', 'Biomedical monitoring', 'Augmented reality', 'Steady-state', 'Layout', 'Visualization']","['Augmented reality (AR)', 'brain–computer interface (BCI)', 'electroencephalography (EEG)', 'Health 40', 'Industry 40', 'instrumentation', 'measurement', 'monitoring', 'steady-state visually evoked potential (SSVEP)', 'real-time systems', 'wearable systems']"
"In the virtual validation of automated driving, trustworthy simulation models of perception sensors are required. Radar sensors are particularly hard to model, as their measurements are notoriously difficult to interpret. This is due to their complex measurement principle, involving multipath propagation of mm-waves, varying backscattering characteristics of objects, and further factors such as limited measurement ranges and resolutions that introduce uncertainty to the measurements. This work presents a method for studying the backscatter characteristics of vehicles under real-world driving conditions. A slalom-like driving scenario, which is representative of road driving where the vehicle is visible under different aspect angles, has been designed. It aims at a high level of reproducibility of the trajectories driven by the vehicles, hence reducing additional sources of uncertainty that were otherwise present in the measurements. In a large-scale measurement campaign, 13 vehicles have been studied. The vehicles under test are observed by multiple radars, mounted at different heights, and carry reference sensors for obtaining their positions. In this article, we present the measurement campaign and show major findings from our measurement results. Our focus lies on drawing conclusions for trustworthy sensor simulation. Both sensor measurement data and MATLAB code for data analysis are made publicly available alongside this article.","['Radar cross-sections', 'Radar', 'Sensor phenomena and characterization', 'Spaceborne radar', 'Automotive engineering', 'Backscatter', 'Mathematical models']","['Millimeter-wave sensors', 'sensor model analysis', 'sensor testing and evaluation']"
"Distributed acoustic sensing (DAS) system with point-backscattering-enhanced fiber (PBSEF) has the capability of breaking the signal-to-noise ratio limit of traditional DAS; therefore, it would become one of the most powerful DAS technologies and expand the applications of DAS systems. To optimize the performance when designing DAS system with PBSEF, it is crucial to understand the relationship between the system noise and the reflectance enhancement, which is yet to be systematically studied. In this article, both experimental investigation and theoretical analysis of a typical3×3heterodyne DAS system with PBSEF were carried out. The relationship between system noise level and the reflectance enhancement was studied by both experiments and simulations, and the results, with good agreements between each other, show that the system noise level drops exponentially as the reflectance increases. With 29.7-dB reflectance enhancement, the system noise level decreased from 9.5 to 0.44pε/√Hz. The influences of intensity noise and phase noise were also analyzed based on the theoretical model. The findings of the quantitative relationship and the noise influence can be used as a universal guidance for the design of PBSEF-based high performance DAS systems.","['Sensors', 'Reflectivity', 'Optical fiber sensors', 'Optical fiber couplers', 'Noise level', 'Optical fiber amplifiers', 'Scattering']","['Distributed acoustic sensing (DAS)', 'fiber Bragg gratings (FBGs)', 'optical backscattering', 'optical fiber sensors']"
"This article presents a compact blood flow measurement system exploiting the Doppler effect and photocurrent sensing technique for wearable health monitoring. Specifically, it helps the patients suffering from peripheral artery disease (PAD) to monitor the blood flow continuously without involving a physician or medical expert. The system prototype integrates a laser diode, a photodiode, and an analog front-end (AFE) on a 54 mm \times64mm three-layer PCB using discrete components for proof of concept. The front-end is designed with two output modes providing simultaneous current-to-voltage and current-to-frequency conversion without requiring an external clock source. Moreover, this system does not require optical filters or fibers, dynamic artifacts are therefore significantly reduced. Drawing 33 mA current from a 3.3-V supply, the developed system offers a transimpedance gain of 151.4 dB \Omegafor the voltage mode output. For the frequency output, a nonlinearity of 0.05% is achieved.","['Blood flow', 'Blood', 'Doppler effect', 'Arteries', 'Photodiodes', 'Current measurement', 'Frequency measurement']","['Blood flowmetry', 'current sensing', 'peripheral artery disease (PAD)', 'Doppler effect', 'wearable sensors']"
"Liquid metals are metals in the liquid state at room temperature; they mostly belong to the alloys of Ga and In. These alloys have attracted attention because of their capacity to conduct electricity and heat, coupled with unique mechanical properties, which allow them to undergo extreme deformations, and self-heal. Their many possible applications range from microfluidics to soft robotics. They are particularly interesting for the fabrication of wearable sensing devices. We design and develop a liquid-metal-based hybrid sensor to study the qualitative and semiquantitative aspects of respiratory mechanics, monitoring the expansion and contraction of chest movements, exploiting the coupling of eutectic Ga–In alloy (EGaIn) and polydimethylsiloxane (PDMS), which show optimal characteristics to build a sensor for this application. The liquid metal has a suitable ohmic response and PDMS, due to its hyperelastic behavior and low viscosity, can be deformed easily and can transmit, without distortion, the primary frequency components useful to characterize different respiratory patterns, from slow and deep breathing to fast and shallow breathing. The time course and spectrum of the electric resistance signal from the sensor correspond closely to a standard reference obtained simultaneously by measuring chest deformation via optoelectronic stereophotogrammetry. The fabrication process is easy, cheap, and robust and can be industrialized.","['Sensors', 'Liquids', 'Biomechanics', 'Sensor phenomena and characterization', 'Monitoring', 'Strain', 'Fabrication']","['Biomechanical design', 'breathing monitoring', 'liquid metal', 'mechanical characterization', 'optoelectronic system', 'wearable sensor']"
"We present a readout scheme for CMOS image sensors that can be used to achieve arbitrarily high dynamic range (HDR) in principle. The linear full well capacity (LFWC) in high signal regions was extended 50 times from 20 to 984 ke ^{-}via an interlaced row-wise readout order, while the noise floor remained unchanged in low signal regions, resulting in a 34-dB increase in DR. The peak signal-to-noise ratio (PSNR) is increased in a continuous fashion from 43 to 60 dB. This was achieved by summing user-selected rows that were read out multiple times. Centroiding uncertainties were lowered when template-fitting a projected pattern, compared to the standard readout scheme. Example applications are aimed at scientific imaging due to the linearity and PSNR increase.","['Dark current', 'CMOS image sensors', 'Floors', 'Standards', 'Timing', 'Sensor phenomena and characterization', 'Photodiodes']","['CMOS', 'dynamic range (DR)', 'image sensor', 'local integration time', 'random addressing', 'row-wise']"
"Parkinson’s disease (PD) is characterized by dopaminergic cell loss and the formation of Lewy bodies, of which the main component is aggregated and fibrillizedα-synuclein (aSyn). Recent studies suggested that ultratrace amounts of aSyn aggregates are also present in biofluid specimens, and they can serve as a biomarker for PD. Because aSyn has been shown to possess a prion-like property, we attempted to enhance the sensitivity and specificity of a cantilever microsensor to detect aSyn aggregates by exploiting the properties of self-templating assembly and lipid interaction on the surface of liposome-immobilized cantilever sensor. We found that the liposome-immobilized cantilever sensor was able to successfully detect aSyn fibrils at a very low concentration (100 pg/mL), and the addition of aSyn monomers, which were converted into fibrils in the presence of aSyn aggregates and further acted as a template for fibrillization, lowered the detection limit to 10 pg/mL. The sensitivity of this cantilever sensor was comparable to or slightly superior to that of enzyme-linked immunosorbent assay (ELISA). Moreover, the lag time for the detection of aSyn fibrils has been significantly reduced to 100–120 min, compared to the tens of hours needed in conventional ELISA, real-time quaking-induced conversion (RT-QuIC), and protein misfolding cyclic amplification (PMCA) assays. Finally, preliminary measurements of aSyn aggregates showed the possibilities of discriminating serum from PD and non-PD patients. The liposome-immobilized cantilever sensor could serve as a promising tool for the early or preclinical diagnosis of PD.","['Sensors', 'Aggregates', 'Proteins', 'Sensor phenomena and characterization', 'Temperature sensors', 'Surface stress', 'Diseases']","['α-synuclein (aSyn)', 'cantilever sensor', 'liposome', 'Parkinson’s disease (PD)', 'prion-lipid interaction', 'self-templating assembly']"
"As a unique natural satellite of the Earth, the Moon has always attracted much attention, and one subject of interest is the establishment of an Earth observation platform on the Moon. Compared with the existing spaceborne and airborne observation platforms, a Moon-based Earth observation platform would have the unique advantage of the ability to obtain global and large-scale observation data. At present, the study on Moon-based Earth observations is in theory, so it is necessary to use simulation methods to understand the observation performance of a Moon-based Earth observation platform. In this study, a method for Moon-based Earth observation imaging simulation in the thermal infrared band is developed to study the Moon-based thermal infrared imaging process. The method comprises three parts, including the estimation of Moon-based imaging coverage, the acquisition of the radiation intensity at the entrance pupil, and the simulated image output from the Moon-based thermal infrared sensor. Then, the simulated results are validated with the existing spaceborne observation data. Results show that the absolute error of Moon-based thermal infrared simulations is between 1.3-5.7 K, the RMSE is between 1.38 - 3.12 K, and the relative error is between 0.44% - 2.12%, indicating that the thermal infrared imaging model can more realistically simulate the true conditions of ground surface and that a Moon-based Earth observation platform could observe the Earth in the thermal infrared band.","['Earth', 'Atmospheric modeling', 'Imaging', 'Orbits', 'Planetary orbits', 'Sensor phenomena and characterization', 'Spatial resolution']","['Moon-based Earth observation', 'thermal infrared imaging simulation', 'radiance', 'land surface temperature', 'thermal infrared']"
"This article introduces a novel stereo-camera system for the measurement of solar-induced chlorophyll fluorescence (SIF) at 760 nm. The instrument uses optical interference filters to gain high background radiation-suppression in the telluric oxygen absorption band at 760 nm, to measure the weak SIF signal. Featuring spatially high-resolution images and a lightweight setup, the system was built for ground- and drone-based field applications. The technical setup of the device and the used methodology are presented as well as a theoretical performance simulation, indicating a maximal reduction of the background radiation by a factor of five. The experimental results show that steady-state fluorescence can be measured with signal-to-noise ratios (SNRs) between 5 and 10, depending on the saturation level of the sensor and the aperture settings of the lens. Intensity changes lower than 0.2 mWm ^{-{2}}sr ^{-{1}}nm ^{-{1}}, emitted by a calibrated light emitting diode (LED) reference panel, can be reliably distinguished under direct sun illumination. The system’s capability to detect fast changes in photosynthetic dynamics, with both high spatial and temporal resolution, is demonstrated by a video sequence of a leaf during a dark-light transition. In a static, platform-based operation, the classification of fluorescent and nonfluorescent surfaces under natural conditions is presented.","['Optical filters', 'Fluorescence', 'Cameras', 'Imaging', 'Vegetation mapping', 'Absorption', 'Sun']","['Camera-based solar-induced chlorophyll fluorescence (SIF) measurement', 'photosynthetic dynamics', 'remote sensing of vegetation', 'SIF']"
"One of the critical advantages of a high-altitude airship (HAA) is its ability to carry multiple-task payloads for remote sensing monitoring on the plateau. We developed a multi-payload observation system based on the HAA (ASQ-MPHAAS) for remote sensing monitoring of the Qinghai plateau. In this paper, we demonstrate the design, development, implementation of the ASQ-MPHAAS. The ASQ-MPHAAS includes two parts; one part is our HAA (ASQ-HAA380), and the other part is our multi-payload observation system (ASQ-MPOS-1). The ASQ-MPOS-1 consists of two hyperspectral push-broom imaging sensors, a multispectral imaging sensor, a high-resolution RGB camera, a video sensor, a Position and Orientation System (POS), and other components. The ASQ-MPOS-1 is installed under the ASQ-HAA380. We also introduce some key methods of data processing we propose to solve unique problems brought by HAA. Four real-world field experiments are presented in this paper to demonstrate that the ASQ-MPHAAS can capture different kinds of remote sensing data at a high spatial resolution of a few or dozen centimeters.","['Payloads', 'Cameras', 'Synchronization', 'Hyperspectral sensors', 'Global Positioning System', 'Sensors']","['Multi-payload observation', 'high altitude airship time synchronization', 'hyperspectral imaging', 'multispectral imaging']"
"Underwater pipeline inspection is an important topic in off-shore subsea operations. Remotely operated vehicles (ROVs) can play an important role in multiple application areas including military, ocean science, aquaculture, shipping, and energy. However, using ROVs for inspection is not cost-effective, and the fixed leak detection sensors mounted along the pipeline have limited precision. Although the cost can be significantly reduced by applying autonomous underwater vehicles (AUVs), the unstable current, low visibility, and loss of GPS signal make the navigation of AUVs underwater very challenging. Previous studies have been conducted on coordinate-based, vision-based, and fusion-based navigation algorithms. However, the coordinate-based algorithms suffered from the denial of GPS signals while the vision-based methods typically relied on terrain and landscape knowledge that required collection prior to the mission. As a result of these issues, a navigation system for an AUV that incorporates vision and sonar sensors is presented in this article. In a robot operating system (ROS)/Gazebo-based simulation environment, the AUV had the ability to find and navigate toward the pipeline and continuously traverse along its length. Additionally, with a chemical concentration sensor mounted on the AUV, the system demonstrated the capability of inspecting the pipeline and reporting the leak point with a resolution of 3 m along the pipeline.","['Pipelines', 'Sensors', 'Sonar navigation', 'Inspection', 'Navigation', 'Sonar', 'Cameras']","['Autonomous underwater vehicle (AUV)', 'fuzzy controller', 'pipeline inspection', 'robot operating system (ROS)', 'sensor fusion', 'simulation']"
"Accurately measuring a person’s level of stress can have a wide variety of impacts, not only on human health, but also on the perceived feeling of safety when going after daily habits, such as walking, cycling, or driving from one place to another. While there is a vast amount of research done on stress and the related physiological responses of the human body, there is no go-to method when it comes to measuring acute stress in a live setting. This work proposes an advancement of the rule-based stress detection algorithm proposed by Kyriakou et al., to identify moments of stress (MOS) more reliably, through an adaptation, and individualization of the rules proposed in the original paper. The proposed algorithm leverages electrodermal activity (EDA) and skin temperature (ST), both recorded by the Empatica E4 wristband, for the assessment of an individual’s stress when exposed to an audible stimulus. The algorithm achieves an average recall of 81.31%, with a precision of 46.23%, and an accuracy of 92.74%, measured on 16 test subjects. The tradeoff between precision and recall can be controlled by adjusting the MOS threshold that needs to be reached for an MOS to be detected.","['Stress', 'Skin', 'Temperature measurement', 'Human factors', 'Stress measurement', 'Temperature sensors', 'Recording']","['Electrodermal activity (EDA)', 'individual-specific physiological responses', 'rule-based algorithm', 'skin temperature (ST)', 'stress detection']"
"Crosstalk is a well-known problem in resistive sensor arrays (RSAs). The zero-potential method (ZPM) is a commonly used readout circuit that uses multiplexers and demultiplexers to reduce the main sources of crosstalk. However, the internal resistors of these switches cause a secondary crosstalk in the RSA that alters the RSA sensor values read. The solution to the effects of this secondary crosstalk is still a case of study. In previous literature, these resistances have been considered as known and, in most cases, equal for all switch channels. However, in a real situation, they are unknown and may vary between channels. In this work, a least-squares (LSQR) method is presented to obtain the true values of the switches from the read voltage signals. Calibration columns are added to the RSA for this purpose. To prove the performance of the method proposed, several simulations have been carried out with different values of RSA sensors, switch resistances, size of the RSA, and noise level. Results show that the proposed LSQR method allows obtaining simultaneously accurate values for both the RSA sensors and switch internal resistors. In this way, the problem of secondary crosstalk is neutralized. It also shows better performance when compared with existing approaches.","['Sensors', 'Crosstalk', 'Resistors', 'Mathematical models', 'Voltage', 'Multiplexing', 'Switches']","['Circuit analysis', 'crosstalk', 'numerical methods', 'resistive sensor array (RSA)']"
"A novel highly stretchable gas sensor is reported that is based on popup antenna reconfiguration due to the strain induced by the swelling of a polydimethylsiloxane (PDMS) substrate when exposed to diethyl ether. When the swollen substrate is removed from the volatile solvent environment, the PDMS volume increase is reversed leading to compressive stress in an attached antenna transforming a 2D structure to a 3D structure through mechanically induced shaping. This provides a low cost and simple route to tune the antenna resonant frequency and gain in direct response to a chemical stimulus. Our proposed solvent sensor is able to measure 0 to 60% PDMS swelling corresponding to diethyl ether concentrations up to 1620 ppm via a resonant frequency shift from 4 to 2.4 GHz. A fatigue life study indicated 10 3.5 life cycles which demonstrates the durability of these sensors to accommodate large strain and repeatability of the sensing process. Multiphysics Finite Element Method (FEM) modelling of the mechanical and RF simulations along with analytical results based on an equivalent circuit model were in good agreement with experimental data and demonstrate the potential of these structures as sensors.","['Integrated circuit modeling', 'Dipole antennas', 'Strain', 'Sensors', 'Substrates', 'Ribs']","['Compressive stress', 'popup antenna', 'diethyl ether', 'vapor', 'PDMS', 'sensor']"
"The calibration of multisensor systems can cause significant costs in terms of time and resources, in particular when cross-sensitivities to parasitic influences are to be compensated. Successful calibration ensures the trustworthy subsequent operation of a sensor system, guaranteeing that one or several measurands of interest can be inferred from its output signals with specified uncertainty. As shown in the present study, this goal can be reached by reduced calibration procedures with fewer calibration conditions than parameters that are needed to model the device response. This is achieved using Bayesian inference by combining the calibration data of a sensor system with statistical prior information about the ensemble to which it belongs. Optimal reduced sets of calibration conditions are identified by the method of Bayesian experimental design. The method is demonstrated on a Hall–temperature sensor system whose nonlinear response model requires seven parameters in the temperature range between−30and150∘Cand for magnetic field valuesBbetween −25 and 25 mT. For the prior, a multivariate normal distribution of the model parameters is acquired using 14 specimens of the sensor ensemble. I-optimal calibration at one, two, and three temperatures reduces the root-mean-square (rms) standard deviation ofBinferred from sensor output signals from203μTbefore calibration down to 78, 41, and34μT. Similar conclusions apply to G-optimal calibration. This article describes how to implement the Bayesian prior acquisition, inference, and experimental design. The proposed approach can help save resources and cut costs in sensor calibration.","['Sensors', 'Calibration', 'Bayes methods', 'Sensor systems', 'Temperature sensors', 'Measurement uncertainty', 'Magnetic sensors']","['Bayesian inference', 'calibration', 'compensation', 'experimental design', 'sensor system']"
"For the first time, Bayesian sensor calibration is used to identify efficient calibration procedures for a sensor cross-sensitive to two parasitic influences. The object under study is a thermomechanically cross-sensitive sensor system for determining the magnetic induction ${B}$ . The packaged system comprises a Hall sensor, a stress sensor, and a temperature sensor. The three sensor signals are combined in a polynomial sensor response model with 11 parameters to determine ${B}$ compensated for offset and cross-sensitivities. For the calibration, sensors are exposed to mechanical stress values between 0 and −68 MPa, temperatures between −40 and $100 ^{\circ} \text{C}$ , and ${B}$ values between −25 and 25 mT. A sample of 35 sensors serves to extract the prior model parameter distribution of their fabrication run. The Bayesian experimental design is applied to identify sets of 2–8 optimal calibration conditions under I-optimality and G-optimality. The Bayesian inference then allows to obtain the posterior model parameter distribution of any uncalibrated sensor from the same run. Any such sensor is thereby turned into a ${B}$ measuring device with individually quantified accuracy. The method was successfully applied to 15 validation sensors. In the case of I-optimality, the median root-mean-square (rms) textsigma values of the ±1 σ confidence intervals for the extracted ${B}$ values were found to be 113–71 $\mu \text{T}$ after near-I-optimal calibrations based on 2–8 measurements. Over the entire range of temperature and mechanical stress and for applied $| {B} | \leq $ 25 mT, corresponding experimentally determined medians of the rms deviations between predicted and applied ${B}$ values were found to be 89–71 $\mu \text{T}$ . Analogous observations apply to G-optimality. In short, Bayesian calibration made it possible to obtain functional ${B}$ sensors of known accuracy with significantly fewer calibration measurements than model parameters. This was enabled by prior knowledge collected by the thorough characterization of 35 prior-generating specimens.","['Sensors', 'Temperature sensors', 'Calibration', 'Mechanical sensors', 'Sensor systems', 'Sensor phenomena and characterization', 'Magnetic sensors']","['Bayesian inference', 'calibration', 'compensation', 'experimental design', 'Hall sensor', 'multiple cross-sensitivities', 'multisensor system']"
"Mode-localization is a promising method to realize high sensitivity sensors, especially in the field of MEMS. Since these sensors monitor amplitude change of weakly coupled resonators, it is important to grasp condition that induces multi-valued amplitude-frequency curve. In this paper, we provide an efficient tool to characterize the nonlinear behavior of the weakly coupled resonators. To analyze the nonlinearity, we solve a two-degrees-of-freedom (2-DoF) coupled equation of motion with nonlinear spring terms. Two approximations are employed to solve the equation; Krylov–Bogoliubov averaging method and approximation based on eigenmode amplitude-ratio at the resonances. As a result, we obtain two decoupled Duffing-like amplitude-frequency equations. We show that nonlinearity of the system is described by factors contained in the equations. The factors can be explicitly written in terms of basic parameters of the system, including coupling spring constant and nonlinear terms. Thus, instead of relying on numerical calculations, we can find parameter condition that brings about multi-valued amplitude-frequency curve. This method can also be utilized to find a condition that eliminates the nonlinearity. As an example, we apply this method to a weakly coupled resonator which uses parallel plate electrode as a coupling spring. We demonstrate the effectiveness and validity of this method by comparing the result with FEM simulations. The methodology and results presented here are general one and can be applied to various systems described by nonlinear coupled resonators.","['Sensors', 'Mathematical models', 'Springs', 'Resonators', 'Sensor phenomena and characterization', 'Sensitivity', 'Couplings']","['Duffing resonator', 'MEMS', 'mode-localization', 'nonlinear', 'weakly coupled resonator']"
"This article presents a noninvasive method of classifying gait patterns associated with various movement disorders and/or neurological conditions, utilizing unobtrusive, instrumented socks and a deep-learning network. Seamless instrumented socks were fabricated using three accelerometer-embedded yarns, positioned at the toe (hallux), above the heel, and on the lateral malleolus. Human trials were conducted on 12 able-bodied participants, an instrumented sock was worn on each foot. Participants were asked to complete seven trials consisting of their typical gait and six different gait types that mimicked the typical movement patterns associated with various movement disorders and neurological conditions. Four neural networks and an SVM were tested to ascertain the most effective method of automatic data classification. The bi-long short-term memory (LSTM) generated the most accurate results and illustrates that the use of three accelerometers per foot increased classification accuracy compared to a single accelerometer per foot by 11.4%. When only a single accelerometer was utilized for classification, the ankle accelerometer generated the most accurate results in comparison to the other two. The network was able to correctly classify five different gait types: stomp (100%), shuffle (66.8%), diplegic (66.6%), hemiplegic (66.6%), and “normal walking” (58.0%). The network was incapable of correctly differentiating foot slap (21.2%) and steppage gait (4.8%). This work demonstrates that instrumented textile socks incorporating three accelerometer yarns were capable of generating sufficient data to allow a neural network to distinguish between specific gait patterns. This may enable clinicians and therapists to remotely classify gait alterations and observe changes in gait during rehabilitation.","['Yarn', 'Sensors', 'Foot', 'Accelerometers', 'Instruments', 'Monitoring', 'Biomedical monitoring']","['Biomedical equipment', 'electronic textiles (E-textiles)', 'gait monitoring', 'long short-term memory (LSTM)', 'machine learning', 'sensors', 'smart textiles', 'wearable sensors']"
"In the study of the electrophysiological activity of cultured cells, the integrity of the electrical insulation layer is crucial in ensuring the reliability of microelectrode array (MEA) sensor measurements. The materials used are required to withstand the challenging culture conditions for up to several months. The commonly used silicon nitride (SiN) has been reported to corrode in cell cultures with very little attention in the scientific community. Herein, we show that plasma-enhanced chemical vapor deposited (PECVD) SiN is subject to substantial wear when in contact with different culture media, with the corrosion rate depending on the deposition temperature and media type. A chemically stable insulator is proposed and demonstrated as an alternative for SiN using thermally annealed atomic layer deposited (ALD) titanium dioxide (TiO 2 ) as a transparent protective layer for SiN. Electrochemical impedance spectroscopy (EIS) was successfully applied to periodically assess the integrity and thickness of the insulation layers in two different media, with profilometer measurements performed as an endpoint analysis to confirm the actual change in thickness in four solutions. ALD TiO 2 not only prevented insulator corrosion but also showed excellent cytocompatibility during a four-week culture with human embryonic stem cell (hESC)-derived neurons. The results here underline the need for addressing the corrosion of SiN insulation in MEA sensors. We believe that the integrity and results of cellular measurements can be compromised, especially when reusing MEAs. We propose the ALD TiO 2 protected insulator as a viable solution for this problem, as it increases reliability, particularly during extended cell cultures.","['Insulators', 'Capacitance', 'Insulation', 'Impedance', 'Corrosion', 'Media', 'Microelectrodes']","['Atomic layer deposition', 'biosensor', 'corrosion', 'cytocompatibility', 'electrochemical impedance spectroscopy', 'MEA', 'microelectrode array', 'silicon nitride', 'titanium dioxide']"
"Active Implantable Medical Devices (AIMDs) have seen a significant increase in popularity in recent years due to their ability to provide continuous therapy unobtrusively. One of the key challenges in developing next generation AIMDs is the design of a safe and reliable Wireless Power Transfer (WPT) system. Standard commercial WPT components are not optimized to deliver small amounts of power or to work with other than small air gaps. We developed a near-field resonant inductive coupling WPT system with closed-loop real-time control, based on the commercial off-the-shelf components and leveraging the already integrated wireless communication system. The system was used to recharge a secondary battery and power supply the Nanochannel Delivery System (nDS). We tested the system’s robustness by varying the coil spacing to a maximum of 20mm, and by misaligning the coils laterally and angularly up to 8mm and 12°, respectively. We performed these tests by simulating battery charging from 2.8 V to 4.1 V and we tested four coils, from 6mm to 19mm in diameter, to evaluate the best trade-off between effectiveness and miniaturization. Finally, we recharged the selected battery for two distances, 2.5mm and 6.5mm. The results show that the 17mm and 19mm coils have the highest peak efficiencies of 31.3% and 32.3% (26% and 26.3% on average), respectively, and provide a complete and reliable battery recharge in less than 5 hours while keeping the receiver coil temperature within 2 °C.",[],[]
"Surface plasmon resonance (SPR) spectra on Al films in the Kretschmann configuration is investigated aiming at the rapid detection of the Al corrosion in NaCl solution. The angle of the dip, occurred in the SPR spectra, was observed to decrease in the first 20 min and then increase with the increase of the immersion time. Calculations based on a simple multilayer model suggest that the observed initial decrease and the subsequent increase in the dip angle correspond to the thinning of the oxide layer and that of Al, respectively. The model also explains the observed deepening of the dip. The decrease in the effective Al thickness is estimated to be about 10 nm after the immersion in 3 wt% NaCl solution for 20 hours, though the quantitative discussion becomes difficult for longer immersion time. The corrosion is surely detectable for the change smaller than 5 nm in the effective Al thickness corresponding to the immersion time of 2 hours. The results indicate that the SPR-based method is promising for detecting the initial stage of Al corrosion in nanoscale.","['Films', 'Corrosion', 'Gold', 'Reflectivity', 'Sensors', 'Substrates']","['Surface plasmon resonance', 'optical sensor', 'aluminum film', 'corrosion']"
"The deployment of transducers to perform in situ inspections of industrial components can be complicated, and in many cases is still performed manually by a team of operators, which involves significant costs and can be dangerous. Robots capable of deploying probes in difficult to access locations are becoming available. Electromagnetic acoustic transducers (EMAT) are well suited to be used with robots since they are non-contact transducers that do not require a coupling medium, and can easily perform scans. However, existing acquisition systems for EMATs are generally not suitable to be directly mounted on robots. In this paper, a new wireless acquisition system for EMATs is presented. The system is standalone, it transmits the inspection data over WiFi, and is compatible with the robotics operating system (ROS). In addition, it is designed to be modular, small and lightweight so that it can be easily mounted on robots. The system design in terms of hardware and software is described in this paper. The resulting performance of the system is also reported.","['Inspection', 'Probes', 'Service robots', 'Wireless fidelity', 'Hardware', 'Oscilloscopes']","['Electromagnetic acoustic transducer', 'non-destructive evaluation', 'wireless', 'acquisition system', 'robotics']"
"In the era of artificial intelligence perceptual algorithms used in state-of-the-art Advanced Driver Assistance Systems (ADAS), algorithm validation is not an easy task. To ensure the highest possible safety level of the solution, the performance of the algorithm must be evaluated under a variety of challenging conditions. To test the algorithms, simulators are used to emulate the virtual environment around the car taking into account road traffic, infrastructure and vehicles dynamics. Sensor models are necessary for virtual testing to provide required data to ADAS algorithms. This article introduces the issue of modeling the color filter spaces that are used in the automotive industry. The images generated by the simulator usually have RGB color. In contrast, the automotive industry uses filters such as RCCC and RYYCy. In this paper, the methods for transforming color space from RGB to RYYCy are discussed. Three novel approaches are introduced to solve this problem: analytical, polynomial, and based on a neural network. Moreover, comparative discussion of the presented solutions is shown and with the set of experiments the conversion accuracy and execution time of each algorithm are compared. In addition, introduced solution were compared with modified models that are presented in the literature.","['Image color analysis', 'Sensors', 'Cameras', 'Mathematical models', 'Information filters', 'Filtering algorithms', 'Testing']","['Cameras', 'virtual validation', 'mathematical modeling']"
"The method of hierarchical clustering is for the first time employed to locate the intrusion-induced disturbance on the sensing fibers of a dual Mach–Zehnder interferometer (DMZI). Such an intrusion-induced disturbance is located by finding the {x}coordinate of the centroid of the largest cluster on the Euclidean plane through hierarchical clustering with an appropriate linkage criterion employed for determining the distance between two observations. We compare average linkage and complete linkage criteria in the clustering analysis to see which one provides better locating accuracy. In the clustering analysis, the number of clusters is set to be 3–8 in finding the location of disturbance. To reduce the locating error, we also use differential signals here in the clustering analysis. Twelve intrusion events are simulated by knocking the sensing fibers to induce disturbances at a given location. The location of disturbance is determined through the clustering analysis for each intrusion event. The mean of the absolute values of locating errors [mean absolute error (MAE)] for the 12 intrusion events is then estimated. The experimental results in this study demonstrate a maximum MAE of 11.55 m in locating an intrusion with average linkage criterion employed for five-cluster analysis. Also, the MAE could be 3.55 m smaller by using the differential signals for clustering analysis, compared with the case when directly detected signals are used for clustering analysis. The results also confirm that the average linkage criterion provides only a small amount of improvement in MAE over complete linkage criterion.","['Optical fiber sensors', 'Optical fiber cables', 'Optical fiber polarization', 'Sensors', 'Optical fiber couplers', 'Optical fiber testing', 'Couplings']","['Average linkage criterion', 'complete linkage criterion', 'dual Mach–Zehnder interferometer (DMZI)', 'fiber-optic intrusion detection', 'hierarchical clustering', 'positioning accuracy']"
"Point cloud compression is an essential task for practical applications using point clouds. Most of the previous approaches rely on octree compression which involves voxelization in the coding itself. Distortions derived from voxelization can be reduced without increasing the bitrate by postprocessing. In this article, we propose a super-resolution method for a decoded voxelized point cloud as a postprocessing step in the geometry compression. The proposed method increases the resolution of the voxelized point cloud by predicting the occupancy of higher resolution voxels than those used to compress the original point cloud. For efficient prediction, we propose a deep neural network for super-resolution based on sparse convolution. It can be highly efficient even for a large point cloud since the network applies convolution only to nonempty space. The proposed method predicts the occupancies represented by continuous values for each point and estimates the binary occupancies through a thresholding procedure. We design a dynamic threshold to ensure that at least one of all voxels is predicted to be occupied in order to prevent the generation of regions with missing points. We also introduce an occupancy prediction method to address the sparsity of high-resolution occupied voxels. Experiments on the outdoor and indoor datasets demonstrate the effectiveness of the proposed method.","['Point cloud compression', 'Superresolution', 'Geometry', 'Image reconstruction', 'Distortion', 'Octrees', 'Encoding']","['Deep super-resolution', 'point cloud compression', 'sparse convolution', 'voxelization']"
"Automatic food portion size estimation (FPSE) with minimal user burden is a challenging task. Most of the existing FPSE methods use fiducial markers and/or virtual models as dimensional references. An alternative approach is to estimate the dimensions of the eating containers prior to estimating the portion size. In this article, we propose a wearable sensor system (the automatic ingestion monitor integrated with a ranging sensor) and a related method for the estimation of dimensions of plates and bowls. The contributions of this study are: 1) the model eliminates the need for fiducial markers; 2) the camera system [automatic ingestion monitor version 2 (AIM-2)] is not restricted in terms of positioning relative to the food item; 3) our model accounts for radial lens distortion caused due to lens aberrations; 4) a ranging sensor directly gives the distance between the sensor and the eating surface; 5) the model is not restricted to circular plates; and 6) the proposed system implements a passive method that can be used for assessment of container dimensions with minimum user interaction. The error rates (mean ± std. dev) for dimension estimation were 2.01% ± 4.10% for plate widths/diameters, 2.75% ± 38.11% for bowl heights, and 4.58% ± 6.78% for bowl diameters.","['Sensors', 'Cameras', 'Image sensors', 'Estimation', 'Lenses', 'Sensor systems', 'Fiducial markers']","['Dietary assessment', 'food imaging', 'food portion', 'food volume', 'portion size estimation', 'wearable sensors', 'wearable technology']"
"Odometry is one of most-used techniques used in mobile robotics and autonomous vehicles, especially when the indoor navigation is required or when robot or vehicle moves inside the tunnel. The output of the odometer is usually a count of pulses corresponding to the distance run by given wheel. Due to the quantization noise, estimation of the velocity (first derivative of the distance) is challenging. This article is focused on curve-fitting filter used for the speed estimation and optimization of its parameters, considering the physical constraints of the robot, sampling frequency of the system and the quantization step. The paper proposes an empirical formula for estimating the optimal parameters of the curve fitting filter. The optimized filter has been evaluated using both simulation and real experiment and compared with several standard differentiation methods.","['Finite impulse response filters', 'Acceleration', 'Sensors', 'Robots', 'Estimation', 'Quantization (signal)', 'IIR filters']","['Odometry', 'velocity estimation', 'quantization noise', 'curve fitting filter', 'tunnel navigation']"
"With the increasing proliferation of embedded sensors in wearable devices, there is potential for modeling individual emotional and mental state variations. The popular measure for the quantification of emotions outlines the affective states of arousal and valences, with high and low being the discrete categories of interest. Recent works explore the discernability of digital behavior differences between groups with and without mental disorders. However, the interaction between physiological states and affective states within a predominantly depressive population remains to be studied with the aid of wearables. Despite the pervasiveness of emotional state inference through the tracking of ubiquitous physiological trackers, such as heart rate, blood volume pulse, skin conductance, and motion, a dearth of work is noted in the exploration of physiological markers in single-modal and multimodal settings. This work provides an extensive evaluation of a convolutional neural network with an attention mechanism ensembled with a random forest algorithm to effectively leverage multiple raw signal-to-image transformations as feature inputs to predict depression severity and affective state. The proposed models are assessed on the Daily Ambulatory Psychological and Physiological recording for Emotion Research (DAPPER) dataset and achieve the sensitivity: specificity scores of 58.75%:45.59%, 62.34%:43.41%, and 49.43%:51.70% for predicting depression, valence, and arousal with a mixture of unimodality and bimodality applying continuous wavelet transforms and short-time Fourier transform to motion and skin-conductance readings, respectively. This work is envisioned as a preliminary study to contribute toward the monitoring of affective states among a depressed population by utilizing low-frequency sensor recordings with the DAPPER dataset.","['Depression', 'Monitoring', 'Recording', 'Heart rate', 'Biomedical monitoring', 'Wearable sensors', 'Wearable computers']","['Affective computing', 'deep learning', 'depression', 'galvanic skin response (GSR)', 'heart rate (HR) sensors', 'motion sensors', 'multimodal', 'unimodal', 'wearable devices']"
"This paper addresses the evaluation of object detection performance for non-contact safety-related sensors in low-visibility environments due to adverse weather conditions. An evaluation metric, MOT (Minimum Object-detectable Transmittance), is proposed to quantitatively evaluate the influence of the low-visibility environments on the object detection performance. Traditionally, the Meteorological Optical Range (MOR) has been used for this purpose. However, MOR is not appropriate for the sensor evaluation because it is a meteorological metric which is optimized for human eyes to estimate the distance at which a person can see clearly through an atmosphere. On the other hand, MOT is a physical and optical measure that quantifies the relationship between the spatial transmittance and the distance at which a sensor can detect a specified object. It is specialized for the sensor evaluation and can express the sensor characteristics in the low-visibility environments at each measurement distance. The usefulness of MOT will be demonstrated by presenting the experimental results of MOT measurements for various sensor devices in a fog space reproduced with environmental simulator equipment.","['Sensors', 'Sensor phenomena and characterization', 'Atmospheric measurements', 'Measurement', 'Optical sensors', 'Robot sensing systems', 'Measurement by laser beam']","['Performance evaluation', 'safety-related sensor', 'object detection', 'low-visibility environment', 'spatial transmittance']"
"We propose a peak detection method for measuring fiber Bragg gratings (FBGs) using convolutional neural network (CNN) to improve the performances of wavelength division multiplexing. In wavelength division multiplexing, each FBG occupies a certain wavelength range; therefore, the number of FBGs that can be installed is limited by the wavelength band of the light source. To address this issue, methods for overlapping multiple FBGs of the same wavelength within a single occupied wavelength range have been studied. This contributes to improving the limit of multipoint FBG’s manifold. However, this method results in the complex overlapping of multiple FBG reflectance spectra, making it difficult to accurately measure the peak wavelengths of individual FBGs using conventional peak detection methods. Therefore, we developed a peak detection process using CNN, which is suitable for identifying unique feature data. Each FBG of the same wavelength was characterized to have a unique spectral shape by assigning a different full-width at half-maximum (FWHM) values to each. We introduced noise-additive learning, a well-known method of data augmentation that increases tolerance to variations in the experimental signal. As a result, the standard deviation for peak wavelength detection significantly improved to 2.8 pm and the strain measurements with three complex overlapping FBGs were successfully demonstrated. The CNN model is the first to solve the problem of three overlapping FBGs for arbitrary wavelength changes. Furthermore, the developed peak detection process was found to be applicable to measurements that combined multiplexing of FBGs of either identical or different wavelengths.","['Fiber gratings', 'Convolutional neural networks', 'Wavelength measurement', 'Optical variables measurement', 'Training data', 'Numerical models', 'Reflectivity']","['Convolutional neural networks (CNNs)', 'deep learning', 'fiber Bragg gratings (FBGs)', 'optical fiber sensors', 'wavelength division multiplexing']"
"Metal chalcogenide indium selenide (In2Se3) is attracting increasing research interest for photodetector applications due to its excellent photoresponse and superior stability under ambient conditions. However, the temperature-dependent performance of In2Se3-based photodetectors has rarely been reported. Here, \gamma-In2Se3 thin films were prepared at various deposition pressures using the RF magnetron sputtering for photodetector applications. The formation of single-phase \gamma-In2Se3 films has been confirmed by the X-ray diffraction (XRD) and Raman analyses. Binding energies and elemental composition of \gamma-In2Se3 films were examined by XPS analysis. Field emission scanning electron microscopy (FE-SEM) images show that the prepared \gamma-In2Se3 films were crack- and pore-free, dense, compact, smooth, and have small grains. The optical energy bandgap decreases from 2.2 to 1.7 eV with an increase in deposition pressure. Then, the photoresponse of \gamma-In2Se3-based photodetectors was investigated. The photodetector fabricated with \gamma-In2Se3 at 5 Pa on an ITO-coated interdigital electrode (IDE) exhibited excellent photoresponsivity ( 2.82~\mu \text{A}/W) and detectivity ( 7.06\times 10^{{7}}Jones) with a fast rise time of 0.26 s and a decay time of 0.32 s. Finally, the temperature-dependent photoresponse of the photodetector fabricated with \gamma-In2Se3 at 5 Pa is meticulously investigated. We found that the photodetector properties of a photodetector critically depend on the operating temperature.","['Photodetectors', 'Films', 'Electrodes', 'Substrates', 'Sputtering', 'Radio frequency', 'Indium tin oxide']","['ᵞ-indium selenide (In₂Se₃)', 'detectivity', 'interdigital electrodes (IDEs)', 'photodetector', 'RF sputtering']"
"Piezoelectric microelectromechanical systems (MEMSs) meet the growing demand for sensors with small sizes and low power consumption. For tactile applications and pressure measurements, they are applied in various fields from robotics to healthcare. In this context, flexible devices are very important for their high responsivity and ability to conform to the analyzed surface. This work reports on miniaturized flexible and compliant piezoelectric devices to increase the number of integrable sensors for detecting and discriminating localized pressures and contacts per unit area with minimal crosstalk. For this purpose, a series of aluminum nitride (AlN)-based piezoelectric sensors with different diameters (from 5 to500μm) was realized, and the generated electrical signal by sensor deformation was amplified by a differential voltage amplifier circuit. By the analysis of the shape of the piezoelectric signal as a function of the applied pressure, oscillations due to piezoelectric deformations, superimposed on the initial peak signal corresponding to the touch event, have been observed. The integral of the electrical signal was calculated for the most accurate representation to describe the sensor’s response. The best responsivity was obtained in samples with diameters of 200 and500μm. Furthermore, it was also found that the minimum distance between the edges of closed sensors, observing crosstalk below −20 dB, was around500μm.","['Sensors', 'Sensor phenomena and characterization', 'Substrates', 'III-V semiconductor materials', 'Aluminum nitride', 'Mechanical sensors', 'Electromechanical sensors']","['Microelectromechanical system (MEMS) devices', 'numerical integration of the electrical signal', 'piezoelectricity', 'pressure measurement', 'tactile sensors']"
"Laser ablation (LA) holds great promise for the localized treatment of pancreatic tumors. However, there are still many technical aspects that require improvements before largely applying this technique in clinical practice. This work shed light on the following subjects: the thermal response of the pancreas to LA using a diffusing applicator and delivering different laser wavelengths (i.e., 808 and 1064 nm); the development of a pancreas-specific numerical model with temperature-dependent tissue properties based on non-Fourier heat conduction for predicting the temperature distribution. To experimentally study the pancreas thermal response and to tune the model parameters, 80 fiber Bragg grating (FBG) sensors, embedded in arrays with millimetric spatial resolution, were implanted in ex vivo pancreas undergoing LA to obtain the real-time and 2-D temperature distribution. Based on the temperature measured by the FBG sensors, results show that the laser wavelength affects the pancreas thermal response: at 2 mm from the applicator, LA at 808 nm resulted in a thermally affected area 5%–15% larger than at 1064 nm, whereas no notable difference was observed at 6 mm. The temperature distribution and laser-induced lesion obtained from the non-Fourier numerical model were found to closely match the experimental results when the phase lag of the temperature gradient was five times that of the heat flux vector.","['Temperature measurement', 'Temperature sensors', 'Fiber lasers', 'Applicators', 'Laser applications', 'Sensors', 'Pancreas']","['Diffusing applicator', 'fiber Bragg grating (FBG)', 'laser ablation (LA)', 'mathematical modeling', 'pancreas', 'temperature monitoring']"
"A modular sensor application for measuring athlete performance in skiing sports was developed. Using inertial measurement units (IMUs) and load cells in a modular system, a force orientation measurement system, FOMS, was developed. A functioning prototype capable of measuring ski sports dynamics was created. Data processing using the system, a validation of the prototype in terms of angle measurement IMU accuracy, example data from in-field athlete testing, and visualization by animations are described. The system developed contains four subsystems: a controller, two pole measuring modules, and a terrain-measuring module. The system structure also allows for additional modules, making the system applicable to different sports. The IMUs use orientation-sensing components to measure pole orientations, which are used to calculate decomposed forces relative to the terrain. Data from different modules are synchronized using wireless communication and saved on SD cards with time stamps. A validation experiment was conducted in which the angles from the modules were compared with the Oqus motion capture system from Qualisys. Examples for athlete testing in both cross country and alpine skiing were calculated from the matrix provided by the different modules and are presented in graphs to evaluate the athlete. In addition, the relative pole/terrain coordinates are visualized in 2D and 3D animations for analyzing the movement pattern in connection with the applied forces, opening up a whole new level of sports analysis.","['Sports', 'Sensors', 'Force measurement', 'Sensor phenomena and characterization', 'Prototypes', 'Kinetics']","['Dynamics', 'kinetics', 'kinematics', 'ski poles']"
"This paper presents a high aspect-ratio open grating Fabry-Perot resonator theoretically and experimentally for aqueous refractive index sensing with high figure of merit. Transmission line modelling and associated Smith charts are utilized to give a better intuitive understanding of the physical sensing mechanism. The analysis aids understanding of the relative contribution to refractive index changes both within the grating fingers and external to them. Measurements of refractive index are carried out with Kretschmann coupling configuration for both transverse magnetic (TM) and transverse electric (TE) polarization. The results show an improvement of 10- and 44-fold in figure of merit over conventional SPR for TM and TE, respectively. When the designed system is analyzed for sensitivity, {{1}.{874}\times }{{{10}}}^{-{8}}{\textit {RIU}} is achieved with a single frame even without a reference channel for noise cancellation. With 794 frames averaging (approximately 1-second acquisition time), the sensitivity of the system can be further improved to {{1}.{609}\times }{{{10}}}^{{-{9}}}{\textit {RIU}} . The simulations and experimental results demonstrate strong potential for biosensing applications, e.g., kinetic molecular binding, ultrasensitive refractive index sensing and photoacoustic detection.","['Sensors', 'Gratings', 'Refractive index', 'Gold', 'Optical variables control', 'Optical sensors', 'Optical refraction']","['Refractive index sensing', 'figure of merit', 'optical resonators']"
"A method to produce spatially resolved images of the distribution of absorbing particles in the exhaust plume of a modified helicopter gas turbine engine is presented. Over a small region of the plume, in situ sensing of soot particles by laser-induced incandescence (LII) is demonstrated using fiber lasers with higher power (~10 W), longer pulse duration (>100 ns), and higher pulse repetition rates (>10 kHz) than the conventional LII. The sensitivity of the method is illustrated by the detection of ambient absorbing particles in background conditions with engine at rest. With a running engine, single-beam images are obtained in 0.01 s. The feasibility of using long-pulsed fiber lasers for soot particle concentration measurement is investigated using a representative laboratory system. The time-resolved LII behavior and the measurement linearity are investigated, demonstrating the suitability of using fiber lasers for soot particle measurement for aero-engine emissions. Results for normalized soot concentration are compared with extractive measurements illustrating good correlation across a range of engine speeds. This paper is the first step toward the development of a non-intrusive system for the measurement of 2-D soot concentration in the cross section of an aero-engine exhaust plume.","['Engines', 'Particle measurements', 'Laser beams', 'Atmospheric measurements', 'Measurement by laser beam', 'Optical fiber sensors']","['aero-engine', 'fiber-laser', 'exhaust plume', 'laser induced incandescence', 'soot sensing', 'ambient sensing']"
"This article proposes a hot-wire anemometer based on optical fiber embedding a fiber Bragg grating (FBG) and having a D-shaped transversal Section on whose flat surface a thin metallic layer has been deposited. Due to this geometrical structure, the optical power flowing through the fiber core can achieve the metallic layer and can be converted into heat. The embedded FBG can measure the resulting temperature increase and the temperature fluctuation of the D-fiber caused by the wind flowing. Numerical simulations have been performed in order to select the appropriate design parameters, such as thickness of the metallic layer and its distance from the core. Then, the fabrication process of the device and the experimental results of its characterization in temperature and wind assess its working principle. The developed sensor can work at low power levels of the source and is characterized by small size and high accuracy. Furthermore, it shows high cost-effectiveness and the possibility to modulate its wind sensitivity by setting the source power.","['Optical fibers', 'Optical fiber sensors', 'Sensors', 'Fluid flow measurement', 'Heating systems', 'Gold', 'Temperature sensors']","['D-fiber', 'fiber Bragg gratings (FBGs)', 'fiber-optic anemometer', 'hot-wire anemometer (HWA)', 'thermoplasmonic effect']"
"Aiming at the problem of the failure of the global positioning system in the satellite denied environment, and facing the needs of the navigation and positioning in indoor space, the technology of indoor magnetic field vector positioning and navigation have been studied. Based on the geomagnetic matching technology, the wavelet analysis method is used to estimate the noise components and statistical characteristics of the magnetic field three-dimensional signal; combined with the Kalman filtering algorithm, through estimating and optimizing the magnetic field vector signal, an indoor magnetic positioning signal processing algorithm with wavelet analysis and Kalman fusion is established, realizing the optimization of the measurement accuracy of the geomagnetic signal and the accuracy of the fingerprint map. The experimental results indicate that the signal-to-noise ratio of the test sequenceis increasedby 3db, the rootmean square error is reducedby about30%, and themaximumpositioning error reaches 1.34m, which can meet the requirements of indoor positioning.","['Magnetic fields', 'Fingerprint recognition', 'Filtering', 'Sensors', 'Magnetic sensors', 'Magnetic separation', 'Magnetic field measurement']","['Geomagnetic filtering algorithm', 'indoor navigation', 'Kalman filtering', 'satellite denial', 'wavelet analysis']"
"The charge density in a flame contains important information about the combustion processes that is difficult to obtain due to fast, complex, and highly exothermic reactions. This paper presents a study of using electrostatic probes to measure the charge density in methane fired diffusion and premixed flames. The sensing principle, practical design, and performance assessment of the electrostatic probe are presented. Comparative experimental studies with a reference ion-current sensor carried out on a combustion test rig indicate that the fluctuation of the signal from the electrostatic probe arises from the variation in the charge density in a flame. A dimensionless index, combining the average of local peak values in the electrostatic signal with the flame oscillation frequency, is adopted as an indicator of the charge density. Experimental results demonstrate that the charge density in a methane diffusion flame has an increasing trend with the fuel flowrate varying from 0.80 L/min to 1.20 L/min. The charge density in a methane-air premixed flame yields a decreasing trend with the equivalence ratio ranging from 0.54 to 0.75, then increases and reaches a local peak at the equivalence ratio of 1.03, and continues to increase when the equivalence ratio varies from 1.03 to 3.50. The charge density in the inner cone is higher than that in the outer cone for diffusion and premixed flames. The results obtained suggest that the developed electrostatic probe and the index can be used to indicate the charge density in diffusion and premixed flames.","['Fires', 'Probes', 'Electrostatics', 'Sensors', 'Electrodes', 'Ions', 'Electron tubes']","['Flame', 'charge density', 'electrostatic probe', 'combustion monitoring']"
"Many recent studies have addressed the detection of negative affective states such as stress and anxiety from physiological signals taken from body-worn sensors. Typically, machine learning classifiers are applied to features derived from sensor signals, and several authors have reported high accuracy results from a range of signals including cardiac, skin conductance, and skin temperature. However, the issue of how robust these models are for deployment in the field is rarely addressed. In this article, we use open data from two large experimental studies to evaluate the generalizability of models derived from cardiac signals, focusing on detection of stress and anxiety. We choose the cardiac signal since the commonly used heart rate variability features can be derived from multiple sensor modalities, allowing us to evaluate the robustness of models within, as well as between, experimental settings. We show that consistent classification outside the original experimental setting relies on high-quality training data with minimal artifacts, and that models may often train on proxies within the noise of lower quality data. Our results also underline the importance of including a wide range of emotional states in the training data to minimize erroneous classification from unseen regions of feature space.","['Anxiety disorders', 'Sensors', 'Machine learning', 'Human factors', 'Emotion recognition', 'Band-pass filters', 'Robustness']","['Emotion recognition', 'generalizability', 'machine learning', 'physiological signals']"
"The increased use of light detection and ranging (LiDAR) systems for distance determination requires the investigation of mutual interference. In this article, we describe the conditions for the occurrence of LiDAR interference. We outline suppression methods for different LiDAR types identifying pulse-position modulation (PPM) as a solution for time-of-flight LiDAR with time-correlated single photon counting (TCSPC) histograms. Based on PPM, we present a suppression method, which randomly varies the laser pulse emission times. For optimal suppression, we switch on the suppression only when interference is present. To recognize the occurrence of LiDAR interference, we develop a multipulse detection algorithm that can also extract all pulse positions. Simulations show that the algorithm can be applied for a signal-to-noise ratio greater than 3. Determining the heights of all recognized pulse signatures, an appropriate suppression level can be chosen. We successfully show the optimized interference suppression for an example LiDAR measurement. For safe use by multiple systems, we suggest random numbers. We reuse the TCSPC histograms to generate random numbers, whose generation probability is calculated theoretically and confirmed by simulation and measurement data. For nearly all histogram distributions consisting of background- and laser-generated data, a sufficient amount of random numbers is produced.","['Laser radar', 'Histograms', 'Measurement by laser beam', 'Pulse measurements', 'Time measurement', 'Sensor phenomena and characterization', 'Photonics']","['Mutual light detection and ranging (LiDAR) interference', 'pulse-position modulation (PPM)', 'random number generation', 'time-correlated single photon counting (TCSPC)', 'time-of-flight (TOF)']"
"The ability to detect abnormalities in infrastructure automatically and remotely with a small number of sensors would greatly contribute to efficient and timely infrastructure inspection. Here, we propose a passive sensor that can be installed by painting. The painted waveguide-based sensor detects abnormalities in infrastructure from abnormality-induced changes in the resonance frequency of electromagnetic waves resonating inside the sensor. In this paper, the principle of detecting loose bolts with the painted waveguide-based sensor is demonstrated by simulation and experiment. The sensor is designed and fabricated as a resonator with resonance frequencies from 100 kHz to 2.5 GHz, which includes the UHF radio-frequency identification band, and the measurement frequency was set within this range. The fabricated sensor is capable of detecting not only a single loose bolt but also two or more loose bolts on the waveguide of the sensor to monitor.","['Fasteners', 'Electromagnetic waveguides', 'Sensor phenomena and characterization', 'Resonant frequency', 'Electromagnetic scattering', 'Inspection', 'Wireless sensor networks']","['Electromagnetic wave', 'resonance', 'painting', 'waveguide', 'loose bolt', 'infrastructure']"
"Polyimide (PI) has been utilized as a hole-blocking layer (HBL) in high-resistivity photoconductors such as amorphous selenium (a-Se) and amorphous lead oxide (a-PbO). It prevents hole injection from the positively biased electrode and suppresses the dark current of the detector. To determine the performance parameters of an a-Se/PI detector for readout design and specification for different radiation detection applications, knowledge of the accurate voltage drop within the photoconductor is crucial. Here, we precisely determine the voltage drop across a-Se when interfaced with a thin layer of PI (1μm) and characterize its temporal resolution (rise time and jitter), dynamic range, and quantum efficiency (QE) as a function of the electric field. The photoresponse of the detector is characterized using light-emitting diodes in the ultraviolet (UV, 355–400-nm wavelength) and visible (400–533-nm wavelength) regions of the light spectrum. We find the voltage drop across a-Se as 91% of the total applied voltage. This results in a QE near unity at 405 nm for corrected fields beginning at 45 V/μm. At fields of 50 V/μm, we observe rise times of 3.7 ns and jitter of 130 ps.","['Selenium', 'Detectors', 'Electric fields', 'Polyimides', 'Voltage', 'Performance evaluation', 'Semiconductor device measurement']","['Amorphous selenium (a-Se)', 'hole-blocking layers (HBLs)', 'indirect X-ray detectors', 'photodetectors', 'polyimide (PI)', 'UV response']"
"A standard method for distance determination is light detection and ranging (LiDAR), which relies on the emission and detection of reflected laser pulses. When LiDAR systems become common for every vehicle, many simultaneous laser signals will produce mutual LiDAR interference between LiDAR systems. In this paper, we analyze the possibility to recognize mutual interference in time-correlated single photon counting (TCSPC) LiDAR with particular focus on flash systems. We evaluate the LiDAR interference appearance by deriving the expected event distribution for ego and aggressor signal. From that, we calculate the probability of photon detection within each measured signal. This paper shows the high potential of different pulse repetition frequencies to reduce LiDAR interference. Using signal-to-noise ratio (SNR), we define the extinction distance, beyond which the aggressor signal completely extinguishes the ego signal. Applied on different background and laser event rates, we find the connection between ideal LiDAR system designs and lowest probability for unrecognized LiDAR interference. Furthermore, we show the relationship to a specific LiDAR design, which must fulfill eye safety condition and receives lower intensities with increasing target distances. Finally, we present different solutions for the recognition and reduction of LiDAR interference based on our previous results.","['Laser radar', 'Interference', 'Photonics', 'Measurement by laser beam', 'Lasers', 'Histograms', 'Lighting']","['Light detection and ranging (LiDAR)', 'mutual Lidar interference', 'time-correlated single-photon counting (TCSPC)', 'direct time-of-flight (dTOF)']"
"In this paper the fabrication and characterization of a novel wireless pressure sensor system that can monitor and report real-time changes in pressure on the rotor blade of a low speed axial compressor is demonstrated. The pressure sensor was positioned on the second-stage rotor at approximately the mid-span location on the pressure surface of the blade near the leading edge and hard wired to a circuit box located on the center line of the compressor drum. The output of the pressure sensor is converted from a voltage to a frequency and transmitted wirelessly via circularly polarized antenna to a stationary antenna that was approximately 1 meter away, mounted at the centerline location on the inlet cart. The output signal was recorded on a spectrum analyzer with a LabVIEW program for data acquisition. Compressor rotation was ramped from 0 RPM up to its design speed of 1000 RPM and the airflow was decreased by closure of downstream throttle valve until the compressor stalled. The measurements from the wireless pressure system show increasing pressure as compressor flow rate is decreased, until a precipitous drop in pressure occurs which signals that the compressor has stalled.","['Sensors', 'Wireless communication', 'Pressure sensors', 'Blades', 'Wireless sensor networks', 'Engines', 'Turbines']","['Wireless pressure sensor system', 'pressure sensor', 'low speed axial compressor', 'cylindrical antenna']"
"We designed a skin-attachable sensor for measuring core body temperature (CBT). Past studies on non-invasive CBT measurement did not consider heat loss and measurement errors caused by ambient convection. To address the effect of convection, we designed a heat flow path in the structure of the CBT sensor. Topology optimization provided a systematic design procedure without the need for numerous, complex trials. By using topology optimization, we developed an optimized aluminum structure, a truncated cone with a hole. The sensor with the aluminum structure was evaluated by numerical calculation and experiment. We determined that the heat loss induced by ambient convection was effectively reduced, and the CBT estimation accuracy and robustness to convection were improved. The feasibility of the CBT sensor was demonstrated in an experiment using a phantom with an accuracy of 0.1 °C, showing potential for applications to thermal and fluidic devices.","['Sensors', 'Heating systems', 'Wind speed', 'Aluminum', 'Temperature measurement', 'Convection', 'Optimization']","['Topology optimization', 'core body temperature', 'non-invasive']"
"As the friction between a vehicle’s tires and the road surface correlates with the road-covering water film height, knowledge about the present wetness level is of relevance for drivers and autonomous systems. A promising approach for wetness quantification is based on a capacitive transducer array (TA) that is integrated at a front wheel arch liner and capable of detecting spray water ejected by the tires. While this approach has already shown that wetness classification using capacitive measurement data is feasible, potentially affecting factors such as the water’s electrical conductivity were assumed to be constant in the past. This article presents a study on the effect of the spray water’s conductivity on wetness classification for that approach and also demonstrates the feasibility of classifying conductivity using the same capacitive measurement data. For these purposes, we propose a test bench capable of simulating reproducible spray water scenarios. We show the effect of varying conductivity on the measured capacitance curves complicating the distinction of wetness classes. In partial investigations, we demonstrate the extent of the conductivity’s effect on classifier performance. While wetness classification with conductivities differing from training results in poor classifier performance, the mean accuracy (ACC) can be increased to approximately 0.98 considering all occurring conductivities. Furthermore, we propose a two-stage classification approach that initially determines the conductivity and subsequently considers it as an additional feature in wetness classification. The approach increases classifier performance to more than 0.99 indicating that knowledge of the spray’s present conductivity can optimize road surface wetness classification.","['Conductivity', 'Sensors', 'Roads', 'Wheels', 'Capacitive sensors', 'Tires', 'Sensor arrays']","['Capacitive sensors', 'electrical conductivity', 'machine learning', 'road surface wetness detection', 'vehicle safety', 'wetness classification']"
"Subsurface flows are challenging to map due to the inaccessibility and extreme nature. This coupled with global positioning system (GPS) denied, sensorially deprived, complex, and locally self-similar environments result in most of the traditional mapping methods failing. This results in extensive subsurface environments that remain unknown and unexplored. In this paper, we show that it is possible to detect features using infinite hidden Markov model (iHMM) from inertial measurement unit data (IMU) collected along surface and subsurface flow paths. By assuming zero velocity at the beginning of each feature, we present a data driven model to reconstruct unknown 2D subsurface flow paths using 9 degrees of freedom (dof) IMU sensor data and two known location points along the path. In addition, we show the advantages of using iHMM compared to simpler feature extraction methods. The model is validated on multiple controlled examples, as well as experimental real world datasets.","['Hidden Markov models', 'Feature extraction', 'Sensors', 'Data models', 'Global Positioning System', 'Data collection', 'Accelerometers']","['Sensors modeling and analysis', 'sensor signal processing', 'subsurface mapping']"
"Surface plasmon resonance imaging (SPRI) detects the changes in refractive index in close proximity to the surface of a thin metal film as variations in light intensity reflected from the back of the film and thus does not require labeling for visualization of the structures under investigation. While traditionally, the wave vector scanning is performed via angular rotations, the wave vector can also be scanned though tuning of the wavelength. Here we demonstrate that a combination of a non-monochromatic electrically tunable bandpass filter in conjunction with highly chromatically corrected imaging objectives can yield subcellular resolution for imaging of the interior refractive index of human mesenchymal stem cells.","['Imaging', 'Image resolution', 'Optical filters', 'Sensors', 'Surface plasmons', 'Refractive index', 'Stem cells']","['Biomarker', 'greyscale image', 'liquid crystal filter', 'mesenchymal stem cells']"
"Indoor localization service is an indispensable part of modern intelligent life, among which Wi-Fi based fingerprint localization system is popular in indoor positioning researches due to its advantages of low cost and widely deployment. However, Wi-Fi based localization system is susceptible to dynamic environment, and fingerprint collection and updating are time-consuming and labor-intensive. To address this problem, we propose a novel positioning framework based on multiple transfer learning fusion using Generalized Policy Iteration (GPI). Firstly, a 1-Dimension Convolutional Autoencoder (1-D CAE) is designed to extract features from one-dimensional fingerprint data; similar to Convolutional Neural Network (CNN), it can not only pay more attention to the information of different dimensions of fingerprints, but also compress redundant information and reduce noise. After that, Domain Adversarial Neural Network (DANN) and Passive Aggressive (PA) algorithm are fused to train localization model based on unlabeled fingerprint of target domain using the theory of GPI in offline stage. Finally, the model is fine-tuned with unlabeled fingerprints and few labeled fingerprints in daily online predictions to improve the performance of the localization system. Various evaluations in five typical scenarios validate the effectiveness of proposed algorithm in dynamic environment, with low tendency, easy recalibration, long-term stabilization high accuracy and so on.","['Fingerprint recognition', 'Location awareness', 'Heuristic algorithms', 'Wireless fidelity', 'Feature extraction', 'Convolutional neural networks', 'Databases']","['Feature extracting', 'fingerprint localization', 'generalized policy iteration', 'indoor localization', 'transfer learning']"
"We present a novel transient measurement method to characterize effective values of the thermal conductivity, thermal diffusivity, and emissivity of a thin-film membrane implemented in a micromachined multiparameter wind sensor. Accurate values for these parameters are mandatory as they considerably influence the thermal heat shunt induced by the membrane, which is the main limiting factor for reliable sensor operation in gases with low thermal admittance and thus determines the ultimate device sensitivity. Compared with commonly used measurement techniques for these thermal thin-film parameters, our method does not require custom-built specimens, i.e., the results can directly be obtained from transient excess temperature measurements on the wind sensor device utilizing the embedded thin-film heater and thermistors. The presented method as such is universal and can easily be applied to other configurations of heating and temperature sensing elements placed on a circular thin-film membrane. In this contribution, we describe the theoretical background of this method, provide an efficient semianalytical model for the data evaluation including its validation by computer numerical simulations, and showcase sample measurements on multilayer Si x N y -SiO 2 thin films.","['Heating', 'Thermal conductivity', 'Temperature measurement', 'Thermal sensors', 'Thermistors', 'Conductivity']","['Multiparameter flow sensor', 'thin-film membrane', 'thermal properties', 'conductivity', 'diffusivity and emissivity']"
"With over 16 million horses worldwide and nearly 60000 sport horses registered to the International Federation for Equestrian Sports database, tracking the activities and performance of these equines is becoming an important aspect in horse management. To perform this activity recognition, inertial measurement units (IMUs) are often used in combination with machine learning algorithms. These often require large labeled datasets to be trained. To this end, a data-efficient algorithm is proposed that requires only 3 min of labeled calibration data. This is achieved by combining supervised feature selection, using the tsfresh time-series feature calculation library and the Kendall rank correlation coefficient, with a distance-based clustering algorithm. The generalizability performance of the algorithm is tested by evaluating on a dataset captured with leg-mounted IMUs and on a dataset captured using a neck-mounted IMU. On both datasets, the algorithm achieved the accuracies of 95%, comparable to state-of-the-art deep learning approaches, when calibrating and evaluating using the same horse. When the algorithm was calibrated on data from multiple horses and evaluated on horses that were not in the calibration dataset, a15%drop in classification accuracy is observed. The proposed algorithm is compared with fully supervised algorithms, such as convolutional neutral network, support vector machine, and random forest, in terms of accuracy achieved with respect to the size of the labeled data using calibration. Our approach achieved accuracies that were similar to these classical algorithms while only using 10%–5%the amount of labeled data.","['Horses', 'Feature extraction', 'Clustering algorithms', 'Sensors', 'Deep learning', 'Support vector machines', 'Pipelines']","['Classification', 'clustering', 'equine activity recognition', 'feature selection']"
"Several thousand grapevine varieties exist, with even more naming identifiers. Adequate specialized labor is not available for proper classification or identification of grapevines, making the value of commercial vines uncertain. Traditional methods, such as genetic analysis or ampelometry, are time-consuming, expensive, and often require expert skills that are even rarer. New vision-based systems benefit from advanced and innovative technology and can be used by nonexperts in ampelometry. To this end, ac dl and ac ml approaches have been successfully applied for classification purposes. This work extends the state of the art by applying digital ampelometry techniques to larger grapevine varieties. We benchmarked MobileNet v2, ResNet-34, and VGG-11-BN DL classifiers to assess their ability for digital ampelography. In our experiment, all the models could identify the vines’ varieties through the leaf with a weightedF1score higher than 92%.","['Pipelines', 'Shape', 'Sensor phenomena and characterization', 'Support vector machines', 'Signal to noise ratio', 'Convolutional neural networks', 'Visualization']","['Artificial neural networks (ANN)', 'computer vision', 'image processing', 'precision agriculture', 'vine species identification']"
"In this tutorial, we discuss trade-offs in sensor system design. With examples from engineered systems in our laboratories, we address design considerations at the different system levels, from the choice of a particular sensor to the signal conditioning circuits, the choice of data converter, and data communication physical interfaces. Fundamental concepts from communication and information theory, as well as practical considerations applied to specific case studies, are presented for sensor readout systems. In particular, we address both fundamental trade-offs such as choice of signal representation for signal conditioning and conversion, as well as practical and technological trade-offs such as place of quantization (signal data conversion) application demands and integration technologies. The tutorial deals with what is commonly referred to as the Analog Front End (AFE) up to the point where the physical signal is represented in digital symbols. Although the discussion focuses on all the processing steps of the AFE, a more system-level approach is further explored, including stand-alone system approaches and system communication. Major system trade-offs during system design are discussed, providing important methodology insights towards the design of sensory systems operating within demanding constraints.","['Sensor systems', 'Signal to noise ratio', 'Robot sensing systems', 'Sensor phenomena and characterization', 'Intelligent sensors', 'System analysis and design', 'Signal representation', 'Tutorials']","['End-to-end sensor systems design', 'sensor interfaces', 'design trade-offs', 'signal to noise ratio']"
"There is limited work on understanding Schottky barrier (SB) field-effect transistor (FET) behavior, and subsequent evolution of sensitivity metrics under wet detection conditions for neurotransmitter sensing applications. In this work, we report low-power deep subthreshold (DST) characteristics of SB InGaZnO 4 (IGZO) thin film transistors (TFTs) and implement this regime for sensitive and selective ex vivo detection of Dopamine against Adenosine Triphosphate (ATP). Device regeneration was performed under liquid solution environment over 34 days and sensor response was observed to drift less than two orders of magnitude. Nevertheless, the quantified trends due to buffer solution baseline versus target conjugation were virtually preserved across the TFTs. Widely improved detection metrics were extracted from the detection tests, namely the limit of detection (LoD) in the 100fM-100pM range, a binding affinity ( k_{D}) between 2.03\times 10^{-11}- 3.02\times 10^{-9}\text{M}limits, and high sensitivity around 31-42mV/decade of target concentration. Although a steady drift with clear, repeatable differences between the device state at the start and end of the 34-day period were obtained, these devices were functional even after the regeneration test period. Such results can be used to estimate the regenerative lifetime of TFT biosensors under aptamer-immobilized solution environment detection conditions.","['Sensors', 'Thin film transistors', 'Sensitivity', 'Biosensors', 'Sensor phenomena and characterization', 'Schottky barriers', 'Logic gates']","['Biosensor', 'thin film transistor', 'IGZO', 'aptamer', 'dopamine', 'detection', 'drift', 'regeneration']"
"Future nuclear arms-control agreements may place numerical limits on items that are difficult to monitor with national technical means, even when complemented with onsite inspections. Such items could include small objects and mobile assets, such as non-deployed nuclear warheads and mobile missile launchers. Typically, this verification task can be addressed with unique identifiers, but standard tagging techniques may be unacceptable in this case due to host concerns about safety and intrusiveness. First proposed in the late 1980s and partially developed by Sandia National Laboratories in the early 1990s, the “Proximity Tag” or “Buddy Tag” concept seeks to overcome these concerns by separating the tag from the treaty accountable item itself. A buddy tag has two key elements: a tamper-indicating enclosure and a motion-detection system designed to detect illicit movements in a stand-down period. As part of this project, we have built a buddy-tag prototype for demonstration and evaluation purposes. This paper reviews the design choices and functionalities of the tag's motion-detection subsystem. We pursue a modular approach for the tag's hardware, built around an Arduino-class microcontroller and a non-export-controlled low-noise accelerometer, and use open-source algorithms for the motion-detection software. We discuss the results of an extensive experimental campaign involving both indoor and outdoor measurements assessing the performance of the tag under real-world environmental conditions.","['Accelerometers', 'Monitoring', 'Inspection', 'Missiles', 'Prototypes', 'Sensors', 'Hardware']","['Nuclear arms control', 'verification', 'tagging', 'motion detection', 'IIR filter']"
"Exposure to respirable coal dust and diesel exhaust in underground coal mines can cause detrimental airway diseases such as coal worker’s pneumoconiosis (CWP), silicosis, and lung cancer. In this article, we present the design, fabrication, and experimental evaluation of a low-cost wearable respirable dust monitor (WEARDM) that uses a dual-resonator gravimetric sensing approach for real-time measurement of respirable airborne particulate matter (PM) concentrations. The sensor selects for the ISO-respirable dust fraction using a miniature virtual impactor (VI) and removes moisture from the collected dust to ensure accurate mass measurement. WEARDM uses a novel dual-resonator mass sensor (DRMS) that is composed of a quartz crystal microbalance (QCM) and a film bulk acoustic resonator (FBAR). The QCM measures the mass concentration of particles generated from coal mining operations [typically >2.5-μmaerodynamic diameter (AD)], separated using inertial impaction. Thermophoretic precipitation is used to deposit the fine and ultrafine particles, such as those emitted from diesel sources [typically<0.1 μmAD) on FBAR. This allows the WEARDM system to maintain a large dynamic range and uniform collection efficiency (CE) across the entire respirable fraction. The WEARDM system is optimized for a low flow rate of 250 mL/min which results in low power usage and a small form factor and is an order of magnitude smaller and less expensive than currently available devices.","['Sensors', 'Coal mining', 'Monitoring', 'Coal', 'Film bulk acoustic resonators', 'Solid modeling', 'Particle measurements']","['Coal dust', 'diesel exhaust', 'film bulk acoustic resonator (FBAR)', 'gravimetric mass sensor', 'particulate matter (PM)', 'quartz crystal microbalance (QCM)']"
